{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sparse Autoencoder","text":"<p>A sparse autoencoder for mechanistic interpretability research.</p> <pre><code>pip install sparse_autoencoder\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Check out the demo notebook for a guide to using this library.</p> <p>We also highly recommend skimming the reference docs to see all the features that are available.</p>"},{"location":"#features","title":"Features","text":"<p>This library contains:</p> <ol> <li>A sparse autoencoder model, along with all the underlying PyTorch components you need to       customise and/or build your own:<ul> <li>Encoder, constrained unit norm decoder and tied bias PyTorch modules in     sparse_autoencoder.autoencoder.</li> <li>L1 and L2 loss modules in sparse_autoencoder.loss.</li> <li>Adam module with helper method to reset state in sparse_autoencoder.optimizer.</li> </ul> </li> <li>Activations data generator using TransformerLens, with the underlying steps in case you       want to customise the approach:<ul> <li>Activation store options (in-memory or on disk) in sparse_autoencoder.activation_store.</li> <li>Hook to get the activations from TransformerLens in an efficient way in     sparse_autoencoder.source_model.</li> <li>Source dataset (i.e. prompts to generate these activations) utils in     sparse_autoencoder.source_data, that stream data from HuggingFace and pre-process     (tokenize &amp; shuffle).</li> </ul> </li> <li>Activation resampler to help reduce the number of dead neurons.</li> <li>Metrics that log at various stages of training (e.g. during training, resampling and       validation), and integrate with wandb.</li> <li>Training pipeline that combines everything together, allowing you to run hyperparameter       sweeps and view progress on wandb.</li> </ol>"},{"location":"#designed-for-research","title":"Designed for Research","text":"<p>The library is designed to be modular. By default it takes the approach from Towards Monosemanticity: Decomposing Language Models With Dictionary Learning , so you can pip install the library and get started quickly. Then when you need to customise something, you can just extend the abstract class for that component (e.g. you can extend <code>AbstractEncoder</code> if you want to customise the encoder layer, and then easily drop it in the standard <code>SparseAutoencoder</code> model to keep everything else as is. Every component is fully documented, so it's nice and easy to do this.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Demo</li> <li>Flexible demo</li> <li>Source dataset pre-processing</li> <li>Reference</li> <li>Contributing</li> <li>Citation</li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>Please cite this library as:</p> <pre><code>@misc{cooney2023SparseAutoencoder,\n    title = {Sparse Autoencoder Library},\n    author = {Alan Cooney},\n    year = {2023},\n    howpublished = {\\url{https://github.com/ai-safety-foundation/sparse_autoencoder}},\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#setup","title":"Setup","text":"<p>This project uses Poetry for dependency management, and PoeThePoet for scripts. After checking out the repo, we recommend setting poetry's config to create the <code>.venv</code> in the root directory (note this is a global setting) and then installing with the dev and demos dependencies.</p> <pre><code>poetry config virtualenvs.in-project true\npoetry install --with dev,demos\n</code></pre> <p>If you are using VSCode we highly recommend installing the recommended extensions as well (it will prompt you to do this when you checkout the repo).</p>"},{"location":"contributing/#checks","title":"Checks","text":"<p>For a full list of available commands (e.g. <code>test</code> or <code>typecheck</code>), run this in your terminal (assumes the venv is active already).</p> <pre><code>poe\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Please make sure to add thorough documentation for any features you add. You should do this directly in the docstring, and this will then automatically generate the API docs when merged into <code>main</code>. They will also be automatically checked with pytest (via doctest).</p> <p>If you want to view your documentation changes, run <code>poe docs-hot-reload</code>. This will give you hot-reloading docs (they change in real time as you edit docstrings).</p>"},{"location":"contributing/#docstring-style-guide","title":"Docstring Style Guide","text":"<p>We follow the Google Python Docstring Style for writing docstrings. Some important details below:</p>"},{"location":"contributing/#sections-and-order","title":"Sections and Order","text":"<p>You should follow this order:</p> <pre><code>\"\"\"Title In Title Case.\n\nA description of what the function/class does, including as much detail as is necessary to fully understand it.\n\nWarning:\n\nAny warnings to the user (e.g. common pitfalls).\n\nExamples:\n\nInclude any examples here. They will be checked with doctest.\n\n  &gt;&gt;&gt; print(1 + 2)\n  3\n\nArgs:\n    param_without_type_signature:\n        Each description should be indented once more.\n    param_2:\n        Another example parameter.\n\nReturns:\n    Returns description without type signature.\n\nRaises:\n    Information about the error it may raise (if any).\n\"\"\"\n</code></pre>"},{"location":"contributing/#latex-support","title":"LaTeX support","text":"<p>You can use LaTeX, inside <code>$$</code> for blocks or <code>$</code> for inline</p> <pre><code>Some text $(a + b)^2 = a^2 + 2ab + b^2$\n</code></pre> <pre><code>Some text:\n\n$$\ny    &amp; = &amp; ax^2 + bx + c \\\\\nf(x) &amp; = &amp; x^2 + 2xy + y^2\n$$\n</code></pre>"},{"location":"contributing/#markup","title":"Markup","text":"<ul> <li>Italics - <code>*text*</code></li> <li>Bold - <code>**text**</code></li> <li>Code - <code>``code``</code></li> <li>List items - <code>*item</code></li> <li>Numbered items - <code>1. Item</code></li> <li>Quotes - indent one level</li> <li>External links = <code>`Link text &lt;https://domain.invalid/&gt;`</code></li> </ul>"},{"location":"demo/","title":"Demo","text":"<pre><code># Check if we're in Colab\ntry:\n    import google.colab  # noqa: F401 # type: ignore\n\n    in_colab = True\nexcept ImportError:\n    in_colab = False\n\n#  Install if in Colab\nif in_colab:\n    %pip install sparse_autoencoder transformer_lens transformers wandb\n\n# Otherwise enable hot reloading in dev mode\nif not in_colab:\n    %load_ext autoreload\n    %autoreload 2\n</code></pre> <pre>\n<code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code>\n</pre> <pre><code>import os\n\nfrom sparse_autoencoder import (\n    ActivationResamplerHyperparameters,\n    Hyperparameters,\n    LossHyperparameters,\n    Method,\n    OptimizerHyperparameters,\n    Parameter,\n    SourceDataHyperparameters,\n    SourceModelHyperparameters,\n    sweep,\n    SweepConfig,\n)\nimport wandb\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_NOTEBOOK_NAME\"] = \"demo.ipynb\"\n</code></pre> <p>Customize any hyperparameters you want below (by default we're sweeping over l1 coefficient and learning rate):</p> <pre><code>sweep_config = SweepConfig(\n    parameters=Hyperparameters(\n        activation_resampler=ActivationResamplerHyperparameters(\n            threshold_is_dead_portion_fires=Parameter(1e-6),\n        ),\n        loss=LossHyperparameters(\n            l1_coefficient=Parameter(max=1e-2, min=4e-3),\n        ),\n        optimizer=OptimizerHyperparameters(\n            lr=Parameter(max=1e-3, min=1e-5),\n        ),\n        source_model=SourceModelHyperparameters(\n            name=Parameter(\"gelu-2l\"),\n            hook_site=Parameter(\"mlp_out\"),\n            hook_layer=Parameter(0),\n            hook_dimension=Parameter(512),\n        ),\n        source_data=SourceDataHyperparameters(\n            dataset_path=Parameter(\"NeelNanda/c4-code-tokenized-2b\"),\n        ),\n    ),\n    method=Method.RANDOM,\n)\nsweep_config\n</code></pre> <pre>\n<code>SweepConfig(parameters=Hyperparameters(\n    source_data=SourceDataHyperparameters(dataset_path=Parameter(value=NeelNanda/c4-code-tokenized-2b), context_size=Parameter(value=128))\n    source_model=SourceModelHyperparameters(name=Parameter(value=gelu-2l), hook_site=Parameter(value=mlp_out), hook_layer=Parameter(value=0), hook_dimension=Parameter(value=512), dtype=Parameter(value=float32))\n    activation_resampler=ActivationResamplerHyperparameters(resample_interval=Parameter(value=197885952), max_n_resamples=Parameter(value=4), n_activations_activity_collate=Parameter(value=98942976), resample_dataset_size=Parameter(value=819200), threshold_is_dead_portion_fires=Parameter(value=1e-06))\n    autoencoder=AutoencoderHyperparameters(expansion_factor=Parameter(value=2))\n    loss=LossHyperparameters(l1_coefficient=Parameter(max=0.01, min=0.004))\n    optimizer=OptimizerHyperparameters(lr=Parameter(max=0.001, min=1e-05), adam_beta_1=Parameter(value=0.9), adam_beta_2=Parameter(value=0.99), adam_weight_decay=Parameter(value=0.0), amsgrad=Parameter(value=False), fused=Parameter(value=False))\n    pipeline=PipelineHyperparameters(log_frequency=Parameter(value=100), source_data_batch_size=Parameter(value=16), train_batch_size=Parameter(value=8192), max_store_size=Parameter(value=2998272), max_activations=Parameter(value=1999847424), checkpoint_frequency=Parameter(value=47972352), validation_frequency=Parameter(value=99999744), validation_number_activations=Parameter(value=8192))\n    random_seed=Parameter(value=49)\n), method=&lt;Method.RANDOM: 'random'&gt;, metric=Metric(name=train/loss/total_loss, goal=minimize), command=None, controller=None, description=None, earlyterminate=None, entity=None, imageuri=None, job=None, kind=None, name=None, program=None, project=None)</code>\n</pre> <pre><code>sweep(sweep_config=sweep_config)\n\nwandb.finish()\n</code></pre>"},{"location":"demo/#quick-start-training-demo","title":"Quick Start Training Demo","text":"<p>This is a quick start demo to get training a SAE right away. All you need to do is choose a few hyperparameters (like the model to train on), and then set it off. By default it replicates Neel Nanda's comment on the Anthropic dictionary learning paper.</p>"},{"location":"demo/#setup","title":"Setup","text":""},{"location":"demo/#imports","title":"Imports","text":""},{"location":"demo/#hyperparameters","title":"Hyperparameters","text":""},{"location":"demo/#run-the-sweep","title":"Run the sweep","text":""},{"location":"flexible_demo/","title":"Flexible demo","text":"<pre><code># Check if we're in Colab\ntry:\n    import google.colab  # noqa: F401 # type: ignore\n\n    in_colab = True\nexcept ImportError:\n    in_colab = False\n\n#  Install if in Colab\nif in_colab:\n    %pip install sparse_autoencoder transformer_lens transformers wandb\n\n# Otherwise enable hot reloading in dev mode\nif not in_colab:\n    from IPython import get_ipython  # type: ignore\n\n    ip = get_ipython()\n    if ip is not None and ip.extension_manager is not None and not ip.extension_manager.loaded:\n        ip.extension_manager.load(\"autoreload\")  # type: ignore\n        %autoreload 2\n</code></pre> <pre><code>import os\nfrom pathlib import Path\n\nimport torch\nfrom transformer_lens import HookedTransformer\nfrom transformer_lens.utils import get_device\n\nfrom sparse_autoencoder import (\n    ActivationResampler,\n    AdamWithReset,\n    L2ReconstructionLoss,\n    LearnedActivationsL1Loss,\n    LossReducer,\n    Pipeline,\n    PreTokenizedDataset,\n    SparseAutoencoder,\n)\nimport wandb\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndevice = get_device()\nprint(f\"Using device: {device}\")  # You will need a GPU\n</code></pre> <pre>\n<code>Using device: mps\n</code>\n</pre> <p>The way this library works is that you can define your own hyper-parameters and then setup the underlying components with them. This is extremely flexible, but to help you get started we've included some common ones below along with some sensible defaults. You can also easily sweep through multiple hyperparameters with <code>wandb.sweep</code>.</p> <pre><code>torch.random.manual_seed(49)\n\nhyperparameters = {\n    # Expansion factor is the number of features in the sparse representation, relative to the\n    # number of features in the original MLP layer. The original paper experimented with 1x to 256x,\n    # and we have found that 4x is a good starting point.\n    \"expansion_factor\": 4,\n    # L1 coefficient is the coefficient of the L1 regularization term (used to encourage sparsity).\n    \"l1_coefficient\": 3e-4,\n    # Adam parameters (set to the default ones here)\n    \"lr\": 1e-4,\n    \"adam_beta_1\": 0.9,\n    \"adam_beta_2\": 0.999,\n    \"adam_epsilon\": 1e-8,\n    \"adam_weight_decay\": 0.0,\n    # Batch sizes\n    \"train_batch_size\": 4096,\n    \"context_size\": 128,\n    # Source model hook point\n    \"source_model_name\": \"gelu-2l\",\n    \"source_model_dtype\": \"float32\",\n    \"source_model_hook_point\": \"blocks.0.hook_mlp_out\",\n    \"source_model_hook_point_layer\": 0,\n    # Train pipeline parameters\n    \"max_store_size\": 384 * 4096 * 2,\n    \"max_activations\": 2_000_000_000,\n    \"resample_frequency\": 122_880_000,\n    \"checkpoint_frequency\": 100_000_000,\n    \"validation_frequency\": 384 * 4096 * 2 * 100,  # Every 100 generations\n}\n</code></pre> <p>The source model is just a TransformerLens model (see here for a full list of supported models).</p> <p>In this example we're training a sparse autoencoder on the activations from the first MLP layer, so we'll also get some details about that hook point.</p> <pre><code># Source model setup with TransformerLens\nsrc_model = HookedTransformer.from_pretrained(\n    str(hyperparameters[\"source_model_name\"]), dtype=str(hyperparameters[\"source_model_dtype\"])\n)\n\n# Details about the activations we'll train the sparse autoencoder on\nautoencoder_input_dim: int = src_model.cfg.d_model  # type: ignore (TransformerLens typing is currently broken)\n\nf\"Source: {hyperparameters['source_model_name']}, \\\n    Hook: {hyperparameters['source_model_hook_point']}, \\\n    Features: {autoencoder_input_dim}\"\n</code></pre> <pre>\n<code>Loaded pretrained model gelu-2l into HookedTransformer\n</code>\n</pre> <pre>\n<code>'Source: gelu-2l,     Hook: blocks.0.hook_mlp_out,     Features: 512'</code>\n</pre> <p>We can then setup the sparse autoencoder. The default model (<code>SparseAutoencoder</code>) is setup as per the original Anthropic paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning .</p> <p>However it's just a standard PyTorch model, so you can create your own model instead if you want to use a different architecture. To do this you just need to extend the <code>AbstractAutoencoder</code>, and optionally the underlying <code>AbstractEncoder</code>, <code>AbstractDecoder</code> and <code>AbstractOuterBias</code>. See these classes (which are fully documented) for more details.</p> <pre><code>expansion_factor = hyperparameters[\"expansion_factor\"]\nautoencoder = SparseAutoencoder(\n    n_input_features=autoencoder_input_dim,  # size of the activations we are autoencoding\n    n_learned_features=int(autoencoder_input_dim * expansion_factor),  # size of SAE\n).to(device)\nautoencoder\n</code></pre> <pre>\n<code>SparseAutoencoder(\n  (_pre_encoder_bias): TiedBias(position=pre_encoder)\n  (_encoder): LinearEncoder(\n    in_features=512, out_features=2048\n    (activation_function): ReLU()\n  )\n  (_decoder): UnitNormDecoder(in_features=2048, out_features=512)\n  (_post_decoder_bias): TiedBias(position=post_decoder)\n)</code>\n</pre> <p>We'll also want to setup an Optimizer and Loss function. In this case we'll also use the standard approach from the original Anthropic paper. However you can create your own loss functions and optimizers by extending <code>AbstractLoss</code> and <code>AbstractOptimizerWithReset</code> respectively.</p> <pre><code># We use a loss reducer, which simply adds up the losses from the underlying loss functions.\nloss = LossReducer(\n    LearnedActivationsL1Loss(\n        l1_coefficient=float(hyperparameters[\"l1_coefficient\"]),\n    ),\n    L2ReconstructionLoss(),\n)\nloss\n</code></pre> <pre>\n<code>LossReducer(\n  (0): LearnedActivationsL1Loss(l1_coefficient=0.0003)\n  (1): L2ReconstructionLoss()\n)</code>\n</pre> <pre><code>optimizer = AdamWithReset(\n    params=autoencoder.parameters(),\n    named_parameters=autoencoder.named_parameters(),\n    lr=float(hyperparameters[\"lr\"]),\n    betas=(float(hyperparameters[\"adam_beta_1\"]), float(hyperparameters[\"adam_beta_2\"])),\n    eps=float(hyperparameters[\"adam_epsilon\"]),\n    weight_decay=float(hyperparameters[\"adam_weight_decay\"]),\n)\noptimizer\n</code></pre> <pre>\n<code>AdamWithReset (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0.0\n)</code>\n</pre> <p>Finally we'll initialise an activation resampler.</p> <pre><code>activation_resampler = ActivationResampler(\n    resample_interval=10_000,\n    n_activations_activity_collate=10_000,\n    max_n_resamples=5,\n    n_learned_features=autoencoder.n_learned_features,\n)\n</code></pre> <p>This is just a dataset of tokenized prompts, to be used in generating activations (which are in turn used to train the SAE).</p> <pre><code>source_data = PreTokenizedDataset(\n    dataset_path=\"NeelNanda/c4-code-tokenized-2b\", context_size=int(hyperparameters[\"context_size\"])\n)\n</code></pre> <pre>\n<code>Resolving data files:   0%|          | 0/28 [00:00&lt;?, ?it/s]</code>\n</pre> <p>If you initialise wandb, the pipeline will automatically log all metrics to wandb. However, we should pass in a dictionary with all of our hyperaparameters so they're on  wandb. </p> <p>We strongly encourage users to make use of wandb in order to understand the training process.</p> <pre><code>checkpoint_path = Path(\"../../.checkpoints\")\ncheckpoint_path.mkdir(exist_ok=True)\n</code></pre> <pre><code>Path(\".cache/\").mkdir(exist_ok=True)\nwandb.init(\n    project=\"sparse-autoencoder\",\n    dir=\".cache\",\n    config=hyperparameters,\n)\n</code></pre> <pre><code>pipeline = Pipeline(\n    activation_resampler=activation_resampler,\n    autoencoder=autoencoder,\n    cache_name=str(hyperparameters[\"source_model_hook_point\"]),\n    checkpoint_directory=checkpoint_path,\n    layer=int(hyperparameters[\"source_model_hook_point_layer\"]),\n    loss=loss,\n    optimizer=optimizer,\n    source_data_batch_size=6,\n    source_dataset=source_data,\n    source_model=src_model,\n)\n\npipeline.run_pipeline(\n    train_batch_size=int(hyperparameters[\"train_batch_size\"]),\n    max_store_size=int(hyperparameters[\"max_store_size\"]),\n    max_activations=int(hyperparameters[\"max_activations\"]),\n    checkpoint_frequency=int(hyperparameters[\"checkpoint_frequency\"]),\n    validate_frequency=int(hyperparameters[\"validation_frequency\"]),\n)\n</code></pre> <pre><code>wandb.finish()\n</code></pre>"},{"location":"flexible_demo/#flexible-training-demo","title":"Flexible Training Demo","text":"<p>This demo shows you how to train a sparse autoencoder (SAE) on a TransformerLens model. It replicates Neel Nanda's comment on the Anthropic dictionary learning paper.</p>"},{"location":"flexible_demo/#introduction","title":"Introduction","text":"<p>The way this library works is that we provide all the components necessary to train a sparse autoencoder. For the most part, these are just standard PyTorch modules. For example <code>AdamWithReset</code> is just an extension of <code>torch.optim.Adam</code>, with a few extra bells and whistles that are needed for training a SAE (e.g. a method to reset the optimizer state when you are also resampling dead neurons).</p> <p>This is very flexible - it's easy for you to extend and change just one component if you want, just like you'd do with a standard PyTorch mode. It also means it's very easy to see what is going on under the hood. However to get you started, the following demo sets up a default SAE that uses the implementation that Neel Nanda used in his comment above.</p>"},{"location":"flexible_demo/#approach","title":"Approach","text":"<p>The approach is pretty simple - we run a training pipeline that alternates between generating activations from a source model, and training the sparse autoencoder model on these generated activations.</p>"},{"location":"flexible_demo/#setup","title":"Setup","text":""},{"location":"flexible_demo/#imports","title":"Imports","text":""},{"location":"flexible_demo/#hyperparameters","title":"Hyperparameters","text":""},{"location":"flexible_demo/#source-model","title":"Source Model","text":""},{"location":"flexible_demo/#sparse-autoencoder","title":"Sparse Autoencoder","text":""},{"location":"flexible_demo/#source-dataset","title":"Source dataset","text":""},{"location":"flexible_demo/#training","title":"Training","text":""},{"location":"pre-process-datasets/","title":"Source dataset pre-processing","text":"<p>Note you will also need to login to HuggingFace via the CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <pre><code># Check if we're in Colab\ntry:\n    import google.colab  # noqa: F401 # type: ignore\n\n    in_colab = True\nexcept ImportError:\n    in_colab = False\n\n#  Install if in Colab\nif in_colab:\n    %pip install sparse_autoencoder transformer_lens transformers wandb datasets\n\n# Otherwise enable hot reloading in dev mode\nif not in_colab:\n    %load_ext autoreload\n    %autoreload 2\n</code></pre> <pre><code>from dataclasses import dataclass\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom sparse_autoencoder import TextDataset\n</code></pre> <pre><code>@dataclass\nclass DatasetToPreprocess:\n    \"\"\"Dataset to preprocess info.\"\"\"\n\n    source_path: str\n    \"\"\"Source path from HF (e.g. `roneneldan/TinyStories`).\"\"\"\n\n    tokenizer_name: str\n    \"\"\"HF tokenizer name (e.g. `gpt2`).\"\"\"\n\n    data_dir: str | None = None\n    \"\"\"Data directory to download from the source dataset.\"\"\"\n\n    data_files: list[str] | None = None\n    \"\"\"Data files to download from the source dataset.\"\"\"\n\n    hugging_face_username: str = \"alancooney\"\n    \"\"\"HF username for the upload.\"\"\"\n\n    @property\n    def source_alias(self) -&amp;gt; str:\n        \"\"\"Create a source alias for the destination dataset name.\n\n        Returns:\n            The modified source path as source alias.\n        \"\"\"\n        return self.source_path.replace(\"/\", \"-\")\n\n    @property\n    def tokenizer_alias(self) -&amp;gt; str:\n        \"\"\"Create a tokenizer alias for the destination dataset name.\n\n        Returns:\n            The modified tokenizer name as tokenizer alias.\n        \"\"\"\n        return self.tokenizer_name.replace(\"/\", \"-\")\n\n    @property\n    def destination_repo_name(self) -&amp;gt; str:\n        \"\"\"Destination repo name.\n\n        Returns:\n            The destination repo name.\n        \"\"\"\n        return f\"sae-{self.source_alias}-tokenizer-{self.tokenizer_alias}\"\n\n    @property\n    def destination_repo_id(self) -&amp;gt; str:\n        \"\"\"Destination repo ID.\n\n        Returns:\n            The destination repo ID.\n        \"\"\"\n        return f\"{self.hugging_face_username}/{self.destination_repo_name}\"\n\n\ndef upload_datasets(datasets_to_preprocess: list[DatasetToPreprocess]) -&amp;gt; None:\n    \"\"\"Upload datasets to HF.\n\n    Warning:\n        Assumes you have already created the corresponding repos on HF.\n\n    Args:\n        datasets_to_preprocess: List of datasets to preprocess.\n\n    Raises:\n        ValueError: If the repo doesn't exist.\n    \"\"\"\n    repositories_updating = [dataset.destination_repo_id for dataset in datasets_to_preprocess]\n    print(\"Updating repositories:\\n\" \"\\n\".join(repositories_updating))\n\n    for dataset in datasets_to_preprocess:\n        print(\"Processing dataset: \", dataset.source_path)\n\n        # Preprocess\n        tokenizer = AutoTokenizer.from_pretrained(dataset.tokenizer_name)\n        text_dataset = TextDataset(\n            dataset_path=dataset.source_path,\n            tokenizer=tokenizer,\n            pre_download=True,  # Must be true to upload after pre-processing, to the hub.\n            dataset_files=dataset.data_files,\n            dataset_dir=dataset.data_dir,\n        )\n        print(\"Size: \", text_dataset.dataset.size_in_bytes)\n        print(\"Info: \", text_dataset.dataset.info)\n\n        # Upload\n        text_dataset.push_to_hugging_face_hub(repo_id=dataset.destination_repo_id)\n</code></pre> <pre><code>datasets: list[DatasetToPreprocess] = [\n    DatasetToPreprocess(\n        source_path=\"roneneldan/TinyStories\",\n        tokenizer_name=\"gpt2\",\n        # Get the newer versions (Generated with GPT-4 only)\n        data_files=[\"TinyStoriesV2-GPT4-train.txt\", \"TinyStoriesV2-GPT4-valid.txt\"],\n    ),\n    DatasetToPreprocess(\n        source_path=\"monology/pile-uncopyrighted\",\n        tokenizer_name=\"gpt2\",\n        # Get just the first few (each file is 11GB so this should be enough for a large dataset)\n        data_files=[\n            \"00.jsonl.zst\",\n            \"01.jsonl.zst\",\n            \"02.jsonl.zst\",\n            \"03.jsonl.zst\",\n            \"04.jsonl.zst\",\n            \"05.jsonl.zst\",\n        ],\n        data_dir=\"train\",\n    ),\n    DatasetToPreprocess(\n        source_path=\"monology/pile-uncopyrighted\",\n        tokenizer_name=\"EleutherAI/gpt-neox-20b\",\n        data_files=[\n            \"00.jsonl.zst\",\n            \"01.jsonl.zst\",\n            \"02.jsonl.zst\",\n            \"03.jsonl.zst\",\n            \"04.jsonl.zst\",\n            \"05.jsonl.zst\",\n        ],\n        data_dir=\"train\",\n    ),\n]\n\nupload_datasets(datasets)\n</code></pre> <pre><code>downloaded_dataset = load_dataset(\n    \"alancooney/sae-roneneldan-TinyStories-tokenizer-gpt2\", streaming=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ni = 0\nfirst_k = 3\nfor data_item in iter(downloaded_dataset[\"train\"]):  # type:ignore\n    # Get just the first few\n    i += 1\n    if i &amp;gt;= first_k:\n        break\n\n    # Print the decoded items\n    input_ids = data_item[\"input_ids\"]\n    decoded = tokenizer.decode(input_ids)\n    print(f\"{len(input_ids)} tokens: {decoded}\")\n</code></pre>"},{"location":"pre-process-datasets/#pre-process-datasets","title":"Pre-process datasets","text":"<p>When training a sparse autoencoder (SAE) often you want to use a text dataset such as The Pile. </p> <p>The <code>TextDataset</code> class can pre-process this for you on the fly (i.e. tokenize and split into <code>context_size</code> chunks of tokens), so that you can get started right away. However, if you're experimenting a lot, it can be nicer to run this once and then save the resulting dataset to HuggingFace. You can then use <code>PreTokenizedDataset</code> to load this directly, saving you from running this pre-processing every time you use it.</p> <p>The following code shows you how to do this, and is also used to upload a set of commonly used datasets for SAE training to Alan Cooney's HuggingFace hub.</p>"},{"location":"pre-process-datasets/#setup","title":"Setup","text":""},{"location":"pre-process-datasets/#upload-helper","title":"Upload helper","text":"<p>Here we define a helper function to upload multiple datasets.</p>"},{"location":"pre-process-datasets/#upload-to-hugging-face","title":"Upload to Hugging Face","text":""},{"location":"pre-process-datasets/#check-a-dataset-is-as-expected","title":"Check a dataset is as expected","text":""},{"location":"reference/","title":"Sparse Autoencoder Library","text":"<p>Sparse Autoencoder Library.</p>"},{"location":"reference/#sparse_autoencoder.LossLogType","title":"<code>LossLogType: TypeAlias = dict[str, int | float | str]</code>  <code>module-attribute</code>","text":"<p>Loss log dict.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResampler","title":"<code>ActivationResampler</code>","text":"<p>             Bases: <code>AbstractActivationResampler</code></p> <p>Activation resampler.</p> <p>Collates the number of times each neuron fires over a set number of learned activation vectors, and then provides the parameters necessary to reset any dead neurons.</p> Motivation <p>Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.</p> <p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.</p> <p>This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network.</p> Warning <p>The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases.</p> Warning <p>This approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>class ActivationResampler(AbstractActivationResampler):\n    \"\"\"Activation resampler.\n\n    Collates the number of times each neuron fires over a set number of learned activation vectors,\n    and then provides the parameters necessary to reset any dead neurons.\n\n    Motivation:\n        Over the course of training, a subset of autoencoder neurons will have zero activity across\n        a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language\n        Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training\n        improves the number of likely-interpretable features (i.e., those in the high density\n        cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket\n        Hypothesis and increase the number of chances the network has to find promising feature\n        directions.\n\n        An interesting nuance around dead neurons involves the ultralow density cluster. They found\n        that if we increase the number of training steps then networks will kill off more of these\n        ultralow density neurons. This reinforces the use of the high density cluster as a useful\n        metric because there can exist neurons that are de facto dead but will not appear to be when\n        looking at the number of dead neurons alone.\n\n        This approach is designed to seed new features to fit inputs where the current autoencoder\n        performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled\n        neuron will only fire weakly for inputs similar to the one used for its reinitialization.\n        This was done to minimize interference with the rest of the network.\n\n    Warning:\n        The optimizer should be reset after applying this function, as the Adam state will be\n        incorrect for the modified weights and biases.\n\n    Warning:\n        This approach is also known to create sudden loss spikes, and resampling too frequently\n        causes training to diverge.\n    \"\"\"\n\n    _activations_seen_since_last_resample: int = 0\n    \"\"\"Number of activations since we last resampled.\"\"\"\n\n    _collated_neuron_activity: Float[Tensor, Axis.LEARNT_FEATURE]\n    \"\"\"Collated neuron activity, over the current data collection window.\"\"\"\n\n    _threshold_is_dead_portion_fires: float\n    \"\"\"Threshold for determining if a neuron has fired (or is dead).\"\"\"\n\n    _max_n_resamples: int\n    \"\"\"Maximum number of times that resampling should be performed.\"\"\"\n\n    _n_activations_collated_since_last_resample: int = 0\n    \"\"\"Number of activations collated since we last resampled.\n\n    Number of vectors used to collate neuron activity, over the current collation window.\n    \"\"\"\n\n    _number_times_resampled: int = 0\n    \"\"\"Number of times that resampling has been performed.\"\"\"\n\n    neuron_activity_window_end: int\n    \"\"\"End of the window for collecting neuron activity.\"\"\"\n\n    neuron_activity_window_start: int\n    \"\"\"Start of the window for collecting neuron activity.\"\"\"\n\n    def __init__(\n        self,\n        n_learned_features: int,\n        resample_interval: int = 200_000_000,\n        max_n_resamples: int = 4,\n        n_activations_activity_collate: int = 100_000_000,\n        resample_dataset_size: int = 819_200,\n        threshold_is_dead_portion_fires: float = 0.0,\n    ) -&gt; None:\n        r\"\"\"Initialize the activation resampler.\n\n        Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n        Args:\n            n_learned_features: Number of learned features\n            resample_interval: Interval in number of autoencoder input activation vectors trained\n                on, before resampling.\n            max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n                Set to inf if you want to have no limit.\n            n_activations_activity_collate: Number of autoencoder learned activation vectors to\n                collate before resampling (the activation resampler will start collecting on vector\n                $\\text{resample_interval} - \\text{n_steps_collate}$).\n            resample_dataset_size: Number of autoencoder input activations to use for calculating\n                the loss, as part of the resampling process to create the reset neuron weights.\n            threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n                \"fired\" in less than this portion of the collated sample).\n\n        Raises:\n            ValueError: If any of the arguments are invalid (e.g. negative integers).\n        \"\"\"\n        if n_activations_activity_collate &lt;= 0:\n            error_message = \"Number of steps to collate must be greater than 0.\"\n            raise ValueError(error_message)\n\n        if n_activations_activity_collate &gt; resample_interval:\n            error_message = (\n                \"Number of steps to collate must be less than or equal to the resample interval.\"\n            )\n            raise ValueError(error_message)\n\n        if threshold_is_dead_portion_fires &lt; 0 or threshold_is_dead_portion_fires &gt; 1:\n            error_message = (\n                \"Threshold portion of times that a dead neuron fires, must be between 0 and 1.\"\n            )\n            raise ValueError(error_message)\n\n        if max_n_resamples &lt; 0:\n            error_message = (\n                \"Maximum number of resamples must be greater than 0. For unlimited, use inf.\"\n            )\n            raise ValueError(error_message)\n\n        if resample_dataset_size &lt; 0:\n            error_message = \"Resample dataset size must be greater than 0.\"\n            raise ValueError(error_message)\n\n        super().__init__()\n        self.neuron_activity_window_end = resample_interval\n        self.neuron_activity_window_start = resample_interval - n_activations_activity_collate\n        self._max_n_resamples = max_n_resamples\n        self._collated_neuron_activity = torch.zeros(n_learned_features, dtype=torch.int64)\n        self._resample_dataset_size = resample_dataset_size\n        self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n\n    def _get_dead_neuron_indices(\n        self,\n    ) -&gt; Int64[Tensor, Axis.LEARNT_FEATURE_IDX]:\n        \"\"\"Identify the indices of neurons that are dead.\n\n        Identifies any neurons that have fired less than the threshold portion of the collated\n        sample size.\n\n        Returns:\n            A tensor containing the indices of neurons that are dead.\n\n        Raises:\n            ValueError: If no neuron activity has been collated yet.\n        \"\"\"\n        # Check we have already collated some neuron activity\n        if torch.all(self._collated_neuron_activity == 0):\n            error_message = \"Cannot get dead neuron indices without neuron activity.\"\n            raise ValueError(error_message)\n\n        # Find any neurons that fire less than the threshold portion of times\n        threshold_is_dead_number_fires: int = int(\n            self._n_activations_collated_since_last_resample * self._threshold_is_dead_portion_fires\n        )\n\n        dead_indices = torch.where(\n            self._collated_neuron_activity &lt;= threshold_is_dead_number_fires\n        )[0]\n\n        return dead_indices.to(dtype=torch.int64)\n\n    def compute_loss_and_get_activations(\n        self,\n        store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; tuple[\n        Float[Tensor, Axis.BATCH], Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n    ]:\n        \"\"\"Compute the loss on a random subset of inputs.\n\n        Motivation:\n            Helps find input vectors that have high loss, so that we can resample dead neurons in a\n            way that improves performance on these specific input vectors.\n\n        Args:\n            store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            A tuple of loss per item, and all input activations.\n\n        Raises:\n            ValueError: If the number of items in the store is less than the number of inputs\n        \"\"\"\n        with torch.no_grad():\n            loss_batches: list[Float[Tensor, Axis.BATCH]] = []\n            input_activations_batches: list[\n                Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n            ] = []\n            dataloader = DataLoader(store, batch_size=train_batch_size)\n            num_inputs = self._resample_dataset_size\n            n_batches_required: int = num_inputs // train_batch_size\n            model_device: torch.device = get_model_device(autoencoder)\n\n            for batch_idx, batch in enumerate(iter(dataloader)):\n                input_activations_batches.append(batch)\n                source_activations = batch.to(model_device)\n                learned_activations, reconstructed_activations = autoencoder(source_activations)\n                loss_batches.append(\n                    loss_fn.forward(\n                        source_activations, learned_activations, reconstructed_activations\n                    )\n                )\n                if batch_idx &gt;= n_batches_required:\n                    break\n\n            loss_result = torch.cat(loss_batches).to(model_device)\n            input_activations = torch.cat(input_activations_batches).to(model_device)\n\n            # Check we generated enough data\n            if len(loss_result) &lt; num_inputs:\n                error_message = (\n                    f\"Cannot get {num_inputs} items from the store, \"\n                    f\"as only {len(loss_result)} were available.\"\n                )\n                raise ValueError(error_message)\n\n            return loss_result, input_activations\n\n    @staticmethod\n    def assign_sampling_probabilities(loss: Float[Tensor, Axis.BATCH]) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Assign the sampling probabilities for each input activations vector.\n\n        Assign each input vector a probability of being picked that is proportional to the square of\n        the autoencoder's loss on that input.\n\n        Example:\n            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n            tensor([0.1000, 0.3000, 0.6000])\n\n        Args:\n            loss: Loss per item.\n\n        Returns:\n            A tensor of probabilities for each item.\n        \"\"\"\n        square_loss = loss.pow(2)\n        return square_loss / square_loss.sum()\n\n    @staticmethod\n    def sample_input(\n        probabilities: Float[Tensor, Axis.BATCH],\n        input_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        num_samples: int,\n    ) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Sample an input vector based on the provided probabilities.\n\n        Example:\n            &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n            &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n            ...     probabilities, input_activations, 2\n            ... )\n            &gt;&gt;&gt; sampled_input.tolist()\n            [[5.0, 6.0], [3.0, 4.0]]\n\n        Args:\n            probabilities: Probabilities for each input.\n            input_activations: Input activation vectors.\n            num_samples: Number of samples to take (number of dead neurons).\n\n        Returns:\n            Sampled input activation vector.\n\n        Raises:\n            ValueError: If the number of samples is greater than the number of input activations.\n        \"\"\"\n        if num_samples &gt; len(input_activations):\n            exception_message = (\n                f\"Cannot sample {num_samples} inputs from \"\n                f\"{len(input_activations)} input activations.\"\n            )\n            raise ValueError(exception_message)\n\n        if num_samples == 0:\n            return torch.empty(\n                (0, input_activations.shape[-1]),\n                dtype=input_activations.dtype,\n                device=input_activations.device,\n            ).to(input_activations.device)\n\n        sample_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n            probabilities, num_samples=num_samples\n        )\n        return input_activations[sample_indices, :]\n\n    @staticmethod\n    def renormalize_and_scale(\n        sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n        neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n        encoder_weight: Float[\n            Parameter, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n        Renormalize the input vector to equal the average norm of the encoder weights for alive\n        neurons times 0.2.\n\n        Example:\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n            &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n            ...     sampled_input,\n            ...     neuron_activity,\n            ...     encoder_weight\n            ... )\n            &gt;&gt;&gt; rescaled_input.round(decimals=1)\n            tensor([[0.2000, 0.2000]])\n\n        Args:\n            sampled_input: Tensor of the sampled input activation.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            encoder_weight: Tensor of encoder weights.\n\n        Returns:\n            Rescaled sampled input.\n\n        Raises:\n            ValueError: If there are no alive neurons.\n        \"\"\"\n        alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n        # Check there is at least one alive neuron\n        if not torch.any(alive_neuron_mask):\n            error_message = \"No alive neurons found.\"\n            raise ValueError(error_message)\n\n        # Handle no dead neurons\n        n_dead_neurons = len(sampled_input)\n        if n_dead_neurons == 0:\n            return torch.empty(\n                (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n            )\n\n        # Calculate the average norm of the encoder weights for alive neurons.\n        detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n        alive_encoder_weights: Float[\n            Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = detached_encoder_weight[alive_neuron_mask, :]\n        average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n            dim=-1\n        ).mean()\n\n        # Renormalize the input vector to equal the average norm of the encoder weights for alive\n        # neurons times 0.2.\n        renormalized_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n        return renormalized_input * (average_alive_norm * 0.2)\n\n    def resample_dead_neurons(\n        self,\n        activation_store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n        \"\"\"\n        with torch.no_grad():\n            dead_neuron_indices = self._get_dead_neuron_indices()\n\n            # Compute the loss for the current model on a random subset of inputs and get the\n            # activations.\n            loss, input_activations = self.compute_loss_and_get_activations(\n                store=activation_store,\n                autoencoder=autoencoder,\n                loss_fn=loss_fn,\n                train_batch_size=train_batch_size,\n            )\n\n            # Assign each input vector a probability of being picked that is proportional to the\n            # square of the autoencoder's loss on that input.\n            sample_probabilities: Float[Tensor, Axis.BATCH] = self.assign_sampling_probabilities(\n                loss\n            )\n\n            # For each dead neuron sample an input according to these probabilities.\n            sampled_input: Float[\n                Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n            ] = self.sample_input(sample_probabilities, input_activations, len(dead_neuron_indices))\n\n            # Renormalize each input vector to have unit L2 norm and set this to be the dictionary\n            # vector for the dead autoencoder neuron.\n            renormalized_input: Float[\n                Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n            ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n            dead_decoder_weight_updates = rearrange(\n                renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n            )\n\n            # For the corresponding encoder vector, renormalize the input vector to equal the\n            # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n            # encoder bias element to zero.\n            encoder_weight: Float[\n                Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n            ] = autoencoder.encoder.weight\n            rescaled_sampled_input = self.renormalize_and_scale(\n                sampled_input=sampled_input,\n                neuron_activity=self._collated_neuron_activity,\n                encoder_weight=encoder_weight,\n            )\n            dead_encoder_bias_updates = torch.zeros_like(\n                dead_neuron_indices,\n                dtype=dead_decoder_weight_updates.dtype,\n                device=dead_decoder_weight_updates.device,\n            )\n\n            return ParameterUpdateResults(\n                dead_neuron_indices=dead_neuron_indices,\n                dead_encoder_weight_updates=rescaled_sampled_input,\n                dead_encoder_bias_updates=dead_encoder_bias_updates,\n                dead_decoder_weight_updates=dead_decoder_weight_updates,\n            )\n\n    def step_resampler(\n        self,\n        batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n        activation_store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults | None:\n        \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n        Args:\n            batch_neuron_activity: Number of times each neuron fired in the current batch.\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Parameter update results if resampled, else None.\n        \"\"\"\n        # Update the counter\n        self._activations_seen_since_last_resample += len(activation_store)\n\n        if self._number_times_resampled &lt; self._max_n_resamples:\n            # Collate neuron activity, if in the data collection window. For example in the\n            # Anthropic Towards Monosemanticity paper, the window started collecting at 100m\n            # activations and stopped at 200m (and then repeated this again a few times until the\n            # max times to resample was hit).\n            if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_start:\n                detached_neuron_activity = batch_neuron_activity.detach().cpu()\n                self._collated_neuron_activity.add_(detached_neuron_activity)\n                self._n_activations_collated_since_last_resample += train_batch_size\n\n            # Check if we should resample.\n            if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_end:\n                # Get resampled dictionary vectors\n                resample_res = self.resample_dead_neurons(\n                    activation_store=activation_store,\n                    autoencoder=autoencoder,\n                    loss_fn=loss_fn,\n                    train_batch_size=train_batch_size,\n                )\n\n                # Update counters\n                self._activations_seen_since_last_resample = 0\n                self._n_activations_collated_since_last_resample = 0\n                self._number_times_resampled += 1\n\n                # Reset the collated neuron activity\n                self._collated_neuron_activity.zero_()\n\n                return resample_res\n\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the activation resampler.\"\"\"\n        return (\n            f\"ActivationResampler(\"\n            f\"neuron_activity_window_start={self.neuron_activity_window_end}, \"\n            f\"neuron_activity_window_end={self.neuron_activity_window_end}, \"\n            f\"max_resamples={self._max_n_resamples}, \"\n            f\"resample_dataset_size={self._resample_dataset_size}, \"\n            f\"dead_neuron_threshold={self._threshold_is_dead_portion_fires})\"\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.neuron_activity_window_end","title":"<code>neuron_activity_window_end: int = resample_interval</code>  <code>instance-attribute</code>","text":"<p>End of the window for collecting neuron activity.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.neuron_activity_window_start","title":"<code>neuron_activity_window_start: int = resample_interval - n_activations_activity_collate</code>  <code>instance-attribute</code>","text":"<p>Start of the window for collecting neuron activity.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.__init__","title":"<code>__init__(n_learned_features, resample_interval=200000000, max_n_resamples=4, n_activations_activity_collate=100000000, resample_dataset_size=819200, threshold_is_dead_portion_fires=0.0)</code>","text":"<p>Initialize the activation resampler.</p> <p>Defaults to values used in the Anthropic Towards Monosemanticity paper.</p> <p>Parameters:</p> Name Type Description Default <code>n_learned_features</code> <code>int</code> <p>Number of learned features</p> required <code>resample_interval</code> <code>int</code> <p>Interval in number of autoencoder input activation vectors trained on, before resampling.</p> <code>200000000</code> <code>max_n_resamples</code> <code>int</code> <p>Maximum number of resamples to perform throughout the entire pipeline. Set to inf if you want to have no limit.</p> <code>4</code> <code>n_activations_activity_collate</code> <code>int</code> <p>Number of autoencoder learned activation vectors to collate before resampling (the activation resampler will start collecting on vector \\(\\text{resample_interval} - \\text{n_steps_collate}\\)).</p> <code>100000000</code> <code>resample_dataset_size</code> <code>int</code> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p> <code>819200</code> <code>threshold_is_dead_portion_fires</code> <code>float</code> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the arguments are invalid (e.g. negative integers).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def __init__(\n    self,\n    n_learned_features: int,\n    resample_interval: int = 200_000_000,\n    max_n_resamples: int = 4,\n    n_activations_activity_collate: int = 100_000_000,\n    resample_dataset_size: int = 819_200,\n    threshold_is_dead_portion_fires: float = 0.0,\n) -&gt; None:\n    r\"\"\"Initialize the activation resampler.\n\n    Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n    Args:\n        n_learned_features: Number of learned features\n        resample_interval: Interval in number of autoencoder input activation vectors trained\n            on, before resampling.\n        max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n            Set to inf if you want to have no limit.\n        n_activations_activity_collate: Number of autoencoder learned activation vectors to\n            collate before resampling (the activation resampler will start collecting on vector\n            $\\text{resample_interval} - \\text{n_steps_collate}$).\n        resample_dataset_size: Number of autoencoder input activations to use for calculating\n            the loss, as part of the resampling process to create the reset neuron weights.\n        threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n            \"fired\" in less than this portion of the collated sample).\n\n    Raises:\n        ValueError: If any of the arguments are invalid (e.g. negative integers).\n    \"\"\"\n    if n_activations_activity_collate &lt;= 0:\n        error_message = \"Number of steps to collate must be greater than 0.\"\n        raise ValueError(error_message)\n\n    if n_activations_activity_collate &gt; resample_interval:\n        error_message = (\n            \"Number of steps to collate must be less than or equal to the resample interval.\"\n        )\n        raise ValueError(error_message)\n\n    if threshold_is_dead_portion_fires &lt; 0 or threshold_is_dead_portion_fires &gt; 1:\n        error_message = (\n            \"Threshold portion of times that a dead neuron fires, must be between 0 and 1.\"\n        )\n        raise ValueError(error_message)\n\n    if max_n_resamples &lt; 0:\n        error_message = (\n            \"Maximum number of resamples must be greater than 0. For unlimited, use inf.\"\n        )\n        raise ValueError(error_message)\n\n    if resample_dataset_size &lt; 0:\n        error_message = \"Resample dataset size must be greater than 0.\"\n        raise ValueError(error_message)\n\n    super().__init__()\n    self.neuron_activity_window_end = resample_interval\n    self.neuron_activity_window_start = resample_interval - n_activations_activity_collate\n    self._max_n_resamples = max_n_resamples\n    self._collated_neuron_activity = torch.zeros(n_learned_features, dtype=torch.int64)\n    self._resample_dataset_size = resample_dataset_size\n    self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the activation resampler.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the activation resampler.\"\"\"\n    return (\n        f\"ActivationResampler(\"\n        f\"neuron_activity_window_start={self.neuron_activity_window_end}, \"\n        f\"neuron_activity_window_end={self.neuron_activity_window_end}, \"\n        f\"max_resamples={self._max_n_resamples}, \"\n        f\"resample_dataset_size={self._resample_dataset_size}, \"\n        f\"dead_neuron_threshold={self._threshold_is_dead_portion_fires})\"\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities","title":"<code>assign_sampling_probabilities(loss)</code>  <code>staticmethod</code>","text":"<p>Assign the sampling probabilities for each input activations vector.</p> <p>Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input.</p> Example <p>loss = torch.tensor([1.0, 2.0, 3.0]) ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000])</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Float[Tensor, BATCH]</code> <p>Loss per item.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>A tensor of probabilities for each item.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef assign_sampling_probabilities(loss: Float[Tensor, Axis.BATCH]) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Assign the sampling probabilities for each input activations vector.\n\n    Assign each input vector a probability of being picked that is proportional to the square of\n    the autoencoder's loss on that input.\n\n    Example:\n        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n        tensor([0.1000, 0.3000, 0.6000])\n\n    Args:\n        loss: Loss per item.\n\n    Returns:\n        A tensor of probabilities for each item.\n    \"\"\"\n    square_loss = loss.pow(2)\n    return square_loss / square_loss.sum()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations","title":"<code>compute_loss_and_get_activations(store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Compute the loss on a random subset of inputs.</p> Motivation <p>Helps find input vectors that have high loss, so that we can resample dead neurons in a way that improves performance on these specific input vectors.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>tuple[Float[Tensor, BATCH], Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]]</code> <p>A tuple of loss per item, and all input activations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of items in the store is less than the number of inputs</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def compute_loss_and_get_activations(\n    self,\n    store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; tuple[\n    Float[Tensor, Axis.BATCH], Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n]:\n    \"\"\"Compute the loss on a random subset of inputs.\n\n    Motivation:\n        Helps find input vectors that have high loss, so that we can resample dead neurons in a\n        way that improves performance on these specific input vectors.\n\n    Args:\n        store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        A tuple of loss per item, and all input activations.\n\n    Raises:\n        ValueError: If the number of items in the store is less than the number of inputs\n    \"\"\"\n    with torch.no_grad():\n        loss_batches: list[Float[Tensor, Axis.BATCH]] = []\n        input_activations_batches: list[\n            Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n        ] = []\n        dataloader = DataLoader(store, batch_size=train_batch_size)\n        num_inputs = self._resample_dataset_size\n        n_batches_required: int = num_inputs // train_batch_size\n        model_device: torch.device = get_model_device(autoencoder)\n\n        for batch_idx, batch in enumerate(iter(dataloader)):\n            input_activations_batches.append(batch)\n            source_activations = batch.to(model_device)\n            learned_activations, reconstructed_activations = autoencoder(source_activations)\n            loss_batches.append(\n                loss_fn.forward(\n                    source_activations, learned_activations, reconstructed_activations\n                )\n            )\n            if batch_idx &gt;= n_batches_required:\n                break\n\n        loss_result = torch.cat(loss_batches).to(model_device)\n        input_activations = torch.cat(input_activations_batches).to(model_device)\n\n        # Check we generated enough data\n        if len(loss_result) &lt; num_inputs:\n            error_message = (\n                f\"Cannot get {num_inputs} items from the store, \"\n                f\"as only {len(loss_result)} were available.\"\n            )\n            raise ValueError(error_message)\n\n        return loss_result, input_activations\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.renormalize_and_scale","title":"<code>renormalize_and_scale(sampled_input, neuron_activity, encoder_weight)</code>  <code>staticmethod</code>","text":"<p>Renormalize and scale the resampled dictionary vectors.</p> <p>Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2.</p> Example <p>_seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) encoder_weight = Parameter(torch.ones((6, 2))) rescaled_input = ActivationResampler.renormalize_and_scale( ...     sampled_input, ...     neuron_activity, ...     encoder_weight ... ) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]])</p> <p>Parameters:</p> Name Type Description Default <code>sampled_input</code> <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of the sampled input activation.</p> required <code>neuron_activity</code> <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>encoder_weight</code> <code>Float[Parameter, names(LEARNT_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of encoder weights.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Rescaled sampled input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no alive neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef renormalize_and_scale(\n    sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n    neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n    encoder_weight: Float[\n        Parameter, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n    Renormalize the input vector to equal the average norm of the encoder weights for alive\n    neurons times 0.2.\n\n    Example:\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n        &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n        ...     sampled_input,\n        ...     neuron_activity,\n        ...     encoder_weight\n        ... )\n        &gt;&gt;&gt; rescaled_input.round(decimals=1)\n        tensor([[0.2000, 0.2000]])\n\n    Args:\n        sampled_input: Tensor of the sampled input activation.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        encoder_weight: Tensor of encoder weights.\n\n    Returns:\n        Rescaled sampled input.\n\n    Raises:\n        ValueError: If there are no alive neurons.\n    \"\"\"\n    alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n    # Check there is at least one alive neuron\n    if not torch.any(alive_neuron_mask):\n        error_message = \"No alive neurons found.\"\n        raise ValueError(error_message)\n\n    # Handle no dead neurons\n    n_dead_neurons = len(sampled_input)\n    if n_dead_neurons == 0:\n        return torch.empty(\n            (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n        )\n\n    # Calculate the average norm of the encoder weights for alive neurons.\n    detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n    alive_encoder_weights: Float[\n        Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = detached_encoder_weight[alive_neuron_mask, :]\n    average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n        dim=-1\n    ).mean()\n\n    # Renormalize the input vector to equal the average norm of the encoder weights for alive\n    # neurons times 0.2.\n    renormalized_input: Float[\n        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n    return renormalized_input * (average_alive_norm * 0.2)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.resample_dead_neurons","title":"<code>resample_dead_neurons(activation_store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults</code> <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def resample_dead_neurons(\n    self,\n    activation_store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n    \"\"\"\n    with torch.no_grad():\n        dead_neuron_indices = self._get_dead_neuron_indices()\n\n        # Compute the loss for the current model on a random subset of inputs and get the\n        # activations.\n        loss, input_activations = self.compute_loss_and_get_activations(\n            store=activation_store,\n            autoencoder=autoencoder,\n            loss_fn=loss_fn,\n            train_batch_size=train_batch_size,\n        )\n\n        # Assign each input vector a probability of being picked that is proportional to the\n        # square of the autoencoder's loss on that input.\n        sample_probabilities: Float[Tensor, Axis.BATCH] = self.assign_sampling_probabilities(\n            loss\n        )\n\n        # For each dead neuron sample an input according to these probabilities.\n        sampled_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = self.sample_input(sample_probabilities, input_activations, len(dead_neuron_indices))\n\n        # Renormalize each input vector to have unit L2 norm and set this to be the dictionary\n        # vector for the dead autoencoder neuron.\n        renormalized_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n        dead_decoder_weight_updates = rearrange(\n            renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n        )\n\n        # For the corresponding encoder vector, renormalize the input vector to equal the\n        # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n        # encoder bias element to zero.\n        encoder_weight: Float[\n            Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = autoencoder.encoder.weight\n        rescaled_sampled_input = self.renormalize_and_scale(\n            sampled_input=sampled_input,\n            neuron_activity=self._collated_neuron_activity,\n            encoder_weight=encoder_weight,\n        )\n        dead_encoder_bias_updates = torch.zeros_like(\n            dead_neuron_indices,\n            dtype=dead_decoder_weight_updates.dtype,\n            device=dead_decoder_weight_updates.device,\n        )\n\n        return ParameterUpdateResults(\n            dead_neuron_indices=dead_neuron_indices,\n            dead_encoder_weight_updates=rescaled_sampled_input,\n            dead_encoder_bias_updates=dead_encoder_bias_updates,\n            dead_decoder_weight_updates=dead_decoder_weight_updates,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.sample_input","title":"<code>sample_input(probabilities, input_activations, num_samples)</code>  <code>staticmethod</code>","text":"<p>Sample an input vector based on the provided probabilities.</p> Example <p>probabilities = torch.tensor([0.1, 0.2, 0.7]) input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = ActivationResampler.sample_input( ...     probabilities, input_activations, 2 ... ) sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]]</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>Float[Tensor, BATCH]</code> <p>Probabilities for each input.</p> required <code>input_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Input activation vectors.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to take (number of dead neurons).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Sampled input activation vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of samples is greater than the number of input activations.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef sample_input(\n    probabilities: Float[Tensor, Axis.BATCH],\n    input_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    num_samples: int,\n) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Sample an input vector based on the provided probabilities.\n\n    Example:\n        &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n        &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n        ...     probabilities, input_activations, 2\n        ... )\n        &gt;&gt;&gt; sampled_input.tolist()\n        [[5.0, 6.0], [3.0, 4.0]]\n\n    Args:\n        probabilities: Probabilities for each input.\n        input_activations: Input activation vectors.\n        num_samples: Number of samples to take (number of dead neurons).\n\n    Returns:\n        Sampled input activation vector.\n\n    Raises:\n        ValueError: If the number of samples is greater than the number of input activations.\n    \"\"\"\n    if num_samples &gt; len(input_activations):\n        exception_message = (\n            f\"Cannot sample {num_samples} inputs from \"\n            f\"{len(input_activations)} input activations.\"\n        )\n        raise ValueError(exception_message)\n\n    if num_samples == 0:\n        return torch.empty(\n            (0, input_activations.shape[-1]),\n            dtype=input_activations.dtype,\n            device=input_activations.device,\n        ).to(input_activations.device)\n\n    sample_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n        probabilities, num_samples=num_samples\n    )\n    return input_activations[sample_indices, :]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.step_resampler","title":"<code>step_resampler(batch_neuron_activity, activation_store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Step the resampler, collating neuron activity and resampling if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>batch_neuron_activity</code> <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Number of times each neuron fired in the current batch.</p> required <code>activation_store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults | None</code> <p>Parameter update results if resampled, else None.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def step_resampler(\n    self,\n    batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n    activation_store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults | None:\n    \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n    Args:\n        batch_neuron_activity: Number of times each neuron fired in the current batch.\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Parameter update results if resampled, else None.\n    \"\"\"\n    # Update the counter\n    self._activations_seen_since_last_resample += len(activation_store)\n\n    if self._number_times_resampled &lt; self._max_n_resamples:\n        # Collate neuron activity, if in the data collection window. For example in the\n        # Anthropic Towards Monosemanticity paper, the window started collecting at 100m\n        # activations and stopped at 200m (and then repeated this again a few times until the\n        # max times to resample was hit).\n        if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_start:\n            detached_neuron_activity = batch_neuron_activity.detach().cpu()\n            self._collated_neuron_activity.add_(detached_neuron_activity)\n            self._n_activations_collated_since_last_resample += train_batch_size\n\n        # Check if we should resample.\n        if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_end:\n            # Get resampled dictionary vectors\n            resample_res = self.resample_dead_neurons(\n                activation_store=activation_store,\n                autoencoder=autoencoder,\n                loss_fn=loss_fn,\n                train_batch_size=train_batch_size,\n            )\n\n            # Update counters\n            self._activations_seen_since_last_resample = 0\n            self._n_activations_collated_since_last_resample = 0\n            self._number_times_resampled += 1\n\n            # Reset the collated neuron activity\n            self._collated_neuron_activity.zero_()\n\n            return resample_res\n\n    return None\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters","title":"<code>ActivationResamplerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Activation resampler hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass ActivationResamplerHyperparameters(NestedParameter):\n    \"\"\"Activation resampler hyperparameters.\"\"\"\n\n    resample_interval: Parameter[int] = field(\n        default=Parameter(round_to_multiple(200_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Resample interval.\"\"\"\n\n    max_n_resamples: Parameter[int] = field(default=Parameter(4))\n    \"\"\"Maximum number of resamples.\"\"\"\n\n    n_activations_activity_collate: Parameter[int] = field(\n        default=Parameter(round_to_multiple(100_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Number of steps to collate before resampling.\n\n    Number of autoencoder learned activation vectors to collate before resampling.\n    \"\"\"\n\n    resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))\n    \"\"\"Resample dataset size.\n\n    Number of autoencoder input activations to use for calculating the loss, as part of the\n    resampling process to create the reset neuron weights.\n    \"\"\"\n\n    threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Dead neuron threshold.\n\n    Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the\n    collated sample).\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.max_n_resamples","title":"<code>max_n_resamples: Parameter[int] = field(default=Parameter(4))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of resamples.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.n_activations_activity_collate","title":"<code>n_activations_activity_collate: Parameter[int] = field(default=Parameter(round_to_multiple(100000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of steps to collate before resampling.</p> <p>Number of autoencoder learned activation vectors to collate before resampling.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.resample_dataset_size","title":"<code>resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample dataset size.</p> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.resample_interval","title":"<code>resample_interval: Parameter[int] = field(default=Parameter(round_to_multiple(200000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample interval.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires","title":"<code>threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead neuron threshold.</p> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset","title":"<code>AdamWithReset</code>","text":"<p>             Bases: <code>Adam</code>, <code>AbstractOptimizerWithReset</code></p> <p>Adam Optimizer with a reset method.</p> <p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed.</p> <p>Otherwise this is the same as the standard Adam optimizer.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>@final\nclass AdamWithReset(Adam, AbstractOptimizerWithReset):\n    \"\"\"Adam Optimizer with a reset method.\n\n    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when\n    manually editing the model parameters during training (e.g. when resampling dead neurons). This\n    is because Adam maintains running averages of the gradients and the squares of gradients, which\n    will be incorrect if the parameters are changed.\n\n    Otherwise this is the same as the standard Adam optimizer.\n    \"\"\"\n\n    parameter_names: list[str]\n    \"\"\"Parameter Names.\n\n    The names of the parameters, so that we can find them later when resetting the state.\n    \"\"\"\n\n    def __init__(  # (extending existing implementation)\n        self,\n        params: params_t,\n        lr: float | Tensor = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        *,\n        amsgrad: bool = False,\n        foreach: bool | None = None,\n        maximize: bool = False,\n        capturable: bool = False,\n        differentiable: bool = False,\n        fused: bool | None = None,\n        named_parameters: Iterator[tuple[str, Parameter]],\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer.\n\n        Warning:\n            Named parameters must be with default settings (remove duplicates and not recursive).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n        Args:\n            params: Iterable of parameters to optimize or dicts defining parameter groups.\n            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n                float LR unless specifying fused=True or capturable=True.\n            betas: Coefficients used for computing running averages of gradient and its square.\n            eps: Term added to the denominator to improve numerical stability.\n            weight_decay: Weight decay (L2 penalty).\n            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n                Convergence of Adam and Beyond\".\n            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n                over the for-loop implementation on CUDA if more performant. Note that foreach uses\n                more peak memory.\n            maximize: If True, maximizes the parameters based on the objective, instead of\n                minimizing.\n            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n                ungraphed performance.\n            differentiable: Whether autograd should occur through the optimizer step in training.\n                Setting to True can impair performance.\n            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n                torch.float32, torch.float16, and torch.bfloat16.\n            named_parameters: An iterator over the named parameters of the model. This is used to\n                find the parameters when resetting their state. You should set this as\n                `model.named_parameters()`.\n\n        Raises:\n            ValueError: If the number of parameter names does not match the number of parameters.\n        \"\"\"\n        # Initialise the parent class (note we repeat the parameter names so that type hints work).\n        super().__init__(\n            params=params,\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n            differentiable=differentiable,\n            fused=fused,\n        )\n\n        # Store the names of the parameters, so that we can find them later when resetting the\n        # state.\n        self.parameter_names = [name for name, _value in named_parameters]\n\n        if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n            error_message = (\n                \"The number of parameter names does not match the number of parameters. \"\n                \"If using model.named_parameters() make sure remove_duplicates is True \"\n                \"and recursive is False (the default settings).\"\n            )\n            raise ValueError(error_message)\n\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Iterates over all parameters and resets both the running averages of the gradients and the\n        squares of gradients.\n        \"\"\"\n        # Iterate over every parameter\n        for group in self.param_groups:\n            for parameter in group[\"params\"]:\n                # Get the state\n                state = self.state[parameter]\n\n                # Check if state is initialized\n                if len(state) == 0:\n                    continue\n\n                # Reset running averages\n                exp_avg: Tensor = state[\"exp_avg\"]\n                exp_avg.zero_()\n                exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n                exp_avg_sq.zero_()\n\n                # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n                if \"max_exp_avg_sq\" in state:\n                    max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                    max_exp_avg_sq.zero_()\n\n    def reset_neurons_state(\n        self,\n        parameter: Parameter,\n        neuron_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX],\n        axis: int,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\n            ...     model.decoder.weight,\n            ...     dead_neurons_indices,\n            ...     axis=1\n            ... )\n\n        Args:\n            parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the parameter to reset.\n        \"\"\"\n        # Get the state of the parameter\n        state = self.state[parameter]\n\n        # Check if state is initialized\n        if len(state) == 0:\n            return\n\n        # Check there are any neurons to reset\n        if neuron_indices.numel() == 0:\n            return\n\n        # Reset running averages for the specified neurons\n        if \"exp_avg\" in state:\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.index_fill_(axis, neuron_indices.to(exp_avg.device), 0)\n        if \"exp_avg_sq\" in state:\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.index_fill_(axis, neuron_indices.to(exp_avg_sq.device), 0)\n\n        # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n        if \"max_exp_avg_sq\" in state:\n            max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n            max_exp_avg_sq.index_fill_(axis, neuron_indices.to(max_exp_avg_sq.device), 0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.parameter_names","title":"<code>parameter_names: list[str] = [name for (name, _value) in named_parameters]</code>  <code>instance-attribute</code>","text":"<p>Parameter Names.</p> <p>The names of the parameters, so that we can find them later when resetting the state.</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.__init__","title":"<code>__init__(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, *, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, named_parameters)</code>","text":"<p>Initialize the optimizer.</p> Warning <p>Named parameters must be with default settings (remove duplicates and not recursive).</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... ) optimizer.reset_state_all_parameters()</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>params_t</code> <p>Iterable of parameters to optimize or dicts defining parameter groups.</p> required <code>lr</code> <code>float | Tensor</code> <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a float LR unless specifying fused=True or capturable=True.</p> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients used for computing running averages of gradient and its square.</p> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <p>Term added to the denominator to improve numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (L2 penalty).</p> <code>0</code> <code>amsgrad</code> <code>bool</code> <p>Whether to use the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Whether foreach implementation of optimizer is used. If None, foreach is used over the for-loop implementation on CUDA if more performant. Note that foreach uses more peak memory.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, maximizes the parameters based on the objective, instead of minimizing.</p> <code>False</code> <code>capturable</code> <code>bool</code> <p>Whether this instance is safe to capture in a CUDA graph. True can impair ungraphed performance.</p> <code>False</code> <code>differentiable</code> <code>bool</code> <p>Whether autograd should occur through the optimizer step in training. Setting to True can impair performance.</p> <code>False</code> <code>fused</code> <code>bool | None</code> <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64, torch.float32, torch.float16, and torch.bfloat16.</p> <code>None</code> <code>named_parameters</code> <code>Iterator[tuple[str, Parameter]]</code> <p>An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as <code>model.named_parameters()</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of parameter names does not match the number of parameters.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def __init__(  # (extending existing implementation)\n    self,\n    params: params_t,\n    lr: float | Tensor = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0,\n    *,\n    amsgrad: bool = False,\n    foreach: bool | None = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: bool | None = None,\n    named_parameters: Iterator[tuple[str, Parameter]],\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Warning:\n        Named parameters must be with default settings (remove duplicates and not recursive).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n    Args:\n        params: Iterable of parameters to optimize or dicts defining parameter groups.\n        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n            float LR unless specifying fused=True or capturable=True.\n        betas: Coefficients used for computing running averages of gradient and its square.\n        eps: Term added to the denominator to improve numerical stability.\n        weight_decay: Weight decay (L2 penalty).\n        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n            Convergence of Adam and Beyond\".\n        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n            over the for-loop implementation on CUDA if more performant. Note that foreach uses\n            more peak memory.\n        maximize: If True, maximizes the parameters based on the objective, instead of\n            minimizing.\n        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n            ungraphed performance.\n        differentiable: Whether autograd should occur through the optimizer step in training.\n            Setting to True can impair performance.\n        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n            torch.float32, torch.float16, and torch.bfloat16.\n        named_parameters: An iterator over the named parameters of the model. This is used to\n            find the parameters when resetting their state. You should set this as\n            `model.named_parameters()`.\n\n    Raises:\n        ValueError: If the number of parameter names does not match the number of parameters.\n    \"\"\"\n    # Initialise the parent class (note we repeat the parameter names so that type hints work).\n    super().__init__(\n        params=params,\n        lr=lr,\n        betas=betas,\n        eps=eps,\n        weight_decay=weight_decay,\n        amsgrad=amsgrad,\n        foreach=foreach,\n        maximize=maximize,\n        capturable=capturable,\n        differentiable=differentiable,\n        fused=fused,\n    )\n\n    # Store the names of the parameters, so that we can find them later when resetting the\n    # state.\n    self.parameter_names = [name for name, _value in named_parameters]\n\n    if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n        error_message = (\n            \"The number of parameter names does not match the number of parameters. \"\n            \"If using model.named_parameters() make sure remove_duplicates is True \"\n            \"and recursive is False (the default settings).\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter, neuron_indices, axis)</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... )</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>Parameter</code> <p>The parameter to be reset. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>,</p> required <code>neuron_indices</code> <code>Int[Tensor, LEARNT_FEATURE_IDX]</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the parameter to reset.</p> required Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_neurons_state(\n    self,\n    parameter: Parameter,\n    neuron_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX],\n    axis: int,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\n        ...     model.decoder.weight,\n        ...     dead_neurons_indices,\n        ...     axis=1\n        ... )\n\n    Args:\n        parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the parameter to reset.\n    \"\"\"\n    # Get the state of the parameter\n    state = self.state[parameter]\n\n    # Check if state is initialized\n    if len(state) == 0:\n        return\n\n    # Check there are any neurons to reset\n    if neuron_indices.numel() == 0:\n        return\n\n    # Reset running averages for the specified neurons\n    if \"exp_avg\" in state:\n        exp_avg: Tensor = state[\"exp_avg\"]\n        exp_avg.index_fill_(axis, neuron_indices.to(exp_avg.device), 0)\n    if \"exp_avg_sq\" in state:\n        exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n        exp_avg_sq.index_fill_(axis, neuron_indices.to(exp_avg_sq.device), 0)\n\n    # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n    if \"max_exp_avg_sq\" in state:\n        max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n        max_exp_avg_sq.index_fill_(axis, neuron_indices.to(max_exp_avg_sq.device), 0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","title":"... train the model and then resample some dead neurons, then do this ...","text":"<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","title":"Reset the optimizer state for parameters that have been updated","text":"<p>optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0) optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ...     model.decoder.weight, ...     dead_neurons_indices, ...     axis=1 ... )</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>","text":"<p>Reset the state for all parameters.</p> <p>Iterates over all parameters and resets both the running averages of the gradients and the squares of gradients.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Iterates over all parameters and resets both the running averages of the gradients and the\n    squares of gradients.\n    \"\"\"\n    # Iterate over every parameter\n    for group in self.param_groups:\n        for parameter in group[\"params\"]:\n            # Get the state\n            state = self.state[parameter]\n\n            # Check if state is initialized\n            if len(state) == 0:\n                continue\n\n            # Reset running averages\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.zero_()\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.zero_()\n\n            # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n            if \"max_exp_avg_sq\" in state:\n                max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                max_exp_avg_sq.zero_()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AutoencoderHyperparameters","title":"<code>AutoencoderHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Sparse autoencoder hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass AutoencoderHyperparameters(NestedParameter):\n    \"\"\"Sparse autoencoder hyperparameters.\"\"\"\n\n    expansion_factor: Parameter[int] = field(default=Parameter(2))\n    \"\"\"Expansion Factor.\n\n    Size of the learned features relative to the input features. A good expansion factor to start\n    with is typically 2-4.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AutoencoderHyperparameters.expansion_factor","title":"<code>expansion_factor: Parameter[int] = field(default=Parameter(2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion Factor.</p> <p>Size of the learned features relative to the input features. A good expansion factor to start with is typically 2-4.</p>"},{"location":"reference/#sparse_autoencoder.CapacityMetric","title":"<code>CapacityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Capacities Metrics for Learned Features.</p> <p>Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks.</p> <p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features.</p> <p>If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>class CapacityMetric(AbstractTrainMetric):\n    \"\"\"Capacities Metrics for Learned Features.\n\n    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf).\n\n    Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.\n    Formally it's the ratio of the squared dot product of a feature with itself to the sum of its\n    squared dot products of all features.\n\n    If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is\n    1/n.\n    \"\"\"\n\n    @staticmethod\n    def capacities(\n        features: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Calculate capacities.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n            &gt;&gt;&gt; orthogonal_caps\n            tensor([1., 1., 1.])\n\n        Args:\n            features: A collection of features.\n\n        Returns:\n            A 1D tensor of capacities, where each element is the capacity of the corresponding\n            feature.\n        \"\"\"\n        squared_dot_products = (\n            einops.einsum(\n                features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n            )\n            ** 2\n        )\n        sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n        return torch.diag(squared_dot_products) / sum_of_sq_dot\n\n    @staticmethod\n    def wandb_capacities_histogram(\n        capacities: Float[Tensor, Axis.BATCH],\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the capacities.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n        wandb_capacities_histogram(capacities)})`.\n\n        Args:\n            capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the capacities for a training batch.\"\"\"\n        train_batch_capacities = self.capacities(data.learned_activations)\n        train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n        return {\n            \"train/batch_capacities_histogram\": train_batch_capacities_histogram,\n        }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the capacities for a training batch.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the capacities for a training batch.\"\"\"\n    train_batch_capacities = self.capacities(data.learned_activations)\n    train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n    return {\n        \"train/batch_capacities_histogram\": train_batch_capacities_histogram,\n    }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.capacities","title":"<code>capacities(features)</code>  <code>staticmethod</code>","text":"<p>Calculate capacities.</p> Example <p>import torch orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) orthogonal_caps = CapacityMetric.capacities(orthogonal_features) orthogonal_caps tensor([1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>A collection of features.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p> <code>Float[Tensor, BATCH]</code> <p>feature.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef capacities(\n    features: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Calculate capacities.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n        &gt;&gt;&gt; orthogonal_caps\n        tensor([1., 1., 1.])\n\n    Args:\n        features: A collection of features.\n\n    Returns:\n        A 1D tensor of capacities, where each element is the capacity of the corresponding\n        feature.\n    \"\"\"\n    squared_dot_products = (\n        einops.einsum(\n            features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n        )\n        ** 2\n    )\n    sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n    return torch.diag(squared_dot_products) / sum_of_sq_dot\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.wandb_capacities_histogram","title":"<code>wandb_capacities_histogram(capacities)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the capacities.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>Float[Tensor, BATCH]</code> <p>Capacity of each feature. Can be calculated using :func:<code>calc_capacities</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef wandb_capacities_histogram(\n    capacities: Float[Tensor, Axis.BATCH],\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the capacities.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n    wandb_capacities_histogram(capacities)})`.\n\n    Args:\n        capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Controller","title":"<code>Controller</code>  <code>dataclass</code>","text":"<p>Controller.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass Controller:\n    \"\"\"Controller.\"\"\"\n\n    type: ControllerType  # noqa: A003\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ControllerType","title":"<code>ControllerType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Controller Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ControllerType(LowercaseStrEnum):\n    \"\"\"Controller Type.\"\"\"\n\n    CLOUD = auto()\n    \"\"\"Weights &amp; Biases cloud controller.\n\n    Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all\n    communicate with the Weights &amp; Biases cloud service to coordinate the sweep.\n    \"\"\"\n\n    LOCAL = auto()\n    \"\"\"Local controller.\n\n    Manages the sweep operation locally, without the need for cloud-based coordination or external\n    services.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ControllerType.CLOUD","title":"<code>CLOUD = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weights &amp; Biases cloud controller.</p> <p>Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all communicate with the Weights &amp; Biases cloud service to coordinate the sweep.</p>"},{"location":"reference/#sparse_autoencoder.ControllerType.LOCAL","title":"<code>LOCAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Local controller.</p> <p>Manages the sweep operation locally, without the need for cloud-based coordination or external services.</p>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore","title":"<code>DiskActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Disk Activation Store.</p> <p>Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches.</p> <p>Multiprocess safe (supports writing from multiple GPU workers).</p> <p>Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set <code>empty_dir</code> to <code>True</code>.</p> <p>Note also that :meth:<code>close</code> must be called to ensure all activation vectors are written to disk after the last batch has been added to the store.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>class DiskActivationStore(ActivationStore):\n    \"\"\"Disk Activation Store.\n\n    Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up\n    activation vectors and then write them to the disk in batches.\n\n    Multiprocess safe (supports writing from multiple GPU workers).\n\n    Warning:\n    Unless you want to keep and use existing .pt files in the storage directory when initialized,\n    set `empty_dir` to `True`.\n\n    Note also that :meth:`close` must be called to ensure all activation vectors are written to disk\n    after the last batch has been added to the store.\n    \"\"\"\n\n    _storage_path: Path\n    \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\"\n\n    _cache: ListProxy\n    \"\"\"Cache for Activation Vectors.\n\n    Activation vectors are buffered in memory until the cache is full, at which point they are\n    written to disk.\n    \"\"\"\n\n    _cache_lock: Lock\n    \"\"\"Lock for the Cache.\"\"\"\n\n    _max_cache_size: int\n    \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\"\n\n    _thread_pool: ThreadPoolExecutor\n    \"\"\"Threadpool for non-blocking writes to the file system.\"\"\"\n\n    _disk_n_activation_vectors: ValueProxy[int]\n    \"\"\"Length of the Store (on disk).\n\n    Minus 1 signifies not calculated yet.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n        max_cache_size: int = 10_000,\n        num_workers: int = 6,\n        *,\n        empty_dir: bool = False,\n    ):\n        \"\"\"Initialize the Disk Activation Store.\n\n        Args:\n            storage_path: Path to the directory where the activation vectors will be stored.\n            max_cache_size: The maximum number of activation vectors to cache in memory before\n                writing to disk. Note this is only followed approximately.\n            num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n                that the model can keep running whilst it writes the previous activations to disk).\n                This should be less than the number of CPU cores available. You don't need multiple\n                GPUs to take advantage of this feature.\n            empty_dir: Whether to empty the directory before writing. Generally you want to set this\n                to `True` as otherwise the directory may contain stale activation vectors from\n                previous runs.\n        \"\"\"\n        super().__init__()\n\n        # Setup the storage directory\n        self._storage_path = storage_path\n        self._storage_path.mkdir(parents=True, exist_ok=True)\n\n        # Setup the Cache\n        manager = Manager()\n        self._cache = manager.list()\n        self._max_cache_size = max_cache_size\n        self._cache_lock = manager.Lock()\n        self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n        # Empty the directory if needed\n        if empty_dir:\n            self.empty()\n\n        # Create a threadpool for non-blocking writes to the cache\n        self._thread_pool = ThreadPoolExecutor(num_workers)\n\n    def _write_to_disk(self, *, wait_for_max: bool = False) -&gt; None:\n        \"\"\"Write the contents of the queue to disk.\n\n        Args:\n            wait_for_max: Whether to wait until the cache is full before writing to disk.\n        \"\"\"\n        with self._cache_lock:\n            # Check we have enough items\n            if len(self._cache) == 0:\n                return\n\n            size_to_get = min(self._max_cache_size, len(self._cache))\n            if wait_for_max and size_to_get &lt; self._max_cache_size:\n                return\n\n            # Get the activations from the cache and delete them\n            activations = self._cache[0:size_to_get]\n            del self._cache[0:size_to_get]\n\n            # Update the length cache\n            if self._disk_n_activation_vectors.value != -1:\n                self._disk_n_activation_vectors.value += len(activations)\n\n        stacked_activations = torch.stack(activations)\n\n        filename = f\"{self.__len__}.pt\"\n        torch.save(stacked_activations, self._storage_path / filename)\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        Args:\n            item: Activation vector to add to the store.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        with self._cache_lock:\n            self._cache.append(item)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def extend(\n        self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n        &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        10\n\n        Args:\n            batch: Batch of activation vectors to add to the store.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        items: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] = resize_to_list_vectors(batch)\n\n        with self._cache_lock:\n            self._cache.extend(items)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        This should be called after the last batch has been added to the store. It will wait for\n        all activation vectors to be written to disk.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; print(len(store))\n        1\n        \"\"\"\n        while len(self._cache) &gt; 0:\n            self._write_to_disk()\n\n    @property\n    def _all_filenames(self) -&gt; list[Path]:\n        \"\"\"Return a List of All Activation Vector Filenames.\"\"\"\n        return list(self._storage_path.glob(\"*.pt\"))\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\n\n        Warning:\n        This will delete all .pt files in the top level of the storage directory.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; print(len(store))\n        0\n        \"\"\"\n        for file in self._all_filenames:\n            file.unlink()\n        self._disk_n_activation_vectors.value = 0\n\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get Item Dunder Method.\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        # Find the file containing the activation vector\n        file_index = index // self._max_cache_size\n        file = self._storage_path / f\"{file_index}.pt\"\n\n        # Load the file and return the activation vector\n        activation_vectors = torch.load(file)\n        return activation_vectors[index % self._max_cache_size]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n            &gt;&gt;&gt; print(len(store))\n            0\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        # Calculate the length if not cached\n        if self._disk_n_activation_vectors.value == -1:\n            cache_size: int = 0\n            for file in self._all_filenames:\n                cache_size += len(torch.load(file))\n            self._disk_n_activation_vectors.value = cache_size\n\n        return self._disk_n_activation_vectors.value\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        # Shutdown the thread pool after everything is complete\n        self._thread_pool.shutdown(wait=True, cancel_futures=False)\n        self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    # Shutdown the thread pool after everything is complete\n    self._thread_pool.shutdown(wait=True, cancel_futures=False)\n    self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get Item Dunder Method.\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    # Find the file containing the activation vector\n    file_index = index // self._max_cache_size\n    file = self._storage_path / f\"{file_index}.pt\"\n\n    # Load the file and return the activation vector\n    activation_vectors = torch.load(file)\n    return activation_vectors[index % self._max_cache_size]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__init__","title":"<code>__init__(storage_path=DEFAULT_DISK_ACTIVATION_STORE_PATH, max_cache_size=10000, num_workers=6, *, empty_dir=False)</code>","text":"<p>Initialize the Disk Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>storage_path</code> <code>Path</code> <p>Path to the directory where the activation vectors will be stored.</p> <code>DEFAULT_DISK_ACTIVATION_STORE_PATH</code> <code>max_cache_size</code> <code>int</code> <p>The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately.</p> <code>10000</code> <code>num_workers</code> <code>int</code> <p>Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature.</p> <code>6</code> <code>empty_dir</code> <code>bool</code> <p>Whether to empty the directory before writing. Generally you want to set this to <code>True</code> as otherwise the directory may contain stale activation vectors from previous runs.</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __init__(\n    self,\n    storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n    max_cache_size: int = 10_000,\n    num_workers: int = 6,\n    *,\n    empty_dir: bool = False,\n):\n    \"\"\"Initialize the Disk Activation Store.\n\n    Args:\n        storage_path: Path to the directory where the activation vectors will be stored.\n        max_cache_size: The maximum number of activation vectors to cache in memory before\n            writing to disk. Note this is only followed approximately.\n        num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n            that the model can keep running whilst it writes the previous activations to disk).\n            This should be less than the number of CPU cores available. You don't need multiple\n            GPUs to take advantage of this feature.\n        empty_dir: Whether to empty the directory before writing. Generally you want to set this\n            to `True` as otherwise the directory may contain stale activation vectors from\n            previous runs.\n    \"\"\"\n    super().__init__()\n\n    # Setup the storage directory\n    self._storage_path = storage_path\n    self._storage_path.mkdir(parents=True, exist_ok=True)\n\n    # Setup the Cache\n    manager = Manager()\n    self._cache = manager.list()\n    self._max_cache_size = max_cache_size\n    self._cache_lock = manager.Lock()\n    self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n    # Empty the directory if needed\n    if empty_dir:\n        self.empty()\n\n    # Create a threadpool for non-blocking writes to the cache\n    self._thread_pool = ThreadPoolExecutor(num_workers)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> Example <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; print(len(store))\n        0\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    # Calculate the length if not cached\n    if self._disk_n_activation_vectors.value == -1:\n        cache_size: int = 0\n        for file in self._all_filenames:\n            cache_size += len(torch.load(file))\n        self._disk_n_activation_vectors.value = cache_size\n\n    return self._disk_n_activation_vectors.value\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a Single Item to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>Activation vector to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    Args:\n        item: Activation vector to add to the store.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    with self._cache_lock:\n        self._cache.append(item)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the Store.</p> <p>Warning: This will delete all .pt files in the top level of the storage directory.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>store.empty() print(len(store)) 0</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the Store.\n\n    Warning:\n    This will delete all .pt files in the top level of the storage directory.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; print(len(store))\n    0\n    \"\"\"\n    for file in self._all_filenames:\n        file.unlink()\n    self._disk_n_activation_vectors.value = 0\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a Batch to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>Batch of activation vectors to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def extend(\n    self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n    &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    10\n\n    Args:\n        batch: Batch of activation vectors to add to the store.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    items: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] = resize_to_list_vectors(batch)\n\n    with self._cache_lock:\n        self._cache.extend(items)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    This should be called after the last batch has been added to the store. It will wait for\n    all activation vectors to be written to disk.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; store.wait_for_writes_to_complete()\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n    while len(self._cache) &gt; 0:\n        self._write_to_disk()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Distribution","title":"<code>Distribution</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Sweep Distribution.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Distribution(LowercaseStrEnum):\n    \"\"\"Sweep Distribution.\"\"\"\n\n    BETA = auto()\n    \"\"\"Beta distribution.\n\n    Utilizes the Beta distribution, a family of continuous probability distributions defined on the\n    interval [0, 1], for parameter sampling.\n    \"\"\"\n\n    CATEGORICAL = auto()\n    \"\"\"Categorical distribution.\n\n    Employs a categorical distribution for discrete variable sampling, where each category has an\n    equal probability of being selected.\n    \"\"\"\n\n    CATEGORICAL_W_PROBABILITIES = auto()\n    \"\"\"Categorical distribution with probabilities.\n\n    Similar to categorical distribution but allows assigning different probabilities to each\n    category.\n    \"\"\"\n\n    CONSTANT = auto()\n    \"\"\"Constant distribution.\n\n    Uses a constant value for the parameter, ensuring it remains the same across all runs.\n    \"\"\"\n\n    INT_UNIFORM = auto()\n    \"\"\"Integer uniform distribution.\n\n    Samples integer values uniformly across a specified range.\n    \"\"\"\n\n    INV_LOG_UNIFORM = auto()\n    \"\"\"Inverse log-uniform distribution.\n\n    Samples values according to an inverse log-uniform distribution, useful for parameters that span\n    several orders of magnitude.\n    \"\"\"\n\n    INV_LOG_UNIFORM_VALUES = auto()\n    \"\"\"Inverse log-uniform values distribution.\n\n    Similar to the inverse log-uniform distribution but allows specifying exact values to be\n    sampled.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Distribution.BETA","title":"<code>BETA = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Beta distribution.</p> <p>Utilizes the Beta distribution, a family of continuous probability distributions defined on the interval [0, 1], for parameter sampling.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.CATEGORICAL","title":"<code>CATEGORICAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution.</p> <p>Employs a categorical distribution for discrete variable sampling, where each category has an equal probability of being selected.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.CATEGORICAL_W_PROBABILITIES","title":"<code>CATEGORICAL_W_PROBABILITIES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution with probabilities.</p> <p>Similar to categorical distribution but allows assigning different probabilities to each category.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.CONSTANT","title":"<code>CONSTANT = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant distribution.</p> <p>Uses a constant value for the parameter, ensuring it remains the same across all runs.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.INT_UNIFORM","title":"<code>INT_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Integer uniform distribution.</p> <p>Samples integer values uniformly across a specified range.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.INV_LOG_UNIFORM","title":"<code>INV_LOG_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform distribution.</p> <p>Samples values according to an inverse log-uniform distribution, useful for parameters that span several orders of magnitude.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.INV_LOG_UNIFORM_VALUES","title":"<code>INV_LOG_UNIFORM_VALUES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform values distribution.</p> <p>Similar to the inverse log-uniform distribution but allows specifying exact values to be sampled.</p>"},{"location":"reference/#sparse_autoencoder.Goal","title":"<code>Goal</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Goal.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Goal(LowercaseStrEnum):\n    \"\"\"Goal.\"\"\"\n\n    MAXIMIZE = auto()\n    \"\"\"Maximization goal.\n\n    Sets the objective of the hyperparameter tuning process to maximize a specified metric.\n    \"\"\"\n\n    MINIMIZE = auto()\n    \"\"\"Minimization goal.\n\n    Aims to minimize a specified metric during the hyperparameter tuning process.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Goal.MAXIMIZE","title":"<code>MAXIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximization goal.</p> <p>Sets the objective of the hyperparameter tuning process to maximize a specified metric.</p>"},{"location":"reference/#sparse_autoencoder.Goal.MINIMIZE","title":"<code>MINIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimization goal.</p> <p>Aims to minimize a specified metric during the hyperparameter tuning process.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping","title":"<code>HyperbandStopping</code>  <code>dataclass</code>","text":"<p>Hyperband Stopping Config.</p> <p>Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs.</p> Example <p>HyperbandStopping(type=HyperbandStoppingType.HYPERBAND) HyperbandStopping(type=hyperband)</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass HyperbandStopping:\n    \"\"\"Hyperband Stopping Config.\n\n    Speed up hyperparameter search by killing off runs that appear to have lower performance\n    than successful training runs.\n\n    Example:\n        &gt;&gt;&gt; HyperbandStopping(type=HyperbandStoppingType.HYPERBAND)\n        HyperbandStopping(type=hyperband)\n    \"\"\"\n\n    type: HyperbandStoppingType | None = HyperbandStoppingType.HYPERBAND  # noqa: A003\n\n    eta: float | None = None\n    \"\"\"ETA.\n\n    Specify the bracket multiplier schedule (default: 3).\n    \"\"\"\n\n    maxiter: int | None = None\n    \"\"\"Max Iterations.\n\n    Specify the maximum number of iterations. Note this is number of times the metric is logged, not\n    the number of activations.\n    \"\"\"\n\n    miniter: int | None = None\n    \"\"\"Min Iterations.\n\n    Set the first epoch to start trimming runs, and hyperband will automatically calculate\n    the subsequent epochs to trim runs.\n    \"\"\"\n\n    s: float | None = None\n    \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\"\n\n    strict: bool | None = None\n    \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.eta","title":"<code>eta: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ETA.</p> <p>Specify the bracket multiplier schedule (default: 3).</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.maxiter","title":"<code>maxiter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max Iterations.</p> <p>Specify the maximum number of iterations. Note this is number of times the metric is logged, not the number of activations.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.miniter","title":"<code>miniter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Min Iterations.</p> <p>Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.s","title":"<code>s: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Set the number of steps you trim runs at, working backwards from the max_iter.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.strict","title":"<code>strict: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use a more aggressive condition for termination, stops more runs.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStoppingType","title":"<code>HyperbandStoppingType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Hyperband Stopping Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class HyperbandStoppingType(LowercaseStrEnum):\n    \"\"\"Hyperband Stopping Type.\"\"\"\n\n    HYPERBAND = auto()\n    \"\"\"Hyperband algorithm.\n\n    Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping\n    method to efficiently tune hyperparameters.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStoppingType.HYPERBAND","title":"<code>HYPERBAND = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hyperband algorithm.</p> <p>Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping method to efficiently tune hyperparameters.</p>"},{"location":"reference/#sparse_autoencoder.Hyperparameters","title":"<code>Hyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Parameters</code></p> <p>Sweep Hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass Hyperparameters(Parameters):\n    \"\"\"Sweep Hyperparameters.\"\"\"\n\n    # Required parameters\n    source_data: SourceDataHyperparameters\n\n    source_model: SourceModelHyperparameters\n\n    # Optional parameters\n    activation_resampler: ActivationResamplerHyperparameters = field(\n        default=ActivationResamplerHyperparameters()\n    )\n\n    autoencoder: AutoencoderHyperparameters = field(default=AutoencoderHyperparameters())\n\n    loss: LossHyperparameters = field(default=LossHyperparameters())\n\n    optimizer: OptimizerHyperparameters = field(default=OptimizerHyperparameters())\n\n    pipeline: PipelineHyperparameters = field(default=PipelineHyperparameters())\n\n    random_seed: Parameter[int] = field(default=Parameter(49))\n    \"\"\"Random seed.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\"\"\"\n        # Check the resample dataset size &lt;= the store size (currently only works if value is used\n        # for both).\n        if (\n            self.activation_resampler.resample_dataset_size.value is not None\n            and self.pipeline.max_store_size.value is not None\n            and self.activation_resampler.resample_dataset_size.value\n            &gt; int(self.pipeline.max_store_size.value)\n        ):\n            error_message = (\n                \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n                f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n                f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n            )\n            raise ValueError(error_message)\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \"\\n    \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}(\\n    {joined_items}\\n)\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.random_seed","title":"<code>random_seed: Parameter[int] = field(default=Parameter(49))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\"\"\"\n    # Check the resample dataset size &lt;= the store size (currently only works if value is used\n    # for both).\n    if (\n        self.activation_resampler.resample_dataset_size.value is not None\n        and self.pipeline.max_store_size.value is not None\n        and self.activation_resampler.resample_dataset_size.value\n        &gt; int(self.pipeline.max_store_size.value)\n    ):\n        error_message = (\n            \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n            f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n            f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \"\\n    \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}(\\n    {joined_items}\\n)\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Impute","title":"<code>Impute</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Metric value to use in bayes search for runs that fail, crash, or are killed.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Impute(LowercaseStrEnum):\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\"\n\n    BEST = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ImputeWhileRunning","title":"<code>ImputeWhileRunning</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Appends a calculated metric even when epochs are in a running state.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ImputeWhileRunning(LowercaseStrEnum):\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    BEST = auto()\n    FALSE = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Kind","title":"<code>Kind</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Kind.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Kind(LowercaseStrEnum):\n    \"\"\"Kind.\"\"\"\n\n    SWEEP = auto()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss","title":"<code>L2ReconstructionLoss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>L2 Reconstruction loss.</p> <p>L2 reconstruction loss is calculated as the sum squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with L2 may achieve the same loss for both polysemantic and monosemantic representations of true features.</p> Example <p>import torch loss = L2ReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>@final\nclass L2ReconstructionLoss(AbstractLoss):\n    \"\"\"L2 Reconstruction loss.\n\n    L2 reconstruction loss is calculated as the sum squared error between each each input vector\n    and it's corresponding decoded vector. The original paper found that models trained with some\n    loss functions such as cross-entropy loss generally prefer to represent features\n    polysemantically, whereas models trained with L2 may achieve the same loss for both\n    polysemantic and monosemantic representations of true features.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; loss = L2ReconstructionLoss()\n        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])\n        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)\n        &gt;&gt;&gt; # Outputs both loss and metrics to log\n        &gt;&gt;&gt; loss(input_activations, unused_activations, output_activations)\n        (tensor(5.5000), {'train/loss/l2_reconstruction_loss': 5.5})\n    \"\"\"\n\n    _reduction: LossReductionType\n    \"\"\"MSE reduction type.\"\"\"\n\n    def __init__(self, reduction: LossReductionType = LossReductionType.MEAN) -&gt; None:\n        \"\"\"Initialise the L2 reconstruction loss.\n\n        Args:\n            reduction: MSE reduction type.\n        \"\"\"\n        super().__init__()\n        self._reduction = reduction\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"l2_reconstruction_loss\"\n\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],  # noqa: ARG002\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Calculate the L2 reconstruction loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n        match self._reduction:\n            case LossReductionType.MEAN:\n                return square_error_loss.mean(dim=-1)\n            case LossReductionType.SUM:\n                return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log","title":"Outputs both loss and metrics to log","text":"<p>loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'train/loss/l2_reconstruction_loss': 5.5})</p>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.__init__","title":"<code>__init__(reduction=LossReductionType.MEAN)</code>","text":"<p>Initialise the L2 reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>LossReductionType</code> <p>MSE reduction type.</p> <code>MEAN</code> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def __init__(self, reduction: LossReductionType = LossReductionType.MEAN) -&gt; None:\n    \"\"\"Initialise the L2 reconstruction loss.\n\n    Args:\n        reduction: MSE reduction type.\n    \"\"\"\n    super().__init__()\n    self._reduction = reduction\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Calculate the L2 reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],  # noqa: ARG002\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Calculate the L2 reconstruction loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n    match self._reduction:\n        case LossReductionType.MEAN:\n            return square_error_loss.mean(dim=-1)\n        case LossReductionType.SUM:\n            return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"l2_reconstruction_loss\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss","title":"<code>LearnedActivationsL1Loss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Learned activations L1 (absolute error) loss.</p> <p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity).</p> Example <p>l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>@final\nclass LearnedActivationsL1Loss(AbstractLoss):\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this\n    multiplied by the l1_coefficient (designed to encourage sparsity).\n\n    Example:\n        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)\n        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)\n        &gt;&gt;&gt; # Returns loss and metrics to log\n        &gt;&gt;&gt; l1_loss(unused_activations, learned_activations, unused_activations)[0]\n        tensor(0.5000)\n    \"\"\"\n\n    l1_coefficient: float\n    \"\"\"L1 coefficient.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"learned_activations_l1_loss_penalty\"\n\n    def __init__(self, l1_coefficient: float) -&gt; None:\n        \"\"\"Initialize the absolute error loss.\n\n        Args:\n            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n                approximate guide if you use e.g. 2x this number of tokens you might consider using\n                0.5x the l1 coefficient.\n        \"\"\"\n        self.l1_coefficient = l1_coefficient\n        super().__init__()\n\n    def _l1_loss(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],  # noqa: ARG002\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],  # noqa: ARG002\n    ) -&gt; tuple[Float[Tensor, Axis.BATCH], Float[Tensor, Axis.BATCH]]:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Tuple of itemwise absolute loss, and itemwise absolute loss multiplied by the l1\n            coefficient.\n        \"\"\"\n        absolute_loss = torch.abs(learned_activations).sum(dim=-1)\n        absolute_loss_penalty = absolute_loss * self.l1_coefficient\n        return absolute_loss, absolute_loss_penalty\n\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n\n    # Override to add both the loss and the penalty to the log\n    def batch_scalar_loss_with_log(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n        \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n                (loss before and after the l1 coefficient).\n        \"\"\"\n        absolute_loss, absolute_loss_penalty = self._l1_loss(\n            source_activations, learned_activations, decoded_activations\n        )\n\n        match reduction:\n            case LossReductionType.MEAN:\n                batch_scalar_loss = absolute_loss.mean().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n            case LossReductionType.SUM:\n                batch_scalar_loss = absolute_loss.sum().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n        metrics = {\n            \"train/loss/\" + \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n            \"train/loss/\" + self.log_name(): batch_scalar_loss_penalty.item(),\n        }\n\n        return batch_scalar_loss_penalty, metrics\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Extra representation string.\"\"\"\n        return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","title":"Returns loss and metrics to log","text":"<p>l1_loss(unused_activations, learned_activations, unused_activations)[0] tensor(0.5000)</p>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient","title":"<code>l1_coefficient: float = l1_coefficient</code>  <code>instance-attribute</code>","text":"<p>L1 coefficient.</p>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.__init__","title":"<code>__init__(l1_coefficient)</code>","text":"<p>Initialize the absolute error loss.</p> <p>Parameters:</p> Name Type Description Default <code>l1_coefficient</code> <code>float</code> <p>L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient.</p> required Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def __init__(self, l1_coefficient: float) -&gt; None:\n    \"\"\"Initialize the absolute error loss.\n\n    Args:\n        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n            approximate guide if you use e.g. 2x this number of tokens you might consider using\n            0.5x the l1 coefficient.\n    \"\"\"\n    self.l1_coefficient = l1_coefficient\n    super().__init__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.batch_scalar_loss_with_log","title":"<code>batch_scalar_loss_with_log(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Learned activations L1 (absolute error) loss, with log.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[Float[Tensor, SINGLE_ITEM], LossLogType]</code> <p>Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log (loss before and after the l1 coefficient).</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def batch_scalar_loss_with_log(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n    \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n            (loss before and after the l1 coefficient).\n    \"\"\"\n    absolute_loss, absolute_loss_penalty = self._l1_loss(\n        source_activations, learned_activations, decoded_activations\n    )\n\n    match reduction:\n        case LossReductionType.MEAN:\n            batch_scalar_loss = absolute_loss.mean().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n        case LossReductionType.SUM:\n            batch_scalar_loss = absolute_loss.sum().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n    metrics = {\n        \"train/loss/\" + \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n        \"train/loss/\" + self.log_name(): batch_scalar_loss_penalty.item(),\n    }\n\n    return batch_scalar_loss_penalty, metrics\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Extra representation string.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Extra representation string.\"\"\"\n    return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Learned activations L1 (absolute error) loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"learned_activations_l1_loss_penalty\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore","title":"<code>ListActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>List Activation Store.</p> <p>Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance.</p> <p>Multiprocess safe if the <code>multiprocessing_enabled</code> argument is set to <code>True</code>. This works in two ways:</p> <ol> <li>The list of activation vectors is stored in a multiprocessing manager, which allows multiple     processes (typically multiple GPUs) to read/write to the list.</li> <li>The <code>extend</code> method is non-blocking, and uses a threadpool to write to the list in the     background, which allows the main process to continue working even if there is just one GPU.</li> </ol> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Note that the built-in :meth:<code>shuffle</code> method is much faster than using the <code>shuffle</code> argument on <code>torch.utils.data.DataLoader</code>. You should therefore call this method before passing the dataset to the loader and then set the DataLoader <code>shuffle</code> argument to <code>False</code>.</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = ListActivationStore()\n</code></pre> <p>Add a single activation vector to the dataset (this is blocking):</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a batch of activation vectors to the dataset (non-blocking):</p> <pre><code>&gt;&gt;&gt; batch = torch.randn(10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n11\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>class ListActivationStore(ActivationStore):\n    \"\"\"List Activation Store.\n\n    Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick\n    experiments where you don't want to calculate how much memory you need in advance.\n\n    Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two\n    ways:\n\n    1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple\n        processes (typically multiple GPUs) to read/write to the list.\n    2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the\n        background, which allows the main process to continue working even if there is just one GPU.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument\n    on `torch.utils.data.DataLoader`. You should therefore call this method before passing the\n    dataset to the loader and then set the DataLoader `shuffle` argument to `False`.\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n\n    Add a single activation vector to the dataset (this is blocking):\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a batch of activation vectors to the dataset (non-blocking):\n\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        11\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] | ListProxy\n    \"\"\"Underlying List Data Store.\"\"\"\n\n    _device: torch.device | None\n    \"\"\"Device to Store the Activation Vectors On.\"\"\"\n\n    _pool: ProcessPoolExecutor | None = None\n    \"\"\"Multiprocessing Pool.\"\"\"\n\n    _pool_exceptions: ListProxy | list[Exception]\n    \"\"\"Pool Exceptions.\n\n    Used to keep track of exceptions.\n    \"\"\"\n\n    _pool_futures: list[Future]\n    \"\"\"Pool Futures.\n\n    Used to keep track of processes running in the pool.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] | None = None,\n        device: torch.device | None = None,\n        max_workers: int | None = None,\n        *,\n        multiprocessing_enabled: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the List Activation Store.\n\n        Args:\n            data: Data to initialize the dataset with.\n            device: Device to store the activation vectors on.\n            max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n                Default is the number of cores you have.\n            multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n                workers. This creates significant overhead, so you should only enable it if you have\n                multiple GPUs (and experiment with enabling/disabling it).\n        \"\"\"\n        # Default to empty\n        if data is None:\n            data = []\n\n        # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n        # between processes. Otherwise, just use a normal list.\n        if multiprocessing_enabled:\n            self._pool = ProcessPoolExecutor(max_workers=max_workers)\n            manager = Manager()\n            self._data = manager.list(data)\n            self._data.extend(data)\n            self._pool_exceptions = manager.list()\n        else:\n            self._data = data\n            self._pool_exceptions = []\n\n        self._pool_futures = []\n\n        # Device for storing the activation vectors\n        self._device = device\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return len(self._data)\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Returns:\n            The size of the dataset in bytes.\n        \"\"\"\n        # The list of tensors is really a list of pointers to tensors, so we need to account for\n        # this as well as the size of the tensors themselves.\n        list_of_pointers_size = self._data.__sizeof__()\n\n        # Handle 0 items\n        if len(self._data) == 0:\n            return list_of_pointers_size\n\n        # Otherwise, get the size of the first tensor\n        first_tensor = self._data[0]\n        first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n        num_tensors = len(self._data)\n        total_tensors_size = first_tensor_size * num_tensors\n\n        return total_tensors_size + list_of_pointers_size\n\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.append(torch.tensor([3.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; len(store)\n        3\n\n        \"\"\"\n        self.wait_for_writes_to_complete()\n        random.shuffle(self._data)\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n        \"\"\"Append a single item to the dataset.\n\n        Note **append is blocking**. For better performance use extend instead with batches.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        Args:\n            item: The item to append to the dataset.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        self._data.append(item.to(self._device))\n\n    def _extend(\n        self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; None:\n        \"\"\"Extend threadpool method.\n\n        To be called by :meth:`extend`.\n\n        Args:\n            batch: A batch of items to add to the dataset.\n        \"\"\"\n        try:\n            # Unstack to a list of tensors\n            items: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] = resize_to_list_vectors(batch)\n\n            self._data.extend(items)\n        except Exception as e:  # noqa: BLE001\n            self._pool_exceptions.append(e)\n\n    def extend(\n        self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; Future | None:\n        \"\"\"Extend the dataset with multiple items (non-blocking).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; batch = torch.randn(10, 100)\n            &gt;&gt;&gt; async_result = store.extend(batch)\n            &gt;&gt;&gt; len(store)\n            10\n\n        Args:\n            batch: A batch of items to add to the dataset.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        # Schedule _extend to run in a separate process\n        if self._pool:\n            future = self._pool.submit(self._extend, batch)\n            self._pool_futures.append(future)\n\n        # Fallback to synchronous execution if not multiprocessing\n        self._extend(batch)\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n            &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n            &gt;&gt;&gt; store.wait_for_writes_to_complete()\n            &gt;&gt;&gt; len(store)\n            3\n\n        Raises:\n            RuntimeError: If any exceptions occurred in the background workers.\n        \"\"\"\n        # Restart the pool\n        if self._pool:\n            for _future in as_completed(self._pool_futures):\n                pass\n            self._pool_futures.clear()\n\n        time.sleep(1)\n\n        if self._pool_exceptions:\n            exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n            msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n            raise RuntimeError(msg)\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the dataset.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; len(store)\n        0\n        \"\"\"\n        self.wait_for_writes_to_complete()\n\n        # Clearing a list like this works for both standard and multiprocessing lists\n        self._data[:] = []\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        if self._pool:\n            self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    if self._pool:\n        self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    return self._data[index]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__init__","title":"<code>__init__(data=None, device=None, max_workers=None, *, multiprocessing_enabled=False)</code>","text":"<p>Initialize the List Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[Float[Tensor, INPUT_OUTPUT_FEATURE]] | None</code> <p>Data to initialize the dataset with.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> <code>max_workers</code> <code>int | None</code> <p>Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have.</p> <code>None</code> <code>multiprocessing_enabled</code> <code>bool</code> <p>Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it).</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __init__(\n    self,\n    data: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] | None = None,\n    device: torch.device | None = None,\n    max_workers: int | None = None,\n    *,\n    multiprocessing_enabled: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the List Activation Store.\n\n    Args:\n        data: Data to initialize the dataset with.\n        device: Device to store the activation vectors on.\n        max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n            Default is the number of cores you have.\n        multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n            workers. This creates significant overhead, so you should only enable it if you have\n            multiple GPUs (and experiment with enabling/disabling it).\n    \"\"\"\n    # Default to empty\n    if data is None:\n        data = []\n\n    # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n    # between processes. Otherwise, just use a normal list.\n    if multiprocessing_enabled:\n        self._pool = ProcessPoolExecutor(max_workers=max_workers)\n        manager = Manager()\n        self._data = manager.list(data)\n        self._data.extend(data)\n        self._pool_exceptions = manager.list()\n    else:\n        self._data = data\n        self._pool_exceptions = []\n\n    self._pool_futures = []\n\n    # Device for storing the activation vectors\n    self._device = device\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the dataset in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Returns:\n        The size of the dataset in bytes.\n    \"\"\"\n    # The list of tensors is really a list of pointers to tensors, so we need to account for\n    # this as well as the size of the tensors themselves.\n    list_of_pointers_size = self._data.__sizeof__()\n\n    # Handle 0 items\n    if len(self._data) == 0:\n        return list_of_pointers_size\n\n    # Otherwise, get the size of the first tensor\n    first_tensor = self._data[0]\n    first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n    num_tensors = len(self._data)\n    total_tensors_size = first_tensor_size * num_tensors\n\n    return total_tensors_size + list_of_pointers_size\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.append","title":"<code>append(item)</code>","text":"<p>Append a single item to the dataset.</p> <p>Note append is blocking. For better performance use extend instead with batches.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The item to append to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n    \"\"\"Append a single item to the dataset.\n\n    Note **append is blocking**. For better performance use extend instead with batches.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    Args:\n        item: The item to append to the dataset.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    self._data.append(item.to(self._device))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the dataset.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>store.empty() len(store) 0</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the dataset.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; len(store)\n    0\n    \"\"\"\n    self.wait_for_writes_to_complete()\n\n    # Clearing a list like this works for both standard and multiprocessing lists\n    self._data[:] = []\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Extend the dataset with multiple items (non-blocking).</p> Example <p>import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>A batch of items to add to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def extend(\n    self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n) -&gt; Future | None:\n    \"\"\"Extend the dataset with multiple items (non-blocking).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; async_result = store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        10\n\n    Args:\n        batch: A batch of items to add to the dataset.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    # Schedule _extend to run in a separate process\n    if self._pool:\n        future = self._pool.submit(self._extend, batch)\n        self._pool_futures.append(future)\n\n    # Fallback to synchronous execution if not multiprocessing\n    self._extend(batch)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.append(torch.tensor([3.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; len(store)\n    3\n\n    \"\"\"\n    self.wait_for_writes_to_complete()\n    random.shuffle(self._data)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>Wait for any non-blocking writes (e.g. calls to :meth:<code>append</code>) to complete.</p> Example <p>import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any exceptions occurred in the background workers.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n        &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; len(store)\n        3\n\n    Raises:\n        RuntimeError: If any exceptions occurred in the background workers.\n    \"\"\"\n    # Restart the pool\n    if self._pool:\n        for _future in as_completed(self._pool_futures):\n            pass\n        self._pool_futures.clear()\n\n    time.sleep(1)\n\n    if self._pool_exceptions:\n        exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n        msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossHyperparameters","title":"<code>LossHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Loss hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass LossHyperparameters(NestedParameter):\n    \"\"\"Loss hyperparameters.\"\"\"\n\n    l1_coefficient: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"L1 Penalty Coefficient.\n\n    The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.\n    The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by\n    using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good\n    starting point for the L1 coefficient is 1e-3.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossHyperparameters.l1_coefficient","title":"<code>l1_coefficient: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>L1 Penalty Coefficient.</p> <p>The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good starting point for the L1 coefficient is 1e-3.</p>"},{"location":"reference/#sparse_autoencoder.LossReducer","title":"<code>LossReducer</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Loss reducer.</p> <p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential.</p> Example <p>from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ...     L2ReconstructionLoss(), ...     LearnedActivationsL1Loss(0.001), ... ) LossReducer(   (0): L2ReconstructionLoss()   (1): LearnedActivationsL1Loss(l1_coefficient=0.001) )</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>@final\nclass LossReducer(AbstractLoss):\n    \"\"\"Loss reducer.\n\n    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to\n    nn.Sequential.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss\n        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss\n        &gt;&gt;&gt; LossReducer(\n        ...     L2ReconstructionLoss(),\n        ...     LearnedActivationsL1Loss(0.001),\n        ... )\n        LossReducer(\n          (0): L2ReconstructionLoss()\n          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)\n        )\n\n    \"\"\"\n\n    _modules: dict[str, \"AbstractLoss\"]\n    \"\"\"Children loss modules.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"total_loss\"\n\n    def __init__(\n        self,\n        *loss_modules: AbstractLoss,\n    ):\n        \"\"\"Initialize the loss reducer.\n\n        Args:\n            *loss_modules: Loss modules to reduce.\n\n        Raises:\n            ValueError: If the loss reducer has no loss modules.\n        \"\"\"\n        super().__init__()\n\n        for idx, loss_module in enumerate(loss_modules):\n            self._modules[str(idx)] = loss_module\n\n        if len(self) == 0:\n            error_message = \"Loss reducer must have at least one loss module.\"\n            raise ValueError(error_message)\n\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Reduce loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Mean loss across the batch, summed across the loss modules.\n        \"\"\"\n        all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n            [\n                loss_module.forward(source_activations, learned_activations, decoded_activations)\n                for loss_module in self._modules.values()\n            ]\n        )\n\n        return all_modules_loss.sum(dim=0)\n\n    def __dir__(self) -&gt; list[str]:\n        \"\"\"Dir dunder method.\"\"\"\n        return list(self._modules.__dir__())\n\n    def __getitem__(self, idx: int) -&gt; AbstractLoss:\n        \"\"\"Get item dunder method.\"\"\"\n        return self._modules[str(idx)]\n\n    def __iter__(self) -&gt; Iterator[AbstractLoss]:\n        \"\"\"Iterator dunder method.\"\"\"\n        return iter(self._modules.values())\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length dunder method.\"\"\"\n        return len(self._modules)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__dir__","title":"<code>__dir__()</code>","text":"<p>Dir dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __dir__(self) -&gt; list[str]:\n    \"\"\"Dir dunder method.\"\"\"\n    return list(self._modules.__dir__())\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get item dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; AbstractLoss:\n    \"\"\"Get item dunder method.\"\"\"\n    return self._modules[str(idx)]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__init__","title":"<code>__init__(*loss_modules)</code>","text":"<p>Initialize the loss reducer.</p> <p>Parameters:</p> Name Type Description Default <code>*loss_modules</code> <code>AbstractLoss</code> <p>Loss modules to reduce.</p> <code>()</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loss reducer has no loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __init__(\n    self,\n    *loss_modules: AbstractLoss,\n):\n    \"\"\"Initialize the loss reducer.\n\n    Args:\n        *loss_modules: Loss modules to reduce.\n\n    Raises:\n        ValueError: If the loss reducer has no loss modules.\n    \"\"\"\n    super().__init__()\n\n    for idx, loss_module in enumerate(loss_modules):\n        self._modules[str(idx)] = loss_module\n\n    if len(self) == 0:\n        error_message = \"Loss reducer must have at least one loss module.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterator dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __iter__(self) -&gt; Iterator[AbstractLoss]:\n    \"\"\"Iterator dunder method.\"\"\"\n    return iter(self._modules.values())\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__len__","title":"<code>__len__()</code>","text":"<p>Length dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length dunder method.\"\"\"\n    return len(self._modules)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Reduce loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Mean loss across the batch, summed across the loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Reduce loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Mean loss across the batch, summed across the loss modules.\n    \"\"\"\n    all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n        [\n            loss_module.forward(source_activations, learned_activations, decoded_activations)\n            for loss_module in self._modules.values()\n        ]\n    )\n\n    return all_modules_loss.sum(dim=0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"total_loss\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReductionType","title":"<code>LossReductionType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Loss reduction type (across batch items).</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>class LossReductionType(LowercaseStrEnum):\n    \"\"\"Loss reduction type (across batch items).\"\"\"\n\n    MEAN = \"mean\"\n    \"\"\"Mean loss across batch items.\"\"\"\n\n    SUM = \"sum\"\n    \"\"\"Sum the loss from all batch items.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReductionType.MEAN","title":"<code>MEAN = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean loss across batch items.</p>"},{"location":"reference/#sparse_autoencoder.LossReductionType.SUM","title":"<code>SUM = 'sum'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sum the loss from all batch items.</p>"},{"location":"reference/#sparse_autoencoder.Method","title":"<code>Method</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Method.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Method(LowercaseStrEnum):\n    \"\"\"Method.\"\"\"\n\n    BAYES = auto()\n    \"\"\"Bayesian optimization.\n\n    Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach\n    for finding the optimal set of parameters.\n    \"\"\"\n\n    CUSTOM = auto()\n    \"\"\"Custom method.\n\n    Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the\n    sweep process.\n    \"\"\"\n\n    GRID = auto()\n    \"\"\"Grid search.\n\n    Utilizes a grid search approach for hyperparameter tuning, systematically working through\n    multiple combinations of parameter values.\n    \"\"\"\n\n    RANDOM = auto()\n    \"\"\"Random search.\n\n    Implements a random search strategy for hyperparameter tuning, exploring the parameter space\n    randomly.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Method.BAYES","title":"<code>BAYES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bayesian optimization.</p> <p>Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach for finding the optimal set of parameters.</p>"},{"location":"reference/#sparse_autoencoder.Method.CUSTOM","title":"<code>CUSTOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom method.</p> <p>Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the sweep process.</p>"},{"location":"reference/#sparse_autoencoder.Method.GRID","title":"<code>GRID = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Grid search.</p> <p>Utilizes a grid search approach for hyperparameter tuning, systematically working through multiple combinations of parameter values.</p>"},{"location":"reference/#sparse_autoencoder.Method.RANDOM","title":"<code>RANDOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random search.</p> <p>Implements a random search strategy for hyperparameter tuning, exploring the parameter space randomly.</p>"},{"location":"reference/#sparse_autoencoder.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>Metric to optimize.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Metric:\n    \"\"\"Metric to optimize.\"\"\"\n\n    name: str\n    \"\"\"Name of metric.\"\"\"\n\n    goal: Goal | None = Goal.MINIMIZE\n\n    impute: Impute | None = None\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\"\n\n    imputewhilerunning: ImputeWhileRunning | None = None\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    target: float | None = None\n    \"\"\"The sweep will finish once any run achieves this value.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Metric.impute","title":"<code>impute: Impute | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metric value to use in bayes search for runs that fail, crash, or are killed</p>"},{"location":"reference/#sparse_autoencoder.Metric.imputewhilerunning","title":"<code>imputewhilerunning: ImputeWhileRunning | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Appends a calculated metric even when epochs are in a running state.</p>"},{"location":"reference/#sparse_autoencoder.Metric.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of metric.</p>"},{"location":"reference/#sparse_autoencoder.Metric.target","title":"<code>target: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will finish once any run achieves this value.</p>"},{"location":"reference/#sparse_autoencoder.Metric.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Metric.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NestedParameter","title":"<code>NestedParameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Nested Parameter.</p> Example <p>from dataclasses import field @dataclass(frozen=True) ... class MyNestedParameter(NestedParameter): ...     a: int = field(default=Parameter(1)) ...     b: int = field(default=Parameter(2)) MyNestedParameter().to_dict() {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass NestedParameter(ABC):  # noqa: B024 (abstract so that we can check against it's type)\n    \"\"\"Nested Parameter.\n\n    Example:\n        &gt;&gt;&gt; from dataclasses import field\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class MyNestedParameter(NestedParameter):\n        ...     a: int = field(default=Parameter(1))\n        ...     b: int = field(default=Parameter(2))\n        &gt;&gt;&gt; MyNestedParameter().to_dict()\n        {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}\n    \"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\"\"\"\n\n        def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n            \"\"\"Return dict without None values.\n\n            Args:\n                obj: The object to convert to a dict.\n\n            Returns:\n                The dict representation of the object.\n            \"\"\"\n            dict_none_removed = {}\n            dict_with_none = dict(obj)\n            for key, value in dict_with_none.items():\n                if value is not None:\n                    dict_none_removed[key] = value\n            return dict_none_removed\n\n        return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n\n    def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n        \"\"\"Return dict representation of this object.\"\"\"\n        return self.to_dict()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NestedParameter.__dict__","title":"<code>__dict__()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n    \"\"\"Return dict representation of this object.\"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NestedParameter.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\"\"\"\n\n    def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n        \"\"\"Return dict without None values.\n\n        Args:\n            obj: The object to convert to a dict.\n\n        Returns:\n            The dict representation of the object.\n        \"\"\"\n        dict_none_removed = {}\n        dict_with_none = dict(obj)\n        for key, value in dict_with_none.items():\n            if value is not None:\n                dict_none_removed[key] = value\n        return dict_none_removed\n\n    return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters","title":"<code>OptimizerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Optimizer hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass OptimizerHyperparameters(NestedParameter):\n    \"\"\"Optimizer hyperparameters.\"\"\"\n\n    lr: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"Learning rate.\n\n    A good starting point for the learning rate is 1e-3, but this is one of the key parameters so\n    you should probably tune it.\n    \"\"\"\n\n    adam_beta_1: Parameter[float] = field(default=Parameter(0.9))\n    \"\"\"Adam Beta 1.\n\n    The exponential decay rate for the first moment estimates (mean) of the gradient.\n    \"\"\"\n\n    adam_beta_2: Parameter[float] = field(default=Parameter(0.99))\n    \"\"\"Adam Beta 2.\n\n    The exponential decay rate for the second moment estimates (variance) of the gradient.\n    \"\"\"\n\n    adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Adam Weight Decay.\n\n    Weight decay (L2 penalty).\n    \"\"\"\n\n    amsgrad: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"AMSGrad.\n\n    Whether to use the AMSGrad variant of this algorithm from the paper [On the Convergence of Adam\n    and Beyond](https://arxiv.org/abs/1904.09237).\n    \"\"\"\n\n    fused: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Fused.\n\n    Whether to use a fused implementation of the optimizer (may be faster on CUDA).\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.adam_beta_1","title":"<code>adam_beta_1: Parameter[float] = field(default=Parameter(0.9))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 1.</p> <p>The exponential decay rate for the first moment estimates (mean) of the gradient.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.adam_beta_2","title":"<code>adam_beta_2: Parameter[float] = field(default=Parameter(0.99))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 2.</p> <p>The exponential decay rate for the second moment estimates (variance) of the gradient.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.adam_weight_decay","title":"<code>adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Weight Decay.</p> <p>Weight decay (L2 penalty).</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.amsgrad","title":"<code>amsgrad: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AMSGrad.</p> <p>Whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.fused","title":"<code>fused: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fused.</p> <p>Whether to use a fused implementation of the optimizer (may be faster on CUDA).</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.lr","title":"<code>lr: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate.</p> <p>A good starting point for the learning rate is 1e-3, but this is one of the key parameters so you should probably tune it.</p>"},{"location":"reference/#sparse_autoencoder.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[ParamType]</code></p> <p>Sweep Parameter.</p> <p>https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Parameter(Generic[ParamType]):\n    \"\"\"Sweep Parameter.\n\n    https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters\n    \"\"\"\n\n    value: ParamType | None = None\n    \"\"\"Single value.\n\n    Specifies the single valid value for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    max: ParamType | None = None  # noqa: A003\n    \"\"\"Maximum value.\"\"\"\n\n    min: ParamType | None = None  # noqa: A003\n    \"\"\"Minimum value.\"\"\"\n\n    distribution: Distribution | None = None\n    \"\"\"Distribution\n\n    If not specified, will default to categorical if values is set, to int_uniform if max and min\n    are set to integers, to uniform if max and min are set to floats, or to constant if value is\n    set.\n    \"\"\"\n\n    q: float | None = None\n    \"\"\"Quantization parameter.\n\n    Quantization step size for quantized hyperparameters.\n    \"\"\"\n\n    values: list[ParamType] | None = None\n    \"\"\"Discrete values.\n\n    Specifies all valid values for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    probabilities: list[float] | None = None\n    \"\"\"Probability of each value\"\"\"\n\n    mu: float | None = None\n    \"\"\"Mean for normal or lognormal distributions\"\"\"\n\n    sigma: float | None = None\n    \"\"\"Std Dev for normal or lognormal distributions\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Parameter.distribution","title":"<code>distribution: Distribution | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Distribution</p> <p>If not specified, will default to categorical if values is set, to int_uniform if max and min are set to integers, to uniform if max and min are set to floats, or to constant if value is set.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.max","title":"<code>max: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum value.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.min","title":"<code>min: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.mu","title":"<code>mu: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean for normal or lognormal distributions</p>"},{"location":"reference/#sparse_autoencoder.Parameter.probabilities","title":"<code>probabilities: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Probability of each value</p>"},{"location":"reference/#sparse_autoencoder.Parameter.q","title":"<code>q: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantization parameter.</p> <p>Quantization step size for quantized hyperparameters.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.sigma","title":"<code>sigma: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Std Dev for normal or lognormal distributions</p>"},{"location":"reference/#sparse_autoencoder.Parameter.value","title":"<code>value: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single value.</p> <p>Specifies the single valid value for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.values","title":"<code>values: list[ParamType] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Discrete values.</p> <p>Specifies all valid values for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Parameter.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p> <p>Includes all the key functionality to train a sparse autoencoder, with a specific set of     hyperparameters.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations.\n\n    Includes all the key functionality to train a sparse autoencoder, with a specific set of\n        hyperparameters.\n    \"\"\"\n\n    activation_resampler: AbstractActivationResampler | None\n    \"\"\"Activation resampler to use.\"\"\"\n\n    autoencoder: SparseAutoencoder\n    \"\"\"Sparse autoencoder to train.\"\"\"\n\n    cache_name: str\n    \"\"\"Name of the cache to use in the source model (hook point).\"\"\"\n\n    layer: int\n    \"\"\"Layer to get activations from with the source model.\"\"\"\n\n    log_frequency: int\n    \"\"\"Frequency at which to log metrics (in steps).\"\"\"\n\n    loss: AbstractLoss\n    \"\"\"Loss function to use.\"\"\"\n\n    metrics: MetricsContainer\n    \"\"\"Metrics to use.\"\"\"\n\n    optimizer: AbstractOptimizerWithReset\n    \"\"\"Optimizer to use.\"\"\"\n\n    progress_bar: tqdm | None\n    \"\"\"Progress bar for the pipeline.\"\"\"\n\n    source_data: Iterable[TorchTokenizedPrompts]\n    \"\"\"Iterable over the source data.\"\"\"\n\n    source_dataset: SourceDataset\n    \"\"\"Source dataset to generate activation data from (tokenized prompts).\"\"\"\n\n    source_model: HookedTransformer\n    \"\"\"Source model to get activations from.\"\"\"\n\n    total_activations_trained_on: int = 0\n    \"\"\"Total number of activations trained on state.\"\"\"\n\n    @final\n    def __init__(\n        self,\n        activation_resampler: AbstractActivationResampler | None,\n        autoencoder: SparseAutoencoder,\n        cache_name: str,\n        layer: int,\n        loss: AbstractLoss,\n        optimizer: AbstractOptimizerWithReset,\n        source_dataset: SourceDataset,\n        source_model: HookedTransformer,\n        run_name: str = \"sparse_autoencoder\",\n        checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n        log_frequency: int = 100,\n        metrics: MetricsContainer = default_metrics,\n        source_data_batch_size: int = 12,\n    ) -&gt; None:\n        \"\"\"Initialize the pipeline.\n\n        Args:\n            activation_resampler: Activation resampler to use.\n            autoencoder: Sparse autoencoder to train.\n            cache_name: Name of the cache to use in the source model (hook point).\n            layer: Layer to get activations from with the source model.\n            loss: Loss function to use.\n            optimizer: Optimizer to use.\n            source_dataset: Source dataset to get data from.\n            source_model: Source model to get activations from.\n            run_name: Name of the run for saving checkpoints.\n            checkpoint_directory: Directory to save checkpoints to.\n            log_frequency: Frequency at which to log metrics (in steps)\n            metrics: Metrics to use.\n            source_data_batch_size: Batch size for the source data.\n        \"\"\"\n        self.activation_resampler = activation_resampler\n        self.autoencoder = autoencoder\n        self.cache_name = cache_name\n        self.checkpoint_directory = checkpoint_directory\n        self.layer = layer\n        self.log_frequency = log_frequency\n        self.loss = loss\n        self.metrics = metrics\n        self.optimizer = optimizer\n        self.run_name = run_name\n        self.source_data_batch_size = source_data_batch_size\n        self.source_dataset = source_dataset\n        self.source_model = source_model\n\n        source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n        self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n\n    def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n        \"\"\"Generate activations.\n\n        Args:\n            store_size: Number of activations to generate.\n\n        Returns:\n            Activation store for the train section.\n\n        Raises:\n            ValueError: If the store size is not positive or is not divisible by the batch size.\n        \"\"\"\n        # Check the store size is positive and divisible by the batch size\n        if store_size &lt;= 0:\n            error_message = f\"Store size must be positive, got {store_size}\"\n            raise ValueError(error_message)\n        if store_size % self.source_data_batch_size != 0:\n            error_message = (\n                f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n                f\"got {store_size}\"\n            )\n            raise ValueError(error_message)\n\n        # Setup the store\n        num_neurons: int = self.autoencoder.n_input_features\n        source_model_device: torch.device = get_model_device(self.source_model)\n        store = TensorActivationStore(store_size, num_neurons)\n\n        # Add the hook to the model (will automatically store the activations every time the model\n        # runs)\n        self.source_model.remove_all_hook_fns()\n        hook = partial(store_activations_hook, store=store)\n        self.source_model.add_hook(self.cache_name, hook)\n\n        # Loop through the dataloader until the store reaches the desired size\n        with torch.no_grad():\n            for batch in self.source_data:\n                input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                    \"input_ids\"\n                ].to(source_model_device)\n                self.source_model.forward(\n                    input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n                )  # type: ignore (TLens is typed incorrectly)\n\n                if len(store) &gt;= store_size:\n                    break\n\n        self.source_model.remove_all_hook_fns()\n        store.shuffle()\n\n        return store\n\n    def train_autoencoder(\n        self, activation_store: TensorActivationStore, train_batch_size: int\n    ) -&gt; Int64[Tensor, Axis.LEARNT_FEATURE]:\n        \"\"\"Train the sparse autoencoder.\n\n        Args:\n            activation_store: Activation store from the generate section.\n            train_batch_size: Train batch size.\n\n        Returns:\n            Number of times each neuron fired.\n        \"\"\"\n        autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n        activations_dataloader = DataLoader(\n            activation_store,\n            batch_size=train_batch_size,\n        )\n\n        learned_activations_fired_count: Int64[Tensor, Axis.LEARNT_FEATURE] = torch.zeros(\n            self.autoencoder.n_learned_features, dtype=torch.int64, device=autoencoder_device\n        )\n\n        for store_batch in activations_dataloader:\n            # Zero the gradients\n            self.optimizer.zero_grad()\n\n            # Move the batch to the device (in place)\n            batch = store_batch.detach().to(autoencoder_device)\n\n            # Forward pass\n            learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n            # Get loss &amp; metrics\n            metrics = {}\n            total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n                batch, learned_activations, reconstructed_activations\n            )\n            metrics.update(loss_metrics)\n\n            with torch.no_grad():\n                for metric in self.metrics.train_metrics:\n                    calculated = metric.calculate(\n                        TrainMetricData(batch, learned_activations, reconstructed_activations)\n                    )\n                    metrics.update(calculated)\n\n            # Store count of how many neurons have fired\n            with torch.no_grad():\n                fired = learned_activations &gt; 0\n                learned_activations_fired_count.add_(fired.sum(dim=0))\n\n            # Backwards pass\n            total_loss.backward()\n            self.optimizer.step()\n            self.autoencoder.decoder.constrain_weights_unit_norm()\n\n            # Log training metrics\n            self.total_activations_trained_on += train_batch_size\n            if (\n                wandb.run is not None\n                and int(self.total_activations_trained_on / train_batch_size) % self.log_frequency\n                == 0\n            ):\n                wandb.log(\n                    data={**metrics, **loss_metrics},\n                    step=self.total_activations_trained_on,\n                    commit=True,\n                )\n\n        return learned_activations_fired_count\n\n    def update_parameters(self, parameter_updates: ParameterUpdateResults) -&gt; None:\n        \"\"\"Update the parameters of the model from the results of the resampler.\n\n        Args:\n            parameter_updates: Parameter updates from the resampler.\n        \"\"\"\n        # Update the weights and biases\n        self.autoencoder.encoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_weight_updates,\n        )\n        self.autoencoder.encoder.update_bias(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_bias_updates,\n        )\n        self.autoencoder.decoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_decoder_weight_updates,\n        )\n\n        # Reset the optimizer\n        for parameter, axis in self.autoencoder.reset_optimizer_parameter_details:\n            self.optimizer.reset_neurons_state(\n                parameter=parameter,\n                neuron_indices=parameter_updates.dead_neuron_indices,\n                axis=axis,\n            )\n\n    def validate_sae(self, validation_number_activations: int) -&gt; None:\n        \"\"\"Get validation metrics.\n\n        Args:\n            validation_number_activations: Number of activations to use for validation.\n        \"\"\"\n        losses: list[float] = []\n        losses_with_reconstruction: list[float] = []\n        losses_with_zero_ablation: list[float] = []\n        source_model_device: torch.device = get_model_device(self.source_model)\n\n        for batch in self.source_data:\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n\n            # Run a forward pass with and without the replaced activations\n            self.source_model.remove_all_hook_fns()\n            replacement_hook = partial(\n                replace_activations_hook, sparse_autoencoder=self.autoencoder\n            )\n\n            loss = self.source_model.forward(input_ids, return_type=\"loss\")\n            loss_with_reconstruction = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[\n                    (\n                        self.cache_name,\n                        replacement_hook,\n                    )\n                ],\n            )\n            loss_with_zero_ablation = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n            )\n\n            losses.append(loss.sum().item())\n            losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n            losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n            if len(losses) &gt;= validation_number_activations // input_ids.numel():\n                break\n\n        # Log\n        validation_data = ValidationMetricData(\n            source_model_loss=torch.tensor(losses),\n            source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n            source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n        )\n        for metric in self.metrics.validation_metrics:\n            calculated = metric.calculate(validation_data)\n            if wandb.run is not None:\n                wandb.log(data=calculated, commit=False)\n\n    @final\n    def save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n        \"\"\"Save the model as a checkpoint.\n\n        Args:\n            is_final: Whether this is the final checkpoint.\n\n        Returns:\n            Path to the saved checkpoint.\n        \"\"\"\n        # Create the name\n        name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n        safe_name = quote_plus(name, safe=\"_\")\n\n        # Save locally\n        self.checkpoint_directory.mkdir(parents=True, exist_ok=True)\n        file_path: Path = self.checkpoint_directory / f\"{safe_name}.pt\"\n        torch.save(\n            self.autoencoder.state_dict(),\n            file_path,\n        )\n\n        # Upload to wandb\n        if wandb.run is not None:\n            artifact = wandb.Artifact(safe_name, type=\"model\")\n            artifact.add_file(str(file_path))\n            wandb.log_artifact(artifact)\n\n        return file_path\n\n    def run_pipeline(\n        self,\n        train_batch_size: int,\n        max_store_size: int,\n        max_activations: int,\n        validation_number_activations: int = 1024,\n        validate_frequency: int | None = None,\n        checkpoint_frequency: int | None = None,\n    ) -&gt; None:\n        \"\"\"Run the full training pipeline.\n\n        Args:\n            train_batch_size: Train batch size.\n            max_store_size: Maximum size of the activation store.\n            max_activations: Maximum total number of activations to train on (the original paper\n                used 8bn, although others have had success with 100m+).\n            validation_number_activations: Number of activations to use for validation.\n            validate_frequency: Frequency at which to get validation metrics.\n            checkpoint_frequency: Frequency at which to save a checkpoint.\n        \"\"\"\n        last_validated: int = 0\n        last_checkpoint: int = 0\n\n        self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n        # Get the store size\n        store_size: int = max_store_size - max_store_size % (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n\n        with tqdm(\n            desc=\"Activations trained on\",\n            total=max_activations,\n        ) as progress_bar:\n            for _ in range(0, max_activations, store_size):\n                # Generate\n                progress_bar.set_postfix({\"stage\": \"generate\"})\n                activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n                # Update the counters\n                num_activation_vectors_in_store = len(activation_store)\n                last_validated += num_activation_vectors_in_store\n                last_checkpoint += num_activation_vectors_in_store\n\n                # Train\n                progress_bar.set_postfix({\"stage\": \"train\"})\n                batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE] = self.train_autoencoder(\n                    activation_store, train_batch_size=train_batch_size\n                )\n\n                # Resample dead neurons (if needed)\n                progress_bar.set_postfix({\"stage\": \"resample\"})\n                if self.activation_resampler is not None:\n                    # Get the updates\n                    parameter_updates = self.activation_resampler.step_resampler(\n                        batch_neuron_activity=batch_neuron_activity,\n                        activation_store=activation_store,\n                        autoencoder=self.autoencoder,\n                        loss_fn=self.loss,\n                        train_batch_size=train_batch_size,\n                    )\n\n                    if parameter_updates is not None:\n                        if wandb.run is not None:\n                            wandb.log(\n                                {\n                                    \"resample/dead_neurons\": len(\n                                        parameter_updates.dead_neuron_indices\n                                    )\n                                },\n                                commit=False,\n                            )\n\n                        # Update the parameters\n                        self.update_parameters(parameter_updates)\n\n                # Get validation metrics (if needed)\n                progress_bar.set_postfix({\"stage\": \"validate\"})\n                if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                    self.validate_sae(validation_number_activations)\n                    last_validated = 0\n\n                # Checkpoint (if needed)\n                progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n                if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                    last_checkpoint = 0\n                    self.save_checkpoint()\n\n                # Update the progress bar\n                progress_bar.update(store_size)\n\n        # Save the final checkpoint\n        self.save_checkpoint(is_final=True)\n\n    @staticmethod\n    def stateful_dataloader_iterable(\n        dataloader: DataLoader[TorchTokenizedPrompts],\n    ) -&gt; Iterable[TorchTokenizedPrompts]:\n        \"\"\"Create a stateful dataloader iterable.\n\n        Create an iterable that maintains it's position in the dataloader between loops.\n\n        Examples:\n            Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n            (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n            position from where the first loop left off.\n\n            &gt;&gt;&gt; from datasets import Dataset\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; def gen():\n            ...     yield {\"int\": 0}\n            ...     yield {\"int\": 1}\n            &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n            &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n            (tensor([0]), tensor([0]))\n\n            By contrast if you create a stateful iterable from the dataloader, each loop will get\n            different data.\n\n            &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n            &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n            (tensor([0]), tensor([1]))\n\n        Args:\n            dataloader: PyTorch DataLoader.\n\n        Returns:\n            Stateful iterable over the data in the dataloader.\n\n        Yields:\n            Data from the dataloader.\n        \"\"\"\n        yield from dataloader\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.activation_resampler","title":"<code>activation_resampler: AbstractActivationResampler | None = activation_resampler</code>  <code>instance-attribute</code>","text":"<p>Activation resampler to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.autoencoder","title":"<code>autoencoder: SparseAutoencoder = autoencoder</code>  <code>instance-attribute</code>","text":"<p>Sparse autoencoder to train.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.cache_name","title":"<code>cache_name: str = cache_name</code>  <code>instance-attribute</code>","text":"<p>Name of the cache to use in the source model (hook point).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.layer","title":"<code>layer: int = layer</code>  <code>instance-attribute</code>","text":"<p>Layer to get activations from with the source model.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.log_frequency","title":"<code>log_frequency: int = log_frequency</code>  <code>instance-attribute</code>","text":"<p>Frequency at which to log metrics (in steps).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.loss","title":"<code>loss: AbstractLoss = loss</code>  <code>instance-attribute</code>","text":"<p>Loss function to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.metrics","title":"<code>metrics: MetricsContainer = metrics</code>  <code>instance-attribute</code>","text":"<p>Metrics to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.optimizer","title":"<code>optimizer: AbstractOptimizerWithReset = optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.progress_bar","title":"<code>progress_bar: tqdm | None</code>  <code>instance-attribute</code>","text":"<p>Progress bar for the pipeline.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_data","title":"<code>source_data: Iterable[TorchTokenizedPrompts] = self.stateful_dataloader_iterable(source_dataloader)</code>  <code>instance-attribute</code>","text":"<p>Iterable over the source data.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_dataset","title":"<code>source_dataset: SourceDataset = source_dataset</code>  <code>instance-attribute</code>","text":"<p>Source dataset to generate activation data from (tokenized prompts).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_model","title":"<code>source_model: HookedTransformer = source_model</code>  <code>instance-attribute</code>","text":"<p>Source model to get activations from.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.total_activations_trained_on","title":"<code>total_activations_trained_on: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of activations trained on state.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.__init__","title":"<code>__init__(activation_resampler, autoencoder, cache_name, layer, loss, optimizer, source_dataset, source_model, run_name='sparse_autoencoder', checkpoint_directory=DEFAULT_CHECKPOINT_DIRECTORY, log_frequency=100, metrics=default_metrics, source_data_batch_size=12)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>activation_resampler</code> <code>AbstractActivationResampler | None</code> <p>Activation resampler to use.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder to train.</p> required <code>cache_name</code> <code>str</code> <p>Name of the cache to use in the source model (hook point).</p> required <code>layer</code> <code>int</code> <p>Layer to get activations from with the source model.</p> required <code>loss</code> <code>AbstractLoss</code> <p>Loss function to use.</p> required <code>optimizer</code> <code>AbstractOptimizerWithReset</code> <p>Optimizer to use.</p> required <code>source_dataset</code> <code>SourceDataset</code> <p>Source dataset to get data from.</p> required <code>source_model</code> <code>HookedTransformer</code> <p>Source model to get activations from.</p> required <code>run_name</code> <code>str</code> <p>Name of the run for saving checkpoints.</p> <code>'sparse_autoencoder'</code> <code>checkpoint_directory</code> <code>Path</code> <p>Directory to save checkpoints to.</p> <code>DEFAULT_CHECKPOINT_DIRECTORY</code> <code>log_frequency</code> <code>int</code> <p>Frequency at which to log metrics (in steps)</p> <code>100</code> <code>metrics</code> <code>MetricsContainer</code> <p>Metrics to use.</p> <code>default_metrics</code> <code>source_data_batch_size</code> <code>int</code> <p>Batch size for the source data.</p> <code>12</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef __init__(\n    self,\n    activation_resampler: AbstractActivationResampler | None,\n    autoencoder: SparseAutoencoder,\n    cache_name: str,\n    layer: int,\n    loss: AbstractLoss,\n    optimizer: AbstractOptimizerWithReset,\n    source_dataset: SourceDataset,\n    source_model: HookedTransformer,\n    run_name: str = \"sparse_autoencoder\",\n    checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n    log_frequency: int = 100,\n    metrics: MetricsContainer = default_metrics,\n    source_data_batch_size: int = 12,\n) -&gt; None:\n    \"\"\"Initialize the pipeline.\n\n    Args:\n        activation_resampler: Activation resampler to use.\n        autoencoder: Sparse autoencoder to train.\n        cache_name: Name of the cache to use in the source model (hook point).\n        layer: Layer to get activations from with the source model.\n        loss: Loss function to use.\n        optimizer: Optimizer to use.\n        source_dataset: Source dataset to get data from.\n        source_model: Source model to get activations from.\n        run_name: Name of the run for saving checkpoints.\n        checkpoint_directory: Directory to save checkpoints to.\n        log_frequency: Frequency at which to log metrics (in steps)\n        metrics: Metrics to use.\n        source_data_batch_size: Batch size for the source data.\n    \"\"\"\n    self.activation_resampler = activation_resampler\n    self.autoencoder = autoencoder\n    self.cache_name = cache_name\n    self.checkpoint_directory = checkpoint_directory\n    self.layer = layer\n    self.log_frequency = log_frequency\n    self.loss = loss\n    self.metrics = metrics\n    self.optimizer = optimizer\n    self.run_name = run_name\n    self.source_data_batch_size = source_data_batch_size\n    self.source_dataset = source_dataset\n    self.source_model = source_model\n\n    source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n    self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.generate_activations","title":"<code>generate_activations(store_size)</code>","text":"<p>Generate activations.</p> <p>Parameters:</p> Name Type Description Default <code>store_size</code> <code>int</code> <p>Number of activations to generate.</p> required <p>Returns:</p> Type Description <code>TensorActivationStore</code> <p>Activation store for the train section.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store size is not positive or is not divisible by the batch size.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n    \"\"\"Generate activations.\n\n    Args:\n        store_size: Number of activations to generate.\n\n    Returns:\n        Activation store for the train section.\n\n    Raises:\n        ValueError: If the store size is not positive or is not divisible by the batch size.\n    \"\"\"\n    # Check the store size is positive and divisible by the batch size\n    if store_size &lt;= 0:\n        error_message = f\"Store size must be positive, got {store_size}\"\n        raise ValueError(error_message)\n    if store_size % self.source_data_batch_size != 0:\n        error_message = (\n            f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n            f\"got {store_size}\"\n        )\n        raise ValueError(error_message)\n\n    # Setup the store\n    num_neurons: int = self.autoencoder.n_input_features\n    source_model_device: torch.device = get_model_device(self.source_model)\n    store = TensorActivationStore(store_size, num_neurons)\n\n    # Add the hook to the model (will automatically store the activations every time the model\n    # runs)\n    self.source_model.remove_all_hook_fns()\n    hook = partial(store_activations_hook, store=store)\n    self.source_model.add_hook(self.cache_name, hook)\n\n    # Loop through the dataloader until the store reaches the desired size\n    with torch.no_grad():\n        for batch in self.source_data:\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n            self.source_model.forward(\n                input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n            )  # type: ignore (TLens is typed incorrectly)\n\n            if len(store) &gt;= store_size:\n                break\n\n    self.source_model.remove_all_hook_fns()\n    store.shuffle()\n\n    return store\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.run_pipeline","title":"<code>run_pipeline(train_batch_size, max_store_size, max_activations, validation_number_activations=1024, validate_frequency=None, checkpoint_frequency=None)</code>","text":"<p>Run the full training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <code>max_store_size</code> <code>int</code> <p>Maximum size of the activation store.</p> required <code>max_activations</code> <code>int</code> <p>Maximum total number of activations to train on (the original paper used 8bn, although others have had success with 100m+).</p> required <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> <code>1024</code> <code>validate_frequency</code> <code>int | None</code> <p>Frequency at which to get validation metrics.</p> <code>None</code> <code>checkpoint_frequency</code> <code>int | None</code> <p>Frequency at which to save a checkpoint.</p> <code>None</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def run_pipeline(\n    self,\n    train_batch_size: int,\n    max_store_size: int,\n    max_activations: int,\n    validation_number_activations: int = 1024,\n    validate_frequency: int | None = None,\n    checkpoint_frequency: int | None = None,\n) -&gt; None:\n    \"\"\"Run the full training pipeline.\n\n    Args:\n        train_batch_size: Train batch size.\n        max_store_size: Maximum size of the activation store.\n        max_activations: Maximum total number of activations to train on (the original paper\n            used 8bn, although others have had success with 100m+).\n        validation_number_activations: Number of activations to use for validation.\n        validate_frequency: Frequency at which to get validation metrics.\n        checkpoint_frequency: Frequency at which to save a checkpoint.\n    \"\"\"\n    last_validated: int = 0\n    last_checkpoint: int = 0\n\n    self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n    # Get the store size\n    store_size: int = max_store_size - max_store_size % (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n\n    with tqdm(\n        desc=\"Activations trained on\",\n        total=max_activations,\n    ) as progress_bar:\n        for _ in range(0, max_activations, store_size):\n            # Generate\n            progress_bar.set_postfix({\"stage\": \"generate\"})\n            activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n            # Update the counters\n            num_activation_vectors_in_store = len(activation_store)\n            last_validated += num_activation_vectors_in_store\n            last_checkpoint += num_activation_vectors_in_store\n\n            # Train\n            progress_bar.set_postfix({\"stage\": \"train\"})\n            batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE] = self.train_autoencoder(\n                activation_store, train_batch_size=train_batch_size\n            )\n\n            # Resample dead neurons (if needed)\n            progress_bar.set_postfix({\"stage\": \"resample\"})\n            if self.activation_resampler is not None:\n                # Get the updates\n                parameter_updates = self.activation_resampler.step_resampler(\n                    batch_neuron_activity=batch_neuron_activity,\n                    activation_store=activation_store,\n                    autoencoder=self.autoencoder,\n                    loss_fn=self.loss,\n                    train_batch_size=train_batch_size,\n                )\n\n                if parameter_updates is not None:\n                    if wandb.run is not None:\n                        wandb.log(\n                            {\n                                \"resample/dead_neurons\": len(\n                                    parameter_updates.dead_neuron_indices\n                                )\n                            },\n                            commit=False,\n                        )\n\n                    # Update the parameters\n                    self.update_parameters(parameter_updates)\n\n            # Get validation metrics (if needed)\n            progress_bar.set_postfix({\"stage\": \"validate\"})\n            if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                self.validate_sae(validation_number_activations)\n                last_validated = 0\n\n            # Checkpoint (if needed)\n            progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n            if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                last_checkpoint = 0\n                self.save_checkpoint()\n\n            # Update the progress bar\n            progress_bar.update(store_size)\n\n    # Save the final checkpoint\n    self.save_checkpoint(is_final=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.save_checkpoint","title":"<code>save_checkpoint(*, is_final=False)</code>","text":"<p>Save the model as a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>is_final</code> <code>bool</code> <p>Whether this is the final checkpoint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved checkpoint.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n    \"\"\"Save the model as a checkpoint.\n\n    Args:\n        is_final: Whether this is the final checkpoint.\n\n    Returns:\n        Path to the saved checkpoint.\n    \"\"\"\n    # Create the name\n    name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n    safe_name = quote_plus(name, safe=\"_\")\n\n    # Save locally\n    self.checkpoint_directory.mkdir(parents=True, exist_ok=True)\n    file_path: Path = self.checkpoint_directory / f\"{safe_name}.pt\"\n    torch.save(\n        self.autoencoder.state_dict(),\n        file_path,\n    )\n\n    # Upload to wandb\n    if wandb.run is not None:\n        artifact = wandb.Artifact(safe_name, type=\"model\")\n        artifact.add_file(str(file_path))\n        wandb.log_artifact(artifact)\n\n    return file_path\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.stateful_dataloader_iterable","title":"<code>stateful_dataloader_iterable(dataloader)</code>  <code>staticmethod</code>","text":"<p>Create a stateful dataloader iterable.</p> <p>Create an iterable that maintains it's position in the dataloader between loops.</p> <p>Examples:</p> <p>Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off.</p> <pre><code>&gt;&gt;&gt; from datasets import Dataset\n&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; def gen():\n...     yield {\"int\": 0}\n...     yield {\"int\": 1}\n&gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n&gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n(tensor([0]), tensor([0]))\n</code></pre> <p>By contrast if you create a stateful iterable from the dataloader, each loop will get different data.</p> <pre><code>&gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n&gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n(tensor([0]), tensor([1]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> required <p>Returns:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Stateful iterable over the data in the dataloader.</p> <p>Yields:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Data from the dataloader.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@staticmethod\ndef stateful_dataloader_iterable(\n    dataloader: DataLoader[TorchTokenizedPrompts],\n) -&gt; Iterable[TorchTokenizedPrompts]:\n    \"\"\"Create a stateful dataloader iterable.\n\n    Create an iterable that maintains it's position in the dataloader between loops.\n\n    Examples:\n        Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n        (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n        position from where the first loop left off.\n\n        &gt;&gt;&gt; from datasets import Dataset\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; def gen():\n        ...     yield {\"int\": 0}\n        ...     yield {\"int\": 1}\n        &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n        &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n        (tensor([0]), tensor([0]))\n\n        By contrast if you create a stateful iterable from the dataloader, each loop will get\n        different data.\n\n        &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n        &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n        (tensor([0]), tensor([1]))\n\n    Args:\n        dataloader: PyTorch DataLoader.\n\n    Returns:\n        Stateful iterable over the data in the dataloader.\n\n    Yields:\n        Data from the dataloader.\n    \"\"\"\n    yield from dataloader\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.train_autoencoder","title":"<code>train_autoencoder(activation_store, train_batch_size)</code>","text":"<p>Train the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store from the generate section.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <p>Returns:</p> Type Description <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Number of times each neuron fired.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def train_autoencoder(\n    self, activation_store: TensorActivationStore, train_batch_size: int\n) -&gt; Int64[Tensor, Axis.LEARNT_FEATURE]:\n    \"\"\"Train the sparse autoencoder.\n\n    Args:\n        activation_store: Activation store from the generate section.\n        train_batch_size: Train batch size.\n\n    Returns:\n        Number of times each neuron fired.\n    \"\"\"\n    autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n    activations_dataloader = DataLoader(\n        activation_store,\n        batch_size=train_batch_size,\n    )\n\n    learned_activations_fired_count: Int64[Tensor, Axis.LEARNT_FEATURE] = torch.zeros(\n        self.autoencoder.n_learned_features, dtype=torch.int64, device=autoencoder_device\n    )\n\n    for store_batch in activations_dataloader:\n        # Zero the gradients\n        self.optimizer.zero_grad()\n\n        # Move the batch to the device (in place)\n        batch = store_batch.detach().to(autoencoder_device)\n\n        # Forward pass\n        learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n        # Get loss &amp; metrics\n        metrics = {}\n        total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n            batch, learned_activations, reconstructed_activations\n        )\n        metrics.update(loss_metrics)\n\n        with torch.no_grad():\n            for metric in self.metrics.train_metrics:\n                calculated = metric.calculate(\n                    TrainMetricData(batch, learned_activations, reconstructed_activations)\n                )\n                metrics.update(calculated)\n\n        # Store count of how many neurons have fired\n        with torch.no_grad():\n            fired = learned_activations &gt; 0\n            learned_activations_fired_count.add_(fired.sum(dim=0))\n\n        # Backwards pass\n        total_loss.backward()\n        self.optimizer.step()\n        self.autoencoder.decoder.constrain_weights_unit_norm()\n\n        # Log training metrics\n        self.total_activations_trained_on += train_batch_size\n        if (\n            wandb.run is not None\n            and int(self.total_activations_trained_on / train_batch_size) % self.log_frequency\n            == 0\n        ):\n            wandb.log(\n                data={**metrics, **loss_metrics},\n                step=self.total_activations_trained_on,\n                commit=True,\n            )\n\n    return learned_activations_fired_count\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.update_parameters","title":"<code>update_parameters(parameter_updates)</code>","text":"<p>Update the parameters of the model from the results of the resampler.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_updates</code> <code>ParameterUpdateResults</code> <p>Parameter updates from the resampler.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def update_parameters(self, parameter_updates: ParameterUpdateResults) -&gt; None:\n    \"\"\"Update the parameters of the model from the results of the resampler.\n\n    Args:\n        parameter_updates: Parameter updates from the resampler.\n    \"\"\"\n    # Update the weights and biases\n    self.autoencoder.encoder.update_dictionary_vectors(\n        parameter_updates.dead_neuron_indices,\n        parameter_updates.dead_encoder_weight_updates,\n    )\n    self.autoencoder.encoder.update_bias(\n        parameter_updates.dead_neuron_indices,\n        parameter_updates.dead_encoder_bias_updates,\n    )\n    self.autoencoder.decoder.update_dictionary_vectors(\n        parameter_updates.dead_neuron_indices,\n        parameter_updates.dead_decoder_weight_updates,\n    )\n\n    # Reset the optimizer\n    for parameter, axis in self.autoencoder.reset_optimizer_parameter_details:\n        self.optimizer.reset_neurons_state(\n            parameter=parameter,\n            neuron_indices=parameter_updates.dead_neuron_indices,\n            axis=axis,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.validate_sae","title":"<code>validate_sae(validation_number_activations)</code>","text":"<p>Get validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def validate_sae(self, validation_number_activations: int) -&gt; None:\n    \"\"\"Get validation metrics.\n\n    Args:\n        validation_number_activations: Number of activations to use for validation.\n    \"\"\"\n    losses: list[float] = []\n    losses_with_reconstruction: list[float] = []\n    losses_with_zero_ablation: list[float] = []\n    source_model_device: torch.device = get_model_device(self.source_model)\n\n    for batch in self.source_data:\n        input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n            \"input_ids\"\n        ].to(source_model_device)\n\n        # Run a forward pass with and without the replaced activations\n        self.source_model.remove_all_hook_fns()\n        replacement_hook = partial(\n            replace_activations_hook, sparse_autoencoder=self.autoencoder\n        )\n\n        loss = self.source_model.forward(input_ids, return_type=\"loss\")\n        loss_with_reconstruction = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[\n                (\n                    self.cache_name,\n                    replacement_hook,\n                )\n            ],\n        )\n        loss_with_zero_ablation = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n        )\n\n        losses.append(loss.sum().item())\n        losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n        losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n        if len(losses) &gt;= validation_number_activations // input_ids.numel():\n            break\n\n    # Log\n    validation_data = ValidationMetricData(\n        source_model_loss=torch.tensor(losses),\n        source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n        source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n    )\n    for metric in self.metrics.validation_metrics:\n        calculated = metric.calculate(validation_data)\n        if wandb.run is not None:\n            wandb.log(data=calculated, commit=False)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters","title":"<code>PipelineHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Pipeline hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass PipelineHyperparameters(NestedParameter):\n    \"\"\"Pipeline hyperparameters.\"\"\"\n\n    log_frequency: Parameter[int] = field(default=Parameter(100))\n    \"\"\"Training log frequency.\"\"\"\n\n    source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))\n    \"\"\"Source data batch size.\"\"\"\n\n    train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))\n    \"\"\"Train batch size.\"\"\"\n\n    max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))\n    \"\"\"Max store size.\"\"\"\n\n    max_activations: Parameter[int] = field(\n        default=Parameter(round_to_multiple(2e9, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Max activations.\"\"\"\n\n    checkpoint_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(5e7, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Checkpoint frequency.\"\"\"\n\n    validation_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(1e8, DEFAULT_BATCH_SIZE))\n    )\n    \"\"\"Validation frequency.\"\"\"\n\n    validation_number_activations: Parameter[int] = field(\n        # Default to a single batch of source data prompts\n        default=Parameter(DEFAULT_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 16)\n    )\n    \"\"\"Number of activations to use for validation.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.checkpoint_frequency","title":"<code>checkpoint_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(50000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoint frequency.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.log_frequency","title":"<code>log_frequency: Parameter[int] = field(default=Parameter(100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training log frequency.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.max_activations","title":"<code>max_activations: Parameter[int] = field(default=Parameter(round_to_multiple(2000000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max activations.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.max_store_size","title":"<code>max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max store size.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.source_data_batch_size","title":"<code>source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source data batch size.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.train_batch_size","title":"<code>train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Train batch size.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.validation_frequency","title":"<code>validation_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(100000000.0, DEFAULT_BATCH_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation frequency.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.validation_number_activations","title":"<code>validation_number_activations: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 16))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of activations to use for validation.</p>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset","title":"<code>PreTokenizedDataset</code>","text":"<p>             Bases: <code>SourceDataset[PreTokenizedDataBatch]</code></p> <p>General Pre-Tokenized Dataset from Hugging Face.</p> <p>Can be used for various datasets available on Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@final\nclass PreTokenizedDataset(SourceDataset[PreTokenizedDataBatch]):\n    \"\"\"General Pre-Tokenized Dataset from Hugging Face.\n\n    Can be used for various datasets available on Hugging Face.\n    \"\"\"\n\n    def preprocess(\n        self,\n        source_batch: PreTokenizedDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        The method splits each pre-tokenized item based on the context size.\n\n        Args:\n            source_batch: A batch of source data.\n            context_size: The context size to use for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n        # Chunk each tokenized prompt into blocks of context_size,\n        # discarding the last block if too small.\n        context_size_prompts = []\n        for encoding in tokenized_prompts:\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        dataset_path: str,\n        context_size: int = 256,\n        buffer_size: int = 1000,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        preprocess_batch_size: int = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face (e.g.\n                `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n            context_size: The context size for tokenized prompts.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g. `train`).\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset.__init__","title":"<code>__init__(dataset_path, context_size=256, buffer_size=1000, dataset_dir=None, dataset_files=None, dataset_split='train', preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a pre-tokenized dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face (e.g. `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</p> required <code>context_size</code> <code>int</code> <p>The context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> <code>preprocess_batch_size</code> <code>int</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_path: str,\n    context_size: int = 256,\n    buffer_size: int = 1000,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    preprocess_batch_size: int = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face (e.g.\n            `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n        context_size: The context size for tokenized prompts.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g. `train`).\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>The method splits each pre-tokenized item based on the context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>PreTokenizedDataBatch</code> <p>A batch of source data.</p> required <code>context_size</code> <code>int</code> <p>The context size to use for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: PreTokenizedDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    The method splits each pre-tokenized item based on the context size.\n\n    Args:\n        source_batch: A batch of source data.\n        context_size: The context size to use for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n    # Chunk each tokenized prompt into blocks of context_size,\n    # discarding the last block if too small.\n    context_size_prompts = []\n    for encoding in tokenized_prompts:\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters","title":"<code>SourceDataHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceDataHyperparameters(NestedParameter):\n    \"\"\"Source data hyperparameters.\"\"\"\n\n    dataset_path: Parameter[str]\n    \"\"\"Dataset path.\"\"\"\n\n    context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))\n    \"\"\"Context size.\"\"\"\n\n    dataset_dir: Parameter[str] | None = field(default=None)\n    \"\"\"Dataset directory (within the HF dataset)\"\"\"\n\n    dataset_files: Parameter[list[str]] | None = field(default=None)\n    \"\"\"Dataset files (within the HF dataset).\"\"\"\n\n    pre_download: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Whether to pre-download the dataset.\"\"\"\n\n    pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))\n    \"\"\"If the dataset is pre-tokenized.\"\"\"\n\n    tokenizer_name: Parameter[str] | None = field(default=None)\n    \"\"\"Tokenizer name.\n\n    Only set this if the dataset is not pre-tokenized.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\n\n        Raises:\n            ValueError: If there is an error in the source data hyperparameters.\n        \"\"\"\n        if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n            raise ValueError(error_message)\n\n        if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n            raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.context_size","title":"<code>context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Context size.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_dir","title":"<code>dataset_dir: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset directory (within the HF dataset)</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_files","title":"<code>dataset_files: Parameter[list[str]] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset files (within the HF dataset).</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_path","title":"<code>dataset_path: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Dataset path.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.pre_download","title":"<code>pre_download: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to pre-download the dataset.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.pre_tokenized","title":"<code>pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If the dataset is pre-tokenized.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.tokenizer_name","title":"<code>tokenizer_name: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tokenizer name.</p> <p>Only set this if the dataset is not pre-tokenized.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error in the source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\n\n    Raises:\n        ValueError: If there is an error in the source data hyperparameters.\n    \"\"\"\n    if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n        raise ValueError(error_message)\n\n    if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters","title":"<code>SourceModelHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source model hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceModelHyperparameters(NestedParameter):\n    \"\"\"Source model hyperparameters.\"\"\"\n\n    name: Parameter[str]\n    \"\"\"Source model name.\"\"\"\n\n    hook_site: Parameter[str]\n    \"\"\"Source model hook site.\"\"\"\n\n    hook_layer: Parameter[int]\n    \"\"\"Source model hook point layer.\"\"\"\n\n    hook_dimension: Parameter[int]\n    \"\"\"Source model hook point dimension.\"\"\"\n\n    dtype: Parameter[str] = field(default=Parameter(\"float32\"))\n    \"\"\"Source model dtype.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.dtype","title":"<code>dtype: Parameter[str] = field(default=Parameter('float32'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source model dtype.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.hook_dimension","title":"<code>hook_dimension: Parameter[int]</code>  <code>instance-attribute</code>","text":"<p>Source model hook point dimension.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.hook_layer","title":"<code>hook_layer: Parameter[int]</code>  <code>instance-attribute</code>","text":"<p>Source model hook point layer.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.hook_site","title":"<code>hook_site: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Source model hook site.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.name","title":"<code>name: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Source model name.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelRuntimeHyperparameters","title":"<code>SourceModelRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Source model runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class SourceModelRuntimeHyperparameters(TypedDict):\n    \"\"\"Source model runtime hyperparameters.\"\"\"\n\n    name: str\n    hook_site: str\n    hook_layer: int\n    hook_dimension: int\n    dtype: str\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>AbstractAutoencoder</code></p> <p>Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@final\nclass SparseAutoencoder(AbstractAutoencoder):\n    \"\"\"Sparse Autoencoder Model.\"\"\"\n\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Estimated Geometric Median of the Dataset.\n\n    Used for initialising :attr:`tied_bias`.\n    \"\"\"\n\n    tied_bias: Float[\n        Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Tied Bias Parameter.\n\n    The same bias is used pre-encoder and post-decoder.\n    \"\"\"\n\n    n_components: int | None\n    \"\"\"Number of source model components the SAE is trained on.\"\"\"\n\n    n_input_features: int\n    \"\"\"Number of Input Features.\"\"\"\n\n    n_learned_features: int\n    \"\"\"Number of Learned Features.\"\"\"\n\n    _pre_encoder_bias: TiedBias\n\n    _encoder: LinearEncoder\n\n    _decoder: UnitNormDecoder\n\n    _post_decoder_bias: TiedBias\n\n    @property\n    def pre_encoder_bias(self) -&gt; TiedBias:\n        \"\"\"Pre-encoder bias.\"\"\"\n        return self._pre_encoder_bias\n\n    @property\n    def encoder(self) -&gt; LinearEncoder:\n        \"\"\"Encoder.\"\"\"\n        return self._encoder\n\n    @property\n    def decoder(self) -&gt; UnitNormDecoder:\n        \"\"\"Decoder.\"\"\"\n        return self._decoder\n\n    @property\n    def post_decoder_bias(self) -&gt; TiedBias:\n        \"\"\"Post-decoder bias.\"\"\"\n        return self._post_decoder_bias\n\n    def __init__(\n        self,\n        n_input_features: int,\n        n_learned_features: int,\n        geometric_median_dataset: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ]\n        | None = None,\n        n_components: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder Model.\n\n        Args:\n            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n                from TransformerLens).\n            n_learned_features: Number of learned features. The initial paper experimented with 1 to\n                256 times the number of input features, and primarily used a multiple of 8.\n            geometric_median_dataset: Estimated geometric median of the dataset.\n            n_components: Number of source model components the SAE is trained on. This is useful if\n                you want to train the SAE on several components of the source model at once. If\n                `None`, the SAE is assumed to be trained on just one component (in this case the\n                model won't contain a component axis in any of the parameters).\n        \"\"\"\n        super().__init__()\n\n        self.n_input_features = n_input_features\n        self.n_learned_features = n_learned_features\n        self.n_components = n_components\n\n        # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n        # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n        tied_bias_shape = shape_with_optional_dimensions(n_components, n_input_features)\n        if geometric_median_dataset is not None:\n            self.geometric_median_dataset = geometric_median_dataset.clone()\n            self.geometric_median_dataset.requires_grad = False\n        else:\n            self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n            self.geometric_median_dataset.requires_grad = False\n\n        # Initialize the tied bias\n        self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n        self.initialize_tied_parameters()\n\n        # Initialize the components\n        self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n        self._encoder = LinearEncoder(\n            input_features=n_input_features,\n            learnt_features=n_learned_features,\n            n_components=n_components,\n        )\n\n        self._decoder = UnitNormDecoder(\n            learnt_features=n_learned_features,\n            decoded_features=n_input_features,\n            n_components=n_components,\n        )\n\n        self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; tuple[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)],\n    ]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n        x = self._pre_encoder_bias(x)\n        learned_activations = self._encoder(x)\n        x = self._decoder(learned_activations)\n        decoded_activations = self._post_decoder_bias(x)\n        return learned_activations, decoded_activations\n\n    def initialize_tied_parameters(self) -&gt; None:\n        \"\"\"Initialize the tied parameters.\"\"\"\n        # The tied bias is initialised as the geometric median of the dataset\n        self.tied_bias.data = self.geometric_median_dataset\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n        self.initialize_tied_parameters()\n        for module in self.network:\n            if \"reset_parameters\" in dir(module):\n                module.reset_parameters()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.decoder","title":"<code>decoder: UnitNormDecoder</code>  <code>property</code>","text":"<p>Decoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.encoder","title":"<code>encoder: LinearEncoder</code>  <code>property</code>","text":"<p>Encoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset","title":"<code>geometric_median_dataset: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Estimated Geometric Median of the Dataset.</p> <p>Used for initialising :attr:<code>tied_bias</code>.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_components","title":"<code>n_components: int | None = n_components</code>  <code>instance-attribute</code>","text":"<p>Number of source model components the SAE is trained on.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_input_features","title":"<code>n_input_features: int = n_input_features</code>  <code>instance-attribute</code>","text":"<p>Number of Input Features.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_learned_features","title":"<code>n_learned_features: int = n_learned_features</code>  <code>instance-attribute</code>","text":"<p>Number of Learned Features.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Post-decoder bias.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Pre-encoder bias.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.tied_bias","title":"<code>tied_bias: Float[Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)] = Parameter(torch.empty(tied_bias_shape))</code>  <code>instance-attribute</code>","text":"<p>Tied Bias Parameter.</p> <p>The same bias is used pre-encoder and post-decoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.__init__","title":"<code>__init__(n_input_features, n_learned_features, geometric_median_dataset=None, n_components=None)</code>","text":"<p>Initialize the Sparse Autoencoder Model.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_features</code> <code>int</code> <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p> required <code>n_learned_features</code> <code>int</code> <p>Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8.</p> required <code>geometric_median_dataset</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)] | None</code> <p>Estimated geometric median of the dataset.</p> <code>None</code> <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on. This is useful if you want to train the SAE on several components of the source model at once. If <code>None</code>, the SAE is assumed to be trained on just one component (in this case the model won't contain a component axis in any of the parameters).</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_learned_features: int,\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    | None = None,\n    n_components: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder Model.\n\n    Args:\n        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n            from TransformerLens).\n        n_learned_features: Number of learned features. The initial paper experimented with 1 to\n            256 times the number of input features, and primarily used a multiple of 8.\n        geometric_median_dataset: Estimated geometric median of the dataset.\n        n_components: Number of source model components the SAE is trained on. This is useful if\n            you want to train the SAE on several components of the source model at once. If\n            `None`, the SAE is assumed to be trained on just one component (in this case the\n            model won't contain a component axis in any of the parameters).\n    \"\"\"\n    super().__init__()\n\n    self.n_input_features = n_input_features\n    self.n_learned_features = n_learned_features\n    self.n_components = n_components\n\n    # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n    # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n    tied_bias_shape = shape_with_optional_dimensions(n_components, n_input_features)\n    if geometric_median_dataset is not None:\n        self.geometric_median_dataset = geometric_median_dataset.clone()\n        self.geometric_median_dataset.requires_grad = False\n    else:\n        self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n        self.geometric_median_dataset.requires_grad = False\n\n    # Initialize the tied bias\n    self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n    self.initialize_tied_parameters()\n\n    # Initialize the components\n    self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n    self._encoder = LinearEncoder(\n        input_features=n_input_features,\n        learnt_features=n_learned_features,\n        n_components=n_components,\n    )\n\n    self._decoder = UnitNormDecoder(\n        learnt_features=n_learned_features,\n        decoded_features=n_input_features,\n        n_components=n_components,\n    )\n\n    self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>tuple[Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)], Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]]</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; tuple[\n    Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n    Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)],\n]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n    x = self._pre_encoder_bias(x)\n    learned_activations = self._encoder(x)\n    x = self._decoder(learned_activations)\n    decoded_activations = self._post_decoder_bias(x)\n    return learned_activations, decoded_activations\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters","title":"<code>initialize_tied_parameters()</code>","text":"<p>Initialize the tied parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def initialize_tied_parameters(self) -&gt; None:\n    \"\"\"Initialize the tied parameters.\"\"\"\n    # The tied bias is initialised as the geometric median of the dataset\n    self.tied_bias.data = self.geometric_median_dataset\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n    self.initialize_tied_parameters()\n    for module in self.network:\n        if \"reset_parameters\" in dir(module):\n            module.reset_parameters()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SweepConfig","title":"<code>SweepConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>WandbSweepConfig</code></p> <p>Sweep Config.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass SweepConfig(WandbSweepConfig):\n    \"\"\"Sweep Config.\"\"\"\n\n    parameters: Hyperparameters\n\n    method: Method = Method.GRID\n\n    metric: Metric = field(default=Metric(name=\"train/loss/total_loss\"))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore","title":"<code>TensorActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Tensor Activation Store.</p> <p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n</code></pre> <p>Add a single activation vector to the dataset:</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a [batch, pos, neurons] activation tensor to the dataset:</p> <pre><code>&gt;&gt;&gt; store.empty()\n&gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n100\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>class TensorActivationStore(ActivationStore):\n    \"\"\"Tensor Activation Store.\n\n    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation\n    vectors to be stored to be known in advance. Multiprocess safe.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n\n    Add a single activation vector to the dataset:\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a [batch, pos, neurons] activation tensor to the dataset:\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        100\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: Float[Tensor, Axis.names(Axis.ITEMS, Axis.INPUT_OUTPUT_FEATURE)]\n    \"\"\"Underlying Tensor Data Store.\"\"\"\n\n    items_stored: int = 0\n    \"\"\"Number of items stored.\"\"\"\n\n    max_items: int\n    \"\"\"Maximum Number of Items to Store.\"\"\"\n\n    def __init__(\n        self,\n        max_items: int,\n        num_neurons: int,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the Tensor Activation Store.\n\n        Args:\n            max_items: Maximum number of items to store (individual activation vectors)\n            num_neurons: Number of neurons in each activation vector.\n            device: Device to store the activation vectors on.\n        \"\"\"\n        self._data = torch.empty((max_items, num_neurons), device=device)\n        self._max_items = max_items\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return self.items_stored\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n            800\n\n        Returns:\n            The size of the underlying tensor in bytes.\n        \"\"\"\n        return self._data.element_size() * self._data.nelement()\n\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        # Check in range\n        if index &gt;= self.items_stored:\n            msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n            raise IndexError(msg)\n\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n        &gt;&gt;&gt; store.append(torch.tensor([0.]))\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; [store[i].item() for i in range(3)]\n        [0.0, 2.0, 1.0]\n        \"\"\"\n        # Generate a permutation of the indices for the active data\n        perm = torch.randperm(self.items_stored)\n\n        # Use this permutation to shuffle the active data in-place\n        self._data[: self.items_stored] = self._data[perm]\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; None:\n        \"\"\"Add a single item to the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            item: The item to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        if self.items_stored + 1 &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[self.items_stored] = item.to(\n            self._data.device,\n        )\n        self.items_stored += 1\n\n    def extend(self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]) -&gt; None:\n        \"\"\"Add a batch to the store.\n\n        Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n        &gt;&gt;&gt; store.items_stored\n        9\n\n        Args:\n            batch: The batch to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        reshaped: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n        ] = resize_to_single_item_dimension(\n            batch,\n        )\n\n        # Check we have space\n        num_activation_tensors: int = reshaped.shape[0]\n        if self.items_stored + num_activation_tensors &gt; self._max_items:\n            if reshaped.shape[0] &gt; self._max_items:\n                msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                    total maximum in the store of {self._max_items}.\"\n                raise ValueError(msg)\n\n            raise StoreFullError\n\n        self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n            self._data.device\n        )\n        self.items_stored += num_activation_tensors\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; store.items_stored\n        0\n        \"\"\"\n        # We don't need to zero the data, just reset the number of items stored\n        self.items_stored = 0\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.items_stored","title":"<code>items_stored: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of items stored.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.max_items","title":"<code>max_items: int</code>  <code>instance-attribute</code>","text":"<p>Maximum Number of Items to Store.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The activation store item at the given index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    # Check in range\n    if index &gt;= self.items_stored:\n        msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n        raise IndexError(msg)\n\n    return self._data[index]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__init__","title":"<code>__init__(max_items, num_neurons, device=None)</code>","text":"<p>Initialise the Tensor Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>max_items</code> <code>int</code> <p>Maximum number of items to store (individual activation vectors)</p> required <code>num_neurons</code> <code>int</code> <p>Number of neurons in each activation vector.</p> required <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __init__(\n    self,\n    max_items: int,\n    num_neurons: int,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"Initialise the Tensor Activation Store.\n\n    Args:\n        max_items: Maximum number of items to store (individual activation vectors)\n        num_neurons: Number of neurons in each activation vector.\n        device: Device to store the activation vectors on.\n    \"\"\"\n    self._data = torch.empty((max_items, num_neurons), device=device)\n    self._max_items = max_items\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return self.items_stored\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> Example <p>import torch store = TensorActivationStore(max_items=2, num_neurons=100) store.sizeof() # Pre-allocated tensor of 2x100 800</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the underlying tensor in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n        800\n\n    Returns:\n        The size of the underlying tensor in bytes.\n    \"\"\"\n    return self._data.element_size() * self._data.nelement()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a single item to the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The item to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; None:\n    \"\"\"Add a single item to the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        item: The item to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    if self.items_stored + 1 &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[self.items_stored] = item.to(\n        self._data.device,\n    )\n    self.items_stored += 1\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; store.items_stored\n    0\n    \"\"\"\n    # We don't need to zero the data, just reset the number of items stored\n    self.items_stored = 0\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a batch to the store.</p> <p>Examples:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2</p> <p>store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>The batch to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def extend(self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]) -&gt; None:\n    \"\"\"Add a batch to the store.\n\n    Examples:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n    &gt;&gt;&gt; store.items_stored\n    9\n\n    Args:\n        batch: The batch to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    reshaped: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n    ] = resize_to_single_item_dimension(\n        batch,\n    )\n\n    # Check we have space\n    num_activation_tensors: int = reshaped.shape[0]\n    if self.items_stored + num_activation_tensors &gt; self._max_items:\n        if reshaped.shape[0] &gt; self._max_items:\n            msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                total maximum in the store of {self._max_items}.\"\n            raise ValueError(msg)\n\n        raise StoreFullError\n\n    self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n        self._data.device\n    )\n    self.items_stored += num_activation_tensors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0]</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n    &gt;&gt;&gt; store.append(torch.tensor([0.]))\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; [store[i].item() for i in range(3)]\n    [0.0, 2.0, 1.0]\n    \"\"\"\n    # Generate a permutation of the indices for the active data\n    perm = torch.randperm(self.items_stored)\n\n    # Use this permutation to shuffle the active data in-place\n    self._data[: self.items_stored] = self._data[perm]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset","title":"<code>TextDataset</code>","text":"<p>             Bases: <code>SourceDataset[GenericTextDataBatch]</code></p> <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@final\nclass TextDataset(SourceDataset[GenericTextDataBatch]):\n    \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n\n    def preprocess(\n        self,\n        source_batch: GenericTextDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n        Args:\n            source_batch: A batch of source data, including 'text' with a list of strings.\n            context_size: Context size for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        prompts: list[str] = source_batch[\"text\"]\n\n        tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n        # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n        context_size_prompts = []\n        for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        dataset_path: str,\n        tokenizer: PreTrainedTokenizerBase,\n        buffer_size: int = 1000,\n        context_size: int = 256,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        n_processes_preprocessing: int | None = None,\n        preprocess_batch_size: int = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a generic text dataset from Hugging Face.\n\n        Args:\n            dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n            tokenizer: Tokenizer to process text data.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g., 'train').\n            n_processes_preprocessing: Number of processes to use for preprocessing.\n            preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        self.tokenizer = tokenizer\n\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            n_processes_preprocessing=n_processes_preprocessing,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n\n    def push_to_hugging_face_hub(\n        self,\n        repo_id: str,\n        commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n        max_shard_size: str | None = None,\n        num_shards: int = 64,\n        revision: str = \"main\",\n        *,\n        private: bool = False,\n    ) -&gt; None:\n        \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n        Motivation:\n            Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n            pre-processed dataset with others. This function allows you to do that by pushing the\n            pre-processed dataset to the Hugging Face hub.\n\n        Warning:\n            You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n            to use this.\n\n        Warning:\n            This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n            initializing the dataset).\n\n        Args:\n            repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n            commit_message: Commit message.\n            max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `num_shards`\n                is set.\n            num_shards: Number of shards to split the dataset into. A high number is recommended\n                here to allow for flexible distributed training of SAEs across nodes (where e.g.\n                each node fetches it's own shard).\n            revision: Branch to push to.\n            private: Whether to save the dataset privately.\n\n        Raises:\n            TypeError: If the dataset is streamed.\n        \"\"\"\n        if isinstance(self.dataset, IterableDataset):\n            error_message = (\n                \"Cannot share a streamed dataset to Hugging Face. \"\n                \"Please use `pre_download=True` when initializing the dataset.\"\n            )\n            raise TypeError(error_message)\n\n        self.dataset.push_to_hub(\n            repo_id=repo_id,\n            commit_message=commit_message,\n            max_shard_size=max_shard_size,\n            num_shards=num_shards,\n            private=private,\n            revision=revision,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.__init__","title":"<code>__init__(dataset_path, tokenizer, buffer_size=1000, context_size=256, dataset_dir=None, dataset_files=None, dataset_split='train', n_processes_preprocessing=None, preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a generic text dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the dataset on Hugging Face (e.g. <code>'monology/pile-uncopyright'</code>).</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer to process text data.</p> required <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>256</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., 'train').</p> <code>'train'</code> <code>n_processes_preprocessing</code> <code>int | None</code> <p>Number of processes to use for preprocessing.</p> <code>None</code> <code>preprocess_batch_size</code> <code>int</code> <p>Batch size for preprocessing (tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_path: str,\n    tokenizer: PreTrainedTokenizerBase,\n    buffer_size: int = 1000,\n    context_size: int = 256,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    n_processes_preprocessing: int | None = None,\n    preprocess_batch_size: int = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a generic text dataset from Hugging Face.\n\n    Args:\n        dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n        tokenizer: Tokenizer to process text data.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g., 'train').\n        n_processes_preprocessing: Number of processes to use for preprocessing.\n        preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    self.tokenizer = tokenizer\n\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        n_processes_preprocessing=n_processes_preprocessing,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>GenericTextDataBatch</code> <p>A batch of source data, including 'text' with a list of strings.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: GenericTextDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n    Args:\n        source_batch: A batch of source data, including 'text' with a list of strings.\n        context_size: Context size for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    prompts: list[str] = source_batch[\"text\"]\n\n    tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n    # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n    context_size_prompts = []\n    for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.push_to_hugging_face_hub","title":"<code>push_to_hugging_face_hub(repo_id, commit_message='Upload preprocessed dataset using sparse_autoencoder.', max_shard_size=None, num_shards=64, revision='main', *, private=False)</code>","text":"<p>Share preprocessed dataset to Hugging Face hub.</p> Motivation <p>Pre-processing a dataset can be time-consuming, so it is useful to be able to share the pre-processed dataset with others. This function allows you to do that by pushing the pre-processed dataset to the Hugging Face hub.</p> Warning <p>You must be logged into HuggingFace (e.g with <code>huggingface-cli login</code> from the terminal) to use this.</p> Warning <p>This will only work if the dataset is not streamed (i.e. if <code>pre_download=True</code> when initializing the dataset).</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Hugging Face repo ID to save the dataset to (e.g. <code>username/dataset_name</code>).</p> required <code>commit_message</code> <code>str</code> <p>Commit message.</p> <code>'Upload preprocessed dataset using sparse_autoencoder.'</code> <code>max_shard_size</code> <code>str | None</code> <p>Maximum shard size (e.g. <code>'500MB'</code>). Should not be set if <code>num_shards</code> is set.</p> <code>None</code> <code>num_shards</code> <code>int</code> <p>Number of shards to split the dataset into. A high number is recommended here to allow for flexible distributed training of SAEs across nodes (where e.g. each node fetches it's own shard).</p> <code>64</code> <code>revision</code> <code>str</code> <p>Branch to push to.</p> <code>'main'</code> <code>private</code> <code>bool</code> <p>Whether to save the dataset privately.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset is streamed.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def push_to_hugging_face_hub(\n    self,\n    repo_id: str,\n    commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n    max_shard_size: str | None = None,\n    num_shards: int = 64,\n    revision: str = \"main\",\n    *,\n    private: bool = False,\n) -&gt; None:\n    \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n    Motivation:\n        Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n        pre-processed dataset with others. This function allows you to do that by pushing the\n        pre-processed dataset to the Hugging Face hub.\n\n    Warning:\n        You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n        to use this.\n\n    Warning:\n        This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n        initializing the dataset).\n\n    Args:\n        repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n        commit_message: Commit message.\n        max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `num_shards`\n            is set.\n        num_shards: Number of shards to split the dataset into. A high number is recommended\n            here to allow for flexible distributed training of SAEs across nodes (where e.g.\n            each node fetches it's own shard).\n        revision: Branch to push to.\n        private: Whether to save the dataset privately.\n\n    Raises:\n        TypeError: If the dataset is streamed.\n    \"\"\"\n    if isinstance(self.dataset, IterableDataset):\n        error_message = (\n            \"Cannot share a streamed dataset to Hugging Face. \"\n            \"Please use `pre_download=True` when initializing the dataset.\"\n        )\n        raise TypeError(error_message)\n\n    self.dataset.push_to_hub(\n        repo_id=repo_id,\n        commit_message=commit_message,\n        max_shard_size=max_shard_size,\n        num_shards=num_shards,\n        private=private,\n        revision=revision,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric","title":"<code>TrainBatchFeatureDensityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Train batch feature density.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a training batch.</p> <p>Generally we want a small number of features to be active in each batch, so average feature density should be low. By contrast if the average feature density is high, it means that the features are not sparse enough.</p> Warning <p>This is not the same as the feature density of the entire training set. It's main use is tracking the progress of training.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>class TrainBatchFeatureDensityMetric(AbstractTrainMetric):\n    \"\"\"Train batch feature density.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a\n    training batch.\n\n    Generally we want a small number of features to be active in each batch, so average feature\n    density should be low. By contrast if the average feature density is high, it means that the\n    features are not sparse enough.\n\n    Warning:\n        This is not the same as the feature density of the entire training set. It's main use is\n        tracking the progress of training.\n    \"\"\"\n\n    threshold: float\n\n    def __init__(self, threshold: float = 0.0) -&gt; None:\n        \"\"\"Initialise the train batch feature density metric.\n\n        Args:\n            threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n                This should be close to zero.\n        \"\"\"\n        super().__init__()\n        self.threshold = threshold\n\n    def feature_density(\n        self, activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)]\n    ) -&gt; Float[Tensor, Axis.LEARNT_FEATURE]:\n        \"\"\"Count how many times each feature was active.\n\n        Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n            &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n            [1.0, 0.5, 0.0]\n\n        Args:\n            activations: Sample of cached activations (the Autoencoder's learned features).\n\n        Returns:\n            Number of times each feature was active in a sample.\n        \"\"\"\n        has_fired: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)] = torch.gt(\n            activations, self.threshold\n        ).to(\n            dtype=torch.float  # Move to float so it can be averaged\n        )\n\n        return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n\n    @staticmethod\n    def wandb_feature_density_histogram(\n        feature_density: Float[Tensor, Axis.LEARNT_FEATURE],\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the feature density.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n        wandb_feature_density_histogram(feature_density)})`.\n\n        Args:\n            feature_density: Number of times each feature was active in a sample. Can be calculated\n                using :func:`feature_activity_count`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_feature_density, bins=50)\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the train batch feature density metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary with the train batch feature density metric, and a histogram of the feature\n            density.\n        \"\"\"\n        train_batch_feature_density: Float[Tensor, Axis.LEARNT_FEATURE] = self.feature_density(\n            data.learned_activations\n        )\n\n        train_batch_feature_density_histogram: wandb.Histogram = (\n            self.wandb_feature_density_histogram(train_batch_feature_density)\n        )\n\n        return {\n            \"train/batch_feature_density_histogram\": train_batch_feature_density_histogram,\n        }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__","title":"<code>__init__(threshold=0.0)</code>","text":"<p>Initialise the train batch feature density metric.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero.</p> <code>0.0</code> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def __init__(self, threshold: float = 0.0) -&gt; None:\n    \"\"\"Initialise the train batch feature density metric.\n\n    Args:\n        threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n            This should be close to zero.\n    \"\"\"\n    super().__init__()\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the train batch feature density metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with the train batch feature density metric, and a histogram of the feature</p> <code>dict[str, Any]</code> <p>density.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the train batch feature density metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary with the train batch feature density metric, and a histogram of the feature\n        density.\n    \"\"\"\n    train_batch_feature_density: Float[Tensor, Axis.LEARNT_FEATURE] = self.feature_density(\n        data.learned_activations\n    )\n\n    train_batch_feature_density_histogram: wandb.Histogram = (\n        self.wandb_feature_density_histogram(train_batch_feature_density)\n    )\n\n    return {\n        \"train/batch_feature_density_histogram\": train_batch_feature_density_histogram,\n    }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density","title":"<code>feature_density(activations)</code>","text":"<p>Count how many times each feature was active.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").</p> Example <p>import torch activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist() [1.0, 0.5, 0.0]</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Sample of cached activations (the Autoencoder's learned features).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, LEARNT_FEATURE]</code> <p>Number of times each feature was active in a sample.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def feature_density(\n    self, activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)]\n) -&gt; Float[Tensor, Axis.LEARNT_FEATURE]:\n    \"\"\"Count how many times each feature was active.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n        &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n        [1.0, 0.5, 0.0]\n\n    Args:\n        activations: Sample of cached activations (the Autoencoder's learned features).\n\n    Returns:\n        Number of times each feature was active in a sample.\n    \"\"\"\n    has_fired: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)] = torch.gt(\n        activations, self.threshold\n    ).to(\n        dtype=torch.float  # Move to float so it can be averaged\n    )\n\n    return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram","title":"<code>wandb_feature_density_histogram(feature_density)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the feature density.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature_density</code> <code>Float[Tensor, LEARNT_FEATURE]</code> <p>Number of times each feature was active in a sample. Can be calculated using :func:<code>feature_activity_count</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>@staticmethod\ndef wandb_feature_density_histogram(\n    feature_density: Float[Tensor, Axis.LEARNT_FEATURE],\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the feature density.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n    wandb_feature_density_histogram(feature_density)})`.\n\n    Args:\n        feature_density: Number of times each feature was active in a sample. Can be calculated\n            using :func:`feature_activity_count`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_feature_density, bins=50)\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.sweep","title":"<code>sweep(sweep_config)</code>","text":"<p>Main function to run the training pipeline with wandb hyperparameter sweep.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def sweep(sweep_config: SweepConfig) -&gt; None:\n    \"\"\"Main function to run the training pipeline with wandb hyperparameter sweep.\"\"\"\n    sweep_id = wandb.sweep(sweep_config.to_dict(), project=\"sparse-autoencoder\")\n\n    wandb.agent(sweep_id, train)\n    wandb.finish()\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>activation_resampler<ul> <li>abstract_activation_resampler</li> <li>activation_resampler</li> </ul> </li> <li>activation_store<ul> <li>base_store</li> <li>disk_store</li> <li>list_store</li> <li>tensor_store</li> <li>utils<ul> <li>extend_resize</li> </ul> </li> </ul> </li> <li>autoencoder<ul> <li>abstract_autoencoder</li> <li>components<ul> <li>abstract_decoder</li> <li>abstract_encoder</li> <li>abstract_outer_bias</li> <li>linear_encoder</li> <li>tied_bias</li> <li>unit_norm_decoder</li> </ul> </li> <li>model</li> <li>utils<ul> <li>tensor_shape</li> </ul> </li> </ul> </li> <li>loss<ul> <li>abstract_loss</li> <li>decoded_activations_l2</li> <li>learned_activations_l1</li> <li>reducer</li> </ul> </li> <li>metrics<ul> <li>generate<ul> <li>abstract_generate_metric</li> </ul> </li> <li>metrics_container</li> <li>train<ul> <li>abstract_train_metric</li> <li>capacity</li> <li>feature_density</li> <li>l0_norm_metric</li> <li>neuron_activity_metric</li> </ul> </li> <li>validate<ul> <li>abstract_validate_metric</li> <li>model_reconstruction_score</li> </ul> </li> </ul> </li> <li>optimizer<ul> <li>abstract_optimizer</li> <li>adam_with_reset</li> </ul> </li> <li>source_data<ul> <li>abstract_dataset</li> <li>mock_dataset</li> <li>pretokenized_dataset</li> <li>text_dataset</li> </ul> </li> <li>source_model<ul> <li>replace_activations_hook</li> <li>store_activations_hook</li> <li>zero_ablate_hook</li> </ul> </li> <li>tensor_types</li> <li>train<ul> <li>pipeline</li> <li>sweep</li> <li>sweep_config</li> <li>utils<ul> <li>get_model_device</li> <li>round_down</li> <li>wandb_sweep_types</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/tensor_types/","title":"Tensor Axis Types","text":"<p>Tensor Axis Types.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis","title":"<code>Axis</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Tensor axis names.</p> <p>Used to annotate tensor types.</p> Example <p>When used directly it prints a string:</p> <p>print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature</p> <p>The primary use is to annotate tensor types:</p> <p>from jaxtyping import Float from torch import Tensor from typing import TypeAlias batch: TypeAlias = Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] print(batch)  <p>You can also join multiple axis together to represent the dimensions of a tensor:</p> <p>print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature</p> Source code in <code>sparse_autoencoder/tensor_types.py</code> <pre><code>class Axis(LowercaseStrEnum):\n    \"\"\"Tensor axis names.\n\n    Used to annotate tensor types.\n\n    Example:\n        When used directly it prints a string:\n\n        &gt;&gt;&gt; print(Axis.INPUT_OUTPUT_FEATURE)\n        input_output_feature\n\n        The primary use is to annotate tensor types:\n\n        &gt;&gt;&gt; from jaxtyping import Float\n        &gt;&gt;&gt; from torch import Tensor\n        &gt;&gt;&gt; from typing import TypeAlias\n        &gt;&gt;&gt; batch: TypeAlias = Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n        &gt;&gt;&gt; print(batch)\n        &lt;class 'jaxtyping.Float[Tensor, 'batch input_output_feature']'&gt;\n\n        You can also join multiple axis together to represent the dimensions of a tensor:\n\n        &gt;&gt;&gt; print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n        batch input_output_feature\n    \"\"\"\n\n    # Component idx\n    COMPONENT = auto()\n    \"\"\"Component index.\"\"\"\n\n    COMPONENT_OPTIONAL = \"*component\"\n    \"\"\"Optional component index.\"\"\"\n\n    # Batches\n    SOURCE_DATA_BATCH = auto()\n    \"\"\"Batch of prompts used to generate source model activations.\"\"\"\n\n    BATCH = auto()\n    \"\"\"Batch of items that the SAE is being trained on.\"\"\"\n\n    ITEMS = auto()\n    \"\"\"Arbitrary number of items.\"\"\"\n\n    # Features\n    INPUT_OUTPUT_FEATURE = auto()\n    \"\"\"Input or output feature (e.g. feature in activation vector from source model).\"\"\"\n\n    LEARNT_FEATURE = auto()\n    \"\"\"Learn feature (e.g. feature in learnt activation vector).\"\"\"\n\n    DEAD_FEATURE = auto()\n    \"\"\"Dead feature.\"\"\"\n\n    ALIVE_FEATURE = auto()\n    \"\"\"Alive feature.\"\"\"\n\n    # Feature indices\n    INPUT_OUTPUT_FEATURE_IDX = auto()\n    \"\"\"Input or output feature index.\"\"\"\n\n    LEARNT_FEATURE_IDX = auto()\n    \"\"\"Learn feature index.\"\"\"\n\n    # Other\n    POSITION = auto()\n    \"\"\"Token position.\"\"\"\n\n    SINGLE_ITEM = \"\"\n    \"\"\"Single item axis.\"\"\"\n\n    ANY = \"*any\"\n    \"\"\"Any number of axis.\"\"\"\n\n    @staticmethod\n    def names(*axis: \"Axis\") -&gt; str:\n        \"\"\"Join multiple axis together, to represent the dimensions of a tensor.\n\n        Example:\n            &gt;&gt;&gt; print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n            batch input_output_feature\n\n        Args:\n            *axis: Axis to join.\n\n        Returns:\n            Joined axis string.\n        \"\"\"\n        return \" \".join(a.value for a in axis)\n</code></pre>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ALIVE_FEATURE","title":"<code>ALIVE_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Alive feature.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ANY","title":"<code>ANY = '*any'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Any number of axis.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH","title":"<code>BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of items that the SAE is being trained on.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT","title":"<code>COMPONENT = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Component index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL","title":"<code>COMPONENT_OPTIONAL = '*component'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional component index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE","title":"<code>DEAD_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead feature.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE","title":"<code>INPUT_OUTPUT_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Input or output feature (e.g. feature in activation vector from source model).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE_IDX","title":"<code>INPUT_OUTPUT_FEATURE_IDX = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Input or output feature index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ITEMS","title":"<code>ITEMS = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrary number of items.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE","title":"<code>LEARNT_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learn feature (e.g. feature in learnt activation vector).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE_IDX","title":"<code>LEARNT_FEATURE_IDX = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learn feature index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.POSITION","title":"<code>POSITION = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Token position.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SINGLE_ITEM","title":"<code>SINGLE_ITEM = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single item axis.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SOURCE_DATA_BATCH","title":"<code>SOURCE_DATA_BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of prompts used to generate source model activations.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.names","title":"<code>names(*axis)</code>  <code>staticmethod</code>","text":"<p>Join multiple axis together, to represent the dimensions of a tensor.</p> Example <p>print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature</p> <p>Parameters:</p> Name Type Description Default <code>*axis</code> <code>Axis</code> <p>Axis to join.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>Joined axis string.</p> Source code in <code>sparse_autoencoder/tensor_types.py</code> <pre><code>@staticmethod\ndef names(*axis: \"Axis\") -&gt; str:\n    \"\"\"Join multiple axis together, to represent the dimensions of a tensor.\n\n    Example:\n        &gt;&gt;&gt; print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n        batch input_output_feature\n\n    Args:\n        *axis: Axis to join.\n\n    Returns:\n        Joined axis string.\n    \"\"\"\n    return \" \".join(a.value for a in axis)\n</code></pre>"},{"location":"reference/activation_resampler/","title":"Activation Resampler","text":"<p>Activation Resampler.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/","title":"Abstract activation resampler","text":"<p>Abstract activation resampler.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler","title":"<code>AbstractActivationResampler</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract activation resampler.</p> <p>Developer guide:</p> <pre><code>This is just an interface (there are no implemented methods, so it is just a glorified type\nsignature).\n\nIt is setup this way so that users can add their own alternative activation resampler with\nthe same interface, and then easily drop them in as a replacement for the training pipeline.\nIf you find you need it to be more flexible (e.g. have more inputs), please do just submit\nan issue and/or a PR.\n\nIf you want to implement your own activation resampler (i.e. you want to experiment with\nchanging the way the weight updates are calculated), you should probably create your own\ncustom resampler by extending this class. By default this should be done by installing the\nsparse_autoencoder library (and then extending this class in your own codebase), but if you\nthink it would be useful for others as well please do also submit an issue/PR for this.\n</code></pre> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>class AbstractActivationResampler(ABC):\n    \"\"\"Abstract activation resampler.\n\n    Developer guide:\n\n        This is just an interface (there are no implemented methods, so it is just a glorified type\n        signature).\n\n        It is setup this way so that users can add their own alternative activation resampler with\n        the same interface, and then easily drop them in as a replacement for the training pipeline.\n        If you find you need it to be more flexible (e.g. have more inputs), please do just submit\n        an issue and/or a PR.\n\n        If you want to implement your own activation resampler (i.e. you want to experiment with\n        changing the way the weight updates are calculated), you should probably create your own\n        custom resampler by extending this class. By default this should be done by installing the\n        sparse_autoencoder library (and then extending this class in your own codebase), but if you\n        think it would be useful for others as well please do also submit an issue/PR for this.\n    \"\"\"\n\n    @abstractmethod\n    def step_resampler(\n        self,\n        batch_neuron_activity: Int[Tensor, Axis.LEARNT_FEATURE],\n        activation_store: TensorActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults | None:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            batch_neuron_activity: Number of times each neuron fired in current batch.\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n        \"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler.step_resampler","title":"<code>step_resampler(batch_neuron_activity, activation_store, autoencoder, loss_fn, train_batch_size)</code>  <code>abstractmethod</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>batch_neuron_activity</code> <code>Int[Tensor, LEARNT_FEATURE]</code> <p>Number of times each neuron fired in current batch.</p> required <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults | None</code> <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>@abstractmethod\ndef step_resampler(\n    self,\n    batch_neuron_activity: Int[Tensor, Axis.LEARNT_FEATURE],\n    activation_store: TensorActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults | None:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        batch_neuron_activity: Number of times each neuron fired in current batch.\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n    \"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults","title":"<code>ParameterUpdateResults</code>  <code>dataclass</code>","text":"<p>Parameter update results from resampling dead neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>@dataclass\nclass ParameterUpdateResults:\n    \"\"\"Parameter update results from resampling dead neurons.\"\"\"\n\n    dead_neuron_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX]\n    \"\"\"Dead neuron indices.\"\"\"\n\n    dead_encoder_weight_updates: Float[\n        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Dead encoder weight updates.\"\"\"\n\n    dead_encoder_bias_updates: Float[Tensor, Axis.DEAD_FEATURE]\n    \"\"\"Dead encoder bias updates.\"\"\"\n\n    dead_decoder_weight_updates: Float[\n        Tensor, Axis.names(Axis.INPUT_OUTPUT_FEATURE, Axis.DEAD_FEATURE)\n    ]\n    \"\"\"Dead decoder weight updates.\"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_decoder_weight_updates","title":"<code>dead_decoder_weight_updates: Float[Tensor, Axis.names(Axis.INPUT_OUTPUT_FEATURE, Axis.DEAD_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Dead decoder weight updates.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_encoder_bias_updates","title":"<code>dead_encoder_bias_updates: Float[Tensor, Axis.DEAD_FEATURE]</code>  <code>instance-attribute</code>","text":"<p>Dead encoder bias updates.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_encoder_weight_updates","title":"<code>dead_encoder_weight_updates: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Dead encoder weight updates.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_neuron_indices","title":"<code>dead_neuron_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX]</code>  <code>instance-attribute</code>","text":"<p>Dead neuron indices.</p>"},{"location":"reference/activation_resampler/activation_resampler/","title":"Activation resampler","text":"<p>Activation resampler.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler","title":"<code>ActivationResampler</code>","text":"<p>             Bases: <code>AbstractActivationResampler</code></p> <p>Activation resampler.</p> <p>Collates the number of times each neuron fires over a set number of learned activation vectors, and then provides the parameters necessary to reset any dead neurons.</p> Motivation <p>Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.</p> <p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.</p> <p>This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network.</p> Warning <p>The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases.</p> Warning <p>This approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>class ActivationResampler(AbstractActivationResampler):\n    \"\"\"Activation resampler.\n\n    Collates the number of times each neuron fires over a set number of learned activation vectors,\n    and then provides the parameters necessary to reset any dead neurons.\n\n    Motivation:\n        Over the course of training, a subset of autoencoder neurons will have zero activity across\n        a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language\n        Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training\n        improves the number of likely-interpretable features (i.e., those in the high density\n        cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket\n        Hypothesis and increase the number of chances the network has to find promising feature\n        directions.\n\n        An interesting nuance around dead neurons involves the ultralow density cluster. They found\n        that if we increase the number of training steps then networks will kill off more of these\n        ultralow density neurons. This reinforces the use of the high density cluster as a useful\n        metric because there can exist neurons that are de facto dead but will not appear to be when\n        looking at the number of dead neurons alone.\n\n        This approach is designed to seed new features to fit inputs where the current autoencoder\n        performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled\n        neuron will only fire weakly for inputs similar to the one used for its reinitialization.\n        This was done to minimize interference with the rest of the network.\n\n    Warning:\n        The optimizer should be reset after applying this function, as the Adam state will be\n        incorrect for the modified weights and biases.\n\n    Warning:\n        This approach is also known to create sudden loss spikes, and resampling too frequently\n        causes training to diverge.\n    \"\"\"\n\n    _activations_seen_since_last_resample: int = 0\n    \"\"\"Number of activations since we last resampled.\"\"\"\n\n    _collated_neuron_activity: Float[Tensor, Axis.LEARNT_FEATURE]\n    \"\"\"Collated neuron activity, over the current data collection window.\"\"\"\n\n    _threshold_is_dead_portion_fires: float\n    \"\"\"Threshold for determining if a neuron has fired (or is dead).\"\"\"\n\n    _max_n_resamples: int\n    \"\"\"Maximum number of times that resampling should be performed.\"\"\"\n\n    _n_activations_collated_since_last_resample: int = 0\n    \"\"\"Number of activations collated since we last resampled.\n\n    Number of vectors used to collate neuron activity, over the current collation window.\n    \"\"\"\n\n    _number_times_resampled: int = 0\n    \"\"\"Number of times that resampling has been performed.\"\"\"\n\n    neuron_activity_window_end: int\n    \"\"\"End of the window for collecting neuron activity.\"\"\"\n\n    neuron_activity_window_start: int\n    \"\"\"Start of the window for collecting neuron activity.\"\"\"\n\n    def __init__(\n        self,\n        n_learned_features: int,\n        resample_interval: int = 200_000_000,\n        max_n_resamples: int = 4,\n        n_activations_activity_collate: int = 100_000_000,\n        resample_dataset_size: int = 819_200,\n        threshold_is_dead_portion_fires: float = 0.0,\n    ) -&gt; None:\n        r\"\"\"Initialize the activation resampler.\n\n        Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n        Args:\n            n_learned_features: Number of learned features\n            resample_interval: Interval in number of autoencoder input activation vectors trained\n                on, before resampling.\n            max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n                Set to inf if you want to have no limit.\n            n_activations_activity_collate: Number of autoencoder learned activation vectors to\n                collate before resampling (the activation resampler will start collecting on vector\n                $\\text{resample_interval} - \\text{n_steps_collate}$).\n            resample_dataset_size: Number of autoencoder input activations to use for calculating\n                the loss, as part of the resampling process to create the reset neuron weights.\n            threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n                \"fired\" in less than this portion of the collated sample).\n\n        Raises:\n            ValueError: If any of the arguments are invalid (e.g. negative integers).\n        \"\"\"\n        if n_activations_activity_collate &lt;= 0:\n            error_message = \"Number of steps to collate must be greater than 0.\"\n            raise ValueError(error_message)\n\n        if n_activations_activity_collate &gt; resample_interval:\n            error_message = (\n                \"Number of steps to collate must be less than or equal to the resample interval.\"\n            )\n            raise ValueError(error_message)\n\n        if threshold_is_dead_portion_fires &lt; 0 or threshold_is_dead_portion_fires &gt; 1:\n            error_message = (\n                \"Threshold portion of times that a dead neuron fires, must be between 0 and 1.\"\n            )\n            raise ValueError(error_message)\n\n        if max_n_resamples &lt; 0:\n            error_message = (\n                \"Maximum number of resamples must be greater than 0. For unlimited, use inf.\"\n            )\n            raise ValueError(error_message)\n\n        if resample_dataset_size &lt; 0:\n            error_message = \"Resample dataset size must be greater than 0.\"\n            raise ValueError(error_message)\n\n        super().__init__()\n        self.neuron_activity_window_end = resample_interval\n        self.neuron_activity_window_start = resample_interval - n_activations_activity_collate\n        self._max_n_resamples = max_n_resamples\n        self._collated_neuron_activity = torch.zeros(n_learned_features, dtype=torch.int64)\n        self._resample_dataset_size = resample_dataset_size\n        self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n\n    def _get_dead_neuron_indices(\n        self,\n    ) -&gt; Int64[Tensor, Axis.LEARNT_FEATURE_IDX]:\n        \"\"\"Identify the indices of neurons that are dead.\n\n        Identifies any neurons that have fired less than the threshold portion of the collated\n        sample size.\n\n        Returns:\n            A tensor containing the indices of neurons that are dead.\n\n        Raises:\n            ValueError: If no neuron activity has been collated yet.\n        \"\"\"\n        # Check we have already collated some neuron activity\n        if torch.all(self._collated_neuron_activity == 0):\n            error_message = \"Cannot get dead neuron indices without neuron activity.\"\n            raise ValueError(error_message)\n\n        # Find any neurons that fire less than the threshold portion of times\n        threshold_is_dead_number_fires: int = int(\n            self._n_activations_collated_since_last_resample * self._threshold_is_dead_portion_fires\n        )\n\n        dead_indices = torch.where(\n            self._collated_neuron_activity &lt;= threshold_is_dead_number_fires\n        )[0]\n\n        return dead_indices.to(dtype=torch.int64)\n\n    def compute_loss_and_get_activations(\n        self,\n        store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; tuple[\n        Float[Tensor, Axis.BATCH], Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n    ]:\n        \"\"\"Compute the loss on a random subset of inputs.\n\n        Motivation:\n            Helps find input vectors that have high loss, so that we can resample dead neurons in a\n            way that improves performance on these specific input vectors.\n\n        Args:\n            store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            A tuple of loss per item, and all input activations.\n\n        Raises:\n            ValueError: If the number of items in the store is less than the number of inputs\n        \"\"\"\n        with torch.no_grad():\n            loss_batches: list[Float[Tensor, Axis.BATCH]] = []\n            input_activations_batches: list[\n                Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n            ] = []\n            dataloader = DataLoader(store, batch_size=train_batch_size)\n            num_inputs = self._resample_dataset_size\n            n_batches_required: int = num_inputs // train_batch_size\n            model_device: torch.device = get_model_device(autoencoder)\n\n            for batch_idx, batch in enumerate(iter(dataloader)):\n                input_activations_batches.append(batch)\n                source_activations = batch.to(model_device)\n                learned_activations, reconstructed_activations = autoencoder(source_activations)\n                loss_batches.append(\n                    loss_fn.forward(\n                        source_activations, learned_activations, reconstructed_activations\n                    )\n                )\n                if batch_idx &gt;= n_batches_required:\n                    break\n\n            loss_result = torch.cat(loss_batches).to(model_device)\n            input_activations = torch.cat(input_activations_batches).to(model_device)\n\n            # Check we generated enough data\n            if len(loss_result) &lt; num_inputs:\n                error_message = (\n                    f\"Cannot get {num_inputs} items from the store, \"\n                    f\"as only {len(loss_result)} were available.\"\n                )\n                raise ValueError(error_message)\n\n            return loss_result, input_activations\n\n    @staticmethod\n    def assign_sampling_probabilities(loss: Float[Tensor, Axis.BATCH]) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Assign the sampling probabilities for each input activations vector.\n\n        Assign each input vector a probability of being picked that is proportional to the square of\n        the autoencoder's loss on that input.\n\n        Example:\n            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n            tensor([0.1000, 0.3000, 0.6000])\n\n        Args:\n            loss: Loss per item.\n\n        Returns:\n            A tensor of probabilities for each item.\n        \"\"\"\n        square_loss = loss.pow(2)\n        return square_loss / square_loss.sum()\n\n    @staticmethod\n    def sample_input(\n        probabilities: Float[Tensor, Axis.BATCH],\n        input_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        num_samples: int,\n    ) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Sample an input vector based on the provided probabilities.\n\n        Example:\n            &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n            &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n            ...     probabilities, input_activations, 2\n            ... )\n            &gt;&gt;&gt; sampled_input.tolist()\n            [[5.0, 6.0], [3.0, 4.0]]\n\n        Args:\n            probabilities: Probabilities for each input.\n            input_activations: Input activation vectors.\n            num_samples: Number of samples to take (number of dead neurons).\n\n        Returns:\n            Sampled input activation vector.\n\n        Raises:\n            ValueError: If the number of samples is greater than the number of input activations.\n        \"\"\"\n        if num_samples &gt; len(input_activations):\n            exception_message = (\n                f\"Cannot sample {num_samples} inputs from \"\n                f\"{len(input_activations)} input activations.\"\n            )\n            raise ValueError(exception_message)\n\n        if num_samples == 0:\n            return torch.empty(\n                (0, input_activations.shape[-1]),\n                dtype=input_activations.dtype,\n                device=input_activations.device,\n            ).to(input_activations.device)\n\n        sample_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n            probabilities, num_samples=num_samples\n        )\n        return input_activations[sample_indices, :]\n\n    @staticmethod\n    def renormalize_and_scale(\n        sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n        neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n        encoder_weight: Float[\n            Parameter, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n        Renormalize the input vector to equal the average norm of the encoder weights for alive\n        neurons times 0.2.\n\n        Example:\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n            &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n            ...     sampled_input,\n            ...     neuron_activity,\n            ...     encoder_weight\n            ... )\n            &gt;&gt;&gt; rescaled_input.round(decimals=1)\n            tensor([[0.2000, 0.2000]])\n\n        Args:\n            sampled_input: Tensor of the sampled input activation.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            encoder_weight: Tensor of encoder weights.\n\n        Returns:\n            Rescaled sampled input.\n\n        Raises:\n            ValueError: If there are no alive neurons.\n        \"\"\"\n        alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n        # Check there is at least one alive neuron\n        if not torch.any(alive_neuron_mask):\n            error_message = \"No alive neurons found.\"\n            raise ValueError(error_message)\n\n        # Handle no dead neurons\n        n_dead_neurons = len(sampled_input)\n        if n_dead_neurons == 0:\n            return torch.empty(\n                (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n            )\n\n        # Calculate the average norm of the encoder weights for alive neurons.\n        detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n        alive_encoder_weights: Float[\n            Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = detached_encoder_weight[alive_neuron_mask, :]\n        average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n            dim=-1\n        ).mean()\n\n        # Renormalize the input vector to equal the average norm of the encoder weights for alive\n        # neurons times 0.2.\n        renormalized_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n        return renormalized_input * (average_alive_norm * 0.2)\n\n    def resample_dead_neurons(\n        self,\n        activation_store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n        \"\"\"\n        with torch.no_grad():\n            dead_neuron_indices = self._get_dead_neuron_indices()\n\n            # Compute the loss for the current model on a random subset of inputs and get the\n            # activations.\n            loss, input_activations = self.compute_loss_and_get_activations(\n                store=activation_store,\n                autoencoder=autoencoder,\n                loss_fn=loss_fn,\n                train_batch_size=train_batch_size,\n            )\n\n            # Assign each input vector a probability of being picked that is proportional to the\n            # square of the autoencoder's loss on that input.\n            sample_probabilities: Float[Tensor, Axis.BATCH] = self.assign_sampling_probabilities(\n                loss\n            )\n\n            # For each dead neuron sample an input according to these probabilities.\n            sampled_input: Float[\n                Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n            ] = self.sample_input(sample_probabilities, input_activations, len(dead_neuron_indices))\n\n            # Renormalize each input vector to have unit L2 norm and set this to be the dictionary\n            # vector for the dead autoencoder neuron.\n            renormalized_input: Float[\n                Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n            ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n            dead_decoder_weight_updates = rearrange(\n                renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n            )\n\n            # For the corresponding encoder vector, renormalize the input vector to equal the\n            # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n            # encoder bias element to zero.\n            encoder_weight: Float[\n                Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n            ] = autoencoder.encoder.weight\n            rescaled_sampled_input = self.renormalize_and_scale(\n                sampled_input=sampled_input,\n                neuron_activity=self._collated_neuron_activity,\n                encoder_weight=encoder_weight,\n            )\n            dead_encoder_bias_updates = torch.zeros_like(\n                dead_neuron_indices,\n                dtype=dead_decoder_weight_updates.dtype,\n                device=dead_decoder_weight_updates.device,\n            )\n\n            return ParameterUpdateResults(\n                dead_neuron_indices=dead_neuron_indices,\n                dead_encoder_weight_updates=rescaled_sampled_input,\n                dead_encoder_bias_updates=dead_encoder_bias_updates,\n                dead_decoder_weight_updates=dead_decoder_weight_updates,\n            )\n\n    def step_resampler(\n        self,\n        batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n        activation_store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults | None:\n        \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n        Args:\n            batch_neuron_activity: Number of times each neuron fired in the current batch.\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Parameter update results if resampled, else None.\n        \"\"\"\n        # Update the counter\n        self._activations_seen_since_last_resample += len(activation_store)\n\n        if self._number_times_resampled &lt; self._max_n_resamples:\n            # Collate neuron activity, if in the data collection window. For example in the\n            # Anthropic Towards Monosemanticity paper, the window started collecting at 100m\n            # activations and stopped at 200m (and then repeated this again a few times until the\n            # max times to resample was hit).\n            if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_start:\n                detached_neuron_activity = batch_neuron_activity.detach().cpu()\n                self._collated_neuron_activity.add_(detached_neuron_activity)\n                self._n_activations_collated_since_last_resample += train_batch_size\n\n            # Check if we should resample.\n            if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_end:\n                # Get resampled dictionary vectors\n                resample_res = self.resample_dead_neurons(\n                    activation_store=activation_store,\n                    autoencoder=autoencoder,\n                    loss_fn=loss_fn,\n                    train_batch_size=train_batch_size,\n                )\n\n                # Update counters\n                self._activations_seen_since_last_resample = 0\n                self._n_activations_collated_since_last_resample = 0\n                self._number_times_resampled += 1\n\n                # Reset the collated neuron activity\n                self._collated_neuron_activity.zero_()\n\n                return resample_res\n\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the activation resampler.\"\"\"\n        return (\n            f\"ActivationResampler(\"\n            f\"neuron_activity_window_start={self.neuron_activity_window_end}, \"\n            f\"neuron_activity_window_end={self.neuron_activity_window_end}, \"\n            f\"max_resamples={self._max_n_resamples}, \"\n            f\"resample_dataset_size={self._resample_dataset_size}, \"\n            f\"dead_neuron_threshold={self._threshold_is_dead_portion_fires})\"\n        )\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.neuron_activity_window_end","title":"<code>neuron_activity_window_end: int = resample_interval</code>  <code>instance-attribute</code>","text":"<p>End of the window for collecting neuron activity.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.neuron_activity_window_start","title":"<code>neuron_activity_window_start: int = resample_interval - n_activations_activity_collate</code>  <code>instance-attribute</code>","text":"<p>Start of the window for collecting neuron activity.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.__init__","title":"<code>__init__(n_learned_features, resample_interval=200000000, max_n_resamples=4, n_activations_activity_collate=100000000, resample_dataset_size=819200, threshold_is_dead_portion_fires=0.0)</code>","text":"<p>Initialize the activation resampler.</p> <p>Defaults to values used in the Anthropic Towards Monosemanticity paper.</p> <p>Parameters:</p> Name Type Description Default <code>n_learned_features</code> <code>int</code> <p>Number of learned features</p> required <code>resample_interval</code> <code>int</code> <p>Interval in number of autoencoder input activation vectors trained on, before resampling.</p> <code>200000000</code> <code>max_n_resamples</code> <code>int</code> <p>Maximum number of resamples to perform throughout the entire pipeline. Set to inf if you want to have no limit.</p> <code>4</code> <code>n_activations_activity_collate</code> <code>int</code> <p>Number of autoencoder learned activation vectors to collate before resampling (the activation resampler will start collecting on vector \\(\\text{resample_interval} - \\text{n_steps_collate}\\)).</p> <code>100000000</code> <code>resample_dataset_size</code> <code>int</code> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p> <code>819200</code> <code>threshold_is_dead_portion_fires</code> <code>float</code> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the arguments are invalid (e.g. negative integers).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def __init__(\n    self,\n    n_learned_features: int,\n    resample_interval: int = 200_000_000,\n    max_n_resamples: int = 4,\n    n_activations_activity_collate: int = 100_000_000,\n    resample_dataset_size: int = 819_200,\n    threshold_is_dead_portion_fires: float = 0.0,\n) -&gt; None:\n    r\"\"\"Initialize the activation resampler.\n\n    Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n    Args:\n        n_learned_features: Number of learned features\n        resample_interval: Interval in number of autoencoder input activation vectors trained\n            on, before resampling.\n        max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n            Set to inf if you want to have no limit.\n        n_activations_activity_collate: Number of autoencoder learned activation vectors to\n            collate before resampling (the activation resampler will start collecting on vector\n            $\\text{resample_interval} - \\text{n_steps_collate}$).\n        resample_dataset_size: Number of autoencoder input activations to use for calculating\n            the loss, as part of the resampling process to create the reset neuron weights.\n        threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n            \"fired\" in less than this portion of the collated sample).\n\n    Raises:\n        ValueError: If any of the arguments are invalid (e.g. negative integers).\n    \"\"\"\n    if n_activations_activity_collate &lt;= 0:\n        error_message = \"Number of steps to collate must be greater than 0.\"\n        raise ValueError(error_message)\n\n    if n_activations_activity_collate &gt; resample_interval:\n        error_message = (\n            \"Number of steps to collate must be less than or equal to the resample interval.\"\n        )\n        raise ValueError(error_message)\n\n    if threshold_is_dead_portion_fires &lt; 0 or threshold_is_dead_portion_fires &gt; 1:\n        error_message = (\n            \"Threshold portion of times that a dead neuron fires, must be between 0 and 1.\"\n        )\n        raise ValueError(error_message)\n\n    if max_n_resamples &lt; 0:\n        error_message = (\n            \"Maximum number of resamples must be greater than 0. For unlimited, use inf.\"\n        )\n        raise ValueError(error_message)\n\n    if resample_dataset_size &lt; 0:\n        error_message = \"Resample dataset size must be greater than 0.\"\n        raise ValueError(error_message)\n\n    super().__init__()\n    self.neuron_activity_window_end = resample_interval\n    self.neuron_activity_window_start = resample_interval - n_activations_activity_collate\n    self._max_n_resamples = max_n_resamples\n    self._collated_neuron_activity = torch.zeros(n_learned_features, dtype=torch.int64)\n    self._resample_dataset_size = resample_dataset_size\n    self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the activation resampler.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the activation resampler.\"\"\"\n    return (\n        f\"ActivationResampler(\"\n        f\"neuron_activity_window_start={self.neuron_activity_window_end}, \"\n        f\"neuron_activity_window_end={self.neuron_activity_window_end}, \"\n        f\"max_resamples={self._max_n_resamples}, \"\n        f\"resample_dataset_size={self._resample_dataset_size}, \"\n        f\"dead_neuron_threshold={self._threshold_is_dead_portion_fires})\"\n    )\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.assign_sampling_probabilities","title":"<code>assign_sampling_probabilities(loss)</code>  <code>staticmethod</code>","text":"<p>Assign the sampling probabilities for each input activations vector.</p> <p>Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input.</p> Example <p>loss = torch.tensor([1.0, 2.0, 3.0]) ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000])</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Float[Tensor, BATCH]</code> <p>Loss per item.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>A tensor of probabilities for each item.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef assign_sampling_probabilities(loss: Float[Tensor, Axis.BATCH]) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Assign the sampling probabilities for each input activations vector.\n\n    Assign each input vector a probability of being picked that is proportional to the square of\n    the autoencoder's loss on that input.\n\n    Example:\n        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n        tensor([0.1000, 0.3000, 0.6000])\n\n    Args:\n        loss: Loss per item.\n\n    Returns:\n        A tensor of probabilities for each item.\n    \"\"\"\n    square_loss = loss.pow(2)\n    return square_loss / square_loss.sum()\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.compute_loss_and_get_activations","title":"<code>compute_loss_and_get_activations(store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Compute the loss on a random subset of inputs.</p> Motivation <p>Helps find input vectors that have high loss, so that we can resample dead neurons in a way that improves performance on these specific input vectors.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>tuple[Float[Tensor, BATCH], Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]]</code> <p>A tuple of loss per item, and all input activations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of items in the store is less than the number of inputs</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def compute_loss_and_get_activations(\n    self,\n    store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; tuple[\n    Float[Tensor, Axis.BATCH], Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n]:\n    \"\"\"Compute the loss on a random subset of inputs.\n\n    Motivation:\n        Helps find input vectors that have high loss, so that we can resample dead neurons in a\n        way that improves performance on these specific input vectors.\n\n    Args:\n        store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        A tuple of loss per item, and all input activations.\n\n    Raises:\n        ValueError: If the number of items in the store is less than the number of inputs\n    \"\"\"\n    with torch.no_grad():\n        loss_batches: list[Float[Tensor, Axis.BATCH]] = []\n        input_activations_batches: list[\n            Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n        ] = []\n        dataloader = DataLoader(store, batch_size=train_batch_size)\n        num_inputs = self._resample_dataset_size\n        n_batches_required: int = num_inputs // train_batch_size\n        model_device: torch.device = get_model_device(autoencoder)\n\n        for batch_idx, batch in enumerate(iter(dataloader)):\n            input_activations_batches.append(batch)\n            source_activations = batch.to(model_device)\n            learned_activations, reconstructed_activations = autoencoder(source_activations)\n            loss_batches.append(\n                loss_fn.forward(\n                    source_activations, learned_activations, reconstructed_activations\n                )\n            )\n            if batch_idx &gt;= n_batches_required:\n                break\n\n        loss_result = torch.cat(loss_batches).to(model_device)\n        input_activations = torch.cat(input_activations_batches).to(model_device)\n\n        # Check we generated enough data\n        if len(loss_result) &lt; num_inputs:\n            error_message = (\n                f\"Cannot get {num_inputs} items from the store, \"\n                f\"as only {len(loss_result)} were available.\"\n            )\n            raise ValueError(error_message)\n\n        return loss_result, input_activations\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.renormalize_and_scale","title":"<code>renormalize_and_scale(sampled_input, neuron_activity, encoder_weight)</code>  <code>staticmethod</code>","text":"<p>Renormalize and scale the resampled dictionary vectors.</p> <p>Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2.</p> Example <p>_seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) encoder_weight = Parameter(torch.ones((6, 2))) rescaled_input = ActivationResampler.renormalize_and_scale( ...     sampled_input, ...     neuron_activity, ...     encoder_weight ... ) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]])</p> <p>Parameters:</p> Name Type Description Default <code>sampled_input</code> <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of the sampled input activation.</p> required <code>neuron_activity</code> <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>encoder_weight</code> <code>Float[Parameter, names(LEARNT_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of encoder weights.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Rescaled sampled input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no alive neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef renormalize_and_scale(\n    sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n    neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n    encoder_weight: Float[\n        Parameter, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n    Renormalize the input vector to equal the average norm of the encoder weights for alive\n    neurons times 0.2.\n\n    Example:\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n        &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n        ...     sampled_input,\n        ...     neuron_activity,\n        ...     encoder_weight\n        ... )\n        &gt;&gt;&gt; rescaled_input.round(decimals=1)\n        tensor([[0.2000, 0.2000]])\n\n    Args:\n        sampled_input: Tensor of the sampled input activation.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        encoder_weight: Tensor of encoder weights.\n\n    Returns:\n        Rescaled sampled input.\n\n    Raises:\n        ValueError: If there are no alive neurons.\n    \"\"\"\n    alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n    # Check there is at least one alive neuron\n    if not torch.any(alive_neuron_mask):\n        error_message = \"No alive neurons found.\"\n        raise ValueError(error_message)\n\n    # Handle no dead neurons\n    n_dead_neurons = len(sampled_input)\n    if n_dead_neurons == 0:\n        return torch.empty(\n            (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n        )\n\n    # Calculate the average norm of the encoder weights for alive neurons.\n    detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n    alive_encoder_weights: Float[\n        Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = detached_encoder_weight[alive_neuron_mask, :]\n    average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n        dim=-1\n    ).mean()\n\n    # Renormalize the input vector to equal the average norm of the encoder weights for alive\n    # neurons times 0.2.\n    renormalized_input: Float[\n        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n    return renormalized_input * (average_alive_norm * 0.2)\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.resample_dead_neurons","title":"<code>resample_dead_neurons(activation_store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults</code> <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def resample_dead_neurons(\n    self,\n    activation_store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n    \"\"\"\n    with torch.no_grad():\n        dead_neuron_indices = self._get_dead_neuron_indices()\n\n        # Compute the loss for the current model on a random subset of inputs and get the\n        # activations.\n        loss, input_activations = self.compute_loss_and_get_activations(\n            store=activation_store,\n            autoencoder=autoencoder,\n            loss_fn=loss_fn,\n            train_batch_size=train_batch_size,\n        )\n\n        # Assign each input vector a probability of being picked that is proportional to the\n        # square of the autoencoder's loss on that input.\n        sample_probabilities: Float[Tensor, Axis.BATCH] = self.assign_sampling_probabilities(\n            loss\n        )\n\n        # For each dead neuron sample an input according to these probabilities.\n        sampled_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = self.sample_input(sample_probabilities, input_activations, len(dead_neuron_indices))\n\n        # Renormalize each input vector to have unit L2 norm and set this to be the dictionary\n        # vector for the dead autoencoder neuron.\n        renormalized_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n        dead_decoder_weight_updates = rearrange(\n            renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n        )\n\n        # For the corresponding encoder vector, renormalize the input vector to equal the\n        # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n        # encoder bias element to zero.\n        encoder_weight: Float[\n            Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = autoencoder.encoder.weight\n        rescaled_sampled_input = self.renormalize_and_scale(\n            sampled_input=sampled_input,\n            neuron_activity=self._collated_neuron_activity,\n            encoder_weight=encoder_weight,\n        )\n        dead_encoder_bias_updates = torch.zeros_like(\n            dead_neuron_indices,\n            dtype=dead_decoder_weight_updates.dtype,\n            device=dead_decoder_weight_updates.device,\n        )\n\n        return ParameterUpdateResults(\n            dead_neuron_indices=dead_neuron_indices,\n            dead_encoder_weight_updates=rescaled_sampled_input,\n            dead_encoder_bias_updates=dead_encoder_bias_updates,\n            dead_decoder_weight_updates=dead_decoder_weight_updates,\n        )\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.sample_input","title":"<code>sample_input(probabilities, input_activations, num_samples)</code>  <code>staticmethod</code>","text":"<p>Sample an input vector based on the provided probabilities.</p> Example <p>probabilities = torch.tensor([0.1, 0.2, 0.7]) input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = ActivationResampler.sample_input( ...     probabilities, input_activations, 2 ... ) sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]]</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>Float[Tensor, BATCH]</code> <p>Probabilities for each input.</p> required <code>input_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Input activation vectors.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to take (number of dead neurons).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Sampled input activation vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of samples is greater than the number of input activations.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef sample_input(\n    probabilities: Float[Tensor, Axis.BATCH],\n    input_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    num_samples: int,\n) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Sample an input vector based on the provided probabilities.\n\n    Example:\n        &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n        &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n        ...     probabilities, input_activations, 2\n        ... )\n        &gt;&gt;&gt; sampled_input.tolist()\n        [[5.0, 6.0], [3.0, 4.0]]\n\n    Args:\n        probabilities: Probabilities for each input.\n        input_activations: Input activation vectors.\n        num_samples: Number of samples to take (number of dead neurons).\n\n    Returns:\n        Sampled input activation vector.\n\n    Raises:\n        ValueError: If the number of samples is greater than the number of input activations.\n    \"\"\"\n    if num_samples &gt; len(input_activations):\n        exception_message = (\n            f\"Cannot sample {num_samples} inputs from \"\n            f\"{len(input_activations)} input activations.\"\n        )\n        raise ValueError(exception_message)\n\n    if num_samples == 0:\n        return torch.empty(\n            (0, input_activations.shape[-1]),\n            dtype=input_activations.dtype,\n            device=input_activations.device,\n        ).to(input_activations.device)\n\n    sample_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n        probabilities, num_samples=num_samples\n    )\n    return input_activations[sample_indices, :]\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.step_resampler","title":"<code>step_resampler(batch_neuron_activity, activation_store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Step the resampler, collating neuron activity and resampling if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>batch_neuron_activity</code> <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Number of times each neuron fired in the current batch.</p> required <code>activation_store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults | None</code> <p>Parameter update results if resampled, else None.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def step_resampler(\n    self,\n    batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE],\n    activation_store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults | None:\n    \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n    Args:\n        batch_neuron_activity: Number of times each neuron fired in the current batch.\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Parameter update results if resampled, else None.\n    \"\"\"\n    # Update the counter\n    self._activations_seen_since_last_resample += len(activation_store)\n\n    if self._number_times_resampled &lt; self._max_n_resamples:\n        # Collate neuron activity, if in the data collection window. For example in the\n        # Anthropic Towards Monosemanticity paper, the window started collecting at 100m\n        # activations and stopped at 200m (and then repeated this again a few times until the\n        # max times to resample was hit).\n        if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_start:\n            detached_neuron_activity = batch_neuron_activity.detach().cpu()\n            self._collated_neuron_activity.add_(detached_neuron_activity)\n            self._n_activations_collated_since_last_resample += train_batch_size\n\n        # Check if we should resample.\n        if self._activations_seen_since_last_resample &gt;= self.neuron_activity_window_end:\n            # Get resampled dictionary vectors\n            resample_res = self.resample_dead_neurons(\n                activation_store=activation_store,\n                autoencoder=autoencoder,\n                loss_fn=loss_fn,\n                train_batch_size=train_batch_size,\n            )\n\n            # Update counters\n            self._activations_seen_since_last_resample = 0\n            self._n_activations_collated_since_last_resample = 0\n            self._number_times_resampled += 1\n\n            # Reset the collated neuron activity\n            self._collated_neuron_activity.zero_()\n\n            return resample_res\n\n    return None\n</code></pre>"},{"location":"reference/activation_store/","title":"Activation Stores","text":"<p>Activation Stores.</p>"},{"location":"reference/activation_store/base_store/","title":"Activation Store Base Class","text":"<p>Activation Store Base Class.</p>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore","title":"<code>ActivationStore</code>","text":"<p>             Bases: <code>Dataset[Float[Tensor, INPUT_OUTPUT_FEATURE]]</code>, <code>ABC</code></p> <p>Activation Store Abstract Class.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide an activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a <code>torch.utils.data.DataLoader</code> to iterate over the dataset.</p> <p>Extend this class if you want to create a new activation store (noting you also need to create <code>__getitem__</code> and <code>__len__</code> methods from the underlying <code>torch.utils.data.Dataset</code> class).</p> <p>Example:</p> <p>import torch class MyActivationStore(ActivationStore): ...     def init(self): ...         super().init() ...         self._data = [] # In this example, we just store in a list ... ...     def append(self, item) -&gt; None: ...         self._data.append(item) ... ...     def extend(self, batch): ...         self._data.extend(batch) ... ...     def empty(self): ...         self._data = [] ... ...     def getitem(self, index: int): ...         return self._data[index] ... ...     def len(self) -&gt; int: ...         return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>class ActivationStore(Dataset[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]], ABC):\n    \"\"\"Activation Store Abstract Class.\n\n    Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional\n    :meth:`append` and :meth:`extend` methods (the latter of which should typically be\n    non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader`\n    to iterate over the dataset.\n\n    Extend this class if you want to create a new activation store (noting you also need to create\n    `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class).\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; class MyActivationStore(ActivationStore):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self._data = [] # In this example, we just store in a list\n    ...\n    ...     def append(self, item) -&gt; None:\n    ...         self._data.append(item)\n    ...\n    ...     def extend(self, batch):\n    ...         self._data.extend(batch)\n    ...\n    ...     def empty(self):\n    ...         self._data = []\n    ...\n    ...     def __getitem__(self, index: int):\n    ...         return self._data[index]\n    ...\n    ...     def __len__(self) -&gt; int:\n    ...         return len(self._data)\n    ...\n    &gt;&gt;&gt; store = MyActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n\n    @abstractmethod\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\"\"\"\n\n    @abstractmethod\n    def extend(\n        self, batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\"\"\"\n\n    @abstractmethod\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Get the Length of the Store.\"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get an Item from the Store.\"\"\"\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Optional shuffle method.\"\"\"\n\n    @final\n    def fill_with_test_data(\n        self, num_batches: int = 16, batch_size: int = 16, input_features: int = 256\n    ) -&gt; None:\n        \"\"\"Fill the store with test data.\n\n        For use when testing your code, to ensure it works with a real activation store.\n\n        Warning:\n            You may want to use `torch.seed(0)` to make the random data deterministic, if your test\n            requires inspecting the data itself.\n\n        Example:\n            &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=16*16, num_neurons=256)\n            &gt;&gt;&gt; store.fill_with_test_data()\n            &gt;&gt;&gt; len(store)\n            256\n            &gt;&gt;&gt; store[0].shape\n            torch.Size([256])\n\n        Args:\n            num_batches: Number of batches to fill the store with.\n            batch_size: Number of items per batch.\n            input_features: Number of input features per item.\n        \"\"\"\n        for _ in range(num_batches):\n            sample = torch.rand((batch_size, input_features))\n            self.extend(sample)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>Get an Item from the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get an Item from the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Get the Length of the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Get the Length of the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.append","title":"<code>append(item)</code>  <code>abstractmethod</code>","text":"<p>Add a Single Item to the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.empty","title":"<code>empty()</code>  <code>abstractmethod</code>","text":"<p>Empty the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef empty(self) -&gt; None:\n    \"\"\"Empty the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.extend","title":"<code>extend(batch)</code>  <code>abstractmethod</code>","text":"<p>Add a Batch to the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef extend(\n    self, batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.fill_with_test_data","title":"<code>fill_with_test_data(num_batches=16, batch_size=16, input_features=256)</code>","text":"<p>Fill the store with test data.</p> <p>For use when testing your code, to ensure it works with a real activation store.</p> Warning <p>You may want to use <code>torch.seed(0)</code> to make the random data deterministic, if your test requires inspecting the data itself.</p> Example <p>from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=16*16, num_neurons=256) store.fill_with_test_data() len(store) 256 store[0].shape torch.Size([256])</p> <p>Parameters:</p> Name Type Description Default <code>num_batches</code> <code>int</code> <p>Number of batches to fill the store with.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>Number of items per batch.</p> <code>16</code> <code>input_features</code> <code>int</code> <p>Number of input features per item.</p> <code>256</code> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@final\ndef fill_with_test_data(\n    self, num_batches: int = 16, batch_size: int = 16, input_features: int = 256\n) -&gt; None:\n    \"\"\"Fill the store with test data.\n\n    For use when testing your code, to ensure it works with a real activation store.\n\n    Warning:\n        You may want to use `torch.seed(0)` to make the random data deterministic, if your test\n        requires inspecting the data itself.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=16*16, num_neurons=256)\n        &gt;&gt;&gt; store.fill_with_test_data()\n        &gt;&gt;&gt; len(store)\n        256\n        &gt;&gt;&gt; store[0].shape\n        torch.Size([256])\n\n    Args:\n        num_batches: Number of batches to fill the store with.\n        batch_size: Number of items per batch.\n        input_features: Number of input features per item.\n    \"\"\"\n    for _ in range(num_batches):\n        sample = torch.rand((batch_size, input_features))\n        self.extend(sample)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Optional shuffle method.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Optional shuffle method.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError","title":"<code>StoreFullError</code>","text":"<p>             Bases: <code>IndexError</code></p> <p>Exception raised when the activation store is full.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>class StoreFullError(IndexError):\n    \"\"\"Exception raised when the activation store is full.\"\"\"\n\n    def __init__(self, message: str = \"Activation store is full\"):\n        \"\"\"Initialise the exception.\n\n        Args:\n            message: Override the default message.\n        \"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError.__init__","title":"<code>__init__(message='Activation store is full')</code>","text":"<p>Initialise the exception.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Override the default message.</p> <code>'Activation store is full'</code> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>def __init__(self, message: str = \"Activation store is full\"):\n    \"\"\"Initialise the exception.\n\n    Args:\n        message: Override the default message.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/activation_store/disk_store/","title":"Disk Activation Store","text":"<p>Disk Activation Store.</p>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore","title":"<code>DiskActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Disk Activation Store.</p> <p>Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches.</p> <p>Multiprocess safe (supports writing from multiple GPU workers).</p> <p>Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set <code>empty_dir</code> to <code>True</code>.</p> <p>Note also that :meth:<code>close</code> must be called to ensure all activation vectors are written to disk after the last batch has been added to the store.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>class DiskActivationStore(ActivationStore):\n    \"\"\"Disk Activation Store.\n\n    Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up\n    activation vectors and then write them to the disk in batches.\n\n    Multiprocess safe (supports writing from multiple GPU workers).\n\n    Warning:\n    Unless you want to keep and use existing .pt files in the storage directory when initialized,\n    set `empty_dir` to `True`.\n\n    Note also that :meth:`close` must be called to ensure all activation vectors are written to disk\n    after the last batch has been added to the store.\n    \"\"\"\n\n    _storage_path: Path\n    \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\"\n\n    _cache: ListProxy\n    \"\"\"Cache for Activation Vectors.\n\n    Activation vectors are buffered in memory until the cache is full, at which point they are\n    written to disk.\n    \"\"\"\n\n    _cache_lock: Lock\n    \"\"\"Lock for the Cache.\"\"\"\n\n    _max_cache_size: int\n    \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\"\n\n    _thread_pool: ThreadPoolExecutor\n    \"\"\"Threadpool for non-blocking writes to the file system.\"\"\"\n\n    _disk_n_activation_vectors: ValueProxy[int]\n    \"\"\"Length of the Store (on disk).\n\n    Minus 1 signifies not calculated yet.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n        max_cache_size: int = 10_000,\n        num_workers: int = 6,\n        *,\n        empty_dir: bool = False,\n    ):\n        \"\"\"Initialize the Disk Activation Store.\n\n        Args:\n            storage_path: Path to the directory where the activation vectors will be stored.\n            max_cache_size: The maximum number of activation vectors to cache in memory before\n                writing to disk. Note this is only followed approximately.\n            num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n                that the model can keep running whilst it writes the previous activations to disk).\n                This should be less than the number of CPU cores available. You don't need multiple\n                GPUs to take advantage of this feature.\n            empty_dir: Whether to empty the directory before writing. Generally you want to set this\n                to `True` as otherwise the directory may contain stale activation vectors from\n                previous runs.\n        \"\"\"\n        super().__init__()\n\n        # Setup the storage directory\n        self._storage_path = storage_path\n        self._storage_path.mkdir(parents=True, exist_ok=True)\n\n        # Setup the Cache\n        manager = Manager()\n        self._cache = manager.list()\n        self._max_cache_size = max_cache_size\n        self._cache_lock = manager.Lock()\n        self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n        # Empty the directory if needed\n        if empty_dir:\n            self.empty()\n\n        # Create a threadpool for non-blocking writes to the cache\n        self._thread_pool = ThreadPoolExecutor(num_workers)\n\n    def _write_to_disk(self, *, wait_for_max: bool = False) -&gt; None:\n        \"\"\"Write the contents of the queue to disk.\n\n        Args:\n            wait_for_max: Whether to wait until the cache is full before writing to disk.\n        \"\"\"\n        with self._cache_lock:\n            # Check we have enough items\n            if len(self._cache) == 0:\n                return\n\n            size_to_get = min(self._max_cache_size, len(self._cache))\n            if wait_for_max and size_to_get &lt; self._max_cache_size:\n                return\n\n            # Get the activations from the cache and delete them\n            activations = self._cache[0:size_to_get]\n            del self._cache[0:size_to_get]\n\n            # Update the length cache\n            if self._disk_n_activation_vectors.value != -1:\n                self._disk_n_activation_vectors.value += len(activations)\n\n        stacked_activations = torch.stack(activations)\n\n        filename = f\"{self.__len__}.pt\"\n        torch.save(stacked_activations, self._storage_path / filename)\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        Args:\n            item: Activation vector to add to the store.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        with self._cache_lock:\n            self._cache.append(item)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def extend(\n        self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n        &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        10\n\n        Args:\n            batch: Batch of activation vectors to add to the store.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        items: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] = resize_to_list_vectors(batch)\n\n        with self._cache_lock:\n            self._cache.extend(items)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        This should be called after the last batch has been added to the store. It will wait for\n        all activation vectors to be written to disk.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; print(len(store))\n        1\n        \"\"\"\n        while len(self._cache) &gt; 0:\n            self._write_to_disk()\n\n    @property\n    def _all_filenames(self) -&gt; list[Path]:\n        \"\"\"Return a List of All Activation Vector Filenames.\"\"\"\n        return list(self._storage_path.glob(\"*.pt\"))\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\n\n        Warning:\n        This will delete all .pt files in the top level of the storage directory.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; print(len(store))\n        0\n        \"\"\"\n        for file in self._all_filenames:\n            file.unlink()\n        self._disk_n_activation_vectors.value = 0\n\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get Item Dunder Method.\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        # Find the file containing the activation vector\n        file_index = index // self._max_cache_size\n        file = self._storage_path / f\"{file_index}.pt\"\n\n        # Load the file and return the activation vector\n        activation_vectors = torch.load(file)\n        return activation_vectors[index % self._max_cache_size]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n            &gt;&gt;&gt; print(len(store))\n            0\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        # Calculate the length if not cached\n        if self._disk_n_activation_vectors.value == -1:\n            cache_size: int = 0\n            for file in self._all_filenames:\n                cache_size += len(torch.load(file))\n            self._disk_n_activation_vectors.value = cache_size\n\n        return self._disk_n_activation_vectors.value\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        # Shutdown the thread pool after everything is complete\n        self._thread_pool.shutdown(wait=True, cancel_futures=False)\n        self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    # Shutdown the thread pool after everything is complete\n    self._thread_pool.shutdown(wait=True, cancel_futures=False)\n    self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get Item Dunder Method.\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    # Find the file containing the activation vector\n    file_index = index // self._max_cache_size\n    file = self._storage_path / f\"{file_index}.pt\"\n\n    # Load the file and return the activation vector\n    activation_vectors = torch.load(file)\n    return activation_vectors[index % self._max_cache_size]\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__init__","title":"<code>__init__(storage_path=DEFAULT_DISK_ACTIVATION_STORE_PATH, max_cache_size=10000, num_workers=6, *, empty_dir=False)</code>","text":"<p>Initialize the Disk Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>storage_path</code> <code>Path</code> <p>Path to the directory where the activation vectors will be stored.</p> <code>DEFAULT_DISK_ACTIVATION_STORE_PATH</code> <code>max_cache_size</code> <code>int</code> <p>The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately.</p> <code>10000</code> <code>num_workers</code> <code>int</code> <p>Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature.</p> <code>6</code> <code>empty_dir</code> <code>bool</code> <p>Whether to empty the directory before writing. Generally you want to set this to <code>True</code> as otherwise the directory may contain stale activation vectors from previous runs.</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __init__(\n    self,\n    storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n    max_cache_size: int = 10_000,\n    num_workers: int = 6,\n    *,\n    empty_dir: bool = False,\n):\n    \"\"\"Initialize the Disk Activation Store.\n\n    Args:\n        storage_path: Path to the directory where the activation vectors will be stored.\n        max_cache_size: The maximum number of activation vectors to cache in memory before\n            writing to disk. Note this is only followed approximately.\n        num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n            that the model can keep running whilst it writes the previous activations to disk).\n            This should be less than the number of CPU cores available. You don't need multiple\n            GPUs to take advantage of this feature.\n        empty_dir: Whether to empty the directory before writing. Generally you want to set this\n            to `True` as otherwise the directory may contain stale activation vectors from\n            previous runs.\n    \"\"\"\n    super().__init__()\n\n    # Setup the storage directory\n    self._storage_path = storage_path\n    self._storage_path.mkdir(parents=True, exist_ok=True)\n\n    # Setup the Cache\n    manager = Manager()\n    self._cache = manager.list()\n    self._max_cache_size = max_cache_size\n    self._cache_lock = manager.Lock()\n    self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n    # Empty the directory if needed\n    if empty_dir:\n        self.empty()\n\n    # Create a threadpool for non-blocking writes to the cache\n    self._thread_pool = ThreadPoolExecutor(num_workers)\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> Example <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; print(len(store))\n        0\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    # Calculate the length if not cached\n    if self._disk_n_activation_vectors.value == -1:\n        cache_size: int = 0\n        for file in self._all_filenames:\n            cache_size += len(torch.load(file))\n        self._disk_n_activation_vectors.value = cache_size\n\n    return self._disk_n_activation_vectors.value\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a Single Item to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>Activation vector to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    Args:\n        item: Activation vector to add to the store.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    with self._cache_lock:\n        self._cache.append(item)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the Store.</p> <p>Warning: This will delete all .pt files in the top level of the storage directory.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>store.empty() print(len(store)) 0</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the Store.\n\n    Warning:\n    This will delete all .pt files in the top level of the storage directory.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; print(len(store))\n    0\n    \"\"\"\n    for file in self._all_filenames:\n        file.unlink()\n    self._disk_n_activation_vectors.value = 0\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a Batch to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>Batch of activation vectors to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def extend(\n    self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n    &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    10\n\n    Args:\n        batch: Batch of activation vectors to add to the store.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    items: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] = resize_to_list_vectors(batch)\n\n    with self._cache_lock:\n        self._cache.extend(items)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    This should be called after the last batch has been added to the store. It will wait for\n    all activation vectors to be written to disk.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; store.wait_for_writes_to_complete()\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n    while len(self._cache) &gt; 0:\n        self._write_to_disk()\n</code></pre>"},{"location":"reference/activation_store/list_store/","title":"List Activation Store","text":"<p>List Activation Store.</p>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore","title":"<code>ListActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>List Activation Store.</p> <p>Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance.</p> <p>Multiprocess safe if the <code>multiprocessing_enabled</code> argument is set to <code>True</code>. This works in two ways:</p> <ol> <li>The list of activation vectors is stored in a multiprocessing manager, which allows multiple     processes (typically multiple GPUs) to read/write to the list.</li> <li>The <code>extend</code> method is non-blocking, and uses a threadpool to write to the list in the     background, which allows the main process to continue working even if there is just one GPU.</li> </ol> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Note that the built-in :meth:<code>shuffle</code> method is much faster than using the <code>shuffle</code> argument on <code>torch.utils.data.DataLoader</code>. You should therefore call this method before passing the dataset to the loader and then set the DataLoader <code>shuffle</code> argument to <code>False</code>.</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = ListActivationStore()\n</code></pre> <p>Add a single activation vector to the dataset (this is blocking):</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a batch of activation vectors to the dataset (non-blocking):</p> <pre><code>&gt;&gt;&gt; batch = torch.randn(10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n11\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>class ListActivationStore(ActivationStore):\n    \"\"\"List Activation Store.\n\n    Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick\n    experiments where you don't want to calculate how much memory you need in advance.\n\n    Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two\n    ways:\n\n    1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple\n        processes (typically multiple GPUs) to read/write to the list.\n    2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the\n        background, which allows the main process to continue working even if there is just one GPU.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument\n    on `torch.utils.data.DataLoader`. You should therefore call this method before passing the\n    dataset to the loader and then set the DataLoader `shuffle` argument to `False`.\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n\n    Add a single activation vector to the dataset (this is blocking):\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a batch of activation vectors to the dataset (non-blocking):\n\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        11\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] | ListProxy\n    \"\"\"Underlying List Data Store.\"\"\"\n\n    _device: torch.device | None\n    \"\"\"Device to Store the Activation Vectors On.\"\"\"\n\n    _pool: ProcessPoolExecutor | None = None\n    \"\"\"Multiprocessing Pool.\"\"\"\n\n    _pool_exceptions: ListProxy | list[Exception]\n    \"\"\"Pool Exceptions.\n\n    Used to keep track of exceptions.\n    \"\"\"\n\n    _pool_futures: list[Future]\n    \"\"\"Pool Futures.\n\n    Used to keep track of processes running in the pool.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] | None = None,\n        device: torch.device | None = None,\n        max_workers: int | None = None,\n        *,\n        multiprocessing_enabled: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the List Activation Store.\n\n        Args:\n            data: Data to initialize the dataset with.\n            device: Device to store the activation vectors on.\n            max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n                Default is the number of cores you have.\n            multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n                workers. This creates significant overhead, so you should only enable it if you have\n                multiple GPUs (and experiment with enabling/disabling it).\n        \"\"\"\n        # Default to empty\n        if data is None:\n            data = []\n\n        # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n        # between processes. Otherwise, just use a normal list.\n        if multiprocessing_enabled:\n            self._pool = ProcessPoolExecutor(max_workers=max_workers)\n            manager = Manager()\n            self._data = manager.list(data)\n            self._data.extend(data)\n            self._pool_exceptions = manager.list()\n        else:\n            self._data = data\n            self._pool_exceptions = []\n\n        self._pool_futures = []\n\n        # Device for storing the activation vectors\n        self._device = device\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return len(self._data)\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Returns:\n            The size of the dataset in bytes.\n        \"\"\"\n        # The list of tensors is really a list of pointers to tensors, so we need to account for\n        # this as well as the size of the tensors themselves.\n        list_of_pointers_size = self._data.__sizeof__()\n\n        # Handle 0 items\n        if len(self._data) == 0:\n            return list_of_pointers_size\n\n        # Otherwise, get the size of the first tensor\n        first_tensor = self._data[0]\n        first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n        num_tensors = len(self._data)\n        total_tensors_size = first_tensor_size * num_tensors\n\n        return total_tensors_size + list_of_pointers_size\n\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.append(torch.tensor([3.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; len(store)\n        3\n\n        \"\"\"\n        self.wait_for_writes_to_complete()\n        random.shuffle(self._data)\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n        \"\"\"Append a single item to the dataset.\n\n        Note **append is blocking**. For better performance use extend instead with batches.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        Args:\n            item: The item to append to the dataset.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        self._data.append(item.to(self._device))\n\n    def _extend(\n        self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; None:\n        \"\"\"Extend threadpool method.\n\n        To be called by :meth:`extend`.\n\n        Args:\n            batch: A batch of items to add to the dataset.\n        \"\"\"\n        try:\n            # Unstack to a list of tensors\n            items: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] = resize_to_list_vectors(batch)\n\n            self._data.extend(items)\n        except Exception as e:  # noqa: BLE001\n            self._pool_exceptions.append(e)\n\n    def extend(\n        self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n    ) -&gt; Future | None:\n        \"\"\"Extend the dataset with multiple items (non-blocking).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; batch = torch.randn(10, 100)\n            &gt;&gt;&gt; async_result = store.extend(batch)\n            &gt;&gt;&gt; len(store)\n            10\n\n        Args:\n            batch: A batch of items to add to the dataset.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        # Schedule _extend to run in a separate process\n        if self._pool:\n            future = self._pool.submit(self._extend, batch)\n            self._pool_futures.append(future)\n\n        # Fallback to synchronous execution if not multiprocessing\n        self._extend(batch)\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n            &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n            &gt;&gt;&gt; store.wait_for_writes_to_complete()\n            &gt;&gt;&gt; len(store)\n            3\n\n        Raises:\n            RuntimeError: If any exceptions occurred in the background workers.\n        \"\"\"\n        # Restart the pool\n        if self._pool:\n            for _future in as_completed(self._pool_futures):\n                pass\n            self._pool_futures.clear()\n\n        time.sleep(1)\n\n        if self._pool_exceptions:\n            exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n            msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n            raise RuntimeError(msg)\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the dataset.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; len(store)\n        0\n        \"\"\"\n        self.wait_for_writes_to_complete()\n\n        # Clearing a list like this works for both standard and multiprocessing lists\n        self._data[:] = []\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        if self._pool:\n            self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    if self._pool:\n        self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    return self._data[index]\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__init__","title":"<code>__init__(data=None, device=None, max_workers=None, *, multiprocessing_enabled=False)</code>","text":"<p>Initialize the List Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[Float[Tensor, INPUT_OUTPUT_FEATURE]] | None</code> <p>Data to initialize the dataset with.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> <code>max_workers</code> <code>int | None</code> <p>Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have.</p> <code>None</code> <code>multiprocessing_enabled</code> <code>bool</code> <p>Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it).</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __init__(\n    self,\n    data: list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]] | None = None,\n    device: torch.device | None = None,\n    max_workers: int | None = None,\n    *,\n    multiprocessing_enabled: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the List Activation Store.\n\n    Args:\n        data: Data to initialize the dataset with.\n        device: Device to store the activation vectors on.\n        max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n            Default is the number of cores you have.\n        multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n            workers. This creates significant overhead, so you should only enable it if you have\n            multiple GPUs (and experiment with enabling/disabling it).\n    \"\"\"\n    # Default to empty\n    if data is None:\n        data = []\n\n    # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n    # between processes. Otherwise, just use a normal list.\n    if multiprocessing_enabled:\n        self._pool = ProcessPoolExecutor(max_workers=max_workers)\n        manager = Manager()\n        self._data = manager.list(data)\n        self._data.extend(data)\n        self._pool_exceptions = manager.list()\n    else:\n        self._data = data\n        self._pool_exceptions = []\n\n    self._pool_futures = []\n\n    # Device for storing the activation vectors\n    self._device = device\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the dataset in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Returns:\n        The size of the dataset in bytes.\n    \"\"\"\n    # The list of tensors is really a list of pointers to tensors, so we need to account for\n    # this as well as the size of the tensors themselves.\n    list_of_pointers_size = self._data.__sizeof__()\n\n    # Handle 0 items\n    if len(self._data) == 0:\n        return list_of_pointers_size\n\n    # Otherwise, get the size of the first tensor\n    first_tensor = self._data[0]\n    first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n    num_tensors = len(self._data)\n    total_tensors_size = first_tensor_size * num_tensors\n\n    return total_tensors_size + list_of_pointers_size\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.append","title":"<code>append(item)</code>","text":"<p>Append a single item to the dataset.</p> <p>Note append is blocking. For better performance use extend instead with batches.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The item to append to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; Future | None:\n    \"\"\"Append a single item to the dataset.\n\n    Note **append is blocking**. For better performance use extend instead with batches.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    Args:\n        item: The item to append to the dataset.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    self._data.append(item.to(self._device))\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the dataset.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>store.empty() len(store) 0</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the dataset.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; len(store)\n    0\n    \"\"\"\n    self.wait_for_writes_to_complete()\n\n    # Clearing a list like this works for both standard and multiprocessing lists\n    self._data[:] = []\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Extend the dataset with multiple items (non-blocking).</p> Example <p>import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>A batch of items to add to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def extend(\n    self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]\n) -&gt; Future | None:\n    \"\"\"Extend the dataset with multiple items (non-blocking).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; async_result = store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        10\n\n    Args:\n        batch: A batch of items to add to the dataset.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    # Schedule _extend to run in a separate process\n    if self._pool:\n        future = self._pool.submit(self._extend, batch)\n        self._pool_futures.append(future)\n\n    # Fallback to synchronous execution if not multiprocessing\n    self._extend(batch)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.append(torch.tensor([3.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; len(store)\n    3\n\n    \"\"\"\n    self.wait_for_writes_to_complete()\n    random.shuffle(self._data)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>Wait for any non-blocking writes (e.g. calls to :meth:<code>append</code>) to complete.</p> Example <p>import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any exceptions occurred in the background workers.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n        &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; len(store)\n        3\n\n    Raises:\n        RuntimeError: If any exceptions occurred in the background workers.\n    \"\"\"\n    # Restart the pool\n    if self._pool:\n        for _future in as_completed(self._pool_futures):\n            pass\n        self._pool_futures.clear()\n\n    time.sleep(1)\n\n    if self._pool_exceptions:\n        exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n        msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"reference/activation_store/tensor_store/","title":"Tensor Activation Store","text":"<p>Tensor Activation Store.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore","title":"<code>TensorActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Tensor Activation Store.</p> <p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n</code></pre> <p>Add a single activation vector to the dataset:</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a [batch, pos, neurons] activation tensor to the dataset:</p> <pre><code>&gt;&gt;&gt; store.empty()\n&gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n100\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>class TensorActivationStore(ActivationStore):\n    \"\"\"Tensor Activation Store.\n\n    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation\n    vectors to be stored to be known in advance. Multiprocess safe.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n\n    Add a single activation vector to the dataset:\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a [batch, pos, neurons] activation tensor to the dataset:\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        100\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: Float[Tensor, Axis.names(Axis.ITEMS, Axis.INPUT_OUTPUT_FEATURE)]\n    \"\"\"Underlying Tensor Data Store.\"\"\"\n\n    items_stored: int = 0\n    \"\"\"Number of items stored.\"\"\"\n\n    max_items: int\n    \"\"\"Maximum Number of Items to Store.\"\"\"\n\n    def __init__(\n        self,\n        max_items: int,\n        num_neurons: int,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the Tensor Activation Store.\n\n        Args:\n            max_items: Maximum number of items to store (individual activation vectors)\n            num_neurons: Number of neurons in each activation vector.\n            device: Device to store the activation vectors on.\n        \"\"\"\n        self._data = torch.empty((max_items, num_neurons), device=device)\n        self._max_items = max_items\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return self.items_stored\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n            800\n\n        Returns:\n            The size of the underlying tensor in bytes.\n        \"\"\"\n        return self._data.element_size() * self._data.nelement()\n\n    def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        # Check in range\n        if index &gt;= self.items_stored:\n            msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n            raise IndexError(msg)\n\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n        &gt;&gt;&gt; store.append(torch.tensor([0.]))\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; [store[i].item() for i in range(3)]\n        [0.0, 2.0, 1.0]\n        \"\"\"\n        # Generate a permutation of the indices for the active data\n        perm = torch.randperm(self.items_stored)\n\n        # Use this permutation to shuffle the active data in-place\n        self._data[: self.items_stored] = self._data[perm]\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; None:\n        \"\"\"Add a single item to the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            item: The item to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        if self.items_stored + 1 &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[self.items_stored] = item.to(\n            self._data.device,\n        )\n        self.items_stored += 1\n\n    def extend(self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]) -&gt; None:\n        \"\"\"Add a batch to the store.\n\n        Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n        &gt;&gt;&gt; store.items_stored\n        9\n\n        Args:\n            batch: The batch to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        reshaped: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n        ] = resize_to_single_item_dimension(\n            batch,\n        )\n\n        # Check we have space\n        num_activation_tensors: int = reshaped.shape[0]\n        if self.items_stored + num_activation_tensors &gt; self._max_items:\n            if reshaped.shape[0] &gt; self._max_items:\n                msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                    total maximum in the store of {self._max_items}.\"\n                raise ValueError(msg)\n\n            raise StoreFullError\n\n        self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n            self._data.device\n        )\n        self.items_stored += num_activation_tensors\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; store.items_stored\n        0\n        \"\"\"\n        # We don't need to zero the data, just reset the number of items stored\n        self.items_stored = 0\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.items_stored","title":"<code>items_stored: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of items stored.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.max_items","title":"<code>max_items: int</code>  <code>instance-attribute</code>","text":"<p>Maximum Number of Items to Store.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The activation store item at the given index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    # Check in range\n    if index &gt;= self.items_stored:\n        msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n        raise IndexError(msg)\n\n    return self._data[index]\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__init__","title":"<code>__init__(max_items, num_neurons, device=None)</code>","text":"<p>Initialise the Tensor Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>max_items</code> <code>int</code> <p>Maximum number of items to store (individual activation vectors)</p> required <code>num_neurons</code> <code>int</code> <p>Number of neurons in each activation vector.</p> required <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __init__(\n    self,\n    max_items: int,\n    num_neurons: int,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"Initialise the Tensor Activation Store.\n\n    Args:\n        max_items: Maximum number of items to store (individual activation vectors)\n        num_neurons: Number of neurons in each activation vector.\n        device: Device to store the activation vectors on.\n    \"\"\"\n    self._data = torch.empty((max_items, num_neurons), device=device)\n    self._max_items = max_items\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return self.items_stored\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> Example <p>import torch store = TensorActivationStore(max_items=2, num_neurons=100) store.sizeof() # Pre-allocated tensor of 2x100 800</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the underlying tensor in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n        800\n\n    Returns:\n        The size of the underlying tensor in bytes.\n    \"\"\"\n    return self._data.element_size() * self._data.nelement()\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a single item to the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The item to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]) -&gt; None:\n    \"\"\"Add a single item to the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        item: The item to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    if self.items_stored + 1 &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[self.items_stored] = item.to(\n        self._data.device,\n    )\n    self.items_stored += 1\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; store.items_stored\n    0\n    \"\"\"\n    # We don't need to zero the data, just reset the number of items stored\n    self.items_stored = 0\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a batch to the store.</p> <p>Examples:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2</p> <p>store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>The batch to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def extend(self, batch: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]) -&gt; None:\n    \"\"\"Add a batch to the store.\n\n    Examples:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n    &gt;&gt;&gt; store.items_stored\n    9\n\n    Args:\n        batch: The batch to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    reshaped: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n    ] = resize_to_single_item_dimension(\n        batch,\n    )\n\n    # Check we have space\n    num_activation_tensors: int = reshaped.shape[0]\n    if self.items_stored + num_activation_tensors &gt; self._max_items:\n        if reshaped.shape[0] &gt; self._max_items:\n            msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                total maximum in the store of {self._max_items}.\"\n            raise ValueError(msg)\n\n        raise StoreFullError\n\n    self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n        self._data.device\n    )\n    self.items_stored += num_activation_tensors\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0]</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n    &gt;&gt;&gt; store.append(torch.tensor([0.]))\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; [store[i].item() for i in range(3)]\n    [0.0, 2.0, 1.0]\n    \"\"\"\n    # Generate a permutation of the indices for the active data\n    perm = torch.randperm(self.items_stored)\n\n    # Use this permutation to shuffle the active data in-place\n    self._data[: self.items_stored] = self._data[perm]\n</code></pre>"},{"location":"reference/activation_store/utils/","title":"Activation Store Utils","text":"<p>Activation Store Utils.</p>"},{"location":"reference/activation_store/utils/extend_resize/","title":"Resize Tensors for Extend Methods","text":"<p>Resize Tensors for Extend Methods.</p>"},{"location":"reference/activation_store/utils/extend_resize/#sparse_autoencoder.activation_store.utils.extend_resize.resize_to_list_vectors","title":"<code>resize_to_list_vectors(batched_tensor)</code>","text":"<p>Resize Extend List Vectors.</p> <p>Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a list of vectors each of size [neurons].</p> <p>Examples: With 2 axis (e.g. pos neuron):</p> <p>import torch input = torch.rand(3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '3 items of shape torch.Size([100])'</p> <p>With 3 axis (e.g. batch, pos, neuron):</p> <p>input = torch.randn(3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '9 items of shape torch.Size([100])'</p> <p>With 4 axis (e.g. batch, pos, head_idx, neuron)</p> <p>input = torch.rand(3, 3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '27 items of shape torch.Size([100])'</p> <p>Parameters:</p> Name Type Description Default <code>batched_tensor</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>Input Activation Store Batch</p> required <p>Returns:</p> Type Description <code>list[Float[Tensor, INPUT_OUTPUT_FEATURE]]</code> <p>List of Activation Store Item Vectors</p> Source code in <code>sparse_autoencoder/activation_store/utils/extend_resize.py</code> <pre><code>def resize_to_list_vectors(\n    batched_tensor: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; list[Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]]:\n    \"\"\"Resize Extend List Vectors.\n\n    Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is\n    the neurons dimension), and returns a list of vectors each of size [neurons].\n\n    Examples:\n    With 2 axis (e.g. pos neuron):\n\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; input = torch.rand(3, 100)\n    &gt;&gt;&gt; res = resize_to_list_vectors(input)\n    &gt;&gt;&gt; f\"{len(res)} items of shape {res[0].shape}\"\n    '3 items of shape torch.Size([100])'\n\n    With 3 axis (e.g. batch, pos, neuron):\n\n    &gt;&gt;&gt; input = torch.randn(3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_list_vectors(input)\n    &gt;&gt;&gt; f\"{len(res)} items of shape {res[0].shape}\"\n    '9 items of shape torch.Size([100])'\n\n    With 4 axis (e.g. batch, pos, head_idx, neuron)\n\n    &gt;&gt;&gt; input = torch.rand(3, 3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_list_vectors(input)\n    &gt;&gt;&gt; f\"{len(res)} items of shape {res[0].shape}\"\n    '27 items of shape torch.Size([100])'\n\n    Args:\n        batched_tensor: Input Activation Store Batch\n\n    Returns:\n        List of Activation Store Item Vectors\n    \"\"\"\n    rearranged: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] = rearrange(\n        batched_tensor,\n        \"... neurons -&gt; (...) neurons\",\n    )\n    res = rearranged.unbind(0)\n    return list(res)\n</code></pre>"},{"location":"reference/activation_store/utils/extend_resize/#sparse_autoencoder.activation_store.utils.extend_resize.resize_to_single_item_dimension","title":"<code>resize_to_single_item_dimension(batch_activations)</code>","text":"<p>Resize Extend Single Item Dimension.</p> <p>Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons].</p> <p>Examples: With 2 axis (e.g. pos neuron):</p> <p>import torch input = torch.rand(3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([3, 100])</p> <p>With 3 axis (e.g. batch, pos, neuron):</p> <p>input = torch.randn(3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([9, 100])</p> <p>With 4 axis (e.g. batch, pos, head_idx, neuron)</p> <p>input = torch.rand(3, 3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([27, 100])</p> <p>Parameters:</p> Name Type Description Default <code>batch_activations</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>Input Activation Store Batch</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Single Tensor of Activation Store Items</p> Source code in <code>sparse_autoencoder/activation_store/utils/extend_resize.py</code> <pre><code>def resize_to_single_item_dimension(\n    batch_activations: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Resize Extend Single Item Dimension.\n\n    Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is\n    the neurons dimension), and returns a single tensor of size [item, neurons].\n\n    Examples:\n    With 2 axis (e.g. pos neuron):\n\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; input = torch.rand(3, 100)\n    &gt;&gt;&gt; res = resize_to_single_item_dimension(input)\n    &gt;&gt;&gt; res.shape\n    torch.Size([3, 100])\n\n    With 3 axis (e.g. batch, pos, neuron):\n\n    &gt;&gt;&gt; input = torch.randn(3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_single_item_dimension(input)\n    &gt;&gt;&gt; res.shape\n    torch.Size([9, 100])\n\n    With 4 axis (e.g. batch, pos, head_idx, neuron)\n\n    &gt;&gt;&gt; input = torch.rand(3, 3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_single_item_dimension(input)\n    &gt;&gt;&gt; res.shape\n    torch.Size([27, 100])\n\n    Args:\n        batch_activations: Input Activation Store Batch\n\n    Returns:\n        Single Tensor of Activation Store Items\n    \"\"\"\n    return rearrange(batch_activations, \"... neurons -&gt; (...) neurons\")\n</code></pre>"},{"location":"reference/autoencoder/","title":"Sparse autoencoder model &amp; components","text":"<p>Sparse autoencoder model &amp; components.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/","title":"Abstract Sparse Autoencoder Model","text":"<p>Abstract Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder","title":"<code>AbstractAutoencoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Sparse Autoencoder Model.</p> Warning <p>All components should support an optional component axis, which comes after the batch axis     (as various PyTorch helpers assume the batch axis is the first axis). This means all     parameters must be created with this dimension if n_components is not None. And all type     signatures should allow for it.</p> Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code> <pre><code>class AbstractAutoencoder(Module, ABC):\n    \"\"\"Abstract Sparse Autoencoder Model.\n\n    Warning:\n        All components should support an optional component axis, which comes after the batch axis\n            (as various PyTorch helpers assume the batch axis is the first axis). This means all\n            parameters must be created with this dimension if n_components is not None. And all type\n            signatures should allow for it.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def encoder(self) -&gt; AbstractEncoder:\n        \"\"\"Encoder.\"\"\"\n\n    @property\n    @abstractmethod\n    def decoder(self) -&gt; AbstractDecoder:\n        \"\"\"Decoder.\"\"\"\n\n    @property\n    @abstractmethod\n    def pre_encoder_bias(self) -&gt; AbstractOuterBias:\n        \"\"\"Pre-encoder bias.\"\"\"\n\n    @property\n    @abstractmethod\n    def post_decoder_bias(self) -&gt; AbstractOuterBias:\n        \"\"\"Post-decoder bias.\"\"\"\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return (\n            self.encoder.reset_optimizer_parameter_details\n            + self.decoder.reset_optimizer_parameter_details\n        )\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; tuple[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)],\n    ]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer (or MLP layers) in a\n                transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n\n    @abstractmethod\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.decoder","title":"<code>decoder: AbstractDecoder</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Decoder.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.encoder","title":"<code>encoder: AbstractEncoder</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Encoder.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: AbstractOuterBias</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Post-decoder bias.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: AbstractOuterBias</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Pre-encoder bias.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer (or MLP layers) in a transformer model).</p> required <p>Returns:</p> Type Description <code>tuple[Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)], Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]]</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; tuple[\n    Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n    Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)],\n]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer (or MLP layers) in a\n            transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code> <pre><code>@abstractmethod\ndef reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/model/","title":"The Sparse Autoencoder Model","text":"<p>The Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>AbstractAutoencoder</code></p> <p>Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@final\nclass SparseAutoencoder(AbstractAutoencoder):\n    \"\"\"Sparse Autoencoder Model.\"\"\"\n\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Estimated Geometric Median of the Dataset.\n\n    Used for initialising :attr:`tied_bias`.\n    \"\"\"\n\n    tied_bias: Float[\n        Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Tied Bias Parameter.\n\n    The same bias is used pre-encoder and post-decoder.\n    \"\"\"\n\n    n_components: int | None\n    \"\"\"Number of source model components the SAE is trained on.\"\"\"\n\n    n_input_features: int\n    \"\"\"Number of Input Features.\"\"\"\n\n    n_learned_features: int\n    \"\"\"Number of Learned Features.\"\"\"\n\n    _pre_encoder_bias: TiedBias\n\n    _encoder: LinearEncoder\n\n    _decoder: UnitNormDecoder\n\n    _post_decoder_bias: TiedBias\n\n    @property\n    def pre_encoder_bias(self) -&gt; TiedBias:\n        \"\"\"Pre-encoder bias.\"\"\"\n        return self._pre_encoder_bias\n\n    @property\n    def encoder(self) -&gt; LinearEncoder:\n        \"\"\"Encoder.\"\"\"\n        return self._encoder\n\n    @property\n    def decoder(self) -&gt; UnitNormDecoder:\n        \"\"\"Decoder.\"\"\"\n        return self._decoder\n\n    @property\n    def post_decoder_bias(self) -&gt; TiedBias:\n        \"\"\"Post-decoder bias.\"\"\"\n        return self._post_decoder_bias\n\n    def __init__(\n        self,\n        n_input_features: int,\n        n_learned_features: int,\n        geometric_median_dataset: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ]\n        | None = None,\n        n_components: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder Model.\n\n        Args:\n            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n                from TransformerLens).\n            n_learned_features: Number of learned features. The initial paper experimented with 1 to\n                256 times the number of input features, and primarily used a multiple of 8.\n            geometric_median_dataset: Estimated geometric median of the dataset.\n            n_components: Number of source model components the SAE is trained on. This is useful if\n                you want to train the SAE on several components of the source model at once. If\n                `None`, the SAE is assumed to be trained on just one component (in this case the\n                model won't contain a component axis in any of the parameters).\n        \"\"\"\n        super().__init__()\n\n        self.n_input_features = n_input_features\n        self.n_learned_features = n_learned_features\n        self.n_components = n_components\n\n        # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n        # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n        tied_bias_shape = shape_with_optional_dimensions(n_components, n_input_features)\n        if geometric_median_dataset is not None:\n            self.geometric_median_dataset = geometric_median_dataset.clone()\n            self.geometric_median_dataset.requires_grad = False\n        else:\n            self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n            self.geometric_median_dataset.requires_grad = False\n\n        # Initialize the tied bias\n        self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n        self.initialize_tied_parameters()\n\n        # Initialize the components\n        self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n        self._encoder = LinearEncoder(\n            input_features=n_input_features,\n            learnt_features=n_learned_features,\n            n_components=n_components,\n        )\n\n        self._decoder = UnitNormDecoder(\n            learnt_features=n_learned_features,\n            decoded_features=n_input_features,\n            n_components=n_components,\n        )\n\n        self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; tuple[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)],\n    ]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n        x = self._pre_encoder_bias(x)\n        learned_activations = self._encoder(x)\n        x = self._decoder(learned_activations)\n        decoded_activations = self._post_decoder_bias(x)\n        return learned_activations, decoded_activations\n\n    def initialize_tied_parameters(self) -&gt; None:\n        \"\"\"Initialize the tied parameters.\"\"\"\n        # The tied bias is initialised as the geometric median of the dataset\n        self.tied_bias.data = self.geometric_median_dataset\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n        self.initialize_tied_parameters()\n        for module in self.network:\n            if \"reset_parameters\" in dir(module):\n                module.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.decoder","title":"<code>decoder: UnitNormDecoder</code>  <code>property</code>","text":"<p>Decoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.encoder","title":"<code>encoder: LinearEncoder</code>  <code>property</code>","text":"<p>Encoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.geometric_median_dataset","title":"<code>geometric_median_dataset: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Estimated Geometric Median of the Dataset.</p> <p>Used for initialising :attr:<code>tied_bias</code>.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_components","title":"<code>n_components: int | None = n_components</code>  <code>instance-attribute</code>","text":"<p>Number of source model components the SAE is trained on.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_input_features","title":"<code>n_input_features: int = n_input_features</code>  <code>instance-attribute</code>","text":"<p>Number of Input Features.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_learned_features","title":"<code>n_learned_features: int = n_learned_features</code>  <code>instance-attribute</code>","text":"<p>Number of Learned Features.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Post-decoder bias.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Pre-encoder bias.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.tied_bias","title":"<code>tied_bias: Float[Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)] = Parameter(torch.empty(tied_bias_shape))</code>  <code>instance-attribute</code>","text":"<p>Tied Bias Parameter.</p> <p>The same bias is used pre-encoder and post-decoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.__init__","title":"<code>__init__(n_input_features, n_learned_features, geometric_median_dataset=None, n_components=None)</code>","text":"<p>Initialize the Sparse Autoencoder Model.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_features</code> <code>int</code> <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p> required <code>n_learned_features</code> <code>int</code> <p>Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8.</p> required <code>geometric_median_dataset</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)] | None</code> <p>Estimated geometric median of the dataset.</p> <code>None</code> <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on. This is useful if you want to train the SAE on several components of the source model at once. If <code>None</code>, the SAE is assumed to be trained on just one component (in this case the model won't contain a component axis in any of the parameters).</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_learned_features: int,\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    | None = None,\n    n_components: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder Model.\n\n    Args:\n        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n            from TransformerLens).\n        n_learned_features: Number of learned features. The initial paper experimented with 1 to\n            256 times the number of input features, and primarily used a multiple of 8.\n        geometric_median_dataset: Estimated geometric median of the dataset.\n        n_components: Number of source model components the SAE is trained on. This is useful if\n            you want to train the SAE on several components of the source model at once. If\n            `None`, the SAE is assumed to be trained on just one component (in this case the\n            model won't contain a component axis in any of the parameters).\n    \"\"\"\n    super().__init__()\n\n    self.n_input_features = n_input_features\n    self.n_learned_features = n_learned_features\n    self.n_components = n_components\n\n    # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n    # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n    tied_bias_shape = shape_with_optional_dimensions(n_components, n_input_features)\n    if geometric_median_dataset is not None:\n        self.geometric_median_dataset = geometric_median_dataset.clone()\n        self.geometric_median_dataset.requires_grad = False\n    else:\n        self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n        self.geometric_median_dataset.requires_grad = False\n\n    # Initialize the tied bias\n    self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n    self.initialize_tied_parameters()\n\n    # Initialize the components\n    self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n    self._encoder = LinearEncoder(\n        input_features=n_input_features,\n        learnt_features=n_learned_features,\n        n_components=n_components,\n    )\n\n    self._decoder = UnitNormDecoder(\n        learnt_features=n_learned_features,\n        decoded_features=n_input_features,\n        n_components=n_components,\n    )\n\n    self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>tuple[Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)], Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]]</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; tuple[\n    Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n    Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)],\n]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n    x = self._pre_encoder_bias(x)\n    learned_activations = self._encoder(x)\n    x = self._decoder(learned_activations)\n    decoded_activations = self._post_decoder_bias(x)\n    return learned_activations, decoded_activations\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.initialize_tied_parameters","title":"<code>initialize_tied_parameters()</code>","text":"<p>Initialize the tied parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def initialize_tied_parameters(self) -&gt; None:\n    \"\"\"Initialize the tied parameters.\"\"\"\n    # The tied bias is initialised as the geometric median of the dataset\n    self.tied_bias.data = self.geometric_median_dataset\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n    self.initialize_tied_parameters()\n    for module in self.network:\n        if \"reset_parameters\" in dir(module):\n            module.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/","title":"Sparse autoencoder components","text":"<p>Sparse autoencoder components.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder","title":"<code>AbstractDecoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Decoder Module.</p> <p>Typically includes just a :attr:<code>weight</code> parameter.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>class AbstractDecoder(Module, ABC):\n    \"\"\"Abstract Decoder Module.\n\n    Typically includes just a :attr:`weight` parameter.\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _decoded_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _n_components: int | None\n\n    def __init__(\n        self,\n        learnt_features: int,\n        decoded_features: int,\n        n_components: int | None,\n    ) -&gt; None:\n        \"\"\"Initialise the decoder.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._decoded_features = decoded_features\n        self._n_components = n_components\n\n    @property\n    @abstractmethod\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]:\n        \"\"\"Weight.\n\n        Each column in the weights matrix (for a specific component) acts as a dictionary vector,\n        representing a single basis element in the learned activation space.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Learned activations.\n\n        Returns:\n            Decoded activations.\n        \"\"\"\n\n    @abstractmethod\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_weights: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE_IDX),\n        ],\n    ) -&gt; None:\n        \"\"\"Update decoder dictionary vectors.\n\n        Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n        this is used when resampling neurons (dictionary vectors) that have died.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if dictionary_vector_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if self._n_components is None:\n                self.weight[:, dictionary_vector_indices] = updated_weights\n            else:\n                for component_idx in range(self._n_components):\n                    self.weight[\n                        component_idx, :, dictionary_vector_indices[component_idx]\n                    ] = updated_weights[component_idx]\n\n    @abstractmethod\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each column in the weights matrix (for a specific component) acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, n_components)</code>","text":"<p>Initialise the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>int</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>def __init__(\n    self,\n    learnt_features: int,\n    decoded_features: int,\n    n_components: int | None,\n) -&gt; None:\n    \"\"\"Initialise the decoder.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._decoded_features = decoded_features\n    self._n_components = n_components\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>  <code>abstractmethod</code>","text":"<p>Constrain the weights to have unit norm.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Learned activations.\n\n    Returns:\n        Decoded activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.reset_parameters","title":"<code>reset_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_weights)</code>","text":"<p>Update decoder dictionary vectors.</p> <p>Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically this is used when resampling neurons (dictionary vectors) that have died.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_weights</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE, LEARNT_FEATURE_IDX)]</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_weights: Float[\n        Tensor,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE_IDX),\n    ],\n) -&gt; None:\n    \"\"\"Update decoder dictionary vectors.\n\n    Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n    this is used when resampling neurons (dictionary vectors) that have died.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if dictionary_vector_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if self._n_components is None:\n            self.weight[:, dictionary_vector_indices] = updated_weights\n        else:\n            for component_idx in range(self._n_components):\n                self.weight[\n                    component_idx, :, dictionary_vector_indices[component_idx]\n                ] = updated_weights[component_idx]\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder","title":"<code>AbstractEncoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract encoder module.</p> <p>Typically includes :attr:<code>weights</code> and :attr:<code>bias</code> parameters, as well as an activation function.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>class AbstractEncoder(Module, ABC):\n    \"\"\"Abstract encoder module.\n\n    Typically includes :attr:`weights` and :attr:`bias` parameters, as well as an activation\n    function.\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _input_features: int\n    \"\"\"Number of input features from the source model.\"\"\"\n\n    _n_components: int | None\n\n    def __init__(\n        self,\n        input_features: int,\n        learnt_features: int,\n        n_components: int | None,\n    ) -&gt; None:\n        \"\"\"Initialise the encoder.\n\n        Args:\n            input_features: Number of input features to the autoencoder.\n            learnt_features: Number of learnt features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n        self._n_components = n_components\n\n    @property\n    @abstractmethod\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]:\n        \"\"\"Weight.\n\n        Each row in the weights matrix (for a specific component) acts as a dictionary vector,\n        representing a single basis element in the learned activation space.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def bias(self) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Bias.\"\"\"\n\n    @property\n    @abstractmethod\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input activations.\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_dictionary_weights: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX, Axis.INPUT_OUTPUT_FEATURE),\n        ],\n    ) -&gt; None:\n        \"\"\"Update encoder dictionary vectors.\n\n        Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_dictionary_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if dictionary_vector_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if self._n_components is None:\n                self.weight[dictionary_vector_indices] = updated_dictionary_weights\n            else:\n                for component_idx in range(self._n_components):\n                    self.weight[\n                        component_idx, dictionary_vector_indices[component_idx]\n                    ] = updated_dictionary_weights[component_idx]\n\n    @final\n    def update_bias(\n        self,\n        update_parameter_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_bias_features: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n    ) -&gt; None:\n        \"\"\"Update encoder bias.\n\n        Args:\n            update_parameter_indices: Indices of the bias features to update.\n            updated_bias_features: Updated bias features for just these indices.\n        \"\"\"\n        if update_parameter_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if self._n_components is None:\n                self.bias[update_parameter_indices] = updated_bias_features\n            else:\n                for component_idx in range(self._n_components):\n                    self.bias[component_idx, update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each row in the weights matrix (for a specific component) acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.__init__","title":"<code>__init__(input_features, learnt_features, n_components)</code>","text":"<p>Initialise the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>int</code> <p>Number of input features to the autoencoder.</p> required <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    input_features: int,\n    learnt_features: int,\n    n_components: int | None,\n) -&gt; None:\n    \"\"\"Initialise the encoder.\n\n    Args:\n        input_features: Number of input features to the autoencoder.\n        learnt_features: Number of learnt features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n    self._n_components = n_components\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input activations.\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.update_bias","title":"<code>update_bias(update_parameter_indices, updated_bias_features)</code>","text":"<p>Update encoder bias.</p> <p>Parameters:</p> Name Type Description Default <code>update_parameter_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the bias features to update.</p> required <code>updated_bias_features</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Updated bias features for just these indices.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_bias(\n    self,\n    update_parameter_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_bias_features: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n) -&gt; None:\n    \"\"\"Update encoder bias.\n\n    Args:\n        update_parameter_indices: Indices of the bias features to update.\n        updated_bias_features: Updated bias features for just these indices.\n    \"\"\"\n    if update_parameter_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if self._n_components is None:\n            self.bias[update_parameter_indices] = updated_bias_features\n        else:\n            for component_idx in range(self._n_components):\n                self.bias[component_idx, update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_dictionary_weights)</code>","text":"<p>Update encoder dictionary vectors.</p> <p>Updates the dictionary vectors (columns in the weight matrix) with the given values.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_dictionary_weights</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX, INPUT_OUTPUT_FEATURE)]</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_dictionary_weights: Float[\n        Tensor,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX, Axis.INPUT_OUTPUT_FEATURE),\n    ],\n) -&gt; None:\n    \"\"\"Update encoder dictionary vectors.\n\n    Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_dictionary_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if dictionary_vector_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if self._n_components is None:\n            self.weight[dictionary_vector_indices] = updated_dictionary_weights\n        else:\n            for component_idx in range(self._n_components):\n                self.weight[\n                    component_idx, dictionary_vector_indices[component_idx]\n                ] = updated_dictionary_weights[component_idx]\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractOuterBias","title":"<code>AbstractOuterBias</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Pre-Encoder or Post-Decoder Bias Module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>class AbstractOuterBias(Module, ABC):\n    \"\"\"Abstract Pre-Encoder or Post-Decoder Bias Module.\"\"\"\n\n    @property\n    @abstractmethod\n    def bias(\n        self,\n    ) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Bias.\n\n        May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractOuterBias.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p> <p>May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractOuterBias.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder","title":"<code>LinearEncoder</code>","text":"<p>             Bases: <code>AbstractEncoder</code></p> <p>Linear encoder layer.</p> <p>Linear encoder layer (essentially <code>nn.Linear</code>, with a ReLU activation function). Designed to be used as the encoder in a sparse autoencoder (excluding any outer tied bias).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\     W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\     b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\     f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output} \\end{align*} \\] Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\nclass LinearEncoder(AbstractEncoder):\n    r\"\"\"Linear encoder layer.\n\n    Linear encoder layer (essentially `nn.Linear`, with a ReLU activation function). Designed to be\n    used as the encoder in a sparse autoencoder (excluding any outer tied bias).\n\n    $$\n    \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\\n        W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\\n        b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\\n        f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output}\n    \\end{align*}\n    $$\n    \"\"\"\n\n    _weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    _bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    \"\"\"Bias parameter internal state.\"\"\"\n\n    @property\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]:\n        \"\"\"Weight parameter.\n\n        Each row in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    @property\n    def bias(self) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Bias parameter.\"\"\"\n        return self._bias\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return [(self.weight, -2), (self.bias, -1)]\n\n    activation_function: ReLU\n    \"\"\"Activation function.\"\"\"\n\n    def __init__(\n        self,\n        input_features: int,\n        learnt_features: int,\n        n_components: int | None,\n    ):\n        \"\"\"Initialize the linear encoder layer.\n\n        Args:\n            input_features: Number of input features to the autoencoder.\n            learnt_features: Number of learnt features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__(\n            input_features=input_features,\n            learnt_features=learnt_features,\n            n_components=n_components,\n        )\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n\n        self._weight = Parameter(\n            torch.empty(\n                (learnt_features, input_features),\n            )\n        )\n        self._bias = Parameter(torch.zeros(learnt_features))\n        self.activation_function = ReLU()\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\"\"\"\n        # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n        # `nonlinerity` must be changed.\n        init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n        # Bias (approach from nn.Linear)\n        fan_in = self._weight.size(1)\n        bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n        init.uniform_(self._bias, -bound, bound)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        z = (\n            einops.einsum(\n                x,\n                self.weight,\n                f\"{Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}, \\\n            ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.BATCH} ... {Axis.LEARNT_FEATURE}\",\n            )\n            + self.bias\n        )\n\n        return self.activation_function(z)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.activation_function","title":"<code>activation_function: ReLU = ReLU()</code>  <code>instance-attribute</code>","text":"<p>Activation function.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]</code>  <code>property</code>","text":"<p>Bias parameter.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.__init__","title":"<code>__init__(input_features, learnt_features, n_components)</code>","text":"<p>Initialize the linear encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>int</code> <p>Number of input features to the autoencoder.</p> required <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def __init__(\n    self,\n    input_features: int,\n    learnt_features: int,\n    n_components: int | None,\n):\n    \"\"\"Initialize the linear encoder layer.\n\n    Args:\n        input_features: Number of input features to the autoencoder.\n        learnt_features: Number of learnt features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__(\n        input_features=input_features,\n        learnt_features=learnt_features,\n        n_components=n_components,\n    )\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n\n    self._weight = Parameter(\n        torch.empty(\n            (learnt_features, input_features),\n        )\n    )\n    self._bias = Parameter(torch.zeros(learnt_features))\n    self.activation_function = ReLU()\n\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    z = (\n        einops.einsum(\n            x,\n            self.weight,\n            f\"{Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}, \\\n        ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n            -&gt; {Axis.BATCH} ... {Axis.LEARNT_FEATURE}\",\n        )\n        + self.bias\n    )\n\n    return self.activation_function(z)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\"\"\"\n    # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n    # `nonlinerity` must be changed.\n    init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n    # Bias (approach from nn.Linear)\n    fan_in = self._weight.size(1)\n    bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n    init.uniform_(self._bias, -bound, bound)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias","title":"<code>TiedBias</code>","text":"<p>             Bases: <code>AbstractOuterBias</code></p> <p>Tied Bias Layer.</p> <p>The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding.</p> <p>The bias parameter must be initialised in the parent module, and then passed to this layer.</p> <p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>@final\nclass TiedBias(AbstractOuterBias):\n    \"\"\"Tied Bias Layer.\n\n    The tied pre-encoder bias is a learned bias term that is subtracted from the input before\n    encoding, and added back after decoding.\n\n    The bias parameter must be initialised in the parent module, and then passed to this layer.\n\n    https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias\n    \"\"\"\n\n    _bias_position: TiedBiasPosition\n\n    _bias_reference: Float[\n        Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n\n    @property\n    def bias(\n        self,\n    ) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Bias.\"\"\"\n        return self._bias_reference\n\n    def __init__(\n        self,\n        bias_reference: Float[\n            Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        position: TiedBiasPosition,\n    ) -&gt; None:\n        \"\"\"Initialize the bias layer.\n\n        Args:\n            bias_reference: Tied bias parameter (initialised in the parent module), used for both\n                the pre-encoder and post-encoder bias. The original paper initialised this using the\n                geometric median of the dataset.\n            position: Whether this is the pre-encoder or post-encoder bias.\n        \"\"\"\n        super().__init__()\n\n        self._bias_reference = bias_reference\n\n        # Support string literals as well as enums\n        self._bias_position = position\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        # If this is the pre-encoder bias, we subtract the bias from the input.\n        if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n            return x - self.bias\n\n        # If it's the post-encoder bias, we add the bias to the input.\n        return x + self.bias\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.__init__","title":"<code>__init__(bias_reference, position)</code>","text":"<p>Initialize the bias layer.</p> <p>Parameters:</p> Name Type Description Default <code>bias_reference</code> <code>Float[Parameter, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset.</p> required <code>position</code> <code>TiedBiasPosition</code> <p>Whether this is the pre-encoder or post-encoder bias.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def __init__(\n    self,\n    bias_reference: Float[\n        Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    position: TiedBiasPosition,\n) -&gt; None:\n    \"\"\"Initialize the bias layer.\n\n    Args:\n        bias_reference: Tied bias parameter (initialised in the parent module), used for both\n            the pre-encoder and post-encoder bias. The original paper initialised this using the\n            geometric median of the dataset.\n        position: Whether this is the pre-encoder or post-encoder bias.\n    \"\"\"\n    super().__init__()\n\n    self._bias_reference = bias_reference\n\n    # Support string literals as well as enums\n    self._bias_position = position\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    # If this is the pre-encoder bias, we subtract the bias from the input.\n    if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n        return x - self.bias\n\n    # If it's the post-encoder bias, we add the bias to the input.\n    return x + self.bias\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBiasPosition","title":"<code>TiedBiasPosition</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Tied Bias Position.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>class TiedBiasPosition(str, Enum):\n    \"\"\"Tied Bias Position.\"\"\"\n\n    PRE_ENCODER = \"pre_encoder\"\n    POST_DECODER = \"post_decoder\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder","title":"<code>UnitNormDecoder</code>","text":"<p>             Bases: <code>AbstractDecoder</code></p> <p>Constrained unit norm linear decoder layer.</p> <p>Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. It also requires <code>constrain_weights_unit_norm</code> to be called after each gradient step, to prevent drift of the dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\     W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\     z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)} \\end{align*} \\] Motivation <p>Normalisation of the columns (dictionary features) prevents the model from reducing the sparsity loss term by increasing the size of the feature vectors in \\(W_d\\).</p> <p>Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>@final\nclass UnitNormDecoder(AbstractDecoder):\n    r\"\"\"Constrained unit norm linear decoder layer.\n\n    Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are\n    constrained to have unit norm. This is done by removing the gradient information parallel to the\n    dictionary vectors before applying the gradient step, using a backward hook. It also requires\n    `constrain_weights_unit_norm` to be called after each gradient step, to prevent drift of the\n    dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the\n    gradient, but instead follow a modified gradient that includes momentum).\n\n    $$ \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\\n        W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\\n        z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)}\n    \\end{align*} $$\n\n    Motivation:\n        Normalisation of the columns (dictionary features) prevents the model from reducing the\n        sparsity loss term by increasing the size of the feature vectors in $W_d$.\n\n        Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary\n        Learning* paper found that removing the gradient information parallel to the dictionary\n        vectors before applying the gradient step, rather than resetting the dictionary vectors to\n        unit norm after each gradient step, results in a small but real reduction in total\n        loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).\n    \"\"\"\n\n    _weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    @property\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]:\n        \"\"\"Weight parameter.\n\n        Each column in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return [(self.weight, -1)]\n\n    def __init__(\n        self,\n        learnt_features: int,\n        decoded_features: int,\n        n_components: int | None,\n        *,\n        enable_gradient_hook: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the constrained unit norm linear layer.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n            enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n                applying the gradient step, to maintain unit norm of the dictionary vectors).\n        \"\"\"\n        # Create the linear layer as per the standard PyTorch linear layer\n        super().__init__(\n            learnt_features=learnt_features,\n            decoded_features=decoded_features,\n            n_components=n_components,\n        )\n        self._weight = Parameter(\n            torch.empty(\n                shape_with_optional_dimensions(n_components, decoded_features, learnt_features),\n            )\n        )\n        self.reset_parameters()\n\n        # Register backward hook to remove any gradient information parallel to the dictionary\n        # vectors (columns of the weight matrix) before applying the gradient step.\n        if enable_gradient_hook:\n            self._weight.register_hook(self._weight_backward_hook)\n\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\n\n        Warning:\n            Note this must be called after each gradient step. This is because optimisers such as\n            Adam don't strictly follow the gradient, but instead follow a modified gradient that\n            includes momentum. This means that the gradient step can change the norm of the\n            dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n            Note this can't be applied directly in the backward hook, as it would interfere with a\n            variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n            with asynchronous operations, etc).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; layer = UnitNormDecoder(3, 3, None)\n            &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n            &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n            &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0]\n\n        \"\"\"\n        with torch.no_grad():\n            torch.nn.functional.normalize(self._weight, dim=-2, out=self._weight)\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n            &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None)\n            &gt;&gt;&gt; layer.reset_parameters()\n            &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n            &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0, 1.0]\n\n        \"\"\"\n        # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n        # normalisation here, since we immediately scale the weights to have unit norm (so the\n        # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n        self._weight: Float[\n            Parameter,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ] = init.normal_(self._weight, mean=0, std=1)  # type: ignore\n\n        # Scale so that each row has unit norm\n        self.constrain_weights_unit_norm()\n\n    def _weight_backward_hook(\n        self,\n        grad: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ],\n    ) -&gt; Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ]:\n        r\"\"\"Unit norm backward hook.\n\n        By subtracting the projection of the gradient onto the dictionary vectors, we remove the\n        component of the gradient that is parallel to the dictionary vectors and just keep the\n        component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere).\n        The result is that the backward pass does not change the norm of the dictionary vectors.\n\n        $$\n        \\begin{align*}\n            W_d &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Decoder weight matrix} \\\\\n            g &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Gradient w.r.t. } W_d\n                \\text{ from the backward pass} \\\\\n            W_{d, \\text{norm}} &amp;= \\frac{W_d}{\\|W_d\\|} = \\text{Normalized decoder weight matrix\n                (over columns)} \\\\\n            g_{\\parallel} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g\n                \\text{ parallel to } W_{d, \\text{norm}} \\\\\n            g_{\\perp} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g \\text{ orthogonal to }\n                W_{d, \\text{norm}} \\\\\n            g_{\\parallel} &amp;= W_{d, \\text{norm}} \\cdot (W_{d, \\text{norm}}^\\top \\cdot g) \\\\\n            g_{\\perp} &amp;= g - g_{\\parallel} =\n                \\text{Adjusted gradient with parallel component removed} \\\\\n        \\end{align*}\n        $$\n\n        Args:\n            grad: Gradient with respect to the weights.\n\n        Returns:\n            Gradient with respect to the weights, with the component parallel to the dictionary\n            vectors removed.\n        \"\"\"\n        # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can\n        # be thought of as vectors that end on the circumference of a hypersphere. The projection of\n        # the gradient onto the dictionary vectors is the component of the gradient that is parallel\n        # to the dictionary vectors, i.e. the component that moves to or from the center of the\n        # hypersphere.\n        normalized_weight: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ] = self._weight / torch.norm(self._weight, dim=-2, keepdim=True)\n\n        scalar_projections = einops.einsum(\n            grad,\n            normalized_weight,\n            f\"... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; ... {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        projection = einops.einsum(\n            scalar_projections,\n            normalized_weight,\n            f\"... {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        # Subtracting the parallel component from the gradient leaves only the component that is\n        # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of\n        # the hypersphere.\n        return grad - projection\n\n    def forward(\n        self, x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        return einops.einsum(\n            x,\n            self.weight,\n            f\"{Axis.BATCH} ... {Axis.LEARNT_FEATURE}, \\\n            ... {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \\\n                -&gt; {Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE)]</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, n_components, *, enable_gradient_hook=True)</code>","text":"<p>Initialize the constrained unit norm linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>int</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required <code>enable_gradient_hook</code> <code>bool</code> <p>Enable the gradient backwards hook (modify the gradient before applying the gradient step, to maintain unit norm of the dictionary vectors).</p> <code>True</code> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def __init__(\n    self,\n    learnt_features: int,\n    decoded_features: int,\n    n_components: int | None,\n    *,\n    enable_gradient_hook: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the constrained unit norm linear layer.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n        enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n            applying the gradient step, to maintain unit norm of the dictionary vectors).\n    \"\"\"\n    # Create the linear layer as per the standard PyTorch linear layer\n    super().__init__(\n        learnt_features=learnt_features,\n        decoded_features=decoded_features,\n        n_components=n_components,\n    )\n    self._weight = Parameter(\n        torch.empty(\n            shape_with_optional_dimensions(n_components, decoded_features, learnt_features),\n        )\n    )\n    self.reset_parameters()\n\n    # Register backward hook to remove any gradient information parallel to the dictionary\n    # vectors (columns of the weight matrix) before applying the gradient step.\n    if enable_gradient_hook:\n        self._weight.register_hook(self._weight_backward_hook)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>","text":"<p>Constrain the weights to have unit norm.</p> Warning <p>Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook <code>_weight_backward_hook</code> is applied.</p> <p>Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc).</p> Example <p>import torch layer = UnitNormDecoder(3, 3, None) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0)) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0]</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\n\n    Warning:\n        Note this must be called after each gradient step. This is because optimisers such as\n        Adam don't strictly follow the gradient, but instead follow a modified gradient that\n        includes momentum. This means that the gradient step can change the norm of the\n        dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n        Note this can't be applied directly in the backward hook, as it would interfere with a\n        variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n        with asynchronous operations, etc).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; layer = UnitNormDecoder(3, 3, None)\n        &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n        &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n        &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0]\n\n    \"\"\"\n    with torch.no_grad():\n        torch.nn.functional.normalize(self._weight, dim=-2, out=self._weight)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    return einops.einsum(\n        x,\n        self.weight,\n        f\"{Axis.BATCH} ... {Axis.LEARNT_FEATURE}, \\\n        ... {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \\\n            -&gt; {Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}\",\n    )\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Example <p>import torch</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n        &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None)\n        &gt;&gt;&gt; layer.reset_parameters()\n        &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n        &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0, 1.0]\n\n    \"\"\"\n    # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n    # normalisation here, since we immediately scale the weights to have unit norm (so the\n    # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n    self._weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ] = init.normal_(self._weight, mean=0, std=1)  # type: ignore\n\n    # Scale so that each row has unit norm\n    self.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features","title":"Create a layer with 4 columns (learnt features) and 3 rows (decoded features)","text":"<p>layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None) layer.reset_parameters()</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns","title":"Get the norm across the rows (by summing across the columns)","text":"<p>column_norms = torch.sum(layer.weight ** 2, dim=0) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0, 1.0]</p>"},{"location":"reference/autoencoder/components/abstract_decoder/","title":"Abstract Sparse Autoencoder Model","text":"<p>Abstract Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder","title":"<code>AbstractDecoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Decoder Module.</p> <p>Typically includes just a :attr:<code>weight</code> parameter.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>class AbstractDecoder(Module, ABC):\n    \"\"\"Abstract Decoder Module.\n\n    Typically includes just a :attr:`weight` parameter.\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _decoded_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _n_components: int | None\n\n    def __init__(\n        self,\n        learnt_features: int,\n        decoded_features: int,\n        n_components: int | None,\n    ) -&gt; None:\n        \"\"\"Initialise the decoder.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._decoded_features = decoded_features\n        self._n_components = n_components\n\n    @property\n    @abstractmethod\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]:\n        \"\"\"Weight.\n\n        Each column in the weights matrix (for a specific component) acts as a dictionary vector,\n        representing a single basis element in the learned activation space.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Learned activations.\n\n        Returns:\n            Decoded activations.\n        \"\"\"\n\n    @abstractmethod\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_weights: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE_IDX),\n        ],\n    ) -&gt; None:\n        \"\"\"Update decoder dictionary vectors.\n\n        Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n        this is used when resampling neurons (dictionary vectors) that have died.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if dictionary_vector_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if self._n_components is None:\n                self.weight[:, dictionary_vector_indices] = updated_weights\n            else:\n                for component_idx in range(self._n_components):\n                    self.weight[\n                        component_idx, :, dictionary_vector_indices[component_idx]\n                    ] = updated_weights[component_idx]\n\n    @abstractmethod\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each column in the weights matrix (for a specific component) acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, n_components)</code>","text":"<p>Initialise the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>int</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>def __init__(\n    self,\n    learnt_features: int,\n    decoded_features: int,\n    n_components: int | None,\n) -&gt; None:\n    \"\"\"Initialise the decoder.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._decoded_features = decoded_features\n    self._n_components = n_components\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>  <code>abstractmethod</code>","text":"<p>Constrain the weights to have unit norm.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Learned activations.\n\n    Returns:\n        Decoded activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.reset_parameters","title":"<code>reset_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_weights)</code>","text":"<p>Update decoder dictionary vectors.</p> <p>Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically this is used when resampling neurons (dictionary vectors) that have died.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_weights</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE, LEARNT_FEATURE_IDX)]</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_weights: Float[\n        Tensor,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE_IDX),\n    ],\n) -&gt; None:\n    \"\"\"Update decoder dictionary vectors.\n\n    Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n    this is used when resampling neurons (dictionary vectors) that have died.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if dictionary_vector_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if self._n_components is None:\n            self.weight[:, dictionary_vector_indices] = updated_weights\n        else:\n            for component_idx in range(self._n_components):\n                self.weight[\n                    component_idx, :, dictionary_vector_indices[component_idx]\n                ] = updated_weights[component_idx]\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/","title":"Abstract Encoder","text":"<p>Abstract Encoder.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder","title":"<code>AbstractEncoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract encoder module.</p> <p>Typically includes :attr:<code>weights</code> and :attr:<code>bias</code> parameters, as well as an activation function.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>class AbstractEncoder(Module, ABC):\n    \"\"\"Abstract encoder module.\n\n    Typically includes :attr:`weights` and :attr:`bias` parameters, as well as an activation\n    function.\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _input_features: int\n    \"\"\"Number of input features from the source model.\"\"\"\n\n    _n_components: int | None\n\n    def __init__(\n        self,\n        input_features: int,\n        learnt_features: int,\n        n_components: int | None,\n    ) -&gt; None:\n        \"\"\"Initialise the encoder.\n\n        Args:\n            input_features: Number of input features to the autoencoder.\n            learnt_features: Number of learnt features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n        self._n_components = n_components\n\n    @property\n    @abstractmethod\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]:\n        \"\"\"Weight.\n\n        Each row in the weights matrix (for a specific component) acts as a dictionary vector,\n        representing a single basis element in the learned activation space.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def bias(self) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Bias.\"\"\"\n\n    @property\n    @abstractmethod\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input activations.\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_dictionary_weights: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX, Axis.INPUT_OUTPUT_FEATURE),\n        ],\n    ) -&gt; None:\n        \"\"\"Update encoder dictionary vectors.\n\n        Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_dictionary_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if dictionary_vector_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if self._n_components is None:\n                self.weight[dictionary_vector_indices] = updated_dictionary_weights\n            else:\n                for component_idx in range(self._n_components):\n                    self.weight[\n                        component_idx, dictionary_vector_indices[component_idx]\n                    ] = updated_dictionary_weights[component_idx]\n\n    @final\n    def update_bias(\n        self,\n        update_parameter_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_bias_features: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n    ) -&gt; None:\n        \"\"\"Update encoder bias.\n\n        Args:\n            update_parameter_indices: Indices of the bias features to update.\n            updated_bias_features: Updated bias features for just these indices.\n        \"\"\"\n        if update_parameter_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if self._n_components is None:\n                self.bias[update_parameter_indices] = updated_bias_features\n            else:\n                for component_idx in range(self._n_components):\n                    self.bias[component_idx, update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each row in the weights matrix (for a specific component) acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.__init__","title":"<code>__init__(input_features, learnt_features, n_components)</code>","text":"<p>Initialise the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>int</code> <p>Number of input features to the autoencoder.</p> required <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>def __init__(\n    self,\n    input_features: int,\n    learnt_features: int,\n    n_components: int | None,\n) -&gt; None:\n    \"\"\"Initialise the encoder.\n\n    Args:\n        input_features: Number of input features to the autoencoder.\n        learnt_features: Number of learnt features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n    self._n_components = n_components\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input activations.\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.update_bias","title":"<code>update_bias(update_parameter_indices, updated_bias_features)</code>","text":"<p>Update encoder bias.</p> <p>Parameters:</p> Name Type Description Default <code>update_parameter_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the bias features to update.</p> required <code>updated_bias_features</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Updated bias features for just these indices.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_bias(\n    self,\n    update_parameter_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_bias_features: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n) -&gt; None:\n    \"\"\"Update encoder bias.\n\n    Args:\n        update_parameter_indices: Indices of the bias features to update.\n        updated_bias_features: Updated bias features for just these indices.\n    \"\"\"\n    if update_parameter_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if self._n_components is None:\n            self.bias[update_parameter_indices] = updated_bias_features\n        else:\n            for component_idx in range(self._n_components):\n                self.bias[component_idx, update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_dictionary_weights)</code>","text":"<p>Update encoder dictionary vectors.</p> <p>Updates the dictionary vectors (columns in the weight matrix) with the given values.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_dictionary_weights</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX, INPUT_OUTPUT_FEATURE)]</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_dictionary_weights: Float[\n        Tensor,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX, Axis.INPUT_OUTPUT_FEATURE),\n    ],\n) -&gt; None:\n    \"\"\"Update encoder dictionary vectors.\n\n    Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_dictionary_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if dictionary_vector_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if self._n_components is None:\n            self.weight[dictionary_vector_indices] = updated_dictionary_weights\n        else:\n            for component_idx in range(self._n_components):\n                self.weight[\n                    component_idx, dictionary_vector_indices[component_idx]\n                ] = updated_dictionary_weights[component_idx]\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_outer_bias/","title":"Abstract Outer Bias","text":"<p>Abstract Outer Bias.</p> <p>This can be extended to create e.g. a pre-encoder and post-decoder bias.</p>"},{"location":"reference/autoencoder/components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias","title":"<code>AbstractOuterBias</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Pre-Encoder or Post-Decoder Bias Module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>class AbstractOuterBias(Module, ABC):\n    \"\"\"Abstract Pre-Encoder or Post-Decoder Bias Module.\"\"\"\n\n    @property\n    @abstractmethod\n    def bias(\n        self,\n    ) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Bias.\n\n        May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p> <p>May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.</p>"},{"location":"reference/autoencoder/components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/","title":"Linear encoder layer","text":"<p>Linear encoder layer.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder","title":"<code>LinearEncoder</code>","text":"<p>             Bases: <code>AbstractEncoder</code></p> <p>Linear encoder layer.</p> <p>Linear encoder layer (essentially <code>nn.Linear</code>, with a ReLU activation function). Designed to be used as the encoder in a sparse autoencoder (excluding any outer tied bias).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\     W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\     b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\     f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output} \\end{align*} \\] Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\nclass LinearEncoder(AbstractEncoder):\n    r\"\"\"Linear encoder layer.\n\n    Linear encoder layer (essentially `nn.Linear`, with a ReLU activation function). Designed to be\n    used as the encoder in a sparse autoencoder (excluding any outer tied bias).\n\n    $$\n    \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\\n        W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\\n        b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\\n        f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output}\n    \\end{align*}\n    $$\n    \"\"\"\n\n    _weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    _bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    \"\"\"Bias parameter internal state.\"\"\"\n\n    @property\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]:\n        \"\"\"Weight parameter.\n\n        Each row in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    @property\n    def bias(self) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Bias parameter.\"\"\"\n        return self._bias\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return [(self.weight, -2), (self.bias, -1)]\n\n    activation_function: ReLU\n    \"\"\"Activation function.\"\"\"\n\n    def __init__(\n        self,\n        input_features: int,\n        learnt_features: int,\n        n_components: int | None,\n    ):\n        \"\"\"Initialize the linear encoder layer.\n\n        Args:\n            input_features: Number of input features to the autoencoder.\n            learnt_features: Number of learnt features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__(\n            input_features=input_features,\n            learnt_features=learnt_features,\n            n_components=n_components,\n        )\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n\n        self._weight = Parameter(\n            torch.empty(\n                (learnt_features, input_features),\n            )\n        )\n        self._bias = Parameter(torch.zeros(learnt_features))\n        self.activation_function = ReLU()\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\"\"\"\n        # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n        # `nonlinerity` must be changed.\n        init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n        # Bias (approach from nn.Linear)\n        fan_in = self._weight.size(1)\n        bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n        init.uniform_(self._bias, -bound, bound)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        z = (\n            einops.einsum(\n                x,\n                self.weight,\n                f\"{Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}, \\\n            ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.BATCH} ... {Axis.LEARNT_FEATURE}\",\n            )\n            + self.bias\n        )\n\n        return self.activation_function(z)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.activation_function","title":"<code>activation_function: ReLU = ReLU()</code>  <code>instance-attribute</code>","text":"<p>Activation function.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]</code>  <code>property</code>","text":"<p>Bias parameter.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.__init__","title":"<code>__init__(input_features, learnt_features, n_components)</code>","text":"<p>Initialize the linear encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>int</code> <p>Number of input features to the autoencoder.</p> required <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def __init__(\n    self,\n    input_features: int,\n    learnt_features: int,\n    n_components: int | None,\n):\n    \"\"\"Initialize the linear encoder layer.\n\n    Args:\n        input_features: Number of input features to the autoencoder.\n        learnt_features: Number of learnt features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__(\n        input_features=input_features,\n        learnt_features=learnt_features,\n        n_components=n_components,\n    )\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n\n    self._weight = Parameter(\n        torch.empty(\n            (learnt_features, input_features),\n        )\n    )\n    self._bias = Parameter(torch.zeros(learnt_features))\n    self.activation_function = ReLU()\n\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    z = (\n        einops.einsum(\n            x,\n            self.weight,\n            f\"{Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}, \\\n        ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n            -&gt; {Axis.BATCH} ... {Axis.LEARNT_FEATURE}\",\n        )\n        + self.bias\n    )\n\n    return self.activation_function(z)\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\"\"\"\n    # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n    # `nonlinerity` must be changed.\n    init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n    # Bias (approach from nn.Linear)\n    fan_in = self._weight.size(1)\n    bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n    init.uniform_(self._bias, -bound, bound)\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/","title":"Tied Biases (Pre-Encoder and Post-Decoder)","text":"<p>Tied Biases (Pre-Encoder and Post-Decoder).</p>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias","title":"<code>TiedBias</code>","text":"<p>             Bases: <code>AbstractOuterBias</code></p> <p>Tied Bias Layer.</p> <p>The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding.</p> <p>The bias parameter must be initialised in the parent module, and then passed to this layer.</p> <p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>@final\nclass TiedBias(AbstractOuterBias):\n    \"\"\"Tied Bias Layer.\n\n    The tied pre-encoder bias is a learned bias term that is subtracted from the input before\n    encoding, and added back after decoding.\n\n    The bias parameter must be initialised in the parent module, and then passed to this layer.\n\n    https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias\n    \"\"\"\n\n    _bias_position: TiedBiasPosition\n\n    _bias_reference: Float[\n        Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n\n    @property\n    def bias(\n        self,\n    ) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Bias.\"\"\"\n        return self._bias_reference\n\n    def __init__(\n        self,\n        bias_reference: Float[\n            Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        position: TiedBiasPosition,\n    ) -&gt; None:\n        \"\"\"Initialize the bias layer.\n\n        Args:\n            bias_reference: Tied bias parameter (initialised in the parent module), used for both\n                the pre-encoder and post-encoder bias. The original paper initialised this using the\n                geometric median of the dataset.\n            position: Whether this is the pre-encoder or post-encoder bias.\n        \"\"\"\n        super().__init__()\n\n        self._bias_reference = bias_reference\n\n        # Support string literals as well as enums\n        self._bias_position = position\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        # If this is the pre-encoder bias, we subtract the bias from the input.\n        if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n            return x - self.bias\n\n        # If it's the post-encoder bias, we add the bias to the input.\n        return x + self.bias\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.__init__","title":"<code>__init__(bias_reference, position)</code>","text":"<p>Initialize the bias layer.</p> <p>Parameters:</p> Name Type Description Default <code>bias_reference</code> <code>Float[Parameter, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset.</p> required <code>position</code> <code>TiedBiasPosition</code> <p>Whether this is the pre-encoder or post-encoder bias.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def __init__(\n    self,\n    bias_reference: Float[\n        Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    position: TiedBiasPosition,\n) -&gt; None:\n    \"\"\"Initialize the bias layer.\n\n    Args:\n        bias_reference: Tied bias parameter (initialised in the parent module), used for both\n            the pre-encoder and post-encoder bias. The original paper initialised this using the\n            geometric median of the dataset.\n        position: Whether this is the pre-encoder or post-encoder bias.\n    \"\"\"\n    super().__init__()\n\n    self._bias_reference = bias_reference\n\n    # Support string literals as well as enums\n    self._bias_position = position\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    # If this is the pre-encoder bias, we subtract the bias from the input.\n    if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n        return x - self.bias\n\n    # If it's the post-encoder bias, we add the bias to the input.\n    return x + self.bias\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBiasPosition","title":"<code>TiedBiasPosition</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Tied Bias Position.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>class TiedBiasPosition(str, Enum):\n    \"\"\"Tied Bias Position.\"\"\"\n\n    PRE_ENCODER = \"pre_encoder\"\n    POST_DECODER = \"post_decoder\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/","title":"Linear layer with unit norm weights","text":"<p>Linear layer with unit norm weights.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder","title":"<code>UnitNormDecoder</code>","text":"<p>             Bases: <code>AbstractDecoder</code></p> <p>Constrained unit norm linear decoder layer.</p> <p>Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. It also requires <code>constrain_weights_unit_norm</code> to be called after each gradient step, to prevent drift of the dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\     W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\     z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)} \\end{align*} \\] Motivation <p>Normalisation of the columns (dictionary features) prevents the model from reducing the sparsity loss term by increasing the size of the feature vectors in \\(W_d\\).</p> <p>Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>@final\nclass UnitNormDecoder(AbstractDecoder):\n    r\"\"\"Constrained unit norm linear decoder layer.\n\n    Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are\n    constrained to have unit norm. This is done by removing the gradient information parallel to the\n    dictionary vectors before applying the gradient step, using a backward hook. It also requires\n    `constrain_weights_unit_norm` to be called after each gradient step, to prevent drift of the\n    dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the\n    gradient, but instead follow a modified gradient that includes momentum).\n\n    $$ \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\\n        W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\\n        z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)}\n    \\end{align*} $$\n\n    Motivation:\n        Normalisation of the columns (dictionary features) prevents the model from reducing the\n        sparsity loss term by increasing the size of the feature vectors in $W_d$.\n\n        Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary\n        Learning* paper found that removing the gradient information parallel to the dictionary\n        vectors before applying the gradient step, rather than resetting the dictionary vectors to\n        unit norm after each gradient step, results in a small but real reduction in total\n        loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).\n    \"\"\"\n\n    _weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    @property\n    def weight(\n        self,\n    ) -&gt; Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]:\n        \"\"\"Weight parameter.\n\n        Each column in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[tuple[Parameter, int]]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return [(self.weight, -1)]\n\n    def __init__(\n        self,\n        learnt_features: int,\n        decoded_features: int,\n        n_components: int | None,\n        *,\n        enable_gradient_hook: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the constrained unit norm linear layer.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n            enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n                applying the gradient step, to maintain unit norm of the dictionary vectors).\n        \"\"\"\n        # Create the linear layer as per the standard PyTorch linear layer\n        super().__init__(\n            learnt_features=learnt_features,\n            decoded_features=decoded_features,\n            n_components=n_components,\n        )\n        self._weight = Parameter(\n            torch.empty(\n                shape_with_optional_dimensions(n_components, decoded_features, learnt_features),\n            )\n        )\n        self.reset_parameters()\n\n        # Register backward hook to remove any gradient information parallel to the dictionary\n        # vectors (columns of the weight matrix) before applying the gradient step.\n        if enable_gradient_hook:\n            self._weight.register_hook(self._weight_backward_hook)\n\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\n\n        Warning:\n            Note this must be called after each gradient step. This is because optimisers such as\n            Adam don't strictly follow the gradient, but instead follow a modified gradient that\n            includes momentum. This means that the gradient step can change the norm of the\n            dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n            Note this can't be applied directly in the backward hook, as it would interfere with a\n            variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n            with asynchronous operations, etc).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; layer = UnitNormDecoder(3, 3, None)\n            &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n            &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n            &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0]\n\n        \"\"\"\n        with torch.no_grad():\n            torch.nn.functional.normalize(self._weight, dim=-2, out=self._weight)\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n            &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None)\n            &gt;&gt;&gt; layer.reset_parameters()\n            &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n            &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0, 1.0]\n\n        \"\"\"\n        # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n        # normalisation here, since we immediately scale the weights to have unit norm (so the\n        # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n        self._weight: Float[\n            Parameter,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ] = init.normal_(self._weight, mean=0, std=1)  # type: ignore\n\n        # Scale so that each row has unit norm\n        self.constrain_weights_unit_norm()\n\n    def _weight_backward_hook(\n        self,\n        grad: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ],\n    ) -&gt; Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ]:\n        r\"\"\"Unit norm backward hook.\n\n        By subtracting the projection of the gradient onto the dictionary vectors, we remove the\n        component of the gradient that is parallel to the dictionary vectors and just keep the\n        component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere).\n        The result is that the backward pass does not change the norm of the dictionary vectors.\n\n        $$\n        \\begin{align*}\n            W_d &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Decoder weight matrix} \\\\\n            g &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Gradient w.r.t. } W_d\n                \\text{ from the backward pass} \\\\\n            W_{d, \\text{norm}} &amp;= \\frac{W_d}{\\|W_d\\|} = \\text{Normalized decoder weight matrix\n                (over columns)} \\\\\n            g_{\\parallel} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g\n                \\text{ parallel to } W_{d, \\text{norm}} \\\\\n            g_{\\perp} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g \\text{ orthogonal to }\n                W_{d, \\text{norm}} \\\\\n            g_{\\parallel} &amp;= W_{d, \\text{norm}} \\cdot (W_{d, \\text{norm}}^\\top \\cdot g) \\\\\n            g_{\\perp} &amp;= g - g_{\\parallel} =\n                \\text{Adjusted gradient with parallel component removed} \\\\\n        \\end{align*}\n        $$\n\n        Args:\n            grad: Gradient with respect to the weights.\n\n        Returns:\n            Gradient with respect to the weights, with the component parallel to the dictionary\n            vectors removed.\n        \"\"\"\n        # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can\n        # be thought of as vectors that end on the circumference of a hypersphere. The projection of\n        # the gradient onto the dictionary vectors is the component of the gradient that is parallel\n        # to the dictionary vectors, i.e. the component that moves to or from the center of the\n        # hypersphere.\n        normalized_weight: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ] = self._weight / torch.norm(self._weight, dim=-2, keepdim=True)\n\n        scalar_projections = einops.einsum(\n            grad,\n            normalized_weight,\n            f\"... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; ... {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        projection = einops.einsum(\n            scalar_projections,\n            normalized_weight,\n            f\"... {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        # Subtracting the parallel component from the gradient leaves only the component that is\n        # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of\n        # the hypersphere.\n        return grad - projection\n\n    def forward(\n        self, x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        return einops.einsum(\n            x,\n            self.weight,\n            f\"{Axis.BATCH} ... {Axis.LEARNT_FEATURE}, \\\n            ... {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \\\n                -&gt; {Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[tuple[Parameter, int]]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[tuple[Parameter, int]]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[tuple[Parameter, int]]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE)]</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, n_components, *, enable_gradient_hook=True)</code>","text":"<p>Initialize the constrained unit norm linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>int</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>n_components</code> <code>int | None</code> <p>Number of source model components the SAE is trained on.</p> required <code>enable_gradient_hook</code> <code>bool</code> <p>Enable the gradient backwards hook (modify the gradient before applying the gradient step, to maintain unit norm of the dictionary vectors).</p> <code>True</code> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def __init__(\n    self,\n    learnt_features: int,\n    decoded_features: int,\n    n_components: int | None,\n    *,\n    enable_gradient_hook: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the constrained unit norm linear layer.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n        enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n            applying the gradient step, to maintain unit norm of the dictionary vectors).\n    \"\"\"\n    # Create the linear layer as per the standard PyTorch linear layer\n    super().__init__(\n        learnt_features=learnt_features,\n        decoded_features=decoded_features,\n        n_components=n_components,\n    )\n    self._weight = Parameter(\n        torch.empty(\n            shape_with_optional_dimensions(n_components, decoded_features, learnt_features),\n        )\n    )\n    self.reset_parameters()\n\n    # Register backward hook to remove any gradient information parallel to the dictionary\n    # vectors (columns of the weight matrix) before applying the gradient step.\n    if enable_gradient_hook:\n        self._weight.register_hook(self._weight_backward_hook)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>","text":"<p>Constrain the weights to have unit norm.</p> Warning <p>Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook <code>_weight_backward_hook</code> is applied.</p> <p>Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc).</p> Example <p>import torch layer = UnitNormDecoder(3, 3, None) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0)) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0]</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\n\n    Warning:\n        Note this must be called after each gradient step. This is because optimisers such as\n        Adam don't strictly follow the gradient, but instead follow a modified gradient that\n        includes momentum. This means that the gradient step can change the norm of the\n        dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n        Note this can't be applied directly in the backward hook, as it would interfere with a\n        variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n        with asynchronous operations, etc).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; layer = UnitNormDecoder(3, 3, None)\n        &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n        &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n        &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0]\n\n    \"\"\"\n    with torch.no_grad():\n        torch.nn.functional.normalize(self._weight, dim=-2, out=self._weight)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    return einops.einsum(\n        x,\n        self.weight,\n        f\"{Axis.BATCH} ... {Axis.LEARNT_FEATURE}, \\\n        ... {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \\\n            -&gt; {Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}\",\n    )\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Example <p>import torch</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n        &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None)\n        &gt;&gt;&gt; layer.reset_parameters()\n        &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n        &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0, 1.0]\n\n    \"\"\"\n    # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n    # normalisation here, since we immediately scale the weights to have unit norm (so the\n    # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n    self._weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ] = init.normal_(self._weight, mean=0, std=1)  # type: ignore\n\n    # Scale so that each row has unit norm\n    self.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features","title":"Create a layer with 4 columns (learnt features) and 3 rows (decoded features)","text":"<p>layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None) layer.reset_parameters()</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns","title":"Get the norm across the rows (by summing across the columns)","text":"<p>column_norms = torch.sum(layer.weight ** 2, dim=0) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0, 1.0]</p>"},{"location":"reference/autoencoder/utils/","title":"Sparse autoencoder model &amp; components utils","text":"<p>Sparse autoencoder model &amp; components utils.</p>"},{"location":"reference/autoencoder/utils/tensor_shape/","title":"Tensor shape utilities","text":"<p>Tensor shape utilities.</p>"},{"location":"reference/autoencoder/utils/tensor_shape/#sparse_autoencoder.autoencoder.utils.tensor_shape.shape_with_optional_dimensions","title":"<code>shape_with_optional_dimensions(*shape)</code>","text":"<p>Create a shape from a tuple of optional dimensions.</p> Motivation <p>By default PyTorch tensor shapes will error if you set an axis to <code>None</code>. This allows you to set that size and then the resulting output simply removes that axis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(1, 2, 3)\n(1, 2, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(1, None, 3)\n(1, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(1, None, None)\n(1,)\n</code></pre> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(None, None, None)\n()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*shape</code> <code>int | None</code> <p>Axis sizes, with <code>None</code> representing an optional axis.</p> <code>()</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Axis sizes.</p> Source code in <code>sparse_autoencoder/autoencoder/utils/tensor_shape.py</code> <pre><code>def shape_with_optional_dimensions(*shape: int | None) -&gt; tuple[int, ...]:\n    \"\"\"Create a shape from a tuple of optional dimensions.\n\n    Motivation:\n        By default PyTorch tensor shapes will error if you set an axis to `None`. This allows\n        you to set that size and then the resulting output simply removes that axis.\n\n    Examples:\n        &gt;&gt;&gt; shape_with_optional_dimensions(1, 2, 3)\n        (1, 2, 3)\n\n        &gt;&gt;&gt; shape_with_optional_dimensions(1, None, 3)\n        (1, 3)\n\n        &gt;&gt;&gt; shape_with_optional_dimensions(1, None, None)\n        (1,)\n\n        &gt;&gt;&gt; shape_with_optional_dimensions(None, None, None)\n        ()\n\n    Args:\n        *shape: Axis sizes, with `None` representing an optional axis.\n\n    Returns:\n        Axis sizes.\n    \"\"\"\n    return tuple(dimension for dimension in shape if dimension is not None)\n</code></pre>"},{"location":"reference/loss/","title":"Loss Modules","text":"<p>Loss Modules.</p> <p>Loss modules are specialised PyTorch modules that calculate the loss for a Sparse Autoencoder. They all inherit from AbstractLoss, which defines the interface for loss modules and some common methods.</p> <p>If you want to create your own loss function, see :class:<code>AbstractLoss</code>.</p> <p>For combining multiple loss modules into a single loss module, see :class:<code>LossReducer</code>.</p>"},{"location":"reference/loss/abstract_loss/","title":"Abstract loss","text":"<p>Abstract loss.</p>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossLogType","title":"<code>LossLogType: TypeAlias = dict[str, int | float | str]</code>  <code>module-attribute</code>","text":"<p>Loss log dict.</p>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss","title":"<code>AbstractLoss</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract loss interface.</p> <p>Interface for implementing batch itemwise loss functions.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>class AbstractLoss(Module, ABC):\n    \"\"\"Abstract loss interface.\n\n    Interface for implementing batch itemwise loss functions.\n    \"\"\"\n\n    _modules: dict[str, \"AbstractLoss\"]  # type: ignore[assignment] (narrowing)\n    \"\"\"Children loss modules.\"\"\"\n\n    @abstractmethod\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Batch itemwise loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n\n    @final\n    def batch_scalar_loss(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; Float[Tensor, Axis.SINGLE_ITEM]:\n        \"\"\"Batch scalar loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Loss for the batch.\n        \"\"\"\n        itemwise_loss = self.forward(source_activations, learned_activations, decoded_activations)\n\n        match reduction:\n            case LossReductionType.MEAN:\n                return itemwise_loss.mean().squeeze()\n            case LossReductionType.SUM:\n                return itemwise_loss.sum().squeeze()\n\n    def batch_scalar_loss_with_log(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n        \"\"\"Batch scalar loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the batch scalar loss and a dict of any properties to log.\n        \"\"\"\n        children_loss_scalars: list[Float[Tensor, Axis.SINGLE_ITEM]] = []\n        metrics: LossLogType = {}\n\n        # If the loss module has children (e.g. it is a reducer):\n        if len(self._modules) &gt; 0:\n            for loss_module in self._modules.values():\n                child_loss, child_metrics = loss_module.batch_scalar_loss_with_log(\n                    source_activations,\n                    learned_activations,\n                    decoded_activations,\n                    reduction=reduction,\n                )\n                children_loss_scalars.append(child_loss)\n                metrics.update(child_metrics)\n\n            # Get the total loss &amp; metric\n            current_module_loss = torch.stack(children_loss_scalars).sum()\n\n        # Otherwise if it is a leaf loss module:\n        else:\n            current_module_loss = self.batch_scalar_loss(\n                source_activations, learned_activations, decoded_activations, reduction\n            )\n\n        # Add in the current loss module's metric\n        log_name = \"train/loss/\" + self.log_name()\n        metrics[log_name] = current_module_loss.detach().cpu().item()\n\n        return current_module_loss, metrics\n\n    @final\n    def __call__(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n        \"\"\"Batch scalar loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the batch scalar loss and a dict of any properties to log.\n        \"\"\"\n        return self.batch_scalar_loss_with_log(\n            source_activations, learned_activations, decoded_activations, reduction\n        )\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.__call__","title":"<code>__call__(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Batch scalar loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[Float[Tensor, SINGLE_ITEM], LossLogType]</code> <p>Tuple of the batch scalar loss and a dict of any properties to log.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@final\ndef __call__(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n    \"\"\"Batch scalar loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the batch scalar loss and a dict of any properties to log.\n    \"\"\"\n    return self.batch_scalar_loss_with_log(\n        source_activations, learned_activations, decoded_activations, reduction\n    )\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.batch_scalar_loss","title":"<code>batch_scalar_loss(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Batch scalar loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>Float[Tensor, SINGLE_ITEM]</code> <p>Loss for the batch.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@final\ndef batch_scalar_loss(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; Float[Tensor, Axis.SINGLE_ITEM]:\n    \"\"\"Batch scalar loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Loss for the batch.\n    \"\"\"\n    itemwise_loss = self.forward(source_activations, learned_activations, decoded_activations)\n\n    match reduction:\n        case LossReductionType.MEAN:\n            return itemwise_loss.mean().squeeze()\n        case LossReductionType.SUM:\n            return itemwise_loss.sum().squeeze()\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.batch_scalar_loss_with_log","title":"<code>batch_scalar_loss_with_log(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Batch scalar loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[Float[Tensor, SINGLE_ITEM], LossLogType]</code> <p>Tuple of the batch scalar loss and a dict of any properties to log.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>def batch_scalar_loss_with_log(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n    \"\"\"Batch scalar loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the batch scalar loss and a dict of any properties to log.\n    \"\"\"\n    children_loss_scalars: list[Float[Tensor, Axis.SINGLE_ITEM]] = []\n    metrics: LossLogType = {}\n\n    # If the loss module has children (e.g. it is a reducer):\n    if len(self._modules) &gt; 0:\n        for loss_module in self._modules.values():\n            child_loss, child_metrics = loss_module.batch_scalar_loss_with_log(\n                source_activations,\n                learned_activations,\n                decoded_activations,\n                reduction=reduction,\n            )\n            children_loss_scalars.append(child_loss)\n            metrics.update(child_metrics)\n\n        # Get the total loss &amp; metric\n        current_module_loss = torch.stack(children_loss_scalars).sum()\n\n    # Otherwise if it is a leaf loss module:\n    else:\n        current_module_loss = self.batch_scalar_loss(\n            source_activations, learned_activations, decoded_activations, reduction\n        )\n\n    # Add in the current loss module's metric\n    log_name = \"train/loss/\" + self.log_name()\n    metrics[log_name] = current_module_loss.detach().cpu().item()\n\n    return current_module_loss, metrics\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>  <code>abstractmethod</code>","text":"<p>Batch itemwise loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Batch itemwise loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.log_name","title":"<code>log_name()</code>  <code>abstractmethod</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@abstractmethod\ndef log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType","title":"<code>LossReductionType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Loss reduction type (across batch items).</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>class LossReductionType(LowercaseStrEnum):\n    \"\"\"Loss reduction type (across batch items).\"\"\"\n\n    MEAN = \"mean\"\n    \"\"\"Mean loss across batch items.\"\"\"\n\n    SUM = \"sum\"\n    \"\"\"Sum the loss from all batch items.\"\"\"\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN","title":"<code>MEAN = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean loss across batch items.</p>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.SUM","title":"<code>SUM = 'sum'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sum the loss from all batch items.</p>"},{"location":"reference/loss/decoded_activations_l2/","title":"L2 Reconstruction loss","text":"<p>L2 Reconstruction loss.</p>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss","title":"<code>L2ReconstructionLoss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>L2 Reconstruction loss.</p> <p>L2 reconstruction loss is calculated as the sum squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with L2 may achieve the same loss for both polysemantic and monosemantic representations of true features.</p> Example <p>import torch loss = L2ReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>@final\nclass L2ReconstructionLoss(AbstractLoss):\n    \"\"\"L2 Reconstruction loss.\n\n    L2 reconstruction loss is calculated as the sum squared error between each each input vector\n    and it's corresponding decoded vector. The original paper found that models trained with some\n    loss functions such as cross-entropy loss generally prefer to represent features\n    polysemantically, whereas models trained with L2 may achieve the same loss for both\n    polysemantic and monosemantic representations of true features.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; loss = L2ReconstructionLoss()\n        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])\n        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)\n        &gt;&gt;&gt; # Outputs both loss and metrics to log\n        &gt;&gt;&gt; loss(input_activations, unused_activations, output_activations)\n        (tensor(5.5000), {'train/loss/l2_reconstruction_loss': 5.5})\n    \"\"\"\n\n    _reduction: LossReductionType\n    \"\"\"MSE reduction type.\"\"\"\n\n    def __init__(self, reduction: LossReductionType = LossReductionType.MEAN) -&gt; None:\n        \"\"\"Initialise the L2 reconstruction loss.\n\n        Args:\n            reduction: MSE reduction type.\n        \"\"\"\n        super().__init__()\n        self._reduction = reduction\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"l2_reconstruction_loss\"\n\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],  # noqa: ARG002\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Calculate the L2 reconstruction loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n        match self._reduction:\n            case LossReductionType.MEAN:\n                return square_error_loss.mean(dim=-1)\n            case LossReductionType.SUM:\n                return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log","title":"Outputs both loss and metrics to log","text":"<p>loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'train/loss/l2_reconstruction_loss': 5.5})</p>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss.__init__","title":"<code>__init__(reduction=LossReductionType.MEAN)</code>","text":"<p>Initialise the L2 reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>LossReductionType</code> <p>MSE reduction type.</p> <code>MEAN</code> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def __init__(self, reduction: LossReductionType = LossReductionType.MEAN) -&gt; None:\n    \"\"\"Initialise the L2 reconstruction loss.\n\n    Args:\n        reduction: MSE reduction type.\n    \"\"\"\n    super().__init__()\n    self._reduction = reduction\n</code></pre>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Calculate the L2 reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],  # noqa: ARG002\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Calculate the L2 reconstruction loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n    match self._reduction:\n        case LossReductionType.MEAN:\n            return square_error_loss.mean(dim=-1)\n        case LossReductionType.SUM:\n            return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"l2_reconstruction_loss\"\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/","title":"Learned activations L1 (absolute error) loss","text":"<p>Learned activations L1 (absolute error) loss.</p>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss","title":"<code>LearnedActivationsL1Loss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Learned activations L1 (absolute error) loss.</p> <p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity).</p> Example <p>l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>@final\nclass LearnedActivationsL1Loss(AbstractLoss):\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this\n    multiplied by the l1_coefficient (designed to encourage sparsity).\n\n    Example:\n        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)\n        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)\n        &gt;&gt;&gt; # Returns loss and metrics to log\n        &gt;&gt;&gt; l1_loss(unused_activations, learned_activations, unused_activations)[0]\n        tensor(0.5000)\n    \"\"\"\n\n    l1_coefficient: float\n    \"\"\"L1 coefficient.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"learned_activations_l1_loss_penalty\"\n\n    def __init__(self, l1_coefficient: float) -&gt; None:\n        \"\"\"Initialize the absolute error loss.\n\n        Args:\n            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n                approximate guide if you use e.g. 2x this number of tokens you might consider using\n                0.5x the l1 coefficient.\n        \"\"\"\n        self.l1_coefficient = l1_coefficient\n        super().__init__()\n\n    def _l1_loss(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],  # noqa: ARG002\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],  # noqa: ARG002\n    ) -&gt; tuple[Float[Tensor, Axis.BATCH], Float[Tensor, Axis.BATCH]]:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Tuple of itemwise absolute loss, and itemwise absolute loss multiplied by the l1\n            coefficient.\n        \"\"\"\n        absolute_loss = torch.abs(learned_activations).sum(dim=-1)\n        absolute_loss_penalty = absolute_loss * self.l1_coefficient\n        return absolute_loss, absolute_loss_penalty\n\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n\n    # Override to add both the loss and the penalty to the log\n    def batch_scalar_loss_with_log(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n        \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n                (loss before and after the l1 coefficient).\n        \"\"\"\n        absolute_loss, absolute_loss_penalty = self._l1_loss(\n            source_activations, learned_activations, decoded_activations\n        )\n\n        match reduction:\n            case LossReductionType.MEAN:\n                batch_scalar_loss = absolute_loss.mean().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n            case LossReductionType.SUM:\n                batch_scalar_loss = absolute_loss.sum().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n        metrics = {\n            \"train/loss/\" + \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n            \"train/loss/\" + self.log_name(): batch_scalar_loss_penalty.item(),\n        }\n\n        return batch_scalar_loss_penalty, metrics\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Extra representation string.\"\"\"\n        return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","title":"Returns loss and metrics to log","text":"<p>l1_loss(unused_activations, learned_activations, unused_activations)[0] tensor(0.5000)</p>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.l1_coefficient","title":"<code>l1_coefficient: float = l1_coefficient</code>  <code>instance-attribute</code>","text":"<p>L1 coefficient.</p>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.__init__","title":"<code>__init__(l1_coefficient)</code>","text":"<p>Initialize the absolute error loss.</p> <p>Parameters:</p> Name Type Description Default <code>l1_coefficient</code> <code>float</code> <p>L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient.</p> required Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def __init__(self, l1_coefficient: float) -&gt; None:\n    \"\"\"Initialize the absolute error loss.\n\n    Args:\n        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n            approximate guide if you use e.g. 2x this number of tokens you might consider using\n            0.5x the l1 coefficient.\n    \"\"\"\n    self.l1_coefficient = l1_coefficient\n    super().__init__()\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.batch_scalar_loss_with_log","title":"<code>batch_scalar_loss_with_log(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Learned activations L1 (absolute error) loss, with log.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[Float[Tensor, SINGLE_ITEM], LossLogType]</code> <p>Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log (loss before and after the l1 coefficient).</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def batch_scalar_loss_with_log(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[Float[Tensor, Axis.SINGLE_ITEM], LossLogType]:\n    \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n            (loss before and after the l1 coefficient).\n    \"\"\"\n    absolute_loss, absolute_loss_penalty = self._l1_loss(\n        source_activations, learned_activations, decoded_activations\n    )\n\n    match reduction:\n        case LossReductionType.MEAN:\n            batch_scalar_loss = absolute_loss.mean().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n        case LossReductionType.SUM:\n            batch_scalar_loss = absolute_loss.sum().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n    metrics = {\n        \"train/loss/\" + \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n        \"train/loss/\" + self.log_name(): batch_scalar_loss_penalty.item(),\n    }\n\n    return batch_scalar_loss_penalty, metrics\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Extra representation string.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Extra representation string.\"\"\"\n    return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Learned activations L1 (absolute error) loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"learned_activations_l1_loss_penalty\"\n</code></pre>"},{"location":"reference/loss/reducer/","title":"Loss reducer","text":"<p>Loss reducer.</p>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer","title":"<code>LossReducer</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Loss reducer.</p> <p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential.</p> Example <p>from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ...     L2ReconstructionLoss(), ...     LearnedActivationsL1Loss(0.001), ... ) LossReducer(   (0): L2ReconstructionLoss()   (1): LearnedActivationsL1Loss(l1_coefficient=0.001) )</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>@final\nclass LossReducer(AbstractLoss):\n    \"\"\"Loss reducer.\n\n    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to\n    nn.Sequential.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss\n        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss\n        &gt;&gt;&gt; LossReducer(\n        ...     L2ReconstructionLoss(),\n        ...     LearnedActivationsL1Loss(0.001),\n        ... )\n        LossReducer(\n          (0): L2ReconstructionLoss()\n          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)\n        )\n\n    \"\"\"\n\n    _modules: dict[str, \"AbstractLoss\"]\n    \"\"\"Children loss modules.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"total_loss\"\n\n    def __init__(\n        self,\n        *loss_modules: AbstractLoss,\n    ):\n        \"\"\"Initialize the loss reducer.\n\n        Args:\n            *loss_modules: Loss modules to reduce.\n\n        Raises:\n            ValueError: If the loss reducer has no loss modules.\n        \"\"\"\n        super().__init__()\n\n        for idx, loss_module in enumerate(loss_modules):\n            self._modules[str(idx)] = loss_module\n\n        if len(self) == 0:\n            error_message = \"Loss reducer must have at least one loss module.\"\n            raise ValueError(error_message)\n\n    def forward(\n        self,\n        source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n        decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Reduce loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Mean loss across the batch, summed across the loss modules.\n        \"\"\"\n        all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n            [\n                loss_module.forward(source_activations, learned_activations, decoded_activations)\n                for loss_module in self._modules.values()\n            ]\n        )\n\n        return all_modules_loss.sum(dim=0)\n\n    def __dir__(self) -&gt; list[str]:\n        \"\"\"Dir dunder method.\"\"\"\n        return list(self._modules.__dir__())\n\n    def __getitem__(self, idx: int) -&gt; AbstractLoss:\n        \"\"\"Get item dunder method.\"\"\"\n        return self._modules[str(idx)]\n\n    def __iter__(self) -&gt; Iterator[AbstractLoss]:\n        \"\"\"Iterator dunder method.\"\"\"\n        return iter(self._modules.values())\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length dunder method.\"\"\"\n        return len(self._modules)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__dir__","title":"<code>__dir__()</code>","text":"<p>Dir dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __dir__(self) -&gt; list[str]:\n    \"\"\"Dir dunder method.\"\"\"\n    return list(self._modules.__dir__())\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get item dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; AbstractLoss:\n    \"\"\"Get item dunder method.\"\"\"\n    return self._modules[str(idx)]\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__init__","title":"<code>__init__(*loss_modules)</code>","text":"<p>Initialize the loss reducer.</p> <p>Parameters:</p> Name Type Description Default <code>*loss_modules</code> <code>AbstractLoss</code> <p>Loss modules to reduce.</p> <code>()</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loss reducer has no loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __init__(\n    self,\n    *loss_modules: AbstractLoss,\n):\n    \"\"\"Initialize the loss reducer.\n\n    Args:\n        *loss_modules: Loss modules to reduce.\n\n    Raises:\n        ValueError: If the loss reducer has no loss modules.\n    \"\"\"\n    super().__init__()\n\n    for idx, loss_module in enumerate(loss_modules):\n        self._modules[str(idx)] = loss_module\n\n    if len(self) == 0:\n        error_message = \"Loss reducer must have at least one loss module.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterator dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __iter__(self) -&gt; Iterator[AbstractLoss]:\n    \"\"\"Iterator dunder method.\"\"\"\n    return iter(self._modules.values())\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__len__","title":"<code>__len__()</code>","text":"<p>Length dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length dunder method.\"\"\"\n    return len(self._modules)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Reduce loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>Mean loss across the batch, summed across the loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def forward(\n    self,\n    source_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Reduce loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Mean loss across the batch, summed across the loss modules.\n    \"\"\"\n    all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n        [\n            loss_module.forward(source_activations, learned_activations, decoded_activations)\n            for loss_module in self._modules.values()\n        ]\n    )\n\n    return all_modules_loss.sum(dim=0)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"total_loss\"\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics","text":"<p>Metrics.</p>"},{"location":"reference/metrics/metrics_container/","title":"Metrics container","text":"<p>Metrics container.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.default_metrics","title":"<code>default_metrics = MetricsContainer(train_metrics=[TrainBatchFeatureDensityMetric(), CapacityMetric(), TrainBatchLearnedActivationsL0(), NeuronActivityMetric()], validation_metrics=[ModelReconstructionScore()])</code>  <code>module-attribute</code>","text":"<p>Default metrics container.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer","title":"<code>MetricsContainer</code>  <code>dataclass</code>","text":"<p>Metrics container.</p> <p>Stores all metrics used in a pipeline.</p> Source code in <code>sparse_autoencoder/metrics/metrics_container.py</code> <pre><code>@dataclass\nclass MetricsContainer:\n    \"\"\"Metrics container.\n\n    Stores all metrics used in a pipeline.\n    \"\"\"\n\n    generate_metrics: list[AbstractGenerateMetric] = field(default_factory=list)\n    \"\"\"Metrics for the generate section.\"\"\"\n\n    train_metrics: list[AbstractTrainMetric] = field(default_factory=list)\n    \"\"\"Metrics for the train section.\"\"\"\n\n    validation_metrics: list[AbstractValidationMetric] = field(default_factory=list)\n    \"\"\"Metrics for the validation section.\"\"\"\n</code></pre>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.generate_metrics","title":"<code>generate_metrics: list[AbstractGenerateMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the generate section.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.train_metrics","title":"<code>train_metrics: list[AbstractTrainMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the train section.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.validation_metrics","title":"<code>validation_metrics: list[AbstractValidationMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the validation section.</p>"},{"location":"reference/metrics/generate/","title":"Generate step metrics","text":"<p>Generate step metrics.</p>"},{"location":"reference/metrics/generate/#sparse_autoencoder.metrics.generate.AbstractGenerateMetric","title":"<code>AbstractGenerateMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract generate metric.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>class AbstractGenerateMetric(ABC):\n    \"\"\"Abstract generate metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/#sparse_autoencoder.metrics.generate.AbstractGenerateMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/abstract_generate_metric/","title":"Abstract generate metric","text":"<p>Abstract generate metric.</p>"},{"location":"reference/metrics/generate/abstract_generate_metric/#sparse_autoencoder.metrics.generate.abstract_generate_metric.AbstractGenerateMetric","title":"<code>AbstractGenerateMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract generate metric.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>class AbstractGenerateMetric(ABC):\n    \"\"\"Abstract generate metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/abstract_generate_metric/#sparse_autoencoder.metrics.generate.abstract_generate_metric.AbstractGenerateMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/abstract_generate_metric/#sparse_autoencoder.metrics.generate.abstract_generate_metric.GenerateMetricData","title":"<code>GenerateMetricData</code>  <code>dataclass</code>","text":"<p>Generate metric data.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>@dataclass\nclass GenerateMetricData:\n    \"\"\"Generate metric data.\"\"\"\n\n    generated_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n</code></pre>"},{"location":"reference/metrics/train/","title":"Train step metrics","text":"<p>Train step metrics.</p>"},{"location":"reference/metrics/train/#sparse_autoencoder.metrics.train.AbstractTrainMetric","title":"<code>AbstractTrainMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract train metric.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>class AbstractTrainMetric(ABC):\n    \"\"\"Abstract train metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/#sparse_autoencoder.metrics.train.AbstractTrainMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/abstract_train_metric/","title":"Abstract train metric","text":"<p>Abstract train metric.</p>"},{"location":"reference/metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric","title":"<code>AbstractTrainMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract train metric.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>class AbstractTrainMetric(ABC):\n    \"\"\"Abstract train metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.TrainMetricData","title":"<code>TrainMetricData</code>  <code>dataclass</code>","text":"<p>Train metric data.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>@dataclass\nclass TrainMetricData:\n    \"\"\"Train metric data.\"\"\"\n\n    input_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n\n    learned_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)]\n\n    decoded_activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n</code></pre>"},{"location":"reference/metrics/train/capacity/","title":"Capacity Metrics","text":"<p>Capacity Metrics.</p>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric","title":"<code>CapacityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Capacities Metrics for Learned Features.</p> <p>Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks.</p> <p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features.</p> <p>If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>class CapacityMetric(AbstractTrainMetric):\n    \"\"\"Capacities Metrics for Learned Features.\n\n    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf).\n\n    Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.\n    Formally it's the ratio of the squared dot product of a feature with itself to the sum of its\n    squared dot products of all features.\n\n    If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is\n    1/n.\n    \"\"\"\n\n    @staticmethod\n    def capacities(\n        features: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.BATCH]:\n        \"\"\"Calculate capacities.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n            &gt;&gt;&gt; orthogonal_caps\n            tensor([1., 1., 1.])\n\n        Args:\n            features: A collection of features.\n\n        Returns:\n            A 1D tensor of capacities, where each element is the capacity of the corresponding\n            feature.\n        \"\"\"\n        squared_dot_products = (\n            einops.einsum(\n                features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n            )\n            ** 2\n        )\n        sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n        return torch.diag(squared_dot_products) / sum_of_sq_dot\n\n    @staticmethod\n    def wandb_capacities_histogram(\n        capacities: Float[Tensor, Axis.BATCH],\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the capacities.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n        wandb_capacities_histogram(capacities)})`.\n\n        Args:\n            capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the capacities for a training batch.\"\"\"\n        train_batch_capacities = self.capacities(data.learned_activations)\n        train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n        return {\n            \"train/batch_capacities_histogram\": train_batch_capacities_histogram,\n        }\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the capacities for a training batch.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the capacities for a training batch.\"\"\"\n    train_batch_capacities = self.capacities(data.learned_activations)\n    train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n    return {\n        \"train/batch_capacities_histogram\": train_batch_capacities_histogram,\n    }\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.capacities","title":"<code>capacities(features)</code>  <code>staticmethod</code>","text":"<p>Calculate capacities.</p> Example <p>import torch orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) orthogonal_caps = CapacityMetric.capacities(orthogonal_features) orthogonal_caps tensor([1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>A collection of features.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, BATCH]</code> <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p> <code>Float[Tensor, BATCH]</code> <p>feature.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef capacities(\n    features: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)],\n) -&gt; Float[Tensor, Axis.BATCH]:\n    \"\"\"Calculate capacities.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n        &gt;&gt;&gt; orthogonal_caps\n        tensor([1., 1., 1.])\n\n    Args:\n        features: A collection of features.\n\n    Returns:\n        A 1D tensor of capacities, where each element is the capacity of the corresponding\n        feature.\n    \"\"\"\n    squared_dot_products = (\n        einops.einsum(\n            features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n        )\n        ** 2\n    )\n    sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n    return torch.diag(squared_dot_products) / sum_of_sq_dot\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.wandb_capacities_histogram","title":"<code>wandb_capacities_histogram(capacities)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the capacities.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>Float[Tensor, BATCH]</code> <p>Capacity of each feature. Can be calculated using :func:<code>calc_capacities</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef wandb_capacities_histogram(\n    capacities: Float[Tensor, Axis.BATCH],\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the capacities.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n    wandb_capacities_histogram(capacities)})`.\n\n    Args:\n        capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/metrics/train/feature_density/","title":"Train batch feature density","text":"<p>Train batch feature density.</p>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric","title":"<code>TrainBatchFeatureDensityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Train batch feature density.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a training batch.</p> <p>Generally we want a small number of features to be active in each batch, so average feature density should be low. By contrast if the average feature density is high, it means that the features are not sparse enough.</p> Warning <p>This is not the same as the feature density of the entire training set. It's main use is tracking the progress of training.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>class TrainBatchFeatureDensityMetric(AbstractTrainMetric):\n    \"\"\"Train batch feature density.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a\n    training batch.\n\n    Generally we want a small number of features to be active in each batch, so average feature\n    density should be low. By contrast if the average feature density is high, it means that the\n    features are not sparse enough.\n\n    Warning:\n        This is not the same as the feature density of the entire training set. It's main use is\n        tracking the progress of training.\n    \"\"\"\n\n    threshold: float\n\n    def __init__(self, threshold: float = 0.0) -&gt; None:\n        \"\"\"Initialise the train batch feature density metric.\n\n        Args:\n            threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n                This should be close to zero.\n        \"\"\"\n        super().__init__()\n        self.threshold = threshold\n\n    def feature_density(\n        self, activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)]\n    ) -&gt; Float[Tensor, Axis.LEARNT_FEATURE]:\n        \"\"\"Count how many times each feature was active.\n\n        Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n            &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n            [1.0, 0.5, 0.0]\n\n        Args:\n            activations: Sample of cached activations (the Autoencoder's learned features).\n\n        Returns:\n            Number of times each feature was active in a sample.\n        \"\"\"\n        has_fired: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)] = torch.gt(\n            activations, self.threshold\n        ).to(\n            dtype=torch.float  # Move to float so it can be averaged\n        )\n\n        return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n\n    @staticmethod\n    def wandb_feature_density_histogram(\n        feature_density: Float[Tensor, Axis.LEARNT_FEATURE],\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the feature density.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n        wandb_feature_density_histogram(feature_density)})`.\n\n        Args:\n            feature_density: Number of times each feature was active in a sample. Can be calculated\n                using :func:`feature_activity_count`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_feature_density, bins=50)\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the train batch feature density metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary with the train batch feature density metric, and a histogram of the feature\n            density.\n        \"\"\"\n        train_batch_feature_density: Float[Tensor, Axis.LEARNT_FEATURE] = self.feature_density(\n            data.learned_activations\n        )\n\n        train_batch_feature_density_histogram: wandb.Histogram = (\n            self.wandb_feature_density_histogram(train_batch_feature_density)\n        )\n\n        return {\n            \"train/batch_feature_density_histogram\": train_batch_feature_density_histogram,\n        }\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.__init__","title":"<code>__init__(threshold=0.0)</code>","text":"<p>Initialise the train batch feature density metric.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero.</p> <code>0.0</code> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def __init__(self, threshold: float = 0.0) -&gt; None:\n    \"\"\"Initialise the train batch feature density metric.\n\n    Args:\n        threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n            This should be close to zero.\n    \"\"\"\n    super().__init__()\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the train batch feature density metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with the train batch feature density metric, and a histogram of the feature</p> <code>dict[str, Any]</code> <p>density.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the train batch feature density metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary with the train batch feature density metric, and a histogram of the feature\n        density.\n    \"\"\"\n    train_batch_feature_density: Float[Tensor, Axis.LEARNT_FEATURE] = self.feature_density(\n        data.learned_activations\n    )\n\n    train_batch_feature_density_histogram: wandb.Histogram = (\n        self.wandb_feature_density_histogram(train_batch_feature_density)\n    )\n\n    return {\n        \"train/batch_feature_density_histogram\": train_batch_feature_density_histogram,\n    }\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.feature_density","title":"<code>feature_density(activations)</code>","text":"<p>Count how many times each feature was active.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").</p> Example <p>import torch activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist() [1.0, 0.5, 0.0]</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Float[Tensor, names(BATCH, LEARNT_FEATURE)]</code> <p>Sample of cached activations (the Autoencoder's learned features).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, LEARNT_FEATURE]</code> <p>Number of times each feature was active in a sample.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def feature_density(\n    self, activations: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)]\n) -&gt; Float[Tensor, Axis.LEARNT_FEATURE]:\n    \"\"\"Count how many times each feature was active.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n        &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n        [1.0, 0.5, 0.0]\n\n    Args:\n        activations: Sample of cached activations (the Autoencoder's learned features).\n\n    Returns:\n        Number of times each feature was active in a sample.\n    \"\"\"\n    has_fired: Float[Tensor, Axis.names(Axis.BATCH, Axis.LEARNT_FEATURE)] = torch.gt(\n        activations, self.threshold\n    ).to(\n        dtype=torch.float  # Move to float so it can be averaged\n    )\n\n    return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram","title":"<code>wandb_feature_density_histogram(feature_density)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the feature density.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature_density</code> <code>Float[Tensor, LEARNT_FEATURE]</code> <p>Number of times each feature was active in a sample. Can be calculated using :func:<code>feature_activity_count</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>@staticmethod\ndef wandb_feature_density_histogram(\n    feature_density: Float[Tensor, Axis.LEARNT_FEATURE],\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the feature density.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n    wandb_feature_density_histogram(feature_density)})`.\n\n    Args:\n        feature_density: Number of times each feature was active in a sample. Can be calculated\n            using :func:`feature_activity_count`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_feature_density, bins=50)\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/metrics/train/l0_norm_metric/","title":"L0 norm sparsity metric","text":"<p>L0 norm sparsity metric.</p>"},{"location":"reference/metrics/train/l0_norm_metric/#sparse_autoencoder.metrics.train.l0_norm_metric.TrainBatchLearnedActivationsL0","title":"<code>TrainBatchLearnedActivationsL0</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Learned activations L0 norm sparsity metric.</p> <p>The L0 norm is the number of non-zero elements in a learned activation vector. We then average this over the batch.</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm_metric.py</code> <pre><code>@final\nclass TrainBatchLearnedActivationsL0(AbstractTrainMetric):\n    \"\"\"Learned activations L0 norm sparsity metric.\n\n    The L0 norm is the number of non-zero elements in a learned activation vector. We then average\n    this over the batch.\n    \"\"\"\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, float]:\n        \"\"\"Create a log item for Weights and Biases.\"\"\"\n        batch_size = data.learned_activations.size(0)\n        n_non_zero_activations = torch.count_nonzero(data.learned_activations)\n        batch_average = n_non_zero_activations / batch_size\n        return {\"train/learned_activations_l0_norm\": batch_average.item()}\n</code></pre>"},{"location":"reference/metrics/train/l0_norm_metric/#sparse_autoencoder.metrics.train.l0_norm_metric.TrainBatchLearnedActivationsL0.calculate","title":"<code>calculate(data)</code>","text":"<p>Create a log item for Weights and Biases.</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm_metric.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, float]:\n    \"\"\"Create a log item for Weights and Biases.\"\"\"\n    batch_size = data.learned_activations.size(0)\n    n_non_zero_activations = torch.count_nonzero(data.learned_activations)\n    batch_average = n_non_zero_activations / batch_size\n    return {\"train/learned_activations_l0_norm\": batch_average.item()}\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/","title":"Neuron activity metric","text":"<p>Neuron activity metric.</p> <p>Logs the number of dead and alive neurons at various horizons. Also logs histograms of neuron activity, and the number of neurons that are almost dead.</p>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.DEFAULT_HORIZONS","title":"<code>DEFAULT_HORIZONS = [10000, 100000, 500000, 1000000, 10000000]</code>  <code>module-attribute</code>","text":"<p>Default horizons.</p>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.DEFAULT_THRESHOLDS","title":"<code>DEFAULT_THRESHOLDS = [1e-05, 1e-06]</code>  <code>module-attribute</code>","text":"<p>Default thresholds for determining if a neuron is almost dead.</p>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityHorizonData","title":"<code>NeuronActivityHorizonData</code>","text":"<p>Neuron activity data for a single horizon.</p> <p>For each time horizon we store some data (e.g. the number of times each neuron fired inside this time horizon). This class also contains some helper methods for then calculating metrics from this data.</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>class NeuronActivityHorizonData:\n    \"\"\"Neuron activity data for a single horizon.\n\n    For each time horizon we store some data (e.g. the number of times each neuron fired inside this\n    time horizon). This class also contains some helper methods for then calculating metrics from\n    this data.\n    \"\"\"\n\n    _horizon_number_activations: int\n    \"\"\"Horizon in number of activations.\"\"\"\n\n    _horizon_steps: int\n    \"\"\"Horizon in number of steps.\"\"\"\n\n    _steps_since_last_calculated: int\n    \"\"\"Steps since last calculated.\"\"\"\n\n    _neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE]\n    \"\"\"Neuron activity since inception.\"\"\"\n\n    _thresholds: list[float]\n    \"\"\"Thresholds for almost dead neurons.\"\"\"\n\n    @property\n    def _dead_count(self) -&gt; int:\n        \"\"\"Dead count.\"\"\"\n        dead_bool_mask: Int64[Tensor, Axis.LEARNT_FEATURE] = self._neuron_activity == 0\n        count_dead: Int64[Tensor, Axis.SINGLE_ITEM] = dead_bool_mask.sum()\n        return int(count_dead.item())\n\n    @property\n    def _dead_fraction(self) -&gt; float:\n        \"\"\"Dead fraction.\"\"\"\n        return self._dead_count / self._neuron_activity.shape[-1]\n\n    @property\n    def _alive_count(self) -&gt; int:\n        \"\"\"Alive count.\"\"\"\n        alive_bool_mask: Int64[Tensor, Axis.LEARNT_FEATURE] = self._neuron_activity &gt; 0\n        count_alive: Int64[Tensor, Axis.SINGLE_ITEM] = alive_bool_mask.sum()\n        return int(count_alive.item())\n\n    def _almost_dead(self, threshold: float) -&gt; int | None:\n        \"\"\"Almost dead count.\"\"\"\n        threshold_in_activations: float = threshold * self._horizon_number_activations\n        if threshold_in_activations &lt; 1:\n            return None\n\n        almost_dead_bool_mask: Int64[Tensor, Axis.LEARNT_FEATURE] = (\n            self._neuron_activity &lt; threshold_in_activations\n        )\n        count_almost_dead: Int64[Tensor, Axis.SINGLE_ITEM] = almost_dead_bool_mask.sum()\n        return int(count_almost_dead.item())\n\n    @property\n    def _activity_histogram(self) -&gt; wandb.Histogram:\n        \"\"\"Activity histogram.\"\"\"\n        numpy_neuron_activity: NDArray[np.float_] = self._neuron_activity.detach().cpu().numpy()\n        bins, values = np.histogram(numpy_neuron_activity, bins=50)\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    @property\n    def _log_activity_histogram(self) -&gt; wandb.Histogram:\n        \"\"\"Log activity histogram.\"\"\"\n        numpy_neuron_activity: NDArray[np.float_] = self._neuron_activity.detach().cpu().numpy()\n        log_epsilon = 0.1  # To avoid log(0)\n        log_neuron_activity = np.log(numpy_neuron_activity + log_epsilon)\n        bins, values = np.histogram(log_neuron_activity, bins=50)\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name.\"\"\"\n        return f\"over_{self._horizon_number_activations}_activations\"\n\n    @property\n    def wandb_log_values(self) -&gt; dict[str, Any]:\n        \"\"\"Wandb log values.\"\"\"\n        log = {\n            f\"train/activity/{self.name}/dead_count\": self._dead_count,\n            f\"train/activity/{self.name}/alive_count\": self._alive_count,\n            f\"train/activity/{self.name}/activity_histogram\": self._activity_histogram,\n            f\"train/activity/{self.name}/log_activity_histogram\": self._log_activity_histogram,\n        }\n\n        for threshold in self._thresholds:\n            almost_dead_count = self._almost_dead(threshold)\n            if almost_dead_count is not None:\n                log[f\"train/activity/{self.name}/almost_dead_{threshold}\"] = almost_dead_count\n\n        return log\n\n    def __init__(\n        self,\n        approximate_activation_horizon: int,\n        train_batch_size: int,\n        number_learned_features: int,\n        thresholds: list[float],\n    ) -&gt; None:\n        \"\"\"Initialise the neuron activity horizon data.\n\n        Args:\n            approximate_activation_horizon: Approximate activation horizon.\n            train_batch_size: Train batch size.\n            number_learned_features: Number of learned features.\n            thresholds: Thresholds for almost dead neurons.\n        \"\"\"\n        self._steps_since_last_calculated = 0\n        self._neuron_activity = torch.zeros(number_learned_features, dtype=torch.int64)\n        self._thresholds = thresholds\n\n        # Get a precise activation_horizon\n        self._horizon_steps = approximate_activation_horizon // train_batch_size\n        self._horizon_number_activations = self._horizon_steps * train_batch_size\n\n    def step(self, neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE]) -&gt; dict[str, Any]:\n        \"\"\"Step the neuron activity horizon data.\n\n        Args:\n            neuron_activity: Neuron activity.\n\n        Returns:\n            Dictionary of metrics (or empty dictionary if no metrics are ready to be logged).\n        \"\"\"\n        self._steps_since_last_calculated += 1\n        self._neuron_activity += neuron_activity\n\n        if self._steps_since_last_calculated &gt;= self._horizon_steps:\n            result = {**self.wandb_log_values}\n            self._steps_since_last_calculated = 0\n            self._neuron_activity = torch.zeros_like(self._neuron_activity)\n            return result\n\n        return {}\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityHorizonData.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Name.</p>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityHorizonData.wandb_log_values","title":"<code>wandb_log_values: dict[str, Any]</code>  <code>property</code>","text":"<p>Wandb log values.</p>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityHorizonData.__init__","title":"<code>__init__(approximate_activation_horizon, train_batch_size, number_learned_features, thresholds)</code>","text":"<p>Initialise the neuron activity horizon data.</p> <p>Parameters:</p> Name Type Description Default <code>approximate_activation_horizon</code> <code>int</code> <p>Approximate activation horizon.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <code>number_learned_features</code> <code>int</code> <p>Number of learned features.</p> required <code>thresholds</code> <code>list[float]</code> <p>Thresholds for almost dead neurons.</p> required Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>def __init__(\n    self,\n    approximate_activation_horizon: int,\n    train_batch_size: int,\n    number_learned_features: int,\n    thresholds: list[float],\n) -&gt; None:\n    \"\"\"Initialise the neuron activity horizon data.\n\n    Args:\n        approximate_activation_horizon: Approximate activation horizon.\n        train_batch_size: Train batch size.\n        number_learned_features: Number of learned features.\n        thresholds: Thresholds for almost dead neurons.\n    \"\"\"\n    self._steps_since_last_calculated = 0\n    self._neuron_activity = torch.zeros(number_learned_features, dtype=torch.int64)\n    self._thresholds = thresholds\n\n    # Get a precise activation_horizon\n    self._horizon_steps = approximate_activation_horizon // train_batch_size\n    self._horizon_number_activations = self._horizon_steps * train_batch_size\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityHorizonData.step","title":"<code>step(neuron_activity)</code>","text":"<p>Step the neuron activity horizon data.</p> <p>Parameters:</p> Name Type Description Default <code>neuron_activity</code> <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Neuron activity.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics (or empty dictionary if no metrics are ready to be logged).</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>def step(self, neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE]) -&gt; dict[str, Any]:\n    \"\"\"Step the neuron activity horizon data.\n\n    Args:\n        neuron_activity: Neuron activity.\n\n    Returns:\n        Dictionary of metrics (or empty dictionary if no metrics are ready to be logged).\n    \"\"\"\n    self._steps_since_last_calculated += 1\n    self._neuron_activity += neuron_activity\n\n    if self._steps_since_last_calculated &gt;= self._horizon_steps:\n        result = {**self.wandb_log_values}\n        self._steps_since_last_calculated = 0\n        self._neuron_activity = torch.zeros_like(self._neuron_activity)\n        return result\n\n    return {}\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityMetric","title":"<code>NeuronActivityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Neuron activity metric.</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>class NeuronActivityMetric(AbstractTrainMetric):\n    \"\"\"Neuron activity metric.\"\"\"\n\n    _approximate_horizons: list[int]\n\n    _data: list[NeuronActivityHorizonData]\n\n    _initialised: bool = False\n\n    _thresholds: list[float]\n\n    def __init__(\n        self,\n        approximate_horizons: list[int] = DEFAULT_HORIZONS,\n        thresholds: list[float] = DEFAULT_THRESHOLDS,\n    ) -&gt; None:\n        \"\"\"Initialise the neuron activity metric.\n\n        time `calculate` is called.\n\n        Args:\n            approximate_horizons: Approximate horizons in number of activations.\n            thresholds: Thresholds for almost dead neurons.\n        \"\"\"\n        super().__init__()\n        self._approximate_horizons = approximate_horizons\n        self._data = []\n        self._thresholds = thresholds\n\n    def initialise_horizons(self, data: TrainMetricData) -&gt; None:\n        \"\"\"Initialise the horizon data structures.\n\n        Args:\n            data: Train metric data.\n        \"\"\"\n        train_batch_size = data.learned_activations.shape[0]\n        number_learned_features = data.learned_activations.shape[-1]\n\n        for horizon in self._approximate_horizons:\n            # Don't add horizons that are smaller than the train batch size\n            if horizon &lt; train_batch_size:\n                continue\n\n            self._data.append(\n                NeuronActivityHorizonData(\n                    approximate_activation_horizon=horizon,\n                    train_batch_size=train_batch_size,\n                    number_learned_features=number_learned_features,\n                    thresholds=self._thresholds,\n                )\n            )\n\n        self._initialised = True\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the neuron activity metrics.\n\n        Args:\n            data: Resample metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n        if not self._initialised:\n            self.initialise_horizons(data)\n\n        log = {}\n\n        for horizon_data in self._data:\n            fired_count: Int64[Tensor, Axis.LEARNT_FEATURE] = (\n                (data.learned_activations &gt; 0).sum(dim=0).detach().cpu()\n            )\n            horizon_specific_log = horizon_data.step(fired_count)\n            log.update(horizon_specific_log)\n\n        return log\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityMetric.__init__","title":"<code>__init__(approximate_horizons=DEFAULT_HORIZONS, thresholds=DEFAULT_THRESHOLDS)</code>","text":"<p>Initialise the neuron activity metric.</p> <p>time <code>calculate</code> is called.</p> <p>Parameters:</p> Name Type Description Default <code>approximate_horizons</code> <code>list[int]</code> <p>Approximate horizons in number of activations.</p> <code>DEFAULT_HORIZONS</code> <code>thresholds</code> <code>list[float]</code> <p>Thresholds for almost dead neurons.</p> <code>DEFAULT_THRESHOLDS</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>def __init__(\n    self,\n    approximate_horizons: list[int] = DEFAULT_HORIZONS,\n    thresholds: list[float] = DEFAULT_THRESHOLDS,\n) -&gt; None:\n    \"\"\"Initialise the neuron activity metric.\n\n    time `calculate` is called.\n\n    Args:\n        approximate_horizons: Approximate horizons in number of activations.\n        thresholds: Thresholds for almost dead neurons.\n    \"\"\"\n    super().__init__()\n    self._approximate_horizons = approximate_horizons\n    self._data = []\n    self._thresholds = thresholds\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the neuron activity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Resample metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the neuron activity metrics.\n\n    Args:\n        data: Resample metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n    if not self._initialised:\n        self.initialise_horizons(data)\n\n    log = {}\n\n    for horizon_data in self._data:\n        fired_count: Int64[Tensor, Axis.LEARNT_FEATURE] = (\n            (data.learned_activations &gt; 0).sum(dim=0).detach().cpu()\n        )\n        horizon_specific_log = horizon_data.step(fired_count)\n        log.update(horizon_specific_log)\n\n    return log\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity_metric/#sparse_autoencoder.metrics.train.neuron_activity_metric.NeuronActivityMetric.initialise_horizons","title":"<code>initialise_horizons(data)</code>","text":"<p>Initialise the horizon data structures.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required Source code in <code>sparse_autoencoder/metrics/train/neuron_activity_metric.py</code> <pre><code>def initialise_horizons(self, data: TrainMetricData) -&gt; None:\n    \"\"\"Initialise the horizon data structures.\n\n    Args:\n        data: Train metric data.\n    \"\"\"\n    train_batch_size = data.learned_activations.shape[0]\n    number_learned_features = data.learned_activations.shape[-1]\n\n    for horizon in self._approximate_horizons:\n        # Don't add horizons that are smaller than the train batch size\n        if horizon &lt; train_batch_size:\n            continue\n\n        self._data.append(\n            NeuronActivityHorizonData(\n                approximate_activation_horizon=horizon,\n                train_batch_size=train_batch_size,\n                number_learned_features=number_learned_features,\n                thresholds=self._thresholds,\n            )\n        )\n\n    self._initialised = True\n</code></pre>"},{"location":"reference/metrics/validate/","title":"Validate step metrics","text":"<p>Validate step metrics.</p>"},{"location":"reference/metrics/validate/#sparse_autoencoder.metrics.validate.AbstractValidationMetric","title":"<code>AbstractValidationMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract validation metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>class AbstractValidationMetric(ABC):\n    \"\"\"Abstract validation metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/#sparse_autoencoder.metrics.validate.AbstractValidationMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/abstract_validate_metric/","title":"Abstract metric classes","text":"<p>Abstract metric classes.</p>"},{"location":"reference/metrics/validate/abstract_validate_metric/#sparse_autoencoder.metrics.validate.abstract_validate_metric.AbstractValidationMetric","title":"<code>AbstractValidationMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract validation metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>class AbstractValidationMetric(ABC):\n    \"\"\"Abstract validation metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/abstract_validate_metric/#sparse_autoencoder.metrics.validate.abstract_validate_metric.AbstractValidationMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/abstract_validate_metric/#sparse_autoencoder.metrics.validate.abstract_validate_metric.ValidationMetricData","title":"<code>ValidationMetricData</code>  <code>dataclass</code>","text":"<p>Validation metric data.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>@dataclass\nclass ValidationMetricData:\n    \"\"\"Validation metric data.\"\"\"\n\n    source_model_loss: Float[Tensor, Axis.ITEMS]\n\n    source_model_loss_with_reconstruction: Float[Tensor, Axis.ITEMS]\n\n    source_model_loss_with_zero_ablation: Float[Tensor, Axis.ITEMS]\n</code></pre>"},{"location":"reference/metrics/validate/model_reconstruction_score/","title":"Model reconstruction score","text":"<p>Model reconstruction score.</p>"},{"location":"reference/metrics/validate/model_reconstruction_score/#sparse_autoencoder.metrics.validate.model_reconstruction_score.ModelReconstructionScore","title":"<code>ModelReconstructionScore</code>","text":"<p>             Bases: <code>AbstractValidationMetric</code></p> <p>Model reconstruction score.</p> <p>Creates a score that measures how well the model can reconstruct the data.</p> \\[ \\begin{align*}     v &amp;= \\text{number of validation items} \\\\     l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\     l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\     l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\     s &amp;= \\text{reconstruction score} \\\\     s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\     s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v \\end{align*} \\] Source code in <code>sparse_autoencoder/metrics/validate/model_reconstruction_score.py</code> <pre><code>class ModelReconstructionScore(AbstractValidationMetric):\n    r\"\"\"Model reconstruction score.\n\n    Creates a score that measures how well the model can reconstruct the data.\n\n    $$\n    \\begin{align*}\n        v &amp;= \\text{number of validation items} \\\\\n        l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\\n        l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\\n        l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\\n        s &amp;= \\text{reconstruction score} \\\\\n        s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\\n        s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v\n    \\end{align*}\n    $$\n    \"\"\"\n\n    def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the model reconstruction score.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; data = ValidationMetricData(\n            ...     source_model_loss=torch.tensor([2.0, 2.0, 2.0]),\n            ...     source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0]),\n            ...     source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0])\n            ... )\n            &gt;&gt;&gt; metric = ModelReconstructionScore()\n            &gt;&gt;&gt; result = metric.calculate(data)\n            &gt;&gt;&gt; round(result['validate/model_reconstruction_score'], 3)\n            0.667\n\n        Args:\n            data: Validation data.\n\n        Returns:\n            Model reconstruction score.\n        \"\"\"\n        # Return no statistics if the data is empty (e.g. if we're at the very end of training)\n        if data.source_model_loss.numel() == 0:\n            return {}\n\n        # Calculate the reconstruction score\n        zero_ablate_loss_minus_default_loss: Float[Tensor, Axis.ITEMS] = (\n            data.source_model_loss_with_zero_ablation - data.source_model_loss\n        )\n        zero_ablate_loss_minus_reconstruction_loss: Float[Tensor, Axis.ITEMS] = (\n            data.source_model_loss_with_zero_ablation - data.source_model_loss_with_reconstruction\n        )\n        model_reconstruction_score: float = (\n            zero_ablate_loss_minus_reconstruction_loss.mean().item()\n            / zero_ablate_loss_minus_default_loss.mean().item()\n        )\n\n        # Get the other metrics\n        validation_baseline_loss: float = data.source_model_loss.mean().item()\n        validation_loss_with_reconstruction: float = (\n            data.source_model_loss_with_reconstruction.mean().item()\n        )\n        validation_loss_with_zero_ablation: float = (\n            data.source_model_loss_with_zero_ablation.mean().item()\n        )\n\n        return {\n            \"validate/baseline_loss\": validation_baseline_loss,\n            \"validate/loss_with_reconstruction\": validation_loss_with_reconstruction,\n            \"validate/loss_with_zero_ablation\": validation_loss_with_zero_ablation,\n            \"validate/model_reconstruction_score\": model_reconstruction_score,\n        }\n</code></pre>"},{"location":"reference/metrics/validate/model_reconstruction_score/#sparse_autoencoder.metrics.validate.model_reconstruction_score.ModelReconstructionScore.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the model reconstruction score.</p> Example <p>import torch data = ValidationMetricData( ...     source_model_loss=torch.tensor([2.0, 2.0, 2.0]), ...     source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0]), ...     source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0]) ... ) metric = ModelReconstructionScore() result = metric.calculate(data) round(result['validate/model_reconstruction_score'], 3) 0.667</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ValidationMetricData</code> <p>Validation data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Model reconstruction score.</p> Source code in <code>sparse_autoencoder/metrics/validate/model_reconstruction_score.py</code> <pre><code>def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the model reconstruction score.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; data = ValidationMetricData(\n        ...     source_model_loss=torch.tensor([2.0, 2.0, 2.0]),\n        ...     source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0]),\n        ...     source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0])\n        ... )\n        &gt;&gt;&gt; metric = ModelReconstructionScore()\n        &gt;&gt;&gt; result = metric.calculate(data)\n        &gt;&gt;&gt; round(result['validate/model_reconstruction_score'], 3)\n        0.667\n\n    Args:\n        data: Validation data.\n\n    Returns:\n        Model reconstruction score.\n    \"\"\"\n    # Return no statistics if the data is empty (e.g. if we're at the very end of training)\n    if data.source_model_loss.numel() == 0:\n        return {}\n\n    # Calculate the reconstruction score\n    zero_ablate_loss_minus_default_loss: Float[Tensor, Axis.ITEMS] = (\n        data.source_model_loss_with_zero_ablation - data.source_model_loss\n    )\n    zero_ablate_loss_minus_reconstruction_loss: Float[Tensor, Axis.ITEMS] = (\n        data.source_model_loss_with_zero_ablation - data.source_model_loss_with_reconstruction\n    )\n    model_reconstruction_score: float = (\n        zero_ablate_loss_minus_reconstruction_loss.mean().item()\n        / zero_ablate_loss_minus_default_loss.mean().item()\n    )\n\n    # Get the other metrics\n    validation_baseline_loss: float = data.source_model_loss.mean().item()\n    validation_loss_with_reconstruction: float = (\n        data.source_model_loss_with_reconstruction.mean().item()\n    )\n    validation_loss_with_zero_ablation: float = (\n        data.source_model_loss_with_zero_ablation.mean().item()\n    )\n\n    return {\n        \"validate/baseline_loss\": validation_baseline_loss,\n        \"validate/loss_with_reconstruction\": validation_loss_with_reconstruction,\n        \"validate/loss_with_zero_ablation\": validation_loss_with_zero_ablation,\n        \"validate/model_reconstruction_score\": model_reconstruction_score,\n    }\n</code></pre>"},{"location":"reference/optimizer/","title":"Optimizers for Sparse Autoencoders","text":"<p>Optimizers for Sparse Autoencoders.</p> <p>When training a Sparse Autoencoder, it can be necessary to manually edit the model parameters (e.g. with neuron resampling to prevent dead neurons). When doing this, it's also necessary to reset the optimizer state for these parameters, as otherwise things like running averages will be incorrect (e.g. the running averages of the gradients and the squares of gradients with Adam).</p> <p>The optimizer used in the original [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning]https://www.anthropic.com/index/towards-monosemanticity-decomposing-language-models-with-dictionary-learning) paper is available here as :class:<code>AdamWithReset</code>.</p> <p>To enable creating other optimizers with reset methods, we also provide the interface :class:<code>AbstractOptimizerWithReset</code>.</p>"},{"location":"reference/optimizer/abstract_optimizer/","title":"Abstract optimizer with reset","text":"<p>Abstract optimizer with reset.</p>"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset","title":"<code>AbstractOptimizerWithReset</code>","text":"<p>             Bases: <code>Optimizer</code>, <code>ABC</code></p> <p>Abstract optimizer with reset.</p> <p>When implementing this interface, we recommend adding a <code>named_parameters</code> argument to the constructor, which can be obtained from <code>named_parameters=model.named_parameters()</code> by the end user. This is so that the optimizer can find the parameters to reset.</p> Source code in <code>sparse_autoencoder/optimizer/abstract_optimizer.py</code> <pre><code>class AbstractOptimizerWithReset(Optimizer, ABC):\n    \"\"\"Abstract optimizer with reset.\n\n    When implementing this interface, we recommend adding a `named_parameters` argument to the\n    constructor, which can be obtained from `named_parameters=model.named_parameters()` by the end\n    user. This is so that the optimizer can find the parameters to reset.\n    \"\"\"\n\n    @abstractmethod\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Resets any optimizer state (e.g. momentum). This is for use after manually editing model\n            parameters (e.g. with activation resampling).\n        \"\"\"\n\n    @abstractmethod\n    def reset_neurons_state(\n        self,\n        parameter: Parameter,\n        neuron_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX],\n        axis: int,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Args:\n            parameter: The parameter to reset, e.g. `encoder.Linear.weight`, `encoder.Linear.bias`,\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the parameter to reset.\n\n        Raises:\n            ValueError: If the parameter name is not found.\n        \"\"\"\n</code></pre>"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter, neuron_indices, axis)</code>  <code>abstractmethod</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>Parameter</code> <p>The parameter to reset, e.g. <code>encoder.Linear.weight</code>, <code>encoder.Linear.bias</code>,</p> required <code>neuron_indices</code> <code>Int64[Tensor, LEARNT_FEATURE_IDX]</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the parameter to reset.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the parameter name is not found.</p> Source code in <code>sparse_autoencoder/optimizer/abstract_optimizer.py</code> <pre><code>@abstractmethod\ndef reset_neurons_state(\n    self,\n    parameter: Parameter,\n    neuron_indices: Int64[Tensor, Axis.LEARNT_FEATURE_IDX],\n    axis: int,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Args:\n        parameter: The parameter to reset, e.g. `encoder.Linear.weight`, `encoder.Linear.bias`,\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the parameter to reset.\n\n    Raises:\n        ValueError: If the parameter name is not found.\n    \"\"\"\n</code></pre>"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the state for all parameters.</p> <p>Resets any optimizer state (e.g. momentum). This is for use after manually editing model     parameters (e.g. with activation resampling).</p> Source code in <code>sparse_autoencoder/optimizer/abstract_optimizer.py</code> <pre><code>@abstractmethod\ndef reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Resets any optimizer state (e.g. momentum). This is for use after manually editing model\n        parameters (e.g. with activation resampling).\n    \"\"\"\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/","title":"Adam Optimizer with a reset method","text":"<p>Adam Optimizer with a reset method.</p> <p>This reset method is useful when resampling dead neurons during training.</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset","title":"<code>AdamWithReset</code>","text":"<p>             Bases: <code>Adam</code>, <code>AbstractOptimizerWithReset</code></p> <p>Adam Optimizer with a reset method.</p> <p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed.</p> <p>Otherwise this is the same as the standard Adam optimizer.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>@final\nclass AdamWithReset(Adam, AbstractOptimizerWithReset):\n    \"\"\"Adam Optimizer with a reset method.\n\n    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when\n    manually editing the model parameters during training (e.g. when resampling dead neurons). This\n    is because Adam maintains running averages of the gradients and the squares of gradients, which\n    will be incorrect if the parameters are changed.\n\n    Otherwise this is the same as the standard Adam optimizer.\n    \"\"\"\n\n    parameter_names: list[str]\n    \"\"\"Parameter Names.\n\n    The names of the parameters, so that we can find them later when resetting the state.\n    \"\"\"\n\n    def __init__(  # (extending existing implementation)\n        self,\n        params: params_t,\n        lr: float | Tensor = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        *,\n        amsgrad: bool = False,\n        foreach: bool | None = None,\n        maximize: bool = False,\n        capturable: bool = False,\n        differentiable: bool = False,\n        fused: bool | None = None,\n        named_parameters: Iterator[tuple[str, Parameter]],\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer.\n\n        Warning:\n            Named parameters must be with default settings (remove duplicates and not recursive).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n        Args:\n            params: Iterable of parameters to optimize or dicts defining parameter groups.\n            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n                float LR unless specifying fused=True or capturable=True.\n            betas: Coefficients used for computing running averages of gradient and its square.\n            eps: Term added to the denominator to improve numerical stability.\n            weight_decay: Weight decay (L2 penalty).\n            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n                Convergence of Adam and Beyond\".\n            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n                over the for-loop implementation on CUDA if more performant. Note that foreach uses\n                more peak memory.\n            maximize: If True, maximizes the parameters based on the objective, instead of\n                minimizing.\n            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n                ungraphed performance.\n            differentiable: Whether autograd should occur through the optimizer step in training.\n                Setting to True can impair performance.\n            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n                torch.float32, torch.float16, and torch.bfloat16.\n            named_parameters: An iterator over the named parameters of the model. This is used to\n                find the parameters when resetting their state. You should set this as\n                `model.named_parameters()`.\n\n        Raises:\n            ValueError: If the number of parameter names does not match the number of parameters.\n        \"\"\"\n        # Initialise the parent class (note we repeat the parameter names so that type hints work).\n        super().__init__(\n            params=params,\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n            differentiable=differentiable,\n            fused=fused,\n        )\n\n        # Store the names of the parameters, so that we can find them later when resetting the\n        # state.\n        self.parameter_names = [name for name, _value in named_parameters]\n\n        if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n            error_message = (\n                \"The number of parameter names does not match the number of parameters. \"\n                \"If using model.named_parameters() make sure remove_duplicates is True \"\n                \"and recursive is False (the default settings).\"\n            )\n            raise ValueError(error_message)\n\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Iterates over all parameters and resets both the running averages of the gradients and the\n        squares of gradients.\n        \"\"\"\n        # Iterate over every parameter\n        for group in self.param_groups:\n            for parameter in group[\"params\"]:\n                # Get the state\n                state = self.state[parameter]\n\n                # Check if state is initialized\n                if len(state) == 0:\n                    continue\n\n                # Reset running averages\n                exp_avg: Tensor = state[\"exp_avg\"]\n                exp_avg.zero_()\n                exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n                exp_avg_sq.zero_()\n\n                # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n                if \"max_exp_avg_sq\" in state:\n                    max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                    max_exp_avg_sq.zero_()\n\n    def reset_neurons_state(\n        self,\n        parameter: Parameter,\n        neuron_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX],\n        axis: int,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\n            ...     model.decoder.weight,\n            ...     dead_neurons_indices,\n            ...     axis=1\n            ... )\n\n        Args:\n            parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the parameter to reset.\n        \"\"\"\n        # Get the state of the parameter\n        state = self.state[parameter]\n\n        # Check if state is initialized\n        if len(state) == 0:\n            return\n\n        # Check there are any neurons to reset\n        if neuron_indices.numel() == 0:\n            return\n\n        # Reset running averages for the specified neurons\n        if \"exp_avg\" in state:\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.index_fill_(axis, neuron_indices.to(exp_avg.device), 0)\n        if \"exp_avg_sq\" in state:\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.index_fill_(axis, neuron_indices.to(exp_avg_sq.device), 0)\n\n        # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n        if \"max_exp_avg_sq\" in state:\n            max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n            max_exp_avg_sq.index_fill_(axis, neuron_indices.to(max_exp_avg_sq.device), 0)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.parameter_names","title":"<code>parameter_names: list[str] = [name for (name, _value) in named_parameters]</code>  <code>instance-attribute</code>","text":"<p>Parameter Names.</p> <p>The names of the parameters, so that we can find them later when resetting the state.</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.__init__","title":"<code>__init__(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, *, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, named_parameters)</code>","text":"<p>Initialize the optimizer.</p> Warning <p>Named parameters must be with default settings (remove duplicates and not recursive).</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... ) optimizer.reset_state_all_parameters()</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>params_t</code> <p>Iterable of parameters to optimize or dicts defining parameter groups.</p> required <code>lr</code> <code>float | Tensor</code> <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a float LR unless specifying fused=True or capturable=True.</p> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients used for computing running averages of gradient and its square.</p> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <p>Term added to the denominator to improve numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (L2 penalty).</p> <code>0</code> <code>amsgrad</code> <code>bool</code> <p>Whether to use the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Whether foreach implementation of optimizer is used. If None, foreach is used over the for-loop implementation on CUDA if more performant. Note that foreach uses more peak memory.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, maximizes the parameters based on the objective, instead of minimizing.</p> <code>False</code> <code>capturable</code> <code>bool</code> <p>Whether this instance is safe to capture in a CUDA graph. True can impair ungraphed performance.</p> <code>False</code> <code>differentiable</code> <code>bool</code> <p>Whether autograd should occur through the optimizer step in training. Setting to True can impair performance.</p> <code>False</code> <code>fused</code> <code>bool | None</code> <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64, torch.float32, torch.float16, and torch.bfloat16.</p> <code>None</code> <code>named_parameters</code> <code>Iterator[tuple[str, Parameter]]</code> <p>An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as <code>model.named_parameters()</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of parameter names does not match the number of parameters.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def __init__(  # (extending existing implementation)\n    self,\n    params: params_t,\n    lr: float | Tensor = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0,\n    *,\n    amsgrad: bool = False,\n    foreach: bool | None = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: bool | None = None,\n    named_parameters: Iterator[tuple[str, Parameter]],\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Warning:\n        Named parameters must be with default settings (remove duplicates and not recursive).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n    Args:\n        params: Iterable of parameters to optimize or dicts defining parameter groups.\n        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n            float LR unless specifying fused=True or capturable=True.\n        betas: Coefficients used for computing running averages of gradient and its square.\n        eps: Term added to the denominator to improve numerical stability.\n        weight_decay: Weight decay (L2 penalty).\n        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n            Convergence of Adam and Beyond\".\n        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n            over the for-loop implementation on CUDA if more performant. Note that foreach uses\n            more peak memory.\n        maximize: If True, maximizes the parameters based on the objective, instead of\n            minimizing.\n        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n            ungraphed performance.\n        differentiable: Whether autograd should occur through the optimizer step in training.\n            Setting to True can impair performance.\n        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n            torch.float32, torch.float16, and torch.bfloat16.\n        named_parameters: An iterator over the named parameters of the model. This is used to\n            find the parameters when resetting their state. You should set this as\n            `model.named_parameters()`.\n\n    Raises:\n        ValueError: If the number of parameter names does not match the number of parameters.\n    \"\"\"\n    # Initialise the parent class (note we repeat the parameter names so that type hints work).\n    super().__init__(\n        params=params,\n        lr=lr,\n        betas=betas,\n        eps=eps,\n        weight_decay=weight_decay,\n        amsgrad=amsgrad,\n        foreach=foreach,\n        maximize=maximize,\n        capturable=capturable,\n        differentiable=differentiable,\n        fused=fused,\n    )\n\n    # Store the names of the parameters, so that we can find them later when resetting the\n    # state.\n    self.parameter_names = [name for name, _value in named_parameters]\n\n    if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n        error_message = (\n            \"The number of parameter names does not match the number of parameters. \"\n            \"If using model.named_parameters() make sure remove_duplicates is True \"\n            \"and recursive is False (the default settings).\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter, neuron_indices, axis)</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... )</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>Parameter</code> <p>The parameter to be reset. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>,</p> required <code>neuron_indices</code> <code>Int[Tensor, LEARNT_FEATURE_IDX]</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the parameter to reset.</p> required Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_neurons_state(\n    self,\n    parameter: Parameter,\n    neuron_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX],\n    axis: int,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\n        ...     model.decoder.weight,\n        ...     dead_neurons_indices,\n        ...     axis=1\n        ... )\n\n    Args:\n        parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the parameter to reset.\n    \"\"\"\n    # Get the state of the parameter\n    state = self.state[parameter]\n\n    # Check if state is initialized\n    if len(state) == 0:\n        return\n\n    # Check there are any neurons to reset\n    if neuron_indices.numel() == 0:\n        return\n\n    # Reset running averages for the specified neurons\n    if \"exp_avg\" in state:\n        exp_avg: Tensor = state[\"exp_avg\"]\n        exp_avg.index_fill_(axis, neuron_indices.to(exp_avg.device), 0)\n    if \"exp_avg_sq\" in state:\n        exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n        exp_avg_sq.index_fill_(axis, neuron_indices.to(exp_avg_sq.device), 0)\n\n    # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n    if \"max_exp_avg_sq\" in state:\n        max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n        max_exp_avg_sq.index_fill_(axis, neuron_indices.to(max_exp_avg_sq.device), 0)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","title":"... train the model and then resample some dead neurons, then do this ...","text":"<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","title":"Reset the optimizer state for parameters that have been updated","text":"<p>optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0) optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ...     model.decoder.weight, ...     dead_neurons_indices, ...     axis=1 ... )</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>","text":"<p>Reset the state for all parameters.</p> <p>Iterates over all parameters and resets both the running averages of the gradients and the squares of gradients.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Iterates over all parameters and resets both the running averages of the gradients and the\n    squares of gradients.\n    \"\"\"\n    # Iterate over every parameter\n    for group in self.param_groups:\n        for parameter in group[\"params\"]:\n            # Get the state\n            state = self.state[parameter]\n\n            # Check if state is initialized\n            if len(state) == 0:\n                continue\n\n            # Reset running averages\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.zero_()\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.zero_()\n\n            # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n            if \"max_exp_avg_sq\" in state:\n                max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                max_exp_avg_sq.zero_()\n</code></pre>"},{"location":"reference/source_data/","title":"Source Data","text":"<p>Source Data.</p>"},{"location":"reference/source_data/abstract_dataset/","title":"Abstract tokenized prompts dataset class","text":"<p>Abstract tokenized prompts dataset class.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem","title":"<code>HuggingFaceDatasetItem = TypeVar('HuggingFaceDatasetItem', bound=Any)</code>  <code>module-attribute</code>","text":"<p>Hugging face dataset item typed dict.</p> <p>When extending :class:<code>SourceDataset</code> you should create a <code>TypedDict</code> that matches the structure of each dataset item in the underlying Hugging Face dataset.</p> Example <p>With the Uncopyrighted Pile this should be a typed dict with text and meta properties.</p> <p>class PileUncopyrightedSourceDataBatch(TypedDict): ...    text: list[str] ...    meta: list[dict[str, dict[str, str]]]</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompt","title":"<code>TokenizedPrompt = list[int]</code>  <code>module-attribute</code>","text":"<p>A tokenized prompt.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset","title":"<code>SourceDataset</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[HuggingFaceDatasetItem]</code></p> <p>Abstract source dataset.</p> <p>Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset.</p> <p>Wraps an HuggingFace IterableDataset.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class SourceDataset(ABC, Generic[HuggingFaceDatasetItem]):\n    \"\"\"Abstract source dataset.\n\n    Source dataset that is used to generate the activations dataset (by running forward passes of\n    the source model with this data). It should contain prompts that have been tokenized with no\n    padding tokens (apart from an optional single first padding token). This enables efficient\n    generation of the activations dataset.\n\n    Wraps an HuggingFace IterableDataset.\n    \"\"\"\n\n    context_size: int\n    \"\"\"Number of tokens in the context window.\n\n    The paper *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n    a context size of 250.\n    \"\"\"\n\n    dataset: Dataset | IterableDataset\n    \"\"\"Underlying HuggingFace Dataset.\n\n    Warning:\n        Hugging Face `Dataset` objects are confusingly not the same as PyTorch `Dataset` objects.\n    \"\"\"\n\n    @abstractmethod\n    def preprocess(\n        self,\n        source_batch: HuggingFaceDatasetItem,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess function.\n\n        Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string\n        prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary\n        length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.\n\n        Applied to the dataset with the [Hugging Face\n        Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)\n        `map` function.\n\n        Warning:\n            The returned tokenized prompts should not have any padding tokens (apart from an\n            optional single first padding token).\n\n        Args:\n            source_batch: A batch of source data. For example, with The Pile dataset this would be a\n                dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        dataset_path: str,\n        dataset_split: str,\n        context_size: int,\n        buffer_size: int = 1000,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        n_processes_preprocessing: int | None = None,\n        preprocess_batch_size: int = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialise the dataset.\n\n        Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the\n        underlying Hugging Face `IterableDataset`.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g. `train`).\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            n_processes_preprocessing: The number of processes to use for preprocessing.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n\n        Raises:\n            TypeError: If the loaded dataset is not a Hugging Face `Dataset` or `IterableDataset`.\n        \"\"\"\n        self.context_size = context_size\n\n        # Load the dataset\n        should_stream = not pre_download\n        dataset = load_dataset(\n            dataset_path,\n            streaming=should_stream,\n            split=dataset_split,\n            data_dir=dataset_dir,\n            data_files=dataset_files,\n        )\n\n        # Setup preprocessing\n        existing_columns: list[str] = list(next(iter(dataset)).keys())\n\n        if pre_download:\n            if not isinstance(dataset, Dataset):\n                error_message = (\n                    f\"Expected Hugging Face dataset to be a Dataset when pre-downloading, but got \"\n                    f\"{type(dataset)}.\"\n                )\n                raise TypeError(error_message)\n\n            # Download the whole dataset\n            mapped_dataset = dataset.map(\n                self.preprocess,\n                batched=True,\n                batch_size=preprocess_batch_size,\n                fn_kwargs={\"context_size\": context_size},\n                remove_columns=existing_columns,\n                num_proc=n_processes_preprocessing,\n            )\n            self.dataset = mapped_dataset.shuffle()\n        else:\n            # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at\n            # least `buffer_size` items and then shuffles just that buffer.\n            # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n            if not isinstance(dataset, IterableDataset):\n                error_message = (\n                    f\"Expected Hugging Face dataset to be an IterableDataset when streaming, but \"\n                    f\"got {type(dataset)}.\"\n                )\n                raise TypeError(error_message)\n\n            mapped_dataset = dataset.map(\n                self.preprocess,\n                batched=True,\n                batch_size=preprocess_batch_size,\n                fn_kwargs={\"context_size\": context_size},\n                remove_columns=existing_columns,\n            )\n            self.dataset = mapped_dataset.shuffle(buffer_size=buffer_size)  # type: ignore\n\n    @final\n    def __iter__(self) -&gt; Any:  # noqa: ANN401\n        \"\"\"Iterate Dunder Method.\n\n        Enables direct access to :attr:`dataset` with e.g. `for` loops.\n        \"\"\"\n        return self.dataset.__iter__()\n\n    @final\n    def __next__(self) -&gt; Any:  # noqa: ANN401\n        \"\"\"Next Dunder Method.\n\n        Enables direct access to :attr:`dataset` with e.g. `next` calls.\n        \"\"\"\n        return next(iter(self))\n\n    @final\n    def get_dataloader(self, batch_size: int) -&gt; DataLoader[TorchTokenizedPrompts]:\n        \"\"\"Get a PyTorch DataLoader.\n\n        Args:\n            batch_size: The batch size to use.\n\n        Returns:\n            PyTorch DataLoader.\n        \"\"\"\n        torch_dataset: TorchDataset[TorchTokenizedPrompts] = self.dataset.with_format(\"torch\")  # type: ignore\n\n        return DataLoader[TorchTokenizedPrompts](\n            torch_dataset,\n            batch_size=batch_size,\n            # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not\n            # here.\n            shuffle=False,\n        )\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.context_size","title":"<code>context_size: int = context_size</code>  <code>instance-attribute</code>","text":"<p>Number of tokens in the context window.</p> <p>The paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.dataset","title":"<code>dataset: Dataset | IterableDataset</code>  <code>instance-attribute</code>","text":"<p>Underlying HuggingFace Dataset.</p> Warning <p>Hugging Face <code>Dataset</code> objects are confusingly not the same as PyTorch <code>Dataset</code> objects.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__init__","title":"<code>__init__(dataset_path, dataset_split, context_size, buffer_size=1000, dataset_dir=None, dataset_files=None, n_processes_preprocessing=None, preprocess_batch_size=1000, *, pre_download=False)</code>  <code>abstractmethod</code>","text":"<p>Initialise the dataset.</p> <p>Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face <code>IterableDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> required <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> required <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> required <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>n_processes_preprocessing</code> <code>int | None</code> <p>The number of processes to use for preprocessing.</p> <code>None</code> <code>preprocess_batch_size</code> <code>int</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the loaded dataset is not a Hugging Face <code>Dataset</code> or <code>IterableDataset</code>.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    dataset_path: str,\n    dataset_split: str,\n    context_size: int,\n    buffer_size: int = 1000,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    n_processes_preprocessing: int | None = None,\n    preprocess_batch_size: int = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialise the dataset.\n\n    Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the\n    underlying Hugging Face `IterableDataset`.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g. `train`).\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        n_processes_preprocessing: The number of processes to use for preprocessing.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n\n    Raises:\n        TypeError: If the loaded dataset is not a Hugging Face `Dataset` or `IterableDataset`.\n    \"\"\"\n    self.context_size = context_size\n\n    # Load the dataset\n    should_stream = not pre_download\n    dataset = load_dataset(\n        dataset_path,\n        streaming=should_stream,\n        split=dataset_split,\n        data_dir=dataset_dir,\n        data_files=dataset_files,\n    )\n\n    # Setup preprocessing\n    existing_columns: list[str] = list(next(iter(dataset)).keys())\n\n    if pre_download:\n        if not isinstance(dataset, Dataset):\n            error_message = (\n                f\"Expected Hugging Face dataset to be a Dataset when pre-downloading, but got \"\n                f\"{type(dataset)}.\"\n            )\n            raise TypeError(error_message)\n\n        # Download the whole dataset\n        mapped_dataset = dataset.map(\n            self.preprocess,\n            batched=True,\n            batch_size=preprocess_batch_size,\n            fn_kwargs={\"context_size\": context_size},\n            remove_columns=existing_columns,\n            num_proc=n_processes_preprocessing,\n        )\n        self.dataset = mapped_dataset.shuffle()\n    else:\n        # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at\n        # least `buffer_size` items and then shuffles just that buffer.\n        # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n        if not isinstance(dataset, IterableDataset):\n            error_message = (\n                f\"Expected Hugging Face dataset to be an IterableDataset when streaming, but \"\n                f\"got {type(dataset)}.\"\n            )\n            raise TypeError(error_message)\n\n        mapped_dataset = dataset.map(\n            self.preprocess,\n            batched=True,\n            batch_size=preprocess_batch_size,\n            fn_kwargs={\"context_size\": context_size},\n            remove_columns=existing_columns,\n        )\n        self.dataset = mapped_dataset.shuffle(buffer_size=buffer_size)  # type: ignore\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate Dunder Method.</p> <p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>for</code> loops.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef __iter__(self) -&gt; Any:  # noqa: ANN401\n    \"\"\"Iterate Dunder Method.\n\n    Enables direct access to :attr:`dataset` with e.g. `for` loops.\n    \"\"\"\n    return self.dataset.__iter__()\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__next__","title":"<code>__next__()</code>","text":"<p>Next Dunder Method.</p> <p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>next</code> calls.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef __next__(self) -&gt; Any:  # noqa: ANN401\n    \"\"\"Next Dunder Method.\n\n    Enables direct access to :attr:`dataset` with e.g. `next` calls.\n    \"\"\"\n    return next(iter(self))\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.get_dataloader","title":"<code>get_dataloader(batch_size)</code>","text":"<p>Get a PyTorch DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size to use.</p> required <p>Returns:</p> Type Description <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef get_dataloader(self, batch_size: int) -&gt; DataLoader[TorchTokenizedPrompts]:\n    \"\"\"Get a PyTorch DataLoader.\n\n    Args:\n        batch_size: The batch size to use.\n\n    Returns:\n        PyTorch DataLoader.\n    \"\"\"\n    torch_dataset: TorchDataset[TorchTokenizedPrompts] = self.dataset.with_format(\"torch\")  # type: ignore\n\n    return DataLoader[TorchTokenizedPrompts](\n        torch_dataset,\n        batch_size=batch_size,\n        # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not\n        # here.\n        shuffle=False,\n    )\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>  <code>abstractmethod</code>","text":"<p>Preprocess function.</p> <p>Takes a <code>preprocess_batch_size</code> (\\(m\\)) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of <code>input_ids</code> and a value of an arbitrary length list (\\(n\\)) of tokenized prompts. Note that \\(m\\) does not have to be equal to \\(n\\).</p> <p>Applied to the dataset with the Hugging Face Dataset <code>map</code> function.</p> Warning <p>The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token).</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>HuggingFaceDatasetItem</code> <p>A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized).</p> required <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@abstractmethod\ndef preprocess(\n    self,\n    source_batch: HuggingFaceDatasetItem,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess function.\n\n    Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string\n    prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary\n    length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.\n\n    Applied to the dataset with the [Hugging Face\n    Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)\n    `map` function.\n\n    Warning:\n        The returned tokenized prompts should not have any padding tokens (apart from an\n        optional single first padding token).\n\n    Args:\n        source_batch: A batch of source data. For example, with The Pile dataset this would be a\n            dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts","title":"<code>TokenizedPrompts</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class TokenizedPrompts(TypedDict):\n    \"\"\"Tokenized prompts.\"\"\"\n\n    input_ids: list[TokenizedPrompt]\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts","title":"<code>TorchTokenizedPrompts</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Tokenized prompts prepared for PyTorch.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class TorchTokenizedPrompts(TypedDict):\n    \"\"\"Tokenized prompts prepared for PyTorch.\"\"\"\n\n    input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)]\n</code></pre>"},{"location":"reference/source_data/mock_dataset/","title":"Mock dataset","text":"<p>Mock dataset.</p> <p>For use with tests and simple examples.</p>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset","title":"<code>ConsecutiveIntHuggingFaceDataset</code>","text":"<p>             Bases: <code>IterableDataset</code></p> <p>Consecutive integers Hugging Face dataset for testing.</p> <p>Creates a dataset where the first item is [0,1,2...], and the second item is [1,2,3...] and so on.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>class ConsecutiveIntHuggingFaceDataset(IterableDataset):\n    \"\"\"Consecutive integers Hugging Face dataset for testing.\n\n    Creates a dataset where the first item is [0,1,2...], and the second item is [1,2,3...] and so\n    on.\n    \"\"\"\n\n    _data: Int[Tensor, \"items context_size\"]\n    \"\"\"Generated data.\"\"\"\n\n    _length: int\n    \"\"\"Size of the dataset.\"\"\"\n\n    _format: Literal[\"torch\", \"list\"] = \"list\"\n    \"\"\"Format of the data.\"\"\"\n\n    def create_data(self, num_items: int, context_size: int) -&gt; Int[Tensor, \"items context_size\"]:\n        \"\"\"Create the data.\n\n        Args:\n            num_items: The number of items in the dataset.\n            context_size: The number of tokens in the context window.\n\n        Returns:\n            The generated data.\n        \"\"\"\n        rows = torch.arange(num_items).unsqueeze(1)\n        columns = torch.arange(context_size).unsqueeze(0)\n        return rows + columns\n\n    def __init__(\n        self, context_size: int, vocab_size: int = 50_000, num_items: int = 10_000\n    ) -&gt; None:\n        \"\"\"Initialize the mock HF dataset.\n\n        Args:\n            context_size: The number of tokens in the context window\n            vocab_size: The size of the vocabulary to use.\n            num_items: The number of items in the dataset.\n\n        Raises:\n            ValueError: If more items are requested than we can create with the vocab size (given\n                that each item is a consecutive list of integers and unique).\n        \"\"\"\n        self._length = num_items\n\n        # Check we can create the data\n        if num_items + context_size &gt; vocab_size:\n            error_message = (\n                f\"num_items ({num_items}) + context_size ({context_size}) must be less than \"\n                f\"vocab_size ({vocab_size})\"\n            )\n            raise ValueError(error_message)\n\n        # Initialise the data\n        self._data = self.create_data(num_items, context_size)\n\n    def __iter__(self) -&gt; Iterator:  # type: ignore (HF typing is incorrect)\n        \"\"\"Initialize the iterator.\n\n        Returns:\n            Iterator.\n        \"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n        \"\"\"Return the next item in the dataset.\n\n        Returns:\n            TokenizedPrompts: The next item in the dataset.\n\n        Raises:\n            StopIteration: If the end of the dataset is reached.\n        \"\"\"\n        if self._index &lt; self._length:\n            item = self[self._index]\n            self._index += 1\n            return item\n\n        raise StopIteration\n\n    def __len__(self) -&gt; int:\n        \"\"\"Len Dunder Method.\"\"\"\n        return self._length\n\n    def __getitem__(self, index: int) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n        \"\"\"Get Item.\"\"\"\n        item = self._data[index]\n\n        if self._format == \"torch\":\n            return {\"input_ids\": item}\n\n        return {\"input_ids\": item.tolist()}\n\n    def with_format(  # type: ignore (only support 2 types)\n        self,\n        type: Literal[\"torch\", \"list\"],  # noqa: A002\n    ) -&gt; \"ConsecutiveIntHuggingFaceDataset\":\n        \"\"\"With Format.\"\"\"\n        self._format = type\n        return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n    \"\"\"Get Item.\"\"\"\n    item = self._data[index]\n\n    if self._format == \"torch\":\n        return {\"input_ids\": item}\n\n    return {\"input_ids\": item.tolist()}\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__init__","title":"<code>__init__(context_size, vocab_size=50000, num_items=10000)</code>","text":"<p>Initialize the mock HF dataset.</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>int</code> <p>The number of tokens in the context window</p> required <code>vocab_size</code> <code>int</code> <p>The size of the vocabulary to use.</p> <code>50000</code> <code>num_items</code> <code>int</code> <p>The number of items in the dataset.</p> <code>10000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If more items are requested than we can create with the vocab size (given that each item is a consecutive list of integers and unique).</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __init__(\n    self, context_size: int, vocab_size: int = 50_000, num_items: int = 10_000\n) -&gt; None:\n    \"\"\"Initialize the mock HF dataset.\n\n    Args:\n        context_size: The number of tokens in the context window\n        vocab_size: The size of the vocabulary to use.\n        num_items: The number of items in the dataset.\n\n    Raises:\n        ValueError: If more items are requested than we can create with the vocab size (given\n            that each item is a consecutive list of integers and unique).\n    \"\"\"\n    self._length = num_items\n\n    # Check we can create the data\n    if num_items + context_size &gt; vocab_size:\n        error_message = (\n            f\"num_items ({num_items}) + context_size ({context_size}) must be less than \"\n            f\"vocab_size ({vocab_size})\"\n        )\n        raise ValueError(error_message)\n\n    # Initialise the data\n    self._data = self.create_data(num_items, context_size)\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Initialize the iterator.</p> <p>Returns:</p> Type Description <code>Iterator</code> <p>Iterator.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __iter__(self) -&gt; Iterator:  # type: ignore (HF typing is incorrect)\n    \"\"\"Initialize the iterator.\n\n    Returns:\n        Iterator.\n    \"\"\"\n    self._index = 0\n    return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__len__","title":"<code>__len__()</code>","text":"<p>Len Dunder Method.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Len Dunder Method.\"\"\"\n    return self._length\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item in the dataset.</p> <p>Returns:</p> Name Type Description <code>TokenizedPrompts</code> <code>TokenizedPrompts | TorchTokenizedPrompts</code> <p>The next item in the dataset.</p> <p>Raises:</p> Type Description <code>StopIteration</code> <p>If the end of the dataset is reached.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __next__(self) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n    \"\"\"Return the next item in the dataset.\n\n    Returns:\n        TokenizedPrompts: The next item in the dataset.\n\n    Raises:\n        StopIteration: If the end of the dataset is reached.\n    \"\"\"\n    if self._index &lt; self._length:\n        item = self[self._index]\n        self._index += 1\n        return item\n\n    raise StopIteration\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.create_data","title":"<code>create_data(num_items, context_size)</code>","text":"<p>Create the data.</p> <p>Parameters:</p> Name Type Description Default <code>num_items</code> <code>int</code> <p>The number of items in the dataset.</p> required <code>context_size</code> <code>int</code> <p>The number of tokens in the context window.</p> required <p>Returns:</p> Type Description <code>Int[Tensor, 'items context_size']</code> <p>The generated data.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def create_data(self, num_items: int, context_size: int) -&gt; Int[Tensor, \"items context_size\"]:\n    \"\"\"Create the data.\n\n    Args:\n        num_items: The number of items in the dataset.\n        context_size: The number of tokens in the context window.\n\n    Returns:\n        The generated data.\n    \"\"\"\n    rows = torch.arange(num_items).unsqueeze(1)\n    columns = torch.arange(context_size).unsqueeze(0)\n    return rows + columns\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.with_format","title":"<code>with_format(type)</code>","text":"<p>With Format.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def with_format(  # type: ignore (only support 2 types)\n    self,\n    type: Literal[\"torch\", \"list\"],  # noqa: A002\n) -&gt; \"ConsecutiveIntHuggingFaceDataset\":\n    \"\"\"With Format.\"\"\"\n    self._format = type\n    return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset","title":"<code>MockDataset</code>","text":"<p>             Bases: <code>SourceDataset[TokenizedPrompts]</code></p> <p>Mock dataset for testing.</p> <p>For use with tests and simple examples.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>@final\nclass MockDataset(SourceDataset[TokenizedPrompts]):\n    \"\"\"Mock dataset for testing.\n\n    For use with tests and simple examples.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerFast\n\n    def preprocess(\n        self,\n        source_batch: TokenizedPrompts,\n        *,\n        context_size: int,  # noqa: ARG002\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\"\"\"\n        # Nothing to do here\n        return source_batch\n\n    def __init__(\n        self,\n        context_size: int = 250,\n        buffer_size: int = 1000,  # noqa: ARG002\n        preprocess_batch_size: int = 1000,  # noqa: ARG002\n        dataset_path: str = \"dummy\",  # noqa: ARG002\n        dataset_split: str = \"train\",  # noqa: ARG002\n    ):\n        \"\"\"Initialize the Random Int Dummy dataset.\n\n        Example:\n            &gt;&gt;&gt; data = MockDataset()\n            &gt;&gt;&gt; first_item = next(iter(data))\n            &gt;&gt;&gt; len(first_item[\"input_ids\"])\n            250\n\n        Args:\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n                streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n                just that buffer. Note that the generated activations should also be shuffled before\n                training the sparse autoencoder, so a large buffer may not be strictly necessary\n                here. Note also that this is the number of items in the dataset (e.g. number of\n                prompts) and is typically significantly less than the number of tokenized prompts\n                once the preprocessing function has been applied.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            dataset_path: The path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g. `train`).\n        \"\"\"\n        self.dataset = ConsecutiveIntHuggingFaceDataset(context_size=context_size)  # type: ignore\n        self.context_size = context_size\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset.__init__","title":"<code>__init__(context_size=250, buffer_size=1000, preprocess_batch_size=1000, dataset_path='dummy', dataset_split='train')</code>","text":"<p>Initialize the Random Int Dummy dataset.</p> Example <p>data = MockDataset() first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>250</code> <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> <code>'dummy'</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __init__(\n    self,\n    context_size: int = 250,\n    buffer_size: int = 1000,  # noqa: ARG002\n    preprocess_batch_size: int = 1000,  # noqa: ARG002\n    dataset_path: str = \"dummy\",  # noqa: ARG002\n    dataset_split: str = \"train\",  # noqa: ARG002\n):\n    \"\"\"Initialize the Random Int Dummy dataset.\n\n    Example:\n        &gt;&gt;&gt; data = MockDataset()\n        &gt;&gt;&gt; first_item = next(iter(data))\n        &gt;&gt;&gt; len(first_item[\"input_ids\"])\n        250\n\n    Args:\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n            streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n            just that buffer. Note that the generated activations should also be shuffled before\n            training the sparse autoencoder, so a large buffer may not be strictly necessary\n            here. Note also that this is the number of items in the dataset (e.g. number of\n            prompts) and is typically significantly less than the number of tokenized prompts\n            once the preprocessing function has been applied.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        dataset_path: The path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g. `train`).\n    \"\"\"\n    self.dataset = ConsecutiveIntHuggingFaceDataset(context_size=context_size)  # type: ignore\n    self.context_size = context_size\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: TokenizedPrompts,\n    *,\n    context_size: int,  # noqa: ARG002\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\"\"\"\n    # Nothing to do here\n    return source_batch\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/","title":"Pre-Tokenized Dataset from Hugging Face","text":"<p>Pre-Tokenized Dataset from Hugging Face.</p> <p>PreTokenizedDataset should work with any of the following tokenized datasets: - NeelNanda/pile-small-tokenized-2b - NeelNanda/pile-tokenized-10b - NeelNanda/openwebtext-tokenized-9b - NeelNanda/c4-tokenized-2b - NeelNanda/code-tokenized - NeelNanda/c4-code-tokenized-2b - NeelNanda/pile-old-tokenized-2b - alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2</p>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch","title":"<code>PreTokenizedDataBatch</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>General Pre-Tokenized Dataset Item.</p> <p>Structure depends on the specific dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>class PreTokenizedDataBatch(TypedDict):\n    \"\"\"General Pre-Tokenized Dataset Item.\n\n    Structure depends on the specific dataset from Hugging Face.\n    \"\"\"\n\n    tokens: list[\n        list[int]\n    ]  # This assumes that the dataset structure is similar to the original Neel Nanda dataset.\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset","title":"<code>PreTokenizedDataset</code>","text":"<p>             Bases: <code>SourceDataset[PreTokenizedDataBatch]</code></p> <p>General Pre-Tokenized Dataset from Hugging Face.</p> <p>Can be used for various datasets available on Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@final\nclass PreTokenizedDataset(SourceDataset[PreTokenizedDataBatch]):\n    \"\"\"General Pre-Tokenized Dataset from Hugging Face.\n\n    Can be used for various datasets available on Hugging Face.\n    \"\"\"\n\n    def preprocess(\n        self,\n        source_batch: PreTokenizedDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        The method splits each pre-tokenized item based on the context size.\n\n        Args:\n            source_batch: A batch of source data.\n            context_size: The context size to use for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n        # Chunk each tokenized prompt into blocks of context_size,\n        # discarding the last block if too small.\n        context_size_prompts = []\n        for encoding in tokenized_prompts:\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        dataset_path: str,\n        context_size: int = 256,\n        buffer_size: int = 1000,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        preprocess_batch_size: int = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face (e.g.\n                `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n            context_size: The context size for tokenized prompts.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g. `train`).\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.__init__","title":"<code>__init__(dataset_path, context_size=256, buffer_size=1000, dataset_dir=None, dataset_files=None, dataset_split='train', preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a pre-tokenized dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face (e.g. `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</p> required <code>context_size</code> <code>int</code> <p>The context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> <code>preprocess_batch_size</code> <code>int</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_path: str,\n    context_size: int = 256,\n    buffer_size: int = 1000,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    preprocess_batch_size: int = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face (e.g.\n            `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n        context_size: The context size for tokenized prompts.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g. `train`).\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>The method splits each pre-tokenized item based on the context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>PreTokenizedDataBatch</code> <p>A batch of source data.</p> required <code>context_size</code> <code>int</code> <p>The context size to use for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: PreTokenizedDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    The method splits each pre-tokenized item based on the context size.\n\n    Args:\n        source_batch: A batch of source data.\n        context_size: The context size to use for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n    # Chunk each tokenized prompt into blocks of context_size,\n    # discarding the last block if too small.\n    context_size_prompts = []\n    for encoding in tokenized_prompts:\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/source_data/text_dataset/","title":"Generic Text Dataset Module for Hugging Face Datasets","text":"<p>Generic Text Dataset Module for Hugging Face Datasets.</p> <p>GenericTextDataset should work with the following datasets: - monology/pile-uncopyrighted - the_pile_openwebtext2 - roneneldan/TinyStories</p>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch","title":"<code>GenericTextDataBatch</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Generic Text Dataset Batch.</p> <p>Assumes the dataset provides a 'text' field with a list of strings.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>class GenericTextDataBatch(TypedDict):\n    \"\"\"Generic Text Dataset Batch.\n\n    Assumes the dataset provides a 'text' field with a list of strings.\n    \"\"\"\n\n    text: list[str]\n    meta: list[dict[str, dict[str, str]]]  # Optional, depending on the dataset structure.\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset","title":"<code>TextDataset</code>","text":"<p>             Bases: <code>SourceDataset[GenericTextDataBatch]</code></p> <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@final\nclass TextDataset(SourceDataset[GenericTextDataBatch]):\n    \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n\n    def preprocess(\n        self,\n        source_batch: GenericTextDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n        Args:\n            source_batch: A batch of source data, including 'text' with a list of strings.\n            context_size: Context size for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        prompts: list[str] = source_batch[\"text\"]\n\n        tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n        # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n        context_size_prompts = []\n        for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        dataset_path: str,\n        tokenizer: PreTrainedTokenizerBase,\n        buffer_size: int = 1000,\n        context_size: int = 256,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        n_processes_preprocessing: int | None = None,\n        preprocess_batch_size: int = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a generic text dataset from Hugging Face.\n\n        Args:\n            dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n            tokenizer: Tokenizer to process text data.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g., 'train').\n            n_processes_preprocessing: Number of processes to use for preprocessing.\n            preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        self.tokenizer = tokenizer\n\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            n_processes_preprocessing=n_processes_preprocessing,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n\n    def push_to_hugging_face_hub(\n        self,\n        repo_id: str,\n        commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n        max_shard_size: str | None = None,\n        num_shards: int = 64,\n        revision: str = \"main\",\n        *,\n        private: bool = False,\n    ) -&gt; None:\n        \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n        Motivation:\n            Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n            pre-processed dataset with others. This function allows you to do that by pushing the\n            pre-processed dataset to the Hugging Face hub.\n\n        Warning:\n            You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n            to use this.\n\n        Warning:\n            This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n            initializing the dataset).\n\n        Args:\n            repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n            commit_message: Commit message.\n            max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `num_shards`\n                is set.\n            num_shards: Number of shards to split the dataset into. A high number is recommended\n                here to allow for flexible distributed training of SAEs across nodes (where e.g.\n                each node fetches it's own shard).\n            revision: Branch to push to.\n            private: Whether to save the dataset privately.\n\n        Raises:\n            TypeError: If the dataset is streamed.\n        \"\"\"\n        if isinstance(self.dataset, IterableDataset):\n            error_message = (\n                \"Cannot share a streamed dataset to Hugging Face. \"\n                \"Please use `pre_download=True` when initializing the dataset.\"\n            )\n            raise TypeError(error_message)\n\n        self.dataset.push_to_hub(\n            repo_id=repo_id,\n            commit_message=commit_message,\n            max_shard_size=max_shard_size,\n            num_shards=num_shards,\n            private=private,\n            revision=revision,\n        )\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.__init__","title":"<code>__init__(dataset_path, tokenizer, buffer_size=1000, context_size=256, dataset_dir=None, dataset_files=None, dataset_split='train', n_processes_preprocessing=None, preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a generic text dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the dataset on Hugging Face (e.g. <code>'monology/pile-uncopyright'</code>).</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer to process text data.</p> required <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>256</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., 'train').</p> <code>'train'</code> <code>n_processes_preprocessing</code> <code>int | None</code> <p>Number of processes to use for preprocessing.</p> <code>None</code> <code>preprocess_batch_size</code> <code>int</code> <p>Batch size for preprocessing (tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_path: str,\n    tokenizer: PreTrainedTokenizerBase,\n    buffer_size: int = 1000,\n    context_size: int = 256,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    n_processes_preprocessing: int | None = None,\n    preprocess_batch_size: int = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a generic text dataset from Hugging Face.\n\n    Args:\n        dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n        tokenizer: Tokenizer to process text data.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g., 'train').\n        n_processes_preprocessing: Number of processes to use for preprocessing.\n        preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    self.tokenizer = tokenizer\n\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        n_processes_preprocessing=n_processes_preprocessing,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>GenericTextDataBatch</code> <p>A batch of source data, including 'text' with a list of strings.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: GenericTextDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n    Args:\n        source_batch: A batch of source data, including 'text' with a list of strings.\n        context_size: Context size for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    prompts: list[str] = source_batch[\"text\"]\n\n    tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n    # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n    context_size_prompts = []\n    for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.push_to_hugging_face_hub","title":"<code>push_to_hugging_face_hub(repo_id, commit_message='Upload preprocessed dataset using sparse_autoencoder.', max_shard_size=None, num_shards=64, revision='main', *, private=False)</code>","text":"<p>Share preprocessed dataset to Hugging Face hub.</p> Motivation <p>Pre-processing a dataset can be time-consuming, so it is useful to be able to share the pre-processed dataset with others. This function allows you to do that by pushing the pre-processed dataset to the Hugging Face hub.</p> Warning <p>You must be logged into HuggingFace (e.g with <code>huggingface-cli login</code> from the terminal) to use this.</p> Warning <p>This will only work if the dataset is not streamed (i.e. if <code>pre_download=True</code> when initializing the dataset).</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Hugging Face repo ID to save the dataset to (e.g. <code>username/dataset_name</code>).</p> required <code>commit_message</code> <code>str</code> <p>Commit message.</p> <code>'Upload preprocessed dataset using sparse_autoencoder.'</code> <code>max_shard_size</code> <code>str | None</code> <p>Maximum shard size (e.g. <code>'500MB'</code>). Should not be set if <code>num_shards</code> is set.</p> <code>None</code> <code>num_shards</code> <code>int</code> <p>Number of shards to split the dataset into. A high number is recommended here to allow for flexible distributed training of SAEs across nodes (where e.g. each node fetches it's own shard).</p> <code>64</code> <code>revision</code> <code>str</code> <p>Branch to push to.</p> <code>'main'</code> <code>private</code> <code>bool</code> <p>Whether to save the dataset privately.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset is streamed.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def push_to_hugging_face_hub(\n    self,\n    repo_id: str,\n    commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n    max_shard_size: str | None = None,\n    num_shards: int = 64,\n    revision: str = \"main\",\n    *,\n    private: bool = False,\n) -&gt; None:\n    \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n    Motivation:\n        Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n        pre-processed dataset with others. This function allows you to do that by pushing the\n        pre-processed dataset to the Hugging Face hub.\n\n    Warning:\n        You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n        to use this.\n\n    Warning:\n        This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n        initializing the dataset).\n\n    Args:\n        repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n        commit_message: Commit message.\n        max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `num_shards`\n            is set.\n        num_shards: Number of shards to split the dataset into. A high number is recommended\n            here to allow for flexible distributed training of SAEs across nodes (where e.g.\n            each node fetches it's own shard).\n        revision: Branch to push to.\n        private: Whether to save the dataset privately.\n\n    Raises:\n        TypeError: If the dataset is streamed.\n    \"\"\"\n    if isinstance(self.dataset, IterableDataset):\n        error_message = (\n            \"Cannot share a streamed dataset to Hugging Face. \"\n            \"Please use `pre_download=True` when initializing the dataset.\"\n        )\n        raise TypeError(error_message)\n\n    self.dataset.push_to_hub(\n        repo_id=repo_id,\n        commit_message=commit_message,\n        max_shard_size=max_shard_size,\n        num_shards=num_shards,\n        private=private,\n        revision=revision,\n    )\n</code></pre>"},{"location":"reference/source_model/","title":"Source Model","text":"<p>Source Model.</p>"},{"location":"reference/source_model/replace_activations_hook/","title":"Replace activations hook","text":"<p>Replace activations hook.</p>"},{"location":"reference/source_model/replace_activations_hook/#sparse_autoencoder.source_model.replace_activations_hook.replace_activations_hook","title":"<code>replace_activations_hook(value, hook, sparse_autoencoder)</code>","text":"<p>Replace activations hook.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The activations to replace.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required <code>sparse_autoencoder</code> <code>AbstractAutoencoder</code> <p>The sparse autoencoder. This should be pre-initialised with <code>functools.partial</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Replaced activations.</p> Source code in <code>sparse_autoencoder/source_model/replace_activations_hook.py</code> <pre><code>def replace_activations_hook(\n    value: Tensor,\n    hook: HookPoint,  # noqa: ARG001\n    sparse_autoencoder: AbstractAutoencoder,\n) -&gt; Tensor:\n    \"\"\"Replace activations hook.\n\n    Args:\n        value: The activations to replace.\n        hook: The hook point.\n        sparse_autoencoder: The sparse autoencoder. This should be pre-initialised with\n            `functools.partial`.\n\n    Returns:\n        Replaced activations.\n    \"\"\"\n    # Squash to just have a \"*items\" and a \"batch\" dimension\n    original_shape = value.shape\n    squashed_value: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] = value.view(\n        -1, value.size(-1)\n    )\n\n    # Get the output activations from a forward pass of the SAE\n    _learned_activations, output_activations = sparse_autoencoder.forward(squashed_value)\n\n    # Reshape to the original shape\n    return output_activations.view(*original_shape)\n</code></pre>"},{"location":"reference/source_model/store_activations_hook/","title":"TransformerLens Hook for storing activations","text":"<p>TransformerLens Hook for storing activations.</p>"},{"location":"reference/source_model/store_activations_hook/#sparse_autoencoder.source_model.store_activations_hook.store_activations_hook","title":"<code>store_activations_hook(value, hook, store)</code>","text":"<p>Store Activations Hook.</p> <p>Useful for getting just the specific activations wanted, rather than the full cache.</p> Example <p>First we'll need a source model from TransformerLens and an activation store.</p> <p>from functools import partial from transformer_lens import HookedTransformer from sparse_autoencoder.activation_store.list_store import ListActivationStore store = ListActivationStore() model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer</p> <p>Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass.</p> <p>model.add_hook( ...     \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store) ... ) tokens = model.to_tokens(\"Hello world\") tokens.shape torch.Size([1, 3])</p> <p>Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set <code>stop_at_layer=1</code> as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer).</p> <p>_output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required len(store) 3</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>The activations to store.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required <code>store</code> <code>ActivationStore</code> <p>The activation store. This should be pre-initialised with <code>functools.partial</code>.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(ANY, INPUT_OUTPUT_FEATURE)]</code> <p>Unmodified activations.</p> Source code in <code>sparse_autoencoder/source_model/store_activations_hook.py</code> <pre><code>def store_activations_hook(\n    value: Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)],\n    hook: HookPoint,  # noqa: ARG001\n    store: ActivationStore,\n) -&gt; Float[Tensor, Axis.names(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Store Activations Hook.\n\n    Useful for getting just the specific activations wanted, rather than the full cache.\n\n    Example:\n        First we'll need a source model from TransformerLens and an activation store.\n\n        &gt;&gt;&gt; from functools import partial\n        &gt;&gt;&gt; from transformer_lens import HookedTransformer\n        &gt;&gt;&gt; from sparse_autoencoder.activation_store.list_store import ListActivationStore\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n        Loaded pretrained model tiny-stories-1M into HookedTransformer\n\n        Next we can add the hook to specific neurons (in this case the first MLP neurons), and\n        create the tokens for a forward pass.\n\n        &gt;&gt;&gt; model.add_hook(\n        ...     \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store)\n        ... )\n        &gt;&gt;&gt; tokens = model.to_tokens(\"Hello world\")\n        &gt;&gt;&gt; tokens.shape\n        torch.Size([1, 3])\n\n        Then when we run the model, we should get one activation vector for each token (as we just\n        have one batch item). Note we also set `stop_at_layer=1` as we don't need the logits or any\n        other activations after the hook point that we've specified (in this case the first MLP\n        layer).\n\n        &gt;&gt;&gt; _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required\n        &gt;&gt;&gt; len(store)\n        3\n\n    Args:\n        value: The activations to store.\n        hook: The hook point.\n        store: The activation store. This should be pre-initialised with `functools.partial`.\n\n    Returns:\n        Unmodified activations.\n    \"\"\"\n    store.extend(value)\n\n    # Return the unmodified value\n    return value\n</code></pre>"},{"location":"reference/source_model/zero_ablate_hook/","title":"Zero ablate hook","text":"<p>Zero ablate hook.</p>"},{"location":"reference/source_model/zero_ablate_hook/#sparse_autoencoder.source_model.zero_ablate_hook.zero_ablate_hook","title":"<code>zero_ablate_hook(value, hook)</code>","text":"<p>Zero ablate hook.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The activations to store.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required Example <p>dummy_hook_point = HookPoint() value = torch.ones(2, 3) zero_ablate_hook(value, dummy_hook_point) tensor([[0., 0., 0.],         [0., 0., 0.]])</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Replaced activations.</p> Source code in <code>sparse_autoencoder/source_model/zero_ablate_hook.py</code> <pre><code>def zero_ablate_hook(\n    value: Tensor,\n    hook: HookPoint,  # noqa: ARG001\n) -&gt; Tensor:\n    \"\"\"Zero ablate hook.\n\n    Args:\n        value: The activations to store.\n        hook: The hook point.\n\n    Example:\n        &gt;&gt;&gt; dummy_hook_point = HookPoint()\n        &gt;&gt;&gt; value = torch.ones(2, 3)\n        &gt;&gt;&gt; zero_ablate_hook(value, dummy_hook_point)\n        tensor([[0., 0., 0.],\n                [0., 0., 0.]])\n\n    Returns:\n        Replaced activations.\n    \"\"\"\n    return torch.zeros_like(value)\n</code></pre>"},{"location":"reference/train/","title":"Train","text":"<p>Train.</p>"},{"location":"reference/train/pipeline/","title":"Default pipeline","text":"<p>Default pipeline.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p> <p>Includes all the key functionality to train a sparse autoencoder, with a specific set of     hyperparameters.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations.\n\n    Includes all the key functionality to train a sparse autoencoder, with a specific set of\n        hyperparameters.\n    \"\"\"\n\n    activation_resampler: AbstractActivationResampler | None\n    \"\"\"Activation resampler to use.\"\"\"\n\n    autoencoder: SparseAutoencoder\n    \"\"\"Sparse autoencoder to train.\"\"\"\n\n    cache_name: str\n    \"\"\"Name of the cache to use in the source model (hook point).\"\"\"\n\n    layer: int\n    \"\"\"Layer to get activations from with the source model.\"\"\"\n\n    log_frequency: int\n    \"\"\"Frequency at which to log metrics (in steps).\"\"\"\n\n    loss: AbstractLoss\n    \"\"\"Loss function to use.\"\"\"\n\n    metrics: MetricsContainer\n    \"\"\"Metrics to use.\"\"\"\n\n    optimizer: AbstractOptimizerWithReset\n    \"\"\"Optimizer to use.\"\"\"\n\n    progress_bar: tqdm | None\n    \"\"\"Progress bar for the pipeline.\"\"\"\n\n    source_data: Iterable[TorchTokenizedPrompts]\n    \"\"\"Iterable over the source data.\"\"\"\n\n    source_dataset: SourceDataset\n    \"\"\"Source dataset to generate activation data from (tokenized prompts).\"\"\"\n\n    source_model: HookedTransformer\n    \"\"\"Source model to get activations from.\"\"\"\n\n    total_activations_trained_on: int = 0\n    \"\"\"Total number of activations trained on state.\"\"\"\n\n    @final\n    def __init__(\n        self,\n        activation_resampler: AbstractActivationResampler | None,\n        autoencoder: SparseAutoencoder,\n        cache_name: str,\n        layer: int,\n        loss: AbstractLoss,\n        optimizer: AbstractOptimizerWithReset,\n        source_dataset: SourceDataset,\n        source_model: HookedTransformer,\n        run_name: str = \"sparse_autoencoder\",\n        checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n        log_frequency: int = 100,\n        metrics: MetricsContainer = default_metrics,\n        source_data_batch_size: int = 12,\n    ) -&gt; None:\n        \"\"\"Initialize the pipeline.\n\n        Args:\n            activation_resampler: Activation resampler to use.\n            autoencoder: Sparse autoencoder to train.\n            cache_name: Name of the cache to use in the source model (hook point).\n            layer: Layer to get activations from with the source model.\n            loss: Loss function to use.\n            optimizer: Optimizer to use.\n            source_dataset: Source dataset to get data from.\n            source_model: Source model to get activations from.\n            run_name: Name of the run for saving checkpoints.\n            checkpoint_directory: Directory to save checkpoints to.\n            log_frequency: Frequency at which to log metrics (in steps)\n            metrics: Metrics to use.\n            source_data_batch_size: Batch size for the source data.\n        \"\"\"\n        self.activation_resampler = activation_resampler\n        self.autoencoder = autoencoder\n        self.cache_name = cache_name\n        self.checkpoint_directory = checkpoint_directory\n        self.layer = layer\n        self.log_frequency = log_frequency\n        self.loss = loss\n        self.metrics = metrics\n        self.optimizer = optimizer\n        self.run_name = run_name\n        self.source_data_batch_size = source_data_batch_size\n        self.source_dataset = source_dataset\n        self.source_model = source_model\n\n        source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n        self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n\n    def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n        \"\"\"Generate activations.\n\n        Args:\n            store_size: Number of activations to generate.\n\n        Returns:\n            Activation store for the train section.\n\n        Raises:\n            ValueError: If the store size is not positive or is not divisible by the batch size.\n        \"\"\"\n        # Check the store size is positive and divisible by the batch size\n        if store_size &lt;= 0:\n            error_message = f\"Store size must be positive, got {store_size}\"\n            raise ValueError(error_message)\n        if store_size % self.source_data_batch_size != 0:\n            error_message = (\n                f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n                f\"got {store_size}\"\n            )\n            raise ValueError(error_message)\n\n        # Setup the store\n        num_neurons: int = self.autoencoder.n_input_features\n        source_model_device: torch.device = get_model_device(self.source_model)\n        store = TensorActivationStore(store_size, num_neurons)\n\n        # Add the hook to the model (will automatically store the activations every time the model\n        # runs)\n        self.source_model.remove_all_hook_fns()\n        hook = partial(store_activations_hook, store=store)\n        self.source_model.add_hook(self.cache_name, hook)\n\n        # Loop through the dataloader until the store reaches the desired size\n        with torch.no_grad():\n            for batch in self.source_data:\n                input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                    \"input_ids\"\n                ].to(source_model_device)\n                self.source_model.forward(\n                    input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n                )  # type: ignore (TLens is typed incorrectly)\n\n                if len(store) &gt;= store_size:\n                    break\n\n        self.source_model.remove_all_hook_fns()\n        store.shuffle()\n\n        return store\n\n    def train_autoencoder(\n        self, activation_store: TensorActivationStore, train_batch_size: int\n    ) -&gt; Int64[Tensor, Axis.LEARNT_FEATURE]:\n        \"\"\"Train the sparse autoencoder.\n\n        Args:\n            activation_store: Activation store from the generate section.\n            train_batch_size: Train batch size.\n\n        Returns:\n            Number of times each neuron fired.\n        \"\"\"\n        autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n        activations_dataloader = DataLoader(\n            activation_store,\n            batch_size=train_batch_size,\n        )\n\n        learned_activations_fired_count: Int64[Tensor, Axis.LEARNT_FEATURE] = torch.zeros(\n            self.autoencoder.n_learned_features, dtype=torch.int64, device=autoencoder_device\n        )\n\n        for store_batch in activations_dataloader:\n            # Zero the gradients\n            self.optimizer.zero_grad()\n\n            # Move the batch to the device (in place)\n            batch = store_batch.detach().to(autoencoder_device)\n\n            # Forward pass\n            learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n            # Get loss &amp; metrics\n            metrics = {}\n            total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n                batch, learned_activations, reconstructed_activations\n            )\n            metrics.update(loss_metrics)\n\n            with torch.no_grad():\n                for metric in self.metrics.train_metrics:\n                    calculated = metric.calculate(\n                        TrainMetricData(batch, learned_activations, reconstructed_activations)\n                    )\n                    metrics.update(calculated)\n\n            # Store count of how many neurons have fired\n            with torch.no_grad():\n                fired = learned_activations &gt; 0\n                learned_activations_fired_count.add_(fired.sum(dim=0))\n\n            # Backwards pass\n            total_loss.backward()\n            self.optimizer.step()\n            self.autoencoder.decoder.constrain_weights_unit_norm()\n\n            # Log training metrics\n            self.total_activations_trained_on += train_batch_size\n            if (\n                wandb.run is not None\n                and int(self.total_activations_trained_on / train_batch_size) % self.log_frequency\n                == 0\n            ):\n                wandb.log(\n                    data={**metrics, **loss_metrics},\n                    step=self.total_activations_trained_on,\n                    commit=True,\n                )\n\n        return learned_activations_fired_count\n\n    def update_parameters(self, parameter_updates: ParameterUpdateResults) -&gt; None:\n        \"\"\"Update the parameters of the model from the results of the resampler.\n\n        Args:\n            parameter_updates: Parameter updates from the resampler.\n        \"\"\"\n        # Update the weights and biases\n        self.autoencoder.encoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_weight_updates,\n        )\n        self.autoencoder.encoder.update_bias(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_bias_updates,\n        )\n        self.autoencoder.decoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_decoder_weight_updates,\n        )\n\n        # Reset the optimizer\n        for parameter, axis in self.autoencoder.reset_optimizer_parameter_details:\n            self.optimizer.reset_neurons_state(\n                parameter=parameter,\n                neuron_indices=parameter_updates.dead_neuron_indices,\n                axis=axis,\n            )\n\n    def validate_sae(self, validation_number_activations: int) -&gt; None:\n        \"\"\"Get validation metrics.\n\n        Args:\n            validation_number_activations: Number of activations to use for validation.\n        \"\"\"\n        losses: list[float] = []\n        losses_with_reconstruction: list[float] = []\n        losses_with_zero_ablation: list[float] = []\n        source_model_device: torch.device = get_model_device(self.source_model)\n\n        for batch in self.source_data:\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n\n            # Run a forward pass with and without the replaced activations\n            self.source_model.remove_all_hook_fns()\n            replacement_hook = partial(\n                replace_activations_hook, sparse_autoencoder=self.autoencoder\n            )\n\n            loss = self.source_model.forward(input_ids, return_type=\"loss\")\n            loss_with_reconstruction = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[\n                    (\n                        self.cache_name,\n                        replacement_hook,\n                    )\n                ],\n            )\n            loss_with_zero_ablation = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n            )\n\n            losses.append(loss.sum().item())\n            losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n            losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n            if len(losses) &gt;= validation_number_activations // input_ids.numel():\n                break\n\n        # Log\n        validation_data = ValidationMetricData(\n            source_model_loss=torch.tensor(losses),\n            source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n            source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n        )\n        for metric in self.metrics.validation_metrics:\n            calculated = metric.calculate(validation_data)\n            if wandb.run is not None:\n                wandb.log(data=calculated, commit=False)\n\n    @final\n    def save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n        \"\"\"Save the model as a checkpoint.\n\n        Args:\n            is_final: Whether this is the final checkpoint.\n\n        Returns:\n            Path to the saved checkpoint.\n        \"\"\"\n        # Create the name\n        name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n        safe_name = quote_plus(name, safe=\"_\")\n\n        # Save locally\n        self.checkpoint_directory.mkdir(parents=True, exist_ok=True)\n        file_path: Path = self.checkpoint_directory / f\"{safe_name}.pt\"\n        torch.save(\n            self.autoencoder.state_dict(),\n            file_path,\n        )\n\n        # Upload to wandb\n        if wandb.run is not None:\n            artifact = wandb.Artifact(safe_name, type=\"model\")\n            artifact.add_file(str(file_path))\n            wandb.log_artifact(artifact)\n\n        return file_path\n\n    def run_pipeline(\n        self,\n        train_batch_size: int,\n        max_store_size: int,\n        max_activations: int,\n        validation_number_activations: int = 1024,\n        validate_frequency: int | None = None,\n        checkpoint_frequency: int | None = None,\n    ) -&gt; None:\n        \"\"\"Run the full training pipeline.\n\n        Args:\n            train_batch_size: Train batch size.\n            max_store_size: Maximum size of the activation store.\n            max_activations: Maximum total number of activations to train on (the original paper\n                used 8bn, although others have had success with 100m+).\n            validation_number_activations: Number of activations to use for validation.\n            validate_frequency: Frequency at which to get validation metrics.\n            checkpoint_frequency: Frequency at which to save a checkpoint.\n        \"\"\"\n        last_validated: int = 0\n        last_checkpoint: int = 0\n\n        self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n        # Get the store size\n        store_size: int = max_store_size - max_store_size % (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n\n        with tqdm(\n            desc=\"Activations trained on\",\n            total=max_activations,\n        ) as progress_bar:\n            for _ in range(0, max_activations, store_size):\n                # Generate\n                progress_bar.set_postfix({\"stage\": \"generate\"})\n                activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n                # Update the counters\n                num_activation_vectors_in_store = len(activation_store)\n                last_validated += num_activation_vectors_in_store\n                last_checkpoint += num_activation_vectors_in_store\n\n                # Train\n                progress_bar.set_postfix({\"stage\": \"train\"})\n                batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE] = self.train_autoencoder(\n                    activation_store, train_batch_size=train_batch_size\n                )\n\n                # Resample dead neurons (if needed)\n                progress_bar.set_postfix({\"stage\": \"resample\"})\n                if self.activation_resampler is not None:\n                    # Get the updates\n                    parameter_updates = self.activation_resampler.step_resampler(\n                        batch_neuron_activity=batch_neuron_activity,\n                        activation_store=activation_store,\n                        autoencoder=self.autoencoder,\n                        loss_fn=self.loss,\n                        train_batch_size=train_batch_size,\n                    )\n\n                    if parameter_updates is not None:\n                        if wandb.run is not None:\n                            wandb.log(\n                                {\n                                    \"resample/dead_neurons\": len(\n                                        parameter_updates.dead_neuron_indices\n                                    )\n                                },\n                                commit=False,\n                            )\n\n                        # Update the parameters\n                        self.update_parameters(parameter_updates)\n\n                # Get validation metrics (if needed)\n                progress_bar.set_postfix({\"stage\": \"validate\"})\n                if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                    self.validate_sae(validation_number_activations)\n                    last_validated = 0\n\n                # Checkpoint (if needed)\n                progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n                if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                    last_checkpoint = 0\n                    self.save_checkpoint()\n\n                # Update the progress bar\n                progress_bar.update(store_size)\n\n        # Save the final checkpoint\n        self.save_checkpoint(is_final=True)\n\n    @staticmethod\n    def stateful_dataloader_iterable(\n        dataloader: DataLoader[TorchTokenizedPrompts],\n    ) -&gt; Iterable[TorchTokenizedPrompts]:\n        \"\"\"Create a stateful dataloader iterable.\n\n        Create an iterable that maintains it's position in the dataloader between loops.\n\n        Examples:\n            Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n            (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n            position from where the first loop left off.\n\n            &gt;&gt;&gt; from datasets import Dataset\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; def gen():\n            ...     yield {\"int\": 0}\n            ...     yield {\"int\": 1}\n            &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n            &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n            (tensor([0]), tensor([0]))\n\n            By contrast if you create a stateful iterable from the dataloader, each loop will get\n            different data.\n\n            &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n            &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n            (tensor([0]), tensor([1]))\n\n        Args:\n            dataloader: PyTorch DataLoader.\n\n        Returns:\n            Stateful iterable over the data in the dataloader.\n\n        Yields:\n            Data from the dataloader.\n        \"\"\"\n        yield from dataloader\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.activation_resampler","title":"<code>activation_resampler: AbstractActivationResampler | None = activation_resampler</code>  <code>instance-attribute</code>","text":"<p>Activation resampler to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.autoencoder","title":"<code>autoencoder: SparseAutoencoder = autoencoder</code>  <code>instance-attribute</code>","text":"<p>Sparse autoencoder to train.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.cache_name","title":"<code>cache_name: str = cache_name</code>  <code>instance-attribute</code>","text":"<p>Name of the cache to use in the source model (hook point).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.layer","title":"<code>layer: int = layer</code>  <code>instance-attribute</code>","text":"<p>Layer to get activations from with the source model.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.log_frequency","title":"<code>log_frequency: int = log_frequency</code>  <code>instance-attribute</code>","text":"<p>Frequency at which to log metrics (in steps).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.loss","title":"<code>loss: AbstractLoss = loss</code>  <code>instance-attribute</code>","text":"<p>Loss function to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.metrics","title":"<code>metrics: MetricsContainer = metrics</code>  <code>instance-attribute</code>","text":"<p>Metrics to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.optimizer","title":"<code>optimizer: AbstractOptimizerWithReset = optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.progress_bar","title":"<code>progress_bar: tqdm | None</code>  <code>instance-attribute</code>","text":"<p>Progress bar for the pipeline.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_data","title":"<code>source_data: Iterable[TorchTokenizedPrompts] = self.stateful_dataloader_iterable(source_dataloader)</code>  <code>instance-attribute</code>","text":"<p>Iterable over the source data.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_dataset","title":"<code>source_dataset: SourceDataset = source_dataset</code>  <code>instance-attribute</code>","text":"<p>Source dataset to generate activation data from (tokenized prompts).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_model","title":"<code>source_model: HookedTransformer = source_model</code>  <code>instance-attribute</code>","text":"<p>Source model to get activations from.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.total_activations_trained_on","title":"<code>total_activations_trained_on: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of activations trained on state.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.__init__","title":"<code>__init__(activation_resampler, autoencoder, cache_name, layer, loss, optimizer, source_dataset, source_model, run_name='sparse_autoencoder', checkpoint_directory=DEFAULT_CHECKPOINT_DIRECTORY, log_frequency=100, metrics=default_metrics, source_data_batch_size=12)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>activation_resampler</code> <code>AbstractActivationResampler | None</code> <p>Activation resampler to use.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder to train.</p> required <code>cache_name</code> <code>str</code> <p>Name of the cache to use in the source model (hook point).</p> required <code>layer</code> <code>int</code> <p>Layer to get activations from with the source model.</p> required <code>loss</code> <code>AbstractLoss</code> <p>Loss function to use.</p> required <code>optimizer</code> <code>AbstractOptimizerWithReset</code> <p>Optimizer to use.</p> required <code>source_dataset</code> <code>SourceDataset</code> <p>Source dataset to get data from.</p> required <code>source_model</code> <code>HookedTransformer</code> <p>Source model to get activations from.</p> required <code>run_name</code> <code>str</code> <p>Name of the run for saving checkpoints.</p> <code>'sparse_autoencoder'</code> <code>checkpoint_directory</code> <code>Path</code> <p>Directory to save checkpoints to.</p> <code>DEFAULT_CHECKPOINT_DIRECTORY</code> <code>log_frequency</code> <code>int</code> <p>Frequency at which to log metrics (in steps)</p> <code>100</code> <code>metrics</code> <code>MetricsContainer</code> <p>Metrics to use.</p> <code>default_metrics</code> <code>source_data_batch_size</code> <code>int</code> <p>Batch size for the source data.</p> <code>12</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef __init__(\n    self,\n    activation_resampler: AbstractActivationResampler | None,\n    autoencoder: SparseAutoencoder,\n    cache_name: str,\n    layer: int,\n    loss: AbstractLoss,\n    optimizer: AbstractOptimizerWithReset,\n    source_dataset: SourceDataset,\n    source_model: HookedTransformer,\n    run_name: str = \"sparse_autoencoder\",\n    checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n    log_frequency: int = 100,\n    metrics: MetricsContainer = default_metrics,\n    source_data_batch_size: int = 12,\n) -&gt; None:\n    \"\"\"Initialize the pipeline.\n\n    Args:\n        activation_resampler: Activation resampler to use.\n        autoencoder: Sparse autoencoder to train.\n        cache_name: Name of the cache to use in the source model (hook point).\n        layer: Layer to get activations from with the source model.\n        loss: Loss function to use.\n        optimizer: Optimizer to use.\n        source_dataset: Source dataset to get data from.\n        source_model: Source model to get activations from.\n        run_name: Name of the run for saving checkpoints.\n        checkpoint_directory: Directory to save checkpoints to.\n        log_frequency: Frequency at which to log metrics (in steps)\n        metrics: Metrics to use.\n        source_data_batch_size: Batch size for the source data.\n    \"\"\"\n    self.activation_resampler = activation_resampler\n    self.autoencoder = autoencoder\n    self.cache_name = cache_name\n    self.checkpoint_directory = checkpoint_directory\n    self.layer = layer\n    self.log_frequency = log_frequency\n    self.loss = loss\n    self.metrics = metrics\n    self.optimizer = optimizer\n    self.run_name = run_name\n    self.source_data_batch_size = source_data_batch_size\n    self.source_dataset = source_dataset\n    self.source_model = source_model\n\n    source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n    self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.generate_activations","title":"<code>generate_activations(store_size)</code>","text":"<p>Generate activations.</p> <p>Parameters:</p> Name Type Description Default <code>store_size</code> <code>int</code> <p>Number of activations to generate.</p> required <p>Returns:</p> Type Description <code>TensorActivationStore</code> <p>Activation store for the train section.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store size is not positive or is not divisible by the batch size.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n    \"\"\"Generate activations.\n\n    Args:\n        store_size: Number of activations to generate.\n\n    Returns:\n        Activation store for the train section.\n\n    Raises:\n        ValueError: If the store size is not positive or is not divisible by the batch size.\n    \"\"\"\n    # Check the store size is positive and divisible by the batch size\n    if store_size &lt;= 0:\n        error_message = f\"Store size must be positive, got {store_size}\"\n        raise ValueError(error_message)\n    if store_size % self.source_data_batch_size != 0:\n        error_message = (\n            f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n            f\"got {store_size}\"\n        )\n        raise ValueError(error_message)\n\n    # Setup the store\n    num_neurons: int = self.autoencoder.n_input_features\n    source_model_device: torch.device = get_model_device(self.source_model)\n    store = TensorActivationStore(store_size, num_neurons)\n\n    # Add the hook to the model (will automatically store the activations every time the model\n    # runs)\n    self.source_model.remove_all_hook_fns()\n    hook = partial(store_activations_hook, store=store)\n    self.source_model.add_hook(self.cache_name, hook)\n\n    # Loop through the dataloader until the store reaches the desired size\n    with torch.no_grad():\n        for batch in self.source_data:\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n            self.source_model.forward(\n                input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n            )  # type: ignore (TLens is typed incorrectly)\n\n            if len(store) &gt;= store_size:\n                break\n\n    self.source_model.remove_all_hook_fns()\n    store.shuffle()\n\n    return store\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.run_pipeline","title":"<code>run_pipeline(train_batch_size, max_store_size, max_activations, validation_number_activations=1024, validate_frequency=None, checkpoint_frequency=None)</code>","text":"<p>Run the full training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <code>max_store_size</code> <code>int</code> <p>Maximum size of the activation store.</p> required <code>max_activations</code> <code>int</code> <p>Maximum total number of activations to train on (the original paper used 8bn, although others have had success with 100m+).</p> required <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> <code>1024</code> <code>validate_frequency</code> <code>int | None</code> <p>Frequency at which to get validation metrics.</p> <code>None</code> <code>checkpoint_frequency</code> <code>int | None</code> <p>Frequency at which to save a checkpoint.</p> <code>None</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def run_pipeline(\n    self,\n    train_batch_size: int,\n    max_store_size: int,\n    max_activations: int,\n    validation_number_activations: int = 1024,\n    validate_frequency: int | None = None,\n    checkpoint_frequency: int | None = None,\n) -&gt; None:\n    \"\"\"Run the full training pipeline.\n\n    Args:\n        train_batch_size: Train batch size.\n        max_store_size: Maximum size of the activation store.\n        max_activations: Maximum total number of activations to train on (the original paper\n            used 8bn, although others have had success with 100m+).\n        validation_number_activations: Number of activations to use for validation.\n        validate_frequency: Frequency at which to get validation metrics.\n        checkpoint_frequency: Frequency at which to save a checkpoint.\n    \"\"\"\n    last_validated: int = 0\n    last_checkpoint: int = 0\n\n    self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n    # Get the store size\n    store_size: int = max_store_size - max_store_size % (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n\n    with tqdm(\n        desc=\"Activations trained on\",\n        total=max_activations,\n    ) as progress_bar:\n        for _ in range(0, max_activations, store_size):\n            # Generate\n            progress_bar.set_postfix({\"stage\": \"generate\"})\n            activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n            # Update the counters\n            num_activation_vectors_in_store = len(activation_store)\n            last_validated += num_activation_vectors_in_store\n            last_checkpoint += num_activation_vectors_in_store\n\n            # Train\n            progress_bar.set_postfix({\"stage\": \"train\"})\n            batch_neuron_activity: Int64[Tensor, Axis.LEARNT_FEATURE] = self.train_autoencoder(\n                activation_store, train_batch_size=train_batch_size\n            )\n\n            # Resample dead neurons (if needed)\n            progress_bar.set_postfix({\"stage\": \"resample\"})\n            if self.activation_resampler is not None:\n                # Get the updates\n                parameter_updates = self.activation_resampler.step_resampler(\n                    batch_neuron_activity=batch_neuron_activity,\n                    activation_store=activation_store,\n                    autoencoder=self.autoencoder,\n                    loss_fn=self.loss,\n                    train_batch_size=train_batch_size,\n                )\n\n                if parameter_updates is not None:\n                    if wandb.run is not None:\n                        wandb.log(\n                            {\n                                \"resample/dead_neurons\": len(\n                                    parameter_updates.dead_neuron_indices\n                                )\n                            },\n                            commit=False,\n                        )\n\n                    # Update the parameters\n                    self.update_parameters(parameter_updates)\n\n            # Get validation metrics (if needed)\n            progress_bar.set_postfix({\"stage\": \"validate\"})\n            if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                self.validate_sae(validation_number_activations)\n                last_validated = 0\n\n            # Checkpoint (if needed)\n            progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n            if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                last_checkpoint = 0\n                self.save_checkpoint()\n\n            # Update the progress bar\n            progress_bar.update(store_size)\n\n    # Save the final checkpoint\n    self.save_checkpoint(is_final=True)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.save_checkpoint","title":"<code>save_checkpoint(*, is_final=False)</code>","text":"<p>Save the model as a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>is_final</code> <code>bool</code> <p>Whether this is the final checkpoint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved checkpoint.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n    \"\"\"Save the model as a checkpoint.\n\n    Args:\n        is_final: Whether this is the final checkpoint.\n\n    Returns:\n        Path to the saved checkpoint.\n    \"\"\"\n    # Create the name\n    name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n    safe_name = quote_plus(name, safe=\"_\")\n\n    # Save locally\n    self.checkpoint_directory.mkdir(parents=True, exist_ok=True)\n    file_path: Path = self.checkpoint_directory / f\"{safe_name}.pt\"\n    torch.save(\n        self.autoencoder.state_dict(),\n        file_path,\n    )\n\n    # Upload to wandb\n    if wandb.run is not None:\n        artifact = wandb.Artifact(safe_name, type=\"model\")\n        artifact.add_file(str(file_path))\n        wandb.log_artifact(artifact)\n\n    return file_path\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.stateful_dataloader_iterable","title":"<code>stateful_dataloader_iterable(dataloader)</code>  <code>staticmethod</code>","text":"<p>Create a stateful dataloader iterable.</p> <p>Create an iterable that maintains it's position in the dataloader between loops.</p> <p>Examples:</p> <p>Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off.</p> <pre><code>&gt;&gt;&gt; from datasets import Dataset\n&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; def gen():\n...     yield {\"int\": 0}\n...     yield {\"int\": 1}\n&gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n&gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n(tensor([0]), tensor([0]))\n</code></pre> <p>By contrast if you create a stateful iterable from the dataloader, each loop will get different data.</p> <pre><code>&gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n&gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n(tensor([0]), tensor([1]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> required <p>Returns:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Stateful iterable over the data in the dataloader.</p> <p>Yields:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Data from the dataloader.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@staticmethod\ndef stateful_dataloader_iterable(\n    dataloader: DataLoader[TorchTokenizedPrompts],\n) -&gt; Iterable[TorchTokenizedPrompts]:\n    \"\"\"Create a stateful dataloader iterable.\n\n    Create an iterable that maintains it's position in the dataloader between loops.\n\n    Examples:\n        Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n        (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n        position from where the first loop left off.\n\n        &gt;&gt;&gt; from datasets import Dataset\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; def gen():\n        ...     yield {\"int\": 0}\n        ...     yield {\"int\": 1}\n        &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n        &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n        (tensor([0]), tensor([0]))\n\n        By contrast if you create a stateful iterable from the dataloader, each loop will get\n        different data.\n\n        &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n        &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n        (tensor([0]), tensor([1]))\n\n    Args:\n        dataloader: PyTorch DataLoader.\n\n    Returns:\n        Stateful iterable over the data in the dataloader.\n\n    Yields:\n        Data from the dataloader.\n    \"\"\"\n    yield from dataloader\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.train_autoencoder","title":"<code>train_autoencoder(activation_store, train_batch_size)</code>","text":"<p>Train the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store from the generate section.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <p>Returns:</p> Type Description <code>Int64[Tensor, LEARNT_FEATURE]</code> <p>Number of times each neuron fired.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def train_autoencoder(\n    self, activation_store: TensorActivationStore, train_batch_size: int\n) -&gt; Int64[Tensor, Axis.LEARNT_FEATURE]:\n    \"\"\"Train the sparse autoencoder.\n\n    Args:\n        activation_store: Activation store from the generate section.\n        train_batch_size: Train batch size.\n\n    Returns:\n        Number of times each neuron fired.\n    \"\"\"\n    autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n    activations_dataloader = DataLoader(\n        activation_store,\n        batch_size=train_batch_size,\n    )\n\n    learned_activations_fired_count: Int64[Tensor, Axis.LEARNT_FEATURE] = torch.zeros(\n        self.autoencoder.n_learned_features, dtype=torch.int64, device=autoencoder_device\n    )\n\n    for store_batch in activations_dataloader:\n        # Zero the gradients\n        self.optimizer.zero_grad()\n\n        # Move the batch to the device (in place)\n        batch = store_batch.detach().to(autoencoder_device)\n\n        # Forward pass\n        learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n        # Get loss &amp; metrics\n        metrics = {}\n        total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n            batch, learned_activations, reconstructed_activations\n        )\n        metrics.update(loss_metrics)\n\n        with torch.no_grad():\n            for metric in self.metrics.train_metrics:\n                calculated = metric.calculate(\n                    TrainMetricData(batch, learned_activations, reconstructed_activations)\n                )\n                metrics.update(calculated)\n\n        # Store count of how many neurons have fired\n        with torch.no_grad():\n            fired = learned_activations &gt; 0\n            learned_activations_fired_count.add_(fired.sum(dim=0))\n\n        # Backwards pass\n        total_loss.backward()\n        self.optimizer.step()\n        self.autoencoder.decoder.constrain_weights_unit_norm()\n\n        # Log training metrics\n        self.total_activations_trained_on += train_batch_size\n        if (\n            wandb.run is not None\n            and int(self.total_activations_trained_on / train_batch_size) % self.log_frequency\n            == 0\n        ):\n            wandb.log(\n                data={**metrics, **loss_metrics},\n                step=self.total_activations_trained_on,\n                commit=True,\n            )\n\n    return learned_activations_fired_count\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.update_parameters","title":"<code>update_parameters(parameter_updates)</code>","text":"<p>Update the parameters of the model from the results of the resampler.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_updates</code> <code>ParameterUpdateResults</code> <p>Parameter updates from the resampler.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def update_parameters(self, parameter_updates: ParameterUpdateResults) -&gt; None:\n    \"\"\"Update the parameters of the model from the results of the resampler.\n\n    Args:\n        parameter_updates: Parameter updates from the resampler.\n    \"\"\"\n    # Update the weights and biases\n    self.autoencoder.encoder.update_dictionary_vectors(\n        parameter_updates.dead_neuron_indices,\n        parameter_updates.dead_encoder_weight_updates,\n    )\n    self.autoencoder.encoder.update_bias(\n        parameter_updates.dead_neuron_indices,\n        parameter_updates.dead_encoder_bias_updates,\n    )\n    self.autoencoder.decoder.update_dictionary_vectors(\n        parameter_updates.dead_neuron_indices,\n        parameter_updates.dead_decoder_weight_updates,\n    )\n\n    # Reset the optimizer\n    for parameter, axis in self.autoencoder.reset_optimizer_parameter_details:\n        self.optimizer.reset_neurons_state(\n            parameter=parameter,\n            neuron_indices=parameter_updates.dead_neuron_indices,\n            axis=axis,\n        )\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.validate_sae","title":"<code>validate_sae(validation_number_activations)</code>","text":"<p>Get validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def validate_sae(self, validation_number_activations: int) -&gt; None:\n    \"\"\"Get validation metrics.\n\n    Args:\n        validation_number_activations: Number of activations to use for validation.\n    \"\"\"\n    losses: list[float] = []\n    losses_with_reconstruction: list[float] = []\n    losses_with_zero_ablation: list[float] = []\n    source_model_device: torch.device = get_model_device(self.source_model)\n\n    for batch in self.source_data:\n        input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n            \"input_ids\"\n        ].to(source_model_device)\n\n        # Run a forward pass with and without the replaced activations\n        self.source_model.remove_all_hook_fns()\n        replacement_hook = partial(\n            replace_activations_hook, sparse_autoencoder=self.autoencoder\n        )\n\n        loss = self.source_model.forward(input_ids, return_type=\"loss\")\n        loss_with_reconstruction = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[\n                (\n                    self.cache_name,\n                    replacement_hook,\n                )\n            ],\n        )\n        loss_with_zero_ablation = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n        )\n\n        losses.append(loss.sum().item())\n        losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n        losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n        if len(losses) &gt;= validation_number_activations // input_ids.numel():\n            break\n\n    # Log\n    validation_data = ValidationMetricData(\n        source_model_loss=torch.tensor(losses),\n        source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n        source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n    )\n    for metric in self.metrics.validation_metrics:\n        calculated = metric.calculate(validation_data)\n        if wandb.run is not None:\n            wandb.log(data=calculated, commit=False)\n</code></pre>"},{"location":"reference/train/sweep/","title":"Sweep","text":"<p>Sweep.</p>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.run_training_pipeline","title":"<code>run_training_pipeline(hyperparameters, source_model, autoencoder, loss, optimizer, activation_resampler, source_data, run_name)</code>","text":"<p>Run the training pipeline for the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <code>source_model</code> <code>HookedTransformer</code> <p>The source model.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>The sparse autoencoder.</p> required <code>loss</code> <code>LossReducer</code> <p>The loss function.</p> required <code>optimizer</code> <code>AdamWithReset</code> <p>The optimizer.</p> required <code>activation_resampler</code> <code>ActivationResampler</code> <p>The activation resampler.</p> required <code>source_data</code> <code>SourceDataset</code> <p>The source data.</p> required <code>run_name</code> <code>str</code> <p>The name of the run.</p> required Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def run_training_pipeline(\n    hyperparameters: RuntimeHyperparameters,\n    source_model: HookedTransformer,\n    autoencoder: SparseAutoencoder,\n    loss: LossReducer,\n    optimizer: AdamWithReset,\n    activation_resampler: ActivationResampler,\n    source_data: SourceDataset,\n    run_name: str,\n) -&gt; None:\n    \"\"\"Run the training pipeline for the sparse autoencoder.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n        source_model: The source model.\n        autoencoder: The sparse autoencoder.\n        loss: The loss function.\n        optimizer: The optimizer.\n        activation_resampler: The activation resampler.\n        source_data: The source data.\n        run_name: The name of the run.\n    \"\"\"\n    checkpoint_path = Path(\"../../.checkpoints\")\n    checkpoint_path.mkdir(exist_ok=True)\n\n    random_seed = hyperparameters[\"random_seed\"]\n    torch.random.manual_seed(random_seed)\n\n    hook_point = get_act_name(\n        hyperparameters[\"source_model\"][\"hook_site\"], hyperparameters[\"source_model\"][\"hook_layer\"]\n    )\n\n    pipeline = Pipeline(\n        activation_resampler=activation_resampler,\n        autoencoder=autoencoder,\n        cache_name=hook_point,\n        checkpoint_directory=checkpoint_path,\n        layer=hyperparameters[\"source_model\"][\"hook_layer\"],\n        loss=loss,\n        optimizer=optimizer,\n        source_data_batch_size=hyperparameters[\"pipeline\"][\"source_data_batch_size\"],\n        source_dataset=source_data,\n        source_model=source_model,\n        log_frequency=hyperparameters[\"pipeline\"][\"log_frequency\"],\n        run_name=run_name,\n    )\n\n    pipeline.run_pipeline(\n        train_batch_size=hyperparameters[\"pipeline\"][\"train_batch_size\"],\n        max_store_size=hyperparameters[\"pipeline\"][\"max_store_size\"],\n        max_activations=hyperparameters[\"pipeline\"][\"max_activations\"],\n        checkpoint_frequency=hyperparameters[\"pipeline\"][\"checkpoint_frequency\"],\n        validate_frequency=hyperparameters[\"pipeline\"][\"validation_frequency\"],\n        validation_number_activations=hyperparameters[\"pipeline\"][\"validation_number_activations\"],\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_activation_resampler","title":"<code>setup_activation_resampler(hyperparameters)</code>","text":"<p>Setup the activation resampler for the autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Name Type Description <code>ActivationResampler</code> <code>ActivationResampler</code> <p>The initialized activation resampler.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_activation_resampler(hyperparameters: RuntimeHyperparameters) -&gt; ActivationResampler:\n    \"\"\"Setup the activation resampler for the autoencoder.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        ActivationResampler: The initialized activation resampler.\n    \"\"\"\n    return ActivationResampler(\n        n_learned_features=hyperparameters[\"autoencoder\"][\"expansion_factor\"]\n        * hyperparameters[\"source_model\"][\"hook_dimension\"],\n        resample_interval=hyperparameters[\"activation_resampler\"][\"resample_interval\"],\n        max_n_resamples=hyperparameters[\"activation_resampler\"][\"max_n_resamples\"],\n        n_activations_activity_collate=hyperparameters[\"activation_resampler\"][\n            \"n_activations_activity_collate\"\n        ],\n        resample_dataset_size=hyperparameters[\"activation_resampler\"][\"resample_dataset_size\"],\n        threshold_is_dead_portion_fires=hyperparameters[\"activation_resampler\"][\n            \"threshold_is_dead_portion_fires\"\n        ],\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_autoencoder","title":"<code>setup_autoencoder(hyperparameters, device)</code>","text":"<p>Setup the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <code>device</code> <code>device</code> <p>The computation device.</p> required <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The initialized sparse autoencoder.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_autoencoder(\n    hyperparameters: RuntimeHyperparameters, device: torch.device\n) -&gt; SparseAutoencoder:\n    \"\"\"Setup the sparse autoencoder.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n        device: The computation device.\n\n    Returns:\n        The initialized sparse autoencoder.\n    \"\"\"\n    autoencoder_input_dim: int = hyperparameters[\"source_model\"][\"hook_dimension\"]\n    expansion_factor = hyperparameters[\"autoencoder\"][\"expansion_factor\"]\n    return SparseAutoencoder(\n        n_input_features=autoencoder_input_dim,\n        n_learned_features=autoencoder_input_dim * expansion_factor,\n    ).to(device)\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_loss_function","title":"<code>setup_loss_function(hyperparameters)</code>","text":"<p>Setup the loss function for the autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>LossReducer</code> <p>The combined loss function.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_loss_function(hyperparameters: RuntimeHyperparameters) -&gt; LossReducer:\n    \"\"\"Setup the loss function for the autoencoder.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The combined loss function.\n    \"\"\"\n    return LossReducer(\n        LearnedActivationsL1Loss(\n            l1_coefficient=hyperparameters[\"loss\"][\"l1_coefficient\"],\n        ),\n        L2ReconstructionLoss(),\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_optimizer","title":"<code>setup_optimizer(autoencoder, hyperparameters)</code>","text":"<p>Setup the optimizer for the autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>autoencoder</code> <code>SparseAutoencoder</code> <p>The sparse autoencoder model.</p> required <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>AdamWithReset</code> <p>The initialized optimizer.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_optimizer(\n    autoencoder: SparseAutoencoder, hyperparameters: RuntimeHyperparameters\n) -&gt; AdamWithReset:\n    \"\"\"Setup the optimizer for the autoencoder.\n\n    Args:\n        autoencoder: The sparse autoencoder model.\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The initialized optimizer.\n    \"\"\"\n    return AdamWithReset(\n        params=autoencoder.parameters(),\n        named_parameters=autoencoder.named_parameters(),\n        lr=hyperparameters[\"optimizer\"][\"lr\"],\n        betas=(\n            hyperparameters[\"optimizer\"][\"adam_beta_1\"],\n            hyperparameters[\"optimizer\"][\"adam_beta_2\"],\n        ),\n        weight_decay=hyperparameters[\"optimizer\"][\"adam_weight_decay\"],\n        amsgrad=hyperparameters[\"optimizer\"][\"amsgrad\"],\n        fused=hyperparameters[\"optimizer\"][\"fused\"],\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_source_data","title":"<code>setup_source_data(hyperparameters)</code>","text":"<p>Setup the source data for training.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>SourceDataset</code> <p>The initialized source dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tokenizer name is not specified, but pre_tokenized is False.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_source_data(hyperparameters: RuntimeHyperparameters) -&gt; SourceDataset:\n    \"\"\"Setup the source data for training.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The initialized source dataset.\n\n    Raises:\n        ValueError: If the tokenizer name is not specified, but pre_tokenized is False.\n    \"\"\"\n    dataset_dir = (\n        hyperparameters[\"source_data\"][\"dataset_dir\"]\n        if \"dataset_dir\" in hyperparameters[\"source_data\"]\n        else None\n    )\n\n    dataset_files = (\n        hyperparameters[\"source_data\"][\"dataset_files\"]\n        if \"dataset_files\" in hyperparameters[\"source_data\"]\n        else None\n    )\n\n    if hyperparameters[\"source_data\"][\"pre_tokenized\"]:\n        return PreTokenizedDataset(\n            context_size=hyperparameters[\"source_data\"][\"context_size\"],\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=hyperparameters[\"source_data\"][\"dataset_path\"],\n            pre_download=hyperparameters[\"source_data\"][\"pre_download\"],\n        )\n\n    if hyperparameters[\"source_data\"][\"tokenizer_name\"] is None:\n        error_message = (\n            \"If pre_tokenized is False, then tokenizer_name must be specified in the \"\n            \"hyperparameters.\"\n        )\n        raise ValueError(error_message)\n\n    tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"source_data\"][\"tokenizer_name\"])\n\n    return TextDataset(\n        context_size=hyperparameters[\"source_data\"][\"context_size\"],\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=hyperparameters[\"source_data\"][\"dataset_path\"],\n        n_processes_preprocessing=4,\n        pre_download=hyperparameters[\"source_data\"][\"pre_download\"],\n        tokenizer=tokenizer,\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_source_model","title":"<code>setup_source_model(hyperparameters)</code>","text":"<p>Setup the source model using HookedTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>HookedTransformer</code> <p>The initialized source model.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_source_model(hyperparameters: RuntimeHyperparameters) -&gt; HookedTransformer:\n    \"\"\"Setup the source model using HookedTransformer.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The initialized source model.\n    \"\"\"\n    return HookedTransformer.from_pretrained(\n        hyperparameters[\"source_model\"][\"name\"],\n        dtype=hyperparameters[\"source_model\"][\"dtype\"],\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_wandb","title":"<code>setup_wandb()</code>","text":"<p>Initialise wandb for experiment tracking.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_wandb() -&gt; RuntimeHyperparameters:\n    \"\"\"Initialise wandb for experiment tracking.\"\"\"\n    wandb.init(project=\"sparse-autoencoder\")\n    return dict(wandb.config)  # type: ignore\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.sweep","title":"<code>sweep(sweep_config)</code>","text":"<p>Main function to run the training pipeline with wandb hyperparameter sweep.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def sweep(sweep_config: SweepConfig) -&gt; None:\n    \"\"\"Main function to run the training pipeline with wandb hyperparameter sweep.\"\"\"\n    sweep_id = wandb.sweep(sweep_config.to_dict(), project=\"sparse-autoencoder\")\n\n    wandb.agent(sweep_id, train)\n    wandb.finish()\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.train","title":"<code>train()</code>","text":"<p>Train the sparse autoencoder using the hyperparameters from the WandB sweep.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def train() -&gt; None:\n    \"\"\"Train the sparse autoencoder using the hyperparameters from the WandB sweep.\"\"\"\n    try:\n        # Set up WandB\n        hyperparameters = setup_wandb()\n        run_name: str = wandb.run.name  # type: ignore\n\n        # Setup the device for training\n        device = get_device()\n\n        # Set up the source model\n        source_model = setup_source_model(hyperparameters)\n\n        # Set up the autoencoder\n        autoencoder = setup_autoencoder(hyperparameters, device)\n\n        # Set up the loss function\n        loss_function = setup_loss_function(hyperparameters)\n\n        # Set up the optimizer\n        optimizer = setup_optimizer(autoencoder, hyperparameters)\n\n        # Set up the activation resampler\n        activation_resampler = setup_activation_resampler(hyperparameters)\n\n        # Set up the source data\n        source_data = setup_source_data(hyperparameters)\n\n        # Run the training pipeline\n        run_training_pipeline(\n            hyperparameters=hyperparameters,\n            source_model=source_model,\n            autoencoder=autoencoder,\n            loss=loss_function,\n            optimizer=optimizer,\n            activation_resampler=activation_resampler,\n            source_data=source_data,\n            run_name=run_name,\n        )\n\n    # Explicit exception catching needed to show the stack trace in wandb sweeps\n    except Exception as _exception:  # noqa: BLE001\n        # Format the stack trace\n        full_stack_trace = traceback.format_exc(50)\n\n        stack_trace = \"\\n\".join(\n            line for line in full_stack_trace.splitlines() if \"wandb/sdk\" not in line\n        )\n\n        # Also print the stack trace to stderr\n        print(stack_trace, file=sys.stderr)  # noqa: T201\n\n        # Exit current run with an error code\n        sys.exit(1)\n</code></pre>"},{"location":"reference/train/sweep_config/","title":"Sweep config","text":"<p>Sweep config.</p> <p>Default hyperparameter setup for quick tuning of a sparse autoencoder.</p> Warning <p>The runtime hyperparameter classes must be manually kept in sync with the hyperparameter classes, so that static type checking works.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters","title":"<code>ActivationResamplerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Activation resampler hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass ActivationResamplerHyperparameters(NestedParameter):\n    \"\"\"Activation resampler hyperparameters.\"\"\"\n\n    resample_interval: Parameter[int] = field(\n        default=Parameter(round_to_multiple(200_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Resample interval.\"\"\"\n\n    max_n_resamples: Parameter[int] = field(default=Parameter(4))\n    \"\"\"Maximum number of resamples.\"\"\"\n\n    n_activations_activity_collate: Parameter[int] = field(\n        default=Parameter(round_to_multiple(100_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Number of steps to collate before resampling.\n\n    Number of autoencoder learned activation vectors to collate before resampling.\n    \"\"\"\n\n    resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))\n    \"\"\"Resample dataset size.\n\n    Number of autoencoder input activations to use for calculating the loss, as part of the\n    resampling process to create the reset neuron weights.\n    \"\"\"\n\n    threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Dead neuron threshold.\n\n    Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the\n    collated sample).\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.max_n_resamples","title":"<code>max_n_resamples: Parameter[int] = field(default=Parameter(4))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of resamples.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.n_activations_activity_collate","title":"<code>n_activations_activity_collate: Parameter[int] = field(default=Parameter(round_to_multiple(100000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of steps to collate before resampling.</p> <p>Number of autoencoder learned activation vectors to collate before resampling.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.resample_dataset_size","title":"<code>resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample dataset size.</p> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.resample_interval","title":"<code>resample_interval: Parameter[int] = field(default=Parameter(round_to_multiple(200000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample interval.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires","title":"<code>threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead neuron threshold.</p> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerRuntimeHyperparameters","title":"<code>ActivationResamplerRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Activation resampler runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class ActivationResamplerRuntimeHyperparameters(TypedDict):\n    \"\"\"Activation resampler runtime hyperparameters.\"\"\"\n\n    resample_interval: int\n    max_n_resamples: int\n    n_activations_activity_collate: int\n    resample_dataset_size: int\n    threshold_is_dead_portion_fires: float\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.AutoencoderHyperparameters","title":"<code>AutoencoderHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Sparse autoencoder hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass AutoencoderHyperparameters(NestedParameter):\n    \"\"\"Sparse autoencoder hyperparameters.\"\"\"\n\n    expansion_factor: Parameter[int] = field(default=Parameter(2))\n    \"\"\"Expansion Factor.\n\n    Size of the learned features relative to the input features. A good expansion factor to start\n    with is typically 2-4.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.AutoencoderHyperparameters.expansion_factor","title":"<code>expansion_factor: Parameter[int] = field(default=Parameter(2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion Factor.</p> <p>Size of the learned features relative to the input features. A good expansion factor to start with is typically 2-4.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.AutoencoderRuntimeHyperparameters","title":"<code>AutoencoderRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Autoencoder runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class AutoencoderRuntimeHyperparameters(TypedDict):\n    \"\"\"Autoencoder runtime hyperparameters.\"\"\"\n\n    expansion_factor: int\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters","title":"<code>Hyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Parameters</code></p> <p>Sweep Hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass Hyperparameters(Parameters):\n    \"\"\"Sweep Hyperparameters.\"\"\"\n\n    # Required parameters\n    source_data: SourceDataHyperparameters\n\n    source_model: SourceModelHyperparameters\n\n    # Optional parameters\n    activation_resampler: ActivationResamplerHyperparameters = field(\n        default=ActivationResamplerHyperparameters()\n    )\n\n    autoencoder: AutoencoderHyperparameters = field(default=AutoencoderHyperparameters())\n\n    loss: LossHyperparameters = field(default=LossHyperparameters())\n\n    optimizer: OptimizerHyperparameters = field(default=OptimizerHyperparameters())\n\n    pipeline: PipelineHyperparameters = field(default=PipelineHyperparameters())\n\n    random_seed: Parameter[int] = field(default=Parameter(49))\n    \"\"\"Random seed.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\"\"\"\n        # Check the resample dataset size &lt;= the store size (currently only works if value is used\n        # for both).\n        if (\n            self.activation_resampler.resample_dataset_size.value is not None\n            and self.pipeline.max_store_size.value is not None\n            and self.activation_resampler.resample_dataset_size.value\n            &gt; int(self.pipeline.max_store_size.value)\n        ):\n            error_message = (\n                \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n                f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n                f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n            )\n            raise ValueError(error_message)\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \"\\n    \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}(\\n    {joined_items}\\n)\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.random_seed","title":"<code>random_seed: Parameter[int] = field(default=Parameter(49))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\"\"\"\n    # Check the resample dataset size &lt;= the store size (currently only works if value is used\n    # for both).\n    if (\n        self.activation_resampler.resample_dataset_size.value is not None\n        and self.pipeline.max_store_size.value is not None\n        and self.activation_resampler.resample_dataset_size.value\n        &gt; int(self.pipeline.max_store_size.value)\n    ):\n        error_message = (\n            \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n            f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n            f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \"\\n    \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}(\\n    {joined_items}\\n)\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.LossHyperparameters","title":"<code>LossHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Loss hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass LossHyperparameters(NestedParameter):\n    \"\"\"Loss hyperparameters.\"\"\"\n\n    l1_coefficient: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"L1 Penalty Coefficient.\n\n    The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.\n    The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by\n    using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good\n    starting point for the L1 coefficient is 1e-3.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.LossHyperparameters.l1_coefficient","title":"<code>l1_coefficient: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>L1 Penalty Coefficient.</p> <p>The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good starting point for the L1 coefficient is 1e-3.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.LossRuntimeHyperparameters","title":"<code>LossRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Loss runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class LossRuntimeHyperparameters(TypedDict):\n    \"\"\"Loss runtime hyperparameters.\"\"\"\n\n    l1_coefficient: float\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters","title":"<code>OptimizerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Optimizer hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass OptimizerHyperparameters(NestedParameter):\n    \"\"\"Optimizer hyperparameters.\"\"\"\n\n    lr: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"Learning rate.\n\n    A good starting point for the learning rate is 1e-3, but this is one of the key parameters so\n    you should probably tune it.\n    \"\"\"\n\n    adam_beta_1: Parameter[float] = field(default=Parameter(0.9))\n    \"\"\"Adam Beta 1.\n\n    The exponential decay rate for the first moment estimates (mean) of the gradient.\n    \"\"\"\n\n    adam_beta_2: Parameter[float] = field(default=Parameter(0.99))\n    \"\"\"Adam Beta 2.\n\n    The exponential decay rate for the second moment estimates (variance) of the gradient.\n    \"\"\"\n\n    adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Adam Weight Decay.\n\n    Weight decay (L2 penalty).\n    \"\"\"\n\n    amsgrad: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"AMSGrad.\n\n    Whether to use the AMSGrad variant of this algorithm from the paper [On the Convergence of Adam\n    and Beyond](https://arxiv.org/abs/1904.09237).\n    \"\"\"\n\n    fused: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Fused.\n\n    Whether to use a fused implementation of the optimizer (may be faster on CUDA).\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.adam_beta_1","title":"<code>adam_beta_1: Parameter[float] = field(default=Parameter(0.9))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 1.</p> <p>The exponential decay rate for the first moment estimates (mean) of the gradient.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.adam_beta_2","title":"<code>adam_beta_2: Parameter[float] = field(default=Parameter(0.99))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 2.</p> <p>The exponential decay rate for the second moment estimates (variance) of the gradient.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.adam_weight_decay","title":"<code>adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Weight Decay.</p> <p>Weight decay (L2 penalty).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.amsgrad","title":"<code>amsgrad: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AMSGrad.</p> <p>Whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.fused","title":"<code>fused: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fused.</p> <p>Whether to use a fused implementation of the optimizer (may be faster on CUDA).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.lr","title":"<code>lr: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate.</p> <p>A good starting point for the learning rate is 1e-3, but this is one of the key parameters so you should probably tune it.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerRuntimeHyperparameters","title":"<code>OptimizerRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Optimizer runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class OptimizerRuntimeHyperparameters(TypedDict):\n    \"\"\"Optimizer runtime hyperparameters.\"\"\"\n\n    lr: float\n    adam_beta_1: float\n    adam_beta_2: float\n    adam_weight_decay: float\n    amsgrad: bool\n    fused: bool\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters","title":"<code>PipelineHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Pipeline hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass PipelineHyperparameters(NestedParameter):\n    \"\"\"Pipeline hyperparameters.\"\"\"\n\n    log_frequency: Parameter[int] = field(default=Parameter(100))\n    \"\"\"Training log frequency.\"\"\"\n\n    source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))\n    \"\"\"Source data batch size.\"\"\"\n\n    train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))\n    \"\"\"Train batch size.\"\"\"\n\n    max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))\n    \"\"\"Max store size.\"\"\"\n\n    max_activations: Parameter[int] = field(\n        default=Parameter(round_to_multiple(2e9, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Max activations.\"\"\"\n\n    checkpoint_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(5e7, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Checkpoint frequency.\"\"\"\n\n    validation_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(1e8, DEFAULT_BATCH_SIZE))\n    )\n    \"\"\"Validation frequency.\"\"\"\n\n    validation_number_activations: Parameter[int] = field(\n        # Default to a single batch of source data prompts\n        default=Parameter(DEFAULT_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 16)\n    )\n    \"\"\"Number of activations to use for validation.\"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.checkpoint_frequency","title":"<code>checkpoint_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(50000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoint frequency.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.log_frequency","title":"<code>log_frequency: Parameter[int] = field(default=Parameter(100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training log frequency.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.max_activations","title":"<code>max_activations: Parameter[int] = field(default=Parameter(round_to_multiple(2000000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max activations.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.max_store_size","title":"<code>max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max store size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.source_data_batch_size","title":"<code>source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source data batch size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.train_batch_size","title":"<code>train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Train batch size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.validation_frequency","title":"<code>validation_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(100000000.0, DEFAULT_BATCH_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation frequency.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.validation_number_activations","title":"<code>validation_number_activations: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 16))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of activations to use for validation.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineRuntimeHyperparameters","title":"<code>PipelineRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Pipeline runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class PipelineRuntimeHyperparameters(TypedDict):\n    \"\"\"Pipeline runtime hyperparameters.\"\"\"\n\n    log_frequency: int\n    source_data_batch_size: int\n    train_batch_size: int\n    max_store_size: int\n    max_activations: int\n    checkpoint_frequency: int\n    validation_frequency: int\n    validation_number_activations: int\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.RuntimeHyperparameters","title":"<code>RuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class RuntimeHyperparameters(TypedDict):\n    \"\"\"Runtime hyperparameters.\"\"\"\n\n    source_data: SourceDataRuntimeHyperparameters\n    source_model: SourceModelRuntimeHyperparameters\n    activation_resampler: ActivationResamplerRuntimeHyperparameters\n    autoencoder: AutoencoderRuntimeHyperparameters\n    loss: LossRuntimeHyperparameters\n    optimizer: OptimizerRuntimeHyperparameters\n    pipeline: PipelineRuntimeHyperparameters\n    random_seed: int\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters","title":"<code>SourceDataHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceDataHyperparameters(NestedParameter):\n    \"\"\"Source data hyperparameters.\"\"\"\n\n    dataset_path: Parameter[str]\n    \"\"\"Dataset path.\"\"\"\n\n    context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))\n    \"\"\"Context size.\"\"\"\n\n    dataset_dir: Parameter[str] | None = field(default=None)\n    \"\"\"Dataset directory (within the HF dataset)\"\"\"\n\n    dataset_files: Parameter[list[str]] | None = field(default=None)\n    \"\"\"Dataset files (within the HF dataset).\"\"\"\n\n    pre_download: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Whether to pre-download the dataset.\"\"\"\n\n    pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))\n    \"\"\"If the dataset is pre-tokenized.\"\"\"\n\n    tokenizer_name: Parameter[str] | None = field(default=None)\n    \"\"\"Tokenizer name.\n\n    Only set this if the dataset is not pre-tokenized.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\n\n        Raises:\n            ValueError: If there is an error in the source data hyperparameters.\n        \"\"\"\n        if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n            raise ValueError(error_message)\n\n        if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n            raise ValueError(error_message)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.context_size","title":"<code>context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Context size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_dir","title":"<code>dataset_dir: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset directory (within the HF dataset)</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_files","title":"<code>dataset_files: Parameter[list[str]] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset files (within the HF dataset).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_path","title":"<code>dataset_path: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Dataset path.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.pre_download","title":"<code>pre_download: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to pre-download the dataset.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.pre_tokenized","title":"<code>pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If the dataset is pre-tokenized.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.tokenizer_name","title":"<code>tokenizer_name: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tokenizer name.</p> <p>Only set this if the dataset is not pre-tokenized.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error in the source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\n\n    Raises:\n        ValueError: If there is an error in the source data hyperparameters.\n    \"\"\"\n    if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n        raise ValueError(error_message)\n\n    if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataRuntimeHyperparameters","title":"<code>SourceDataRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Source data runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class SourceDataRuntimeHyperparameters(TypedDict):\n    \"\"\"Source data runtime hyperparameters.\"\"\"\n\n    context_size: int\n    dataset_dir: str | None\n    dataset_files: list[str] | None\n    dataset_path: str\n    pre_download: bool\n    pre_tokenized: bool\n    tokenizer_name: str | None\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters","title":"<code>SourceModelHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source model hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceModelHyperparameters(NestedParameter):\n    \"\"\"Source model hyperparameters.\"\"\"\n\n    name: Parameter[str]\n    \"\"\"Source model name.\"\"\"\n\n    hook_site: Parameter[str]\n    \"\"\"Source model hook site.\"\"\"\n\n    hook_layer: Parameter[int]\n    \"\"\"Source model hook point layer.\"\"\"\n\n    hook_dimension: Parameter[int]\n    \"\"\"Source model hook point dimension.\"\"\"\n\n    dtype: Parameter[str] = field(default=Parameter(\"float32\"))\n    \"\"\"Source model dtype.\"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.dtype","title":"<code>dtype: Parameter[str] = field(default=Parameter('float32'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source model dtype.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.hook_dimension","title":"<code>hook_dimension: Parameter[int]</code>  <code>instance-attribute</code>","text":"<p>Source model hook point dimension.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.hook_layer","title":"<code>hook_layer: Parameter[int]</code>  <code>instance-attribute</code>","text":"<p>Source model hook point layer.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.hook_site","title":"<code>hook_site: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Source model hook site.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.name","title":"<code>name: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Source model name.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelRuntimeHyperparameters","title":"<code>SourceModelRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Source model runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class SourceModelRuntimeHyperparameters(TypedDict):\n    \"\"\"Source model runtime hyperparameters.\"\"\"\n\n    name: str\n    hook_site: str\n    hook_layer: int\n    hook_dimension: int\n    dtype: str\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig","title":"<code>SweepConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>WandbSweepConfig</code></p> <p>Sweep Config.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass SweepConfig(WandbSweepConfig):\n    \"\"\"Sweep Config.\"\"\"\n\n    parameters: Hyperparameters\n\n    method: Method = Method.GRID\n\n    metric: Metric = field(default=Metric(name=\"train/loss/total_loss\"))\n</code></pre>"},{"location":"reference/train/utils/","title":"Train Utils","text":"<p>Train Utils.</p>"},{"location":"reference/train/utils/get_model_device/","title":"Get the device that the model is on","text":"<p>Get the device that the model is on.</p>"},{"location":"reference/train/utils/get_model_device/#sparse_autoencoder.train.utils.get_model_device.get_model_device","title":"<code>get_model_device(model)</code>","text":"<p>Get the device on which a PyTorch model is on.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The device ('cuda' or 'cpu') where the model is located.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has no parameters.</p> Source code in <code>sparse_autoencoder/train/utils/get_model_device.py</code> <pre><code>def get_model_device(model: Module) -&gt; torch.device:\n    \"\"\"Get the device on which a PyTorch model is on.\n\n    Args:\n        model: The PyTorch model.\n\n    Returns:\n        The device ('cuda' or 'cpu') where the model is located.\n\n    Raises:\n        ValueError: If the model has no parameters.\n    \"\"\"\n    # Check if the model has parameters\n    if len(list(model.parameters())) == 0:\n        exception_message = \"The model has no parameters.\"\n        raise ValueError(exception_message)\n\n    # Return the device of the first parameter\n    return next(model.parameters()).device\n</code></pre>"},{"location":"reference/train/utils/round_down/","title":"Round down to the nearest multiple","text":"<p>Round down to the nearest multiple.</p>"},{"location":"reference/train/utils/round_down/#sparse_autoencoder.train.utils.round_down.round_to_multiple","title":"<code>round_to_multiple(value, multiple)</code>","text":"<p>Round down to the nearest multiple.</p> <p>Helper function for creating default values.</p> Example <p>round_to_multiple(1023, 100) 1000</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int | float</code> <p>The value to round down.</p> required <code>multiple</code> <code>int</code> <p>The multiple to round down to.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The value rounded down to the nearest multiple.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>value</code> is less than <code>multiple</code>.</p> Source code in <code>sparse_autoencoder/train/utils/round_down.py</code> <pre><code>def round_to_multiple(value: int | float, multiple: int) -&gt; int:  # noqa: PYI041\n    \"\"\"Round down to the nearest multiple.\n\n    Helper function for creating default values.\n\n    Example:\n        &gt;&gt;&gt; round_to_multiple(1023, 100)\n        1000\n\n    Args:\n        value: The value to round down.\n        multiple: The multiple to round down to.\n\n    Returns:\n        The value rounded down to the nearest multiple.\n\n    Raises:\n        ValueError: If `value` is less than `multiple`.\n    \"\"\"\n    int_value = int(value)\n\n    if int_value &lt; multiple:\n        error_message = f\"{value=} must be greater than or equal to {multiple=}\"\n        raise ValueError(error_message)\n\n    return int_value - int_value % multiple\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/","title":"Wandb Sweep Config Dataclasses","text":"<p>Wandb Sweep Config Dataclasses.</p> <p>Weights &amp; Biases just provide a JSON Schema, so we've converted here to dataclasses.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Controller","title":"<code>Controller</code>  <code>dataclass</code>","text":"<p>Controller.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass Controller:\n    \"\"\"Controller.\"\"\"\n\n    type: ControllerType  # noqa: A003\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType","title":"<code>ControllerType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Controller Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ControllerType(LowercaseStrEnum):\n    \"\"\"Controller Type.\"\"\"\n\n    CLOUD = auto()\n    \"\"\"Weights &amp; Biases cloud controller.\n\n    Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all\n    communicate with the Weights &amp; Biases cloud service to coordinate the sweep.\n    \"\"\"\n\n    LOCAL = auto()\n    \"\"\"Local controller.\n\n    Manages the sweep operation locally, without the need for cloud-based coordination or external\n    services.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType.CLOUD","title":"<code>CLOUD = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weights &amp; Biases cloud controller.</p> <p>Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all communicate with the Weights &amp; Biases cloud service to coordinate the sweep.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType.LOCAL","title":"<code>LOCAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Local controller.</p> <p>Manages the sweep operation locally, without the need for cloud-based coordination or external services.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution","title":"<code>Distribution</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Sweep Distribution.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Distribution(LowercaseStrEnum):\n    \"\"\"Sweep Distribution.\"\"\"\n\n    BETA = auto()\n    \"\"\"Beta distribution.\n\n    Utilizes the Beta distribution, a family of continuous probability distributions defined on the\n    interval [0, 1], for parameter sampling.\n    \"\"\"\n\n    CATEGORICAL = auto()\n    \"\"\"Categorical distribution.\n\n    Employs a categorical distribution for discrete variable sampling, where each category has an\n    equal probability of being selected.\n    \"\"\"\n\n    CATEGORICAL_W_PROBABILITIES = auto()\n    \"\"\"Categorical distribution with probabilities.\n\n    Similar to categorical distribution but allows assigning different probabilities to each\n    category.\n    \"\"\"\n\n    CONSTANT = auto()\n    \"\"\"Constant distribution.\n\n    Uses a constant value for the parameter, ensuring it remains the same across all runs.\n    \"\"\"\n\n    INT_UNIFORM = auto()\n    \"\"\"Integer uniform distribution.\n\n    Samples integer values uniformly across a specified range.\n    \"\"\"\n\n    INV_LOG_UNIFORM = auto()\n    \"\"\"Inverse log-uniform distribution.\n\n    Samples values according to an inverse log-uniform distribution, useful for parameters that span\n    several orders of magnitude.\n    \"\"\"\n\n    INV_LOG_UNIFORM_VALUES = auto()\n    \"\"\"Inverse log-uniform values distribution.\n\n    Similar to the inverse log-uniform distribution but allows specifying exact values to be\n    sampled.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.BETA","title":"<code>BETA = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Beta distribution.</p> <p>Utilizes the Beta distribution, a family of continuous probability distributions defined on the interval [0, 1], for parameter sampling.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.CATEGORICAL","title":"<code>CATEGORICAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution.</p> <p>Employs a categorical distribution for discrete variable sampling, where each category has an equal probability of being selected.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.CATEGORICAL_W_PROBABILITIES","title":"<code>CATEGORICAL_W_PROBABILITIES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution with probabilities.</p> <p>Similar to categorical distribution but allows assigning different probabilities to each category.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.CONSTANT","title":"<code>CONSTANT = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant distribution.</p> <p>Uses a constant value for the parameter, ensuring it remains the same across all runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.INT_UNIFORM","title":"<code>INT_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Integer uniform distribution.</p> <p>Samples integer values uniformly across a specified range.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.INV_LOG_UNIFORM","title":"<code>INV_LOG_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform distribution.</p> <p>Samples values according to an inverse log-uniform distribution, useful for parameters that span several orders of magnitude.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.INV_LOG_UNIFORM_VALUES","title":"<code>INV_LOG_UNIFORM_VALUES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform values distribution.</p> <p>Similar to the inverse log-uniform distribution but allows specifying exact values to be sampled.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal","title":"<code>Goal</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Goal.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Goal(LowercaseStrEnum):\n    \"\"\"Goal.\"\"\"\n\n    MAXIMIZE = auto()\n    \"\"\"Maximization goal.\n\n    Sets the objective of the hyperparameter tuning process to maximize a specified metric.\n    \"\"\"\n\n    MINIMIZE = auto()\n    \"\"\"Minimization goal.\n\n    Aims to minimize a specified metric during the hyperparameter tuning process.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal.MAXIMIZE","title":"<code>MAXIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximization goal.</p> <p>Sets the objective of the hyperparameter tuning process to maximize a specified metric.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal.MINIMIZE","title":"<code>MINIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimization goal.</p> <p>Aims to minimize a specified metric during the hyperparameter tuning process.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping","title":"<code>HyperbandStopping</code>  <code>dataclass</code>","text":"<p>Hyperband Stopping Config.</p> <p>Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs.</p> Example <p>HyperbandStopping(type=HyperbandStoppingType.HYPERBAND) HyperbandStopping(type=hyperband)</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass HyperbandStopping:\n    \"\"\"Hyperband Stopping Config.\n\n    Speed up hyperparameter search by killing off runs that appear to have lower performance\n    than successful training runs.\n\n    Example:\n        &gt;&gt;&gt; HyperbandStopping(type=HyperbandStoppingType.HYPERBAND)\n        HyperbandStopping(type=hyperband)\n    \"\"\"\n\n    type: HyperbandStoppingType | None = HyperbandStoppingType.HYPERBAND  # noqa: A003\n\n    eta: float | None = None\n    \"\"\"ETA.\n\n    Specify the bracket multiplier schedule (default: 3).\n    \"\"\"\n\n    maxiter: int | None = None\n    \"\"\"Max Iterations.\n\n    Specify the maximum number of iterations. Note this is number of times the metric is logged, not\n    the number of activations.\n    \"\"\"\n\n    miniter: int | None = None\n    \"\"\"Min Iterations.\n\n    Set the first epoch to start trimming runs, and hyperband will automatically calculate\n    the subsequent epochs to trim runs.\n    \"\"\"\n\n    s: float | None = None\n    \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\"\n\n    strict: bool | None = None\n    \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.eta","title":"<code>eta: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ETA.</p> <p>Specify the bracket multiplier schedule (default: 3).</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.maxiter","title":"<code>maxiter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max Iterations.</p> <p>Specify the maximum number of iterations. Note this is number of times the metric is logged, not the number of activations.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.miniter","title":"<code>miniter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Min Iterations.</p> <p>Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.s","title":"<code>s: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Set the number of steps you trim runs at, working backwards from the max_iter.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.strict","title":"<code>strict: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use a more aggressive condition for termination, stops more runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStoppingType","title":"<code>HyperbandStoppingType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Hyperband Stopping Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class HyperbandStoppingType(LowercaseStrEnum):\n    \"\"\"Hyperband Stopping Type.\"\"\"\n\n    HYPERBAND = auto()\n    \"\"\"Hyperband algorithm.\n\n    Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping\n    method to efficiently tune hyperparameters.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStoppingType.HYPERBAND","title":"<code>HYPERBAND = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hyperband algorithm.</p> <p>Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping method to efficiently tune hyperparameters.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Impute","title":"<code>Impute</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Metric value to use in bayes search for runs that fail, crash, or are killed.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Impute(LowercaseStrEnum):\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\"\n\n    BEST = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ImputeWhileRunning","title":"<code>ImputeWhileRunning</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Appends a calculated metric even when epochs are in a running state.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ImputeWhileRunning(LowercaseStrEnum):\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    BEST = auto()\n    FALSE = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Kind","title":"<code>Kind</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Kind.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Kind(LowercaseStrEnum):\n    \"\"\"Kind.\"\"\"\n\n    SWEEP = auto()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method","title":"<code>Method</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Method.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Method(LowercaseStrEnum):\n    \"\"\"Method.\"\"\"\n\n    BAYES = auto()\n    \"\"\"Bayesian optimization.\n\n    Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach\n    for finding the optimal set of parameters.\n    \"\"\"\n\n    CUSTOM = auto()\n    \"\"\"Custom method.\n\n    Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the\n    sweep process.\n    \"\"\"\n\n    GRID = auto()\n    \"\"\"Grid search.\n\n    Utilizes a grid search approach for hyperparameter tuning, systematically working through\n    multiple combinations of parameter values.\n    \"\"\"\n\n    RANDOM = auto()\n    \"\"\"Random search.\n\n    Implements a random search strategy for hyperparameter tuning, exploring the parameter space\n    randomly.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.BAYES","title":"<code>BAYES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bayesian optimization.</p> <p>Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach for finding the optimal set of parameters.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.CUSTOM","title":"<code>CUSTOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom method.</p> <p>Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the sweep process.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.GRID","title":"<code>GRID = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Grid search.</p> <p>Utilizes a grid search approach for hyperparameter tuning, systematically working through multiple combinations of parameter values.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.RANDOM","title":"<code>RANDOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random search.</p> <p>Implements a random search strategy for hyperparameter tuning, exploring the parameter space randomly.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>Metric to optimize.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Metric:\n    \"\"\"Metric to optimize.\"\"\"\n\n    name: str\n    \"\"\"Name of metric.\"\"\"\n\n    goal: Goal | None = Goal.MINIMIZE\n\n    impute: Impute | None = None\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\"\n\n    imputewhilerunning: ImputeWhileRunning | None = None\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    target: float | None = None\n    \"\"\"The sweep will finish once any run achieves this value.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.impute","title":"<code>impute: Impute | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metric value to use in bayes search for runs that fail, crash, or are killed</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.imputewhilerunning","title":"<code>imputewhilerunning: ImputeWhileRunning | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Appends a calculated metric even when epochs are in a running state.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of metric.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.target","title":"<code>target: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will finish once any run achieves this value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter","title":"<code>NestedParameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Nested Parameter.</p> Example <p>from dataclasses import field @dataclass(frozen=True) ... class MyNestedParameter(NestedParameter): ...     a: int = field(default=Parameter(1)) ...     b: int = field(default=Parameter(2)) MyNestedParameter().to_dict() {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass NestedParameter(ABC):  # noqa: B024 (abstract so that we can check against it's type)\n    \"\"\"Nested Parameter.\n\n    Example:\n        &gt;&gt;&gt; from dataclasses import field\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class MyNestedParameter(NestedParameter):\n        ...     a: int = field(default=Parameter(1))\n        ...     b: int = field(default=Parameter(2))\n        &gt;&gt;&gt; MyNestedParameter().to_dict()\n        {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}\n    \"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\"\"\"\n\n        def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n            \"\"\"Return dict without None values.\n\n            Args:\n                obj: The object to convert to a dict.\n\n            Returns:\n                The dict representation of the object.\n            \"\"\"\n            dict_none_removed = {}\n            dict_with_none = dict(obj)\n            for key, value in dict_with_none.items():\n                if value is not None:\n                    dict_none_removed[key] = value\n            return dict_none_removed\n\n        return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n\n    def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n        \"\"\"Return dict representation of this object.\"\"\"\n        return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter.__dict__","title":"<code>__dict__()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n    \"\"\"Return dict representation of this object.\"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\"\"\"\n\n    def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n        \"\"\"Return dict without None values.\n\n        Args:\n            obj: The object to convert to a dict.\n\n        Returns:\n            The dict representation of the object.\n        \"\"\"\n        dict_none_removed = {}\n        dict_with_none = dict(obj)\n        for key, value in dict_with_none.items():\n            if value is not None:\n                dict_none_removed[key] = value\n        return dict_none_removed\n\n    return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[ParamType]</code></p> <p>Sweep Parameter.</p> <p>https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Parameter(Generic[ParamType]):\n    \"\"\"Sweep Parameter.\n\n    https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters\n    \"\"\"\n\n    value: ParamType | None = None\n    \"\"\"Single value.\n\n    Specifies the single valid value for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    max: ParamType | None = None  # noqa: A003\n    \"\"\"Maximum value.\"\"\"\n\n    min: ParamType | None = None  # noqa: A003\n    \"\"\"Minimum value.\"\"\"\n\n    distribution: Distribution | None = None\n    \"\"\"Distribution\n\n    If not specified, will default to categorical if values is set, to int_uniform if max and min\n    are set to integers, to uniform if max and min are set to floats, or to constant if value is\n    set.\n    \"\"\"\n\n    q: float | None = None\n    \"\"\"Quantization parameter.\n\n    Quantization step size for quantized hyperparameters.\n    \"\"\"\n\n    values: list[ParamType] | None = None\n    \"\"\"Discrete values.\n\n    Specifies all valid values for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    probabilities: list[float] | None = None\n    \"\"\"Probability of each value\"\"\"\n\n    mu: float | None = None\n    \"\"\"Mean for normal or lognormal distributions\"\"\"\n\n    sigma: float | None = None\n    \"\"\"Std Dev for normal or lognormal distributions\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.distribution","title":"<code>distribution: Distribution | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Distribution</p> <p>If not specified, will default to categorical if values is set, to int_uniform if max and min are set to integers, to uniform if max and min are set to floats, or to constant if value is set.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.max","title":"<code>max: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.min","title":"<code>min: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.mu","title":"<code>mu: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean for normal or lognormal distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.probabilities","title":"<code>probabilities: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Probability of each value</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.q","title":"<code>q: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantization parameter.</p> <p>Quantization step size for quantized hyperparameters.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.sigma","title":"<code>sigma: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Std Dev for normal or lognormal distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.value","title":"<code>value: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single value.</p> <p>Specifies the single valid value for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.values","title":"<code>values: list[ParamType] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Discrete values.</p> <p>Specifies all valid values for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameters","title":"<code>Parameters</code>  <code>dataclass</code>","text":"<p>Parameters</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass Parameters:\n    \"\"\"Parameters\"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig","title":"<code>WandbSweepConfig</code>  <code>dataclass</code>","text":"<p>Weights &amp; Biases Sweep Configuration.</p> Example <p>config = WandbSweepConfig( ...     parameters={\"lr\": Parameter(value=1e-3)}, ...     method=Method.BAYES, ...     metric=Metric(name=\"loss\"), ...     ) print(config.to_dict()[\"parameters\"]) {'lr': {'value': 0.001}}</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass WandbSweepConfig:\n    \"\"\"Weights &amp; Biases Sweep Configuration.\n\n    Example:\n        &gt;&gt;&gt; config = WandbSweepConfig(\n        ...     parameters={\"lr\": Parameter(value=1e-3)},\n        ...     method=Method.BAYES,\n        ...     metric=Metric(name=\"loss\"),\n        ...     )\n        &gt;&gt;&gt; print(config.to_dict()[\"parameters\"])\n        {'lr': {'value': 0.001}}\n    \"\"\"\n\n    parameters: Parameters | Any\n\n    method: Method\n    \"\"\"Method (search strategy).\"\"\"\n\n    metric: Metric\n    \"\"\"Metric to optimize\"\"\"\n\n    command: list[Any] | None = None\n    \"\"\"Command used to launch the training script\"\"\"\n\n    controller: Controller | None = None\n\n    description: str | None = None\n    \"\"\"Short package description\"\"\"\n\n    earlyterminate: HyperbandStopping | None = None\n\n    entity: str | None = None\n    \"\"\"The entity for this sweep\"\"\"\n\n    imageuri: str | None = None\n    \"\"\"Sweeps on Launch will use this uri instead of a job.\"\"\"\n\n    job: str | None = None\n    \"\"\"Launch Job to run.\"\"\"\n\n    kind: Kind | None = None\n\n    name: str | None = None\n    \"\"\"The name of the sweep, displayed in the W&amp;B UI.\"\"\"\n\n    program: str | None = None\n    \"\"\"Training script to run.\"\"\"\n\n    project: str | None = None\n    \"\"\"The project for this sweep.\"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\n\n        Recursively removes all None values. Handles special cases of dataclass\n        instances and values that are `NestedParameter` instances.\n\n        Returns:\n            dict[str, Any]: The dict representation of the object.\n        \"\"\"\n\n        def recursive_format(obj: Any) -&gt; Any:  # noqa: ANN401\n            \"\"\"Recursively format the dict of hyperparameters.\"\"\"\n            # Handle dataclasses\n            if is_dataclass(obj):\n                cleaned_obj = {}\n                for parameter_name in asdict(obj):\n                    value = getattr(obj, parameter_name)\n\n                    # Remove None values.\n                    if value is None:\n                        continue\n\n                    # Nested parameters have their own `to_dict` method, which we can call.\n                    if isinstance(value, NestedParameter):\n                        cleaned_obj[parameter_name] = value.to_dict()\n                    # Otherwise recurse.\n                    else:\n                        cleaned_obj[parameter_name] = recursive_format(value)\n                return cleaned_obj\n\n            # Handle dicts\n            if isinstance(obj, dict):\n                cleaned_obj = {}\n                for key, value in obj.items():\n                    # Remove None values.\n                    if value is None:\n                        continue\n\n                    # Otherwise recurse.\n                    cleaned_obj[key] = recursive_format(value)\n                return cleaned_obj\n\n            # Handle enums\n            if isinstance(obj, Enum):\n                return obj.value\n\n            # Handle other types (e.g. float, int, str)\n            return obj\n\n        return recursive_format(self)\n\n    def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n        \"\"\"Return dict representation of this object.\"\"\"\n        return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.command","title":"<code>command: list[Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Command used to launch the training script</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.description","title":"<code>description: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Short package description</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.entity","title":"<code>entity: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entity for this sweep</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.imageuri","title":"<code>imageuri: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sweeps on Launch will use this uri instead of a job.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.job","title":"<code>job: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Launch Job to run.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.method","title":"<code>method: Method</code>  <code>instance-attribute</code>","text":"<p>Method (search strategy).</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.metric","title":"<code>metric: Metric</code>  <code>instance-attribute</code>","text":"<p>Metric to optimize</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.name","title":"<code>name: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the sweep, displayed in the W&amp;B UI.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.program","title":"<code>program: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training script to run.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.project","title":"<code>project: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The project for this sweep.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.__dict__","title":"<code>__dict__()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n    \"\"\"Return dict representation of this object.\"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> <p>Recursively removes all None values. Handles special cases of dataclass instances and values that are <code>NestedParameter</code> instances.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The dict representation of the object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\n\n    Recursively removes all None values. Handles special cases of dataclass\n    instances and values that are `NestedParameter` instances.\n\n    Returns:\n        dict[str, Any]: The dict representation of the object.\n    \"\"\"\n\n    def recursive_format(obj: Any) -&gt; Any:  # noqa: ANN401\n        \"\"\"Recursively format the dict of hyperparameters.\"\"\"\n        # Handle dataclasses\n        if is_dataclass(obj):\n            cleaned_obj = {}\n            for parameter_name in asdict(obj):\n                value = getattr(obj, parameter_name)\n\n                # Remove None values.\n                if value is None:\n                    continue\n\n                # Nested parameters have their own `to_dict` method, which we can call.\n                if isinstance(value, NestedParameter):\n                    cleaned_obj[parameter_name] = value.to_dict()\n                # Otherwise recurse.\n                else:\n                    cleaned_obj[parameter_name] = recursive_format(value)\n            return cleaned_obj\n\n        # Handle dicts\n        if isinstance(obj, dict):\n            cleaned_obj = {}\n            for key, value in obj.items():\n                # Remove None values.\n                if value is None:\n                    continue\n\n                # Otherwise recurse.\n                cleaned_obj[key] = recursive_format(value)\n            return cleaned_obj\n\n        # Handle enums\n        if isinstance(obj, Enum):\n            return obj.value\n\n        # Handle other types (e.g. float, int, str)\n        return obj\n\n    return recursive_format(self)\n</code></pre>"}]}