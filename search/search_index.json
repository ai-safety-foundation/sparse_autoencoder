{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home A sparse autoencoder for mechanistic interpretability research. pip install sparse_autoencoder Demo Check out the demo notebook for a guide to using this library.","title":"Home"},{"location":"#home","text":"A sparse autoencoder for mechanistic interpretability research. pip install sparse_autoencoder","title":"Home"},{"location":"#demo","text":"Check out the demo notebook for a guide to using this library.","title":"Demo"},{"location":"SUMMARY/","text":"Home Getting Started Contributing Citation Reference","title":"SUMMARY"},{"location":"citation/","text":"Citation Please cite this library as:","title":"Citation"},{"location":"citation/#citation","text":"Please cite this library as:","title":"Citation"},{"location":"contributing/","text":"Contributing Setup DevContainer For a one-click setup of your development environment, this project includes a DevContainer . It can be used locally with VS Code or with GitHub Codespaces . Manual Setup This project uses Poetry for package management. Install as follows (this will also setup your virtual environment): poetry config virtualenvs.in-project true poetry install --with dev,docs,jupyter Testing If adding a feature, please add unit tests for it. If you need a model, please use one of the ones that are cached by GitHub Actions (so that it runs quickly on the CD). These are gpt2 , attn-only-1l , attn-only-2l , attn-only-3l , attn-only-4l , tiny-stories-1M . Note gpt2 is quite slow (as we only have CPU actions) so the smaller models like attn-only-1l and tiny-stories-1M are preferred if possible. Running the tests Unit tests only via make unit-test Acceptance tests only via make acceptance-test Docstring tests only via make docstring-test Formatting This project uses pycln , isort and black for formatting, pull requests are checked in github actions. Format all files via make format Only check the formatting via make check-format Documentation Please make sure to add thorough documentation for any features you add. You should do this directly in the docstring, and this will then automatically generate the API docs when merged into main . They will also be automatically checked with pytest (via doctest ). If you want to view your documentation changes, run pytest run docs-hot-reload . This will give you hot-reloading docs (they change in real time as you edit docstrings). Docstring Style Guide We follow the Google Python Docstring Style for writing docstrings, with some added features from reStructuredText (reST). Sections and Order You should follow this order: \"\"\"Title In Title Case. A description of what the function/class does, including as much detail as is necessary to fully understand it. Warning: Any warnings to the user (e.g. common pitfalls). Examples: Include any examples here. They will be checked with doctest. >>> print(1 + 2) 3 Args: param_without_type_signature: Each description should be indented once more. param_2: Another example parameter. Returns: Returns description without type signature. Raises: Information about the error it may raise (if any). \"\"\" Supported Sphinx Properties References to Other Functions/Classes You can reference other parts of the codebase using cross-referencing (noting that you can omit the full path if it is in the same file). :mod:transformer_lens # Function or module :const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES` :class:`transformer_lens.HookedTransformer` :meth:`transformer_lens.HookedTransformer.from_pretrained` :attr:`transformer_lens.HookedTransformer.cfg` Maths You can use LaTeX, but note that as you're placing this in python strings the backwards slash ( \\ ) must be repeated (i.e. \\\\ ). You can write LaTeX inline, or in \"display mode\". :math:`(a + b)^2 = a^2 + 2ab + b^2` .. math:: :nowrap: \\\\begin{eqnarray} y & = & ax^2 + bx + c \\\\ f(x) & = & x^2 + 2xy + y^2 \\\\end{eqnarray} Markup Italics - *text* Bold - **text** Code - ``code`` List items - *item Numbered items - 1. Item Quotes - indent one level External links = `Link text <https://domain.invalid/>`","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#setup","text":"","title":"Setup"},{"location":"contributing/#devcontainer","text":"For a one-click setup of your development environment, this project includes a DevContainer . It can be used locally with VS Code or with GitHub Codespaces .","title":"DevContainer"},{"location":"contributing/#manual-setup","text":"This project uses Poetry for package management. Install as follows (this will also setup your virtual environment): poetry config virtualenvs.in-project true poetry install --with dev,docs,jupyter","title":"Manual Setup"},{"location":"contributing/#testing","text":"If adding a feature, please add unit tests for it. If you need a model, please use one of the ones that are cached by GitHub Actions (so that it runs quickly on the CD). These are gpt2 , attn-only-1l , attn-only-2l , attn-only-3l , attn-only-4l , tiny-stories-1M . Note gpt2 is quite slow (as we only have CPU actions) so the smaller models like attn-only-1l and tiny-stories-1M are preferred if possible.","title":"Testing"},{"location":"contributing/#running-the-tests","text":"Unit tests only via make unit-test Acceptance tests only via make acceptance-test Docstring tests only via make docstring-test","title":"Running the tests"},{"location":"contributing/#formatting","text":"This project uses pycln , isort and black for formatting, pull requests are checked in github actions. Format all files via make format Only check the formatting via make check-format","title":"Formatting"},{"location":"contributing/#documentation","text":"Please make sure to add thorough documentation for any features you add. You should do this directly in the docstring, and this will then automatically generate the API docs when merged into main . They will also be automatically checked with pytest (via doctest ). If you want to view your documentation changes, run pytest run docs-hot-reload . This will give you hot-reloading docs (they change in real time as you edit docstrings).","title":"Documentation"},{"location":"contributing/#docstring-style-guide","text":"We follow the Google Python Docstring Style for writing docstrings, with some added features from reStructuredText (reST).","title":"Docstring Style Guide"},{"location":"contributing/#sections-and-order","text":"You should follow this order: \"\"\"Title In Title Case. A description of what the function/class does, including as much detail as is necessary to fully understand it. Warning: Any warnings to the user (e.g. common pitfalls). Examples: Include any examples here. They will be checked with doctest. >>> print(1 + 2) 3 Args: param_without_type_signature: Each description should be indented once more. param_2: Another example parameter. Returns: Returns description without type signature. Raises: Information about the error it may raise (if any). \"\"\"","title":"Sections and Order"},{"location":"contributing/#supported-sphinx-properties","text":"","title":"Supported Sphinx Properties"},{"location":"contributing/#references-to-other-functionsclasses","text":"You can reference other parts of the codebase using cross-referencing (noting that you can omit the full path if it is in the same file). :mod:transformer_lens # Function or module :const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES` :class:`transformer_lens.HookedTransformer` :meth:`transformer_lens.HookedTransformer.from_pretrained` :attr:`transformer_lens.HookedTransformer.cfg`","title":"References to Other Functions/Classes"},{"location":"contributing/#maths","text":"You can use LaTeX, but note that as you're placing this in python strings the backwards slash ( \\ ) must be repeated (i.e. \\\\ ). You can write LaTeX inline, or in \"display mode\". :math:`(a + b)^2 = a^2 + 2ab + b^2` .. math:: :nowrap: \\\\begin{eqnarray} y & = & ax^2 + bx + c \\\\ f(x) & = & x^2 + 2xy + y^2 \\\\end{eqnarray}","title":"Maths"},{"location":"contributing/#markup","text":"Italics - *text* Bold - **text** Code - ``code`` List items - *item Numbered items - 1. Item Quotes - indent one level External links = `Link text <https://domain.invalid/>`","title":"Markup"},{"location":"getting_started/","text":"Getting Started","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"reference/","text":"Sparse Autoencoder Library. LossLogType : TypeAlias = dict [ str , int | float | str ] module-attribute Loss log dict. AbstractLoss Bases: Module , ABC Abstract loss interface. Interface for implementing batch itemwise loss functions. Source code in sparse_autoencoder/loss/abstract_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class AbstractLoss ( Module , ABC ): \"\"\"Abstract loss interface. Interface for implementing batch itemwise loss functions. \"\"\" _modules : dict [ str , \"AbstractLoss\" ] # type: ignore[assignment] (narrowing) \"\"\"Children loss modules.\"\"\" @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction ) __call__ ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction ) batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: ItemTensor \u2013 Loss for the batch. sparse_autoencoder/loss/abstract_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics forward ( source_activations , learned_activations , decoded_activations ) abstractmethod Batch itemwise loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/abstract_loss.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError ActivationStore Bases: Dataset [ InputOutputActivationVector ] , ABC Activation Store Abstract Class. Extends the torch.utils.data.Dataset class to provide an activation store, with additional :meth: append and :meth: extend methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a torch.utils.data.DataLoader to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create __getitem__ and __len__ methods from the underlying torch.utils.data.Dataset class). Example: import torch class MyActivationStore(ActivationStore): ... def init (self): ... super(). init () ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def getitem (self, index: int): ... return self._data[index] ... ... def len (self) -> int: ... return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1 Source code in sparse_autoencoder/activation_store/base_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class ActivationStore ( Dataset [ InputOutputActivationVector ], ABC ): \"\"\"Activation Store Abstract Class. Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader` to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class). Example: >>> import torch >>> class MyActivationStore(ActivationStore): ... def __init__(self): ... super().__init__() ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def __getitem__(self, index: int): ... return self._data[index] ... ... def __len__(self) -> int: ... return len(self._data) ... >>> store = MyActivationStore() >>> store.append(torch.randn(100)) >>> print(len(store)) 1 \"\"\" @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\" @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample ) __getitem__ ( index ) abstractmethod Get an Item from the Store. sparse_autoencoder/activation_store/base_store.py 71 72 73 74 @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError __len__ () abstractmethod Get the Length of the Store. sparse_autoencoder/activation_store/base_store.py 66 67 68 69 @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError append ( item ) abstractmethod Add a Single Item to the Store. sparse_autoencoder/activation_store/base_store.py 51 52 53 54 @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError empty () abstractmethod Empty the Store. sparse_autoencoder/activation_store/base_store.py 61 62 63 64 @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError extend ( batch ) abstractmethod Add a Batch to the Store. sparse_autoencoder/activation_store/base_store.py 56 57 58 59 @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError fill_with_test_data ( num_batches = 16 , batch_size = 16 , input_features = 256 ) Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning You may want to use torch.seed(0) to make the random data deterministic, if your test requires inspecting the data itself. Example from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=16*16, num_neurons=256) store.fill_with_test_data() len(store) 256 store[0].shape torch.Size([256]) Parameters: num_batches ( int , default: 16 ) \u2013 Number of batches to fill the store with. batch_size ( int , default: 16 ) \u2013 Number of items per batch. input_features ( int , default: 256 ) \u2013 Number of input features per item. sparse_autoencoder/activation_store/base_store.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample ) shuffle () Optional shuffle method. sparse_autoencoder/activation_store/base_store.py 76 77 def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\" DiskActivationStore Bases: ActivationStore Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set empty_dir to True . Note also that :meth: close must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. Source code in sparse_autoencoder/activation_store/disk_store.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 class DiskActivationStore ( ActivationStore ): \"\"\"Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set `empty_dir` to `True`. Note also that :meth:`close` must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. \"\"\" _storage_path : Path \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\" _cache : ListProxy \"\"\"Cache for Activation Vectors. Activation vectors are buffered in memory until the cache is full, at which point they are written to disk. \"\"\" _cache_lock : Lock \"\"\"Lock for the Cache.\"\"\" _max_cache_size : int \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\" _thread_pool : ThreadPoolExecutor \"\"\"Threadpool for non-blocking writes to the file system.\"\"\" _disk_n_activation_vectors : ValueProxy [ int ] \"\"\"Length of the Store (on disk). Minus 1 signifies not calculated yet. \"\"\" def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers ) def _write_to_disk ( self , * , wait_for_max : bool = False ) -> None : \"\"\"Write the contents of the queue to disk. Args: wait_for_max: Whether to wait until the cache is full before writing to disk. \"\"\" with self . _cache_lock : # Check we have enough items if len ( self . _cache ) == 0 : return size_to_get = min ( self . _max_cache_size , len ( self . _cache )) if wait_for_max and size_to_get < self . _max_cache_size : return # Get the activations from the cache and delete them activations = self . _cache [ 0 : size_to_get ] del self . _cache [ 0 : size_to_get ] # Update the length cache if self . _disk_n_activation_vectors . value != - 1 : self . _disk_n_activation_vectors . value += len ( activations ) stacked_activations = torch . stack ( activations ) filename = f \" { self . __len__ } .pt\" torch . save ( stacked_activations , self . _storage_path / filename ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk () @property def _all_filenames ( self ) -> list [ Path ]: \"\"\"Return a List of All Activation Vector Filenames.\"\"\" return list ( self . _storage_path . glob ( \"*.pt\" )) def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ] def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete () __del__ () Delete Dunder Method. sparse_autoencoder/activation_store/disk_store.py 267 268 269 270 271 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete () __getitem__ ( index ) Get Item Dunder Method. Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/disk_store.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ] __init__ ( storage_path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size = 10000 , num_workers = 6 , * , empty_dir = False ) Initialize the Disk Activation Store. Parameters: storage_path ( Path , default: DEFAULT_DISK_ACTIVATION_STORE_PATH ) \u2013 Path to the directory where the activation vectors will be stored. max_cache_size ( int , default: 10000 ) \u2013 The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers ( int , default: 6 ) \u2013 Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir ( bool , default: False ) \u2013 Whether to empty the directory before writing. Generally you want to set this to True as otherwise the directory may contain stale activation vectors from previous runs. sparse_autoencoder/activation_store/disk_store.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers ) __len__ () Length Dunder Method. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value append ( item ) Add a Single Item to the Store. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 Parameters: item ( InputOutputActivationVector ) \u2013 Activation vector to add to the store. Returns: Future | None \u2013 Future that completes when the activation vector has queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy empty () Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 store.empty() print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0 extend ( batch ) Add a Batch to the Store. Example: store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10 Parameters: batch ( SourceModelActivations ) \u2013 Batch of activation vectors to add to the store. Returns: Future | None \u2013 Future that completes when the activation vectors have queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy wait_for_writes_to_complete () Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1 sparse_autoencoder/activation_store/disk_store.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk () LearnedActivationsL1Loss Bases: AbstractLoss Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations) Returns loss and metrics to log l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) Source code in sparse_autoencoder/loss/learned_activations_l1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @final class LearnedActivationsL1Loss ( AbstractLoss ): \"\"\"Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example: >>> l1_loss = LearnedActivationsL1Loss(0.1) >>> learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) >>> unused_activations = torch.zeros_like(learned_activations) >>> # Returns loss and metrics to log >>> l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) \"\"\" l1_coefficient : float \"\"\"L1 coefficient.\"\"\" def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \" l1_coefficient : float = l1_coefficient instance-attribute L1 coefficient. __init__ ( l1_coefficient ) Initialize the absolute error loss. Parameters: l1_coefficient ( float ) \u2013 L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. sparse_autoencoder/loss/learned_activations_l1.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () extra_repr () Extra representation string. sparse_autoencoder/loss/learned_activations_l1.py 66 67 68 def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \" forward ( source_activations , learned_activations , decoded_activations ) Learned activations L1 (absolute error) loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/learned_activations_l1.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient ListActivationStore Bases: ActivationStore List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the multiprocessing_enabled argument is set to True . This works in two ways: The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. The extend method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Note that the built-in :meth: shuffle method is much faster than using the shuffle argument on torch.utils.data.DataLoader . You should therefore call this method before passing the dataset to the loader and then set the DataLoader shuffle argument to False . Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/list_store.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class ListActivationStore ( ActivationStore ): \"\"\"List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two ways: 1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. 2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument on `torch.utils.data.DataLoader`. You should therefore call this method before passing the dataset to the loader and then set the DataLoader `shuffle` argument to `False`. Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : list [ InputOutputActivationVector ] | ListProxy \"\"\"Underlying List Data Store.\"\"\" _device : torch . device | None \"\"\"Device to Store the Activation Vectors On.\"\"\" _pool : ProcessPoolExecutor | None = None \"\"\"Multiprocessing Pool.\"\"\" _pool_exceptions : ListProxy | list [ Exception ] \"\"\"Pool Exceptions. Used to keep track of exceptions. \"\"\" _pool_futures : list [ Future ] \"\"\"Pool Futures. Used to keep track of processes running in the pool. \"\"\" def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data ) def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device )) def _extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Extend threadpool method. To be called by :meth:`extend`. Args: batch: A batch of items to add to the dataset. \"\"\" try : # Unstack to a list of tensors items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) self . _data . extend ( items ) except Exception as e : # noqa: BLE001 self . _pool_exceptions . append ( e ) def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch ) def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg ) def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = [] def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True ) __del__ () Delete Dunder Method. sparse_autoencoder/activation_store/list_store.py 312 313 314 315 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True ) __getitem__ ( index ) Get Item Dunder Method. Example: import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/list_store.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ] __init__ ( data = None , device = None , max_workers = None , * , multiprocessing_enabled = False ) Initialize the List Activation Store. Parameters: data ( list [ InputOutputActivationVector ] | None , default: None ) \u2013 Data to initialize the dataset with. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. max_workers ( int | None , default: None ) \u2013 Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled ( bool , default: False ) \u2013 Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). sparse_autoencoder/activation_store/list_store.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device __len__ () Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/list_store.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data ) __sizeof__ () Sizeof Dunder Method. Returns the size of the dataset in bytes. sparse_autoencoder/activation_store/list_store.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size append ( item ) Append a single item to the dataset. Note append is blocking . For better performance use extend instead with batches. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. sparse_autoencoder/activation_store/list_store.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device )) empty () Empty the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 store.empty() len(store) 0 sparse_autoencoder/activation_store/list_store.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = [] extend ( batch ) Extend the dataset with multiple items (non-blocking). Example import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10 Parameters: batch ( SourceModelActivations ) \u2013 A batch of items to add to the dataset. sparse_autoencoder/activation_store/list_store.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch ) shuffle () Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3 sparse_autoencoder/activation_store/list_store.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data ) wait_for_writes_to_complete () Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth: append ) to complete. Example: import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3 sparse_autoencoder/activation_store/list_store.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg ) LossReducer Bases: AbstractLoss Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) Source code in sparse_autoencoder/loss/reducer.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @final class LossReducer ( AbstractLoss ): \"\"\"Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example: >>> from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss >>> from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss >>> LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) \"\"\" _modules : dict [ str , \"AbstractLoss\" ] \"\"\"Children loss modules.\"\"\" def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules ) __dir__ () Dir dunder method. sparse_autoencoder/loss/reducer.py 88 89 90 def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) __getitem__ ( idx ) Get item dunder method. sparse_autoencoder/loss/reducer.py 92 93 94 def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] __init__ ( * loss_modules ) Initialize the loss reducer. Parameters: loss_modules ( AbstractLoss , default: () ) \u2013 Loss modules to reduce. Raises: ValueError \u2013 If the loss reducer has no loss modules. sparse_autoencoder/loss/reducer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) __iter__ () Iterator dunder method. sparse_autoencoder/loss/reducer.py 96 97 98 def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) __len__ () Length dunder method. sparse_autoencoder/loss/reducer.py 100 101 102 def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules ) forward ( source_activations , learned_activations , decoded_activations ) Reduce loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: ItemTensor \u2013 Mean loss across the batch, summed across the loss modules. sparse_autoencoder/loss/reducer.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) LossReductionType Bases: LowercaseStrEnum Loss reduction type (across batch items). Source code in sparse_autoencoder/loss/abstract_loss.py 17 18 19 20 21 22 23 24 class LossReductionType ( LowercaseStrEnum ): \"\"\"Loss reduction type (across batch items).\"\"\" MEAN = \"mean\" \"\"\"Mean loss across batch items.\"\"\" SUM = \"sum\" \"\"\"Sum the loss from all batch items.\"\"\" MEAN = 'mean' class-attribute instance-attribute Mean loss across batch items. SUM = 'sum' class-attribute instance-attribute Sum the loss from all batch items. MSEReconstructionLoss Bases: AbstractLoss MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example import torch loss = MSEReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations) Outputs both loss and metrics to log loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) Source code in sparse_autoencoder/loss/mse_reconstruction_loss.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @final class MSEReconstructionLoss ( AbstractLoss ): \"\"\"MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example: >>> import torch >>> loss = MSEReconstructionLoss() >>> input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) >>> output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) >>> unused_activations = torch.zeros_like(input_activations) >>> # Outputs both loss and metrics to log >>> loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) \"\"\" def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 ) forward ( source_activations , learned_activations , decoded_activations ) MSE Reconstruction loss (mean across features dimension). Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/mse_reconstruction_loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 ) SparseAutoencoder Bases: Module Sparse Autoencoder Model. Source code in sparse_autoencoder/autoencoder/model.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class SparseAutoencoder ( Module ): \"\"\"Sparse Autoencoder Model.\"\"\" geometric_median_dataset : InputOutputActivationVector \"\"\"Estimated Geometric Median of the Dataset. Used for initialising :attr:`tied_bias`. \"\"\" tied_bias : InputOutputActivationBatch \"\"\"Tied Bias Parameter. The same bias is used pre-encoder and post-decoder. \"\"\" n_input_features : int \"\"\"Number of Input Features.\"\"\" n_learned_features : int \"\"\"Number of Learned Features.\"\"\" device : torch . device | None \"\"\"Device to run the model on.\"\"\" dtype : torch . dtype | None \"\"\"Data type to use for the model.\"\"\" encoder : Sequential \"\"\"Encoder Module.\"\"\" decoder : Sequential \"\"\"Decoder Module.\"\"\" def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) ) def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype ) def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters () def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError decoder : Sequential = Sequential ( OrderedDict ({ 'ConstrainedUnitNormLinear' : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), 'TiedBias' : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER )})) instance-attribute Decoder Module. device : torch . device | None = device instance-attribute Device to run the model on. dtype : torch . dtype | None = dtype instance-attribute Data type to use for the model. encoder : Sequential = Sequential ( OrderedDict ({ 'TiedBias' : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), 'Linear' : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), 'ReLU' : ReLU ()})) instance-attribute Encoder Module. geometric_median_dataset : InputOutputActivationVector = geometric_median_dataset . clone () instance-attribute Estimated Geometric Median of the Dataset. Used for initialising :attr: tied_bias . n_input_features : int = n_input_features instance-attribute Number of Input Features. n_learned_features : int = n_learned_features instance-attribute Number of Learned Features. tied_bias : InputOutputActivationBatch = Parameter ( torch . empty ( n_input_features , device = device , dtype = dtype )) instance-attribute Tied Bias Parameter. The same bias is used pre-encoder and post-decoder. __init__ ( n_input_features , n_learned_features , geometric_median_dataset , device = None , dtype = None ) Initialize the Sparse Autoencoder Model. Parameters: n_input_features ( int ) \u2013 Number of input features (e.g. d_mlp if training on MLP activations from TransformerLens). n_learned_features ( int ) \u2013 Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset ( InputOutputActivationVector ) \u2013 Estimated geometric median of the dataset. device ( device | None , default: None ) \u2013 Device to run the model on. dtype ( dtype | None , default: None ) \u2013 Data type to use for the model. sparse_autoencoder/autoencoder/model.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) ) forward ( x ) Forward Pass. Parameters: x ( InputOutputActivationBatch ) \u2013 Input activations (e.g. activations from an MLP layer in a transformer model). Returns: tuple [ LearnedActivationBatch , InputOutputActivationBatch ] \u2013 Tuple of learned activations and decoded activations. sparse_autoencoder/autoencoder/model.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations initialize_tied_parameters () Initialize the tied parameters. sparse_autoencoder/autoencoder/model.py 127 128 129 130 131 132 def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype ) load_from_hf () Load the model from Hugging Face. sparse_autoencoder/autoencoder/model.py 145 146 147 def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError reset_parameters () Reset the parameters. sparse_autoencoder/autoencoder/model.py 134 135 136 137 138 139 def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters () save_to_hf () Save the model to Hugging Face. sparse_autoencoder/autoencoder/model.py 141 142 143 def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError TensorActivationStore Bases: ActivationStore Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/tensor_store.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class TensorActivationStore ( ActivationStore ): \"\"\"Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : StoreActivations \"\"\"Underlying Tensor Data Store.\"\"\" items_stored : int = 0 \"\"\"Number of items stored.\"\"\" max_items : int \"\"\"Maximum Number of Items to Store.\"\"\" def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement () def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ] def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0 items_stored : int = 0 class-attribute instance-attribute Number of items stored. max_items : int instance-attribute Maximum Number of Items to Store. __getitem__ ( index ) Get Item Dunder Method. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. Raises: IndexError \u2013 If the index is out of range. sparse_autoencoder/activation_store/tensor_store.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ] __init__ ( max_items , num_neurons , device = None ) Initialise the Tensor Activation Store. Parameters: max_items ( int ) \u2013 Maximum number of items to store (individual activation vectors) num_neurons ( int ) \u2013 Number of neurons in each activation vector. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. sparse_autoencoder/activation_store/tensor_store.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items __len__ () Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/tensor_store.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored __sizeof__ () Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=100) store. sizeof () # Pre-allocated tensor of 2x100 800 sparse_autoencoder/activation_store/tensor_store.py 100 101 102 103 104 105 106 107 108 109 110 111 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement () append ( item ) Add a single item to the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1 empty () Empty the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0 sparse_autoencoder/activation_store/tensor_store.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0 extend ( batch ) Add a batch to the store. Examples: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9 Parameters: batch ( SourceModelActivations ) \u2013 The batch to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors shuffle () Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] sparse_autoencoder/activation_store/tensor_store.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ] pipeline ( src_model , src_model_activation_hook_point , src_model_activation_layer , source_dataset , activation_store , num_activations_before_training , autoencoder , source_dataset_batch_size = 16 , resample_frequency = 25000000 , sweep_parameters = SweepParametersRuntime (), device = None , max_activations = 100000000 ) Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Parameters: src_model ( HookedTransformer ) \u2013 The model to get activations from. src_model_activation_hook_point ( str ) \u2013 The hook point to get activations from. src_model_activation_layer ( int ) \u2013 The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset ( SourceDataset ) \u2013 Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store ( ActivationStore ) \u2013 The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training ( int ) \u2013 The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder ( SparseAutoencoder ) \u2013 The autoencoder to train. source_dataset_batch_size ( int , default: 16 ) \u2013 Batch size of tokenized prompts for generating the source data. resample_frequency ( int , default: 25000000 ) \u2013 How often to resample neurons (number of activations learnt on). sweep_parameters ( SweepParametersRuntime , default: SweepParametersRuntime () ) \u2013 Parameter config to use. device ( device | None , default: None ) \u2013 Device to run pipeline on. max_activations ( int , default: 100000000 ) \u2013 Maximum number of activations to train with. May train for less if the source dataset is exhausted. sparse_autoencoder/train/pipeline.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def pipeline ( # noqa: PLR0913 src_model : HookedTransformer , src_model_activation_hook_point : str , src_model_activation_layer : int , source_dataset : SourceDataset , activation_store : ActivationStore , num_activations_before_training : int , autoencoder : SparseAutoencoder , source_dataset_batch_size : int = 16 , resample_frequency : int = 25_000_000 , sweep_parameters : SweepParametersRuntime = SweepParametersRuntime (), # noqa: B008 device : torch . device | None = None , max_activations : int = 100_000_000 , ) -> None : \"\"\"Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Args: src_model: The model to get activations from. src_model_activation_hook_point: The hook point to get activations from. src_model_activation_layer: The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset: Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store: The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training: The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder: The autoencoder to train. source_dataset_batch_size: Batch size of tokenized prompts for generating the source data. resample_frequency: How often to resample neurons (number of activations learnt on). sweep_parameters: Parameter config to use. device: Device to run pipeline on. max_activations: Maximum number of activations to train with. May train for less if the source dataset is exhausted. \"\"\" autoencoder . to ( device ) optimizer = AdamWithReset ( autoencoder . parameters (), lr = sweep_parameters . lr , betas = ( sweep_parameters . adam_beta_1 , sweep_parameters . adam_beta_2 ), eps = sweep_parameters . adam_epsilon , weight_decay = sweep_parameters . adam_weight_decay , named_parameters = autoencoder . named_parameters (), ) source_dataloader = source_dataset . get_dataloader ( source_dataset_batch_size ) source_data_iterator = stateful_dataloader_iterable ( source_dataloader ) total_steps : int = 0 activations_since_resampling : int = 0 neuron_activity : NeuronActivity = torch . zeros ( autoencoder . n_learned_features , dtype = torch . int32 , device = device ) total_activations : int = 0 # Run loop until source data is exhausted: with logging_redirect_tqdm (), tqdm ( desc = \"Total activations trained on\" , dynamic_ncols = True , total = max_activations , postfix = { \"Current mode\" : \"initializing\" }, ) as progress_bar : while total_activations < max_activations : # Add activations to the store activation_store . empty () # In case it was filled by a different run progress_bar . set_postfix ({ \"Current mode\" : \"generating\" }) generate_activations ( src_model , src_model_activation_layer , src_model_activation_hook_point , activation_store , source_data_iterator , device = device , context_size = source_dataset . context_size , num_items = num_activations_before_training , batch_size = source_dataset_batch_size , ) if len ( activation_store ) == 0 : break # Shuffle the store if it has a shuffle method - it is often more efficient to # create a shuffle method ourselves rather than get the DataLoader to shuffle activation_store . shuffle () # Train the autoencoder progress_bar . set_postfix ({ \"Current mode\" : \"training\" }) train_steps , learned_activations_fired_count = train_autoencoder ( activation_store = activation_store , autoencoder = autoencoder , optimizer = optimizer , sweep_parameters = sweep_parameters , device = device , previous_steps = total_steps , ) total_steps += train_steps if activations_since_resampling >= resample_frequency / 2 : neuron_activity . add_ ( learned_activations_fired_count ) activations_since_resampling += len ( activation_store ) total_activations += len ( activation_store ) progress_bar . update ( len ( activation_store )) # Resample neurons if required if len ( activation_store ) < DEFAULT_RESAMPLE_N : warn_str = ( f \"Warning: activation store len { len ( activation_store ) } is less than \" f \"DEFAULT_RESAMPLE_N ( { DEFAULT_RESAMPLE_N } ). Resampling with\" f \"num_resample_inputs as { len ( activation_store ) } .\" ) warnings . warn ( warn_str , stacklevel = 2 , ) num_resample_inputs = len ( activation_store ) else : num_resample_inputs = DEFAULT_RESAMPLE_N if activations_since_resampling >= resample_frequency : progress_bar . set_postfix ({ \"Current mode\" : \"resampling\" }) activations_since_resampling = 0 resample_dead_neurons ( neuron_activity = neuron_activity , store = activation_store , autoencoder = autoencoder , sweep_parameters = sweep_parameters , num_inputs = num_resample_inputs , ) learned_activations_fired_count . zero_ () optimizer . reset_state_all_parameters () activation_store . empty ()","title":"Home"},{"location":"reference/#sparse_autoencoder.LossLogType","text":"Loss log dict.","title":"LossLogType"},{"location":"reference/#sparse_autoencoder.AbstractLoss","text":"Bases: Module , ABC Abstract loss interface. Interface for implementing batch itemwise loss functions. Source code in sparse_autoencoder/loss/abstract_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class AbstractLoss ( Module , ABC ): \"\"\"Abstract loss interface. Interface for implementing batch itemwise loss functions. \"\"\" _modules : dict [ str , \"AbstractLoss\" ] # type: ignore[assignment] (narrowing) \"\"\"Children loss modules.\"\"\" @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction )","title":"AbstractLoss"},{"location":"reference/#sparse_autoencoder.AbstractLoss.__call__","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction )","title":"__call__()"},{"location":"reference/#sparse_autoencoder.AbstractLoss.batch_scalar_loss","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: ItemTensor \u2013 Loss for the batch. sparse_autoencoder/loss/abstract_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze ()","title":"batch_scalar_loss()"},{"location":"reference/#sparse_autoencoder.AbstractLoss.batch_scalar_loss_with_log","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics","title":"batch_scalar_loss_with_log()"},{"location":"reference/#sparse_autoencoder.AbstractLoss.forward","text":"Batch itemwise loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/abstract_loss.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"reference/#sparse_autoencoder.ActivationStore","text":"Bases: Dataset [ InputOutputActivationVector ] , ABC Activation Store Abstract Class. Extends the torch.utils.data.Dataset class to provide an activation store, with additional :meth: append and :meth: extend methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a torch.utils.data.DataLoader to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create __getitem__ and __len__ methods from the underlying torch.utils.data.Dataset class). Example: import torch class MyActivationStore(ActivationStore): ... def init (self): ... super(). init () ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def getitem (self, index: int): ... return self._data[index] ... ... def len (self) -> int: ... return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1 Source code in sparse_autoencoder/activation_store/base_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class ActivationStore ( Dataset [ InputOutputActivationVector ], ABC ): \"\"\"Activation Store Abstract Class. Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader` to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class). Example: >>> import torch >>> class MyActivationStore(ActivationStore): ... def __init__(self): ... super().__init__() ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def __getitem__(self, index: int): ... return self._data[index] ... ... def __len__(self) -> int: ... return len(self._data) ... >>> store = MyActivationStore() >>> store.append(torch.randn(100)) >>> print(len(store)) 1 \"\"\" @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\" @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample )","title":"ActivationStore"},{"location":"reference/#sparse_autoencoder.ActivationStore.__getitem__","text":"Get an Item from the Store. sparse_autoencoder/activation_store/base_store.py 71 72 73 74 @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError","title":"__getitem__()"},{"location":"reference/#sparse_autoencoder.ActivationStore.__len__","text":"Get the Length of the Store. sparse_autoencoder/activation_store/base_store.py 66 67 68 69 @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError","title":"__len__()"},{"location":"reference/#sparse_autoencoder.ActivationStore.append","text":"Add a Single Item to the Store. sparse_autoencoder/activation_store/base_store.py 51 52 53 54 @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError","title":"append()"},{"location":"reference/#sparse_autoencoder.ActivationStore.empty","text":"Empty the Store. sparse_autoencoder/activation_store/base_store.py 61 62 63 64 @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError","title":"empty()"},{"location":"reference/#sparse_autoencoder.ActivationStore.extend","text":"Add a Batch to the Store. sparse_autoencoder/activation_store/base_store.py 56 57 58 59 @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError","title":"extend()"},{"location":"reference/#sparse_autoencoder.ActivationStore.fill_with_test_data","text":"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning You may want to use torch.seed(0) to make the random data deterministic, if your test requires inspecting the data itself. Example from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=16*16, num_neurons=256) store.fill_with_test_data() len(store) 256 store[0].shape torch.Size([256]) Parameters: num_batches ( int , default: 16 ) \u2013 Number of batches to fill the store with. batch_size ( int , default: 16 ) \u2013 Number of items per batch. input_features ( int , default: 256 ) \u2013 Number of input features per item. sparse_autoencoder/activation_store/base_store.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample )","title":"fill_with_test_data()"},{"location":"reference/#sparse_autoencoder.ActivationStore.shuffle","text":"Optional shuffle method. sparse_autoencoder/activation_store/base_store.py 76 77 def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\"","title":"shuffle()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore","text":"Bases: ActivationStore Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set empty_dir to True . Note also that :meth: close must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. Source code in sparse_autoencoder/activation_store/disk_store.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 class DiskActivationStore ( ActivationStore ): \"\"\"Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set `empty_dir` to `True`. Note also that :meth:`close` must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. \"\"\" _storage_path : Path \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\" _cache : ListProxy \"\"\"Cache for Activation Vectors. Activation vectors are buffered in memory until the cache is full, at which point they are written to disk. \"\"\" _cache_lock : Lock \"\"\"Lock for the Cache.\"\"\" _max_cache_size : int \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\" _thread_pool : ThreadPoolExecutor \"\"\"Threadpool for non-blocking writes to the file system.\"\"\" _disk_n_activation_vectors : ValueProxy [ int ] \"\"\"Length of the Store (on disk). Minus 1 signifies not calculated yet. \"\"\" def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers ) def _write_to_disk ( self , * , wait_for_max : bool = False ) -> None : \"\"\"Write the contents of the queue to disk. Args: wait_for_max: Whether to wait until the cache is full before writing to disk. \"\"\" with self . _cache_lock : # Check we have enough items if len ( self . _cache ) == 0 : return size_to_get = min ( self . _max_cache_size , len ( self . _cache )) if wait_for_max and size_to_get < self . _max_cache_size : return # Get the activations from the cache and delete them activations = self . _cache [ 0 : size_to_get ] del self . _cache [ 0 : size_to_get ] # Update the length cache if self . _disk_n_activation_vectors . value != - 1 : self . _disk_n_activation_vectors . value += len ( activations ) stacked_activations = torch . stack ( activations ) filename = f \" { self . __len__ } .pt\" torch . save ( stacked_activations , self . _storage_path / filename ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk () @property def _all_filenames ( self ) -> list [ Path ]: \"\"\"Return a List of All Activation Vector Filenames.\"\"\" return list ( self . _storage_path . glob ( \"*.pt\" )) def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ] def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete ()","title":"DiskActivationStore"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__del__","text":"Delete Dunder Method. sparse_autoencoder/activation_store/disk_store.py 267 268 269 270 271 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete ()","title":"__del__()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__getitem__","text":"Get Item Dunder Method. Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/disk_store.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ]","title":"__getitem__()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__init__","text":"Initialize the Disk Activation Store. Parameters: storage_path ( Path , default: DEFAULT_DISK_ACTIVATION_STORE_PATH ) \u2013 Path to the directory where the activation vectors will be stored. max_cache_size ( int , default: 10000 ) \u2013 The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers ( int , default: 6 ) \u2013 Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir ( bool , default: False ) \u2013 Whether to empty the directory before writing. Generally you want to set this to True as otherwise the directory may contain stale activation vectors from previous runs. sparse_autoencoder/activation_store/disk_store.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers )","title":"__init__()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__len__","text":"Length Dunder Method. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value","title":"__len__()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.append","text":"Add a Single Item to the Store. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 Parameters: item ( InputOutputActivationVector ) \u2013 Activation vector to add to the store. Returns: Future | None \u2013 Future that completes when the activation vector has queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy","title":"append()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.empty","text":"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 store.empty() print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0","title":"empty()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.extend","text":"Add a Batch to the Store. Example: store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10 Parameters: batch ( SourceModelActivations ) \u2013 Batch of activation vectors to add to the store. Returns: Future | None \u2013 Future that completes when the activation vectors have queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy","title":"extend()"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete","text":"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1 sparse_autoencoder/activation_store/disk_store.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk ()","title":"wait_for_writes_to_complete()"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss","text":"Bases: AbstractLoss Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)","title":"LearnedActivationsL1Loss"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","text":"l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) Source code in sparse_autoencoder/loss/learned_activations_l1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @final class LearnedActivationsL1Loss ( AbstractLoss ): \"\"\"Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example: >>> l1_loss = LearnedActivationsL1Loss(0.1) >>> learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) >>> unused_activations = torch.zeros_like(learned_activations) >>> # Returns loss and metrics to log >>> l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) \"\"\" l1_coefficient : float \"\"\"L1 coefficient.\"\"\" def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \"","title":"Returns loss and metrics to log"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient","text":"L1 coefficient.","title":"l1_coefficient"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.__init__","text":"Initialize the absolute error loss. Parameters: l1_coefficient ( float ) \u2013 L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. sparse_autoencoder/loss/learned_activations_l1.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ ()","title":"__init__()"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr","text":"Extra representation string. sparse_autoencoder/loss/learned_activations_l1.py 66 67 68 def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \"","title":"extra_repr()"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.forward","text":"Learned activations L1 (absolute error) loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/learned_activations_l1.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient","title":"forward()"},{"location":"reference/#sparse_autoencoder.ListActivationStore","text":"Bases: ActivationStore List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the multiprocessing_enabled argument is set to True . This works in two ways: The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. The extend method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Note that the built-in :meth: shuffle method is much faster than using the shuffle argument on torch.utils.data.DataLoader . You should therefore call this method before passing the dataset to the loader and then set the DataLoader shuffle argument to False . Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/list_store.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class ListActivationStore ( ActivationStore ): \"\"\"List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two ways: 1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. 2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument on `torch.utils.data.DataLoader`. You should therefore call this method before passing the dataset to the loader and then set the DataLoader `shuffle` argument to `False`. Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : list [ InputOutputActivationVector ] | ListProxy \"\"\"Underlying List Data Store.\"\"\" _device : torch . device | None \"\"\"Device to Store the Activation Vectors On.\"\"\" _pool : ProcessPoolExecutor | None = None \"\"\"Multiprocessing Pool.\"\"\" _pool_exceptions : ListProxy | list [ Exception ] \"\"\"Pool Exceptions. Used to keep track of exceptions. \"\"\" _pool_futures : list [ Future ] \"\"\"Pool Futures. Used to keep track of processes running in the pool. \"\"\" def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data ) def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device )) def _extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Extend threadpool method. To be called by :meth:`extend`. Args: batch: A batch of items to add to the dataset. \"\"\" try : # Unstack to a list of tensors items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) self . _data . extend ( items ) except Exception as e : # noqa: BLE001 self . _pool_exceptions . append ( e ) def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch ) def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg ) def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = [] def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True )","title":"ListActivationStore"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__del__","text":"Delete Dunder Method. sparse_autoencoder/activation_store/list_store.py 312 313 314 315 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True )","title":"__del__()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__getitem__","text":"Get Item Dunder Method. Example: import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/list_store.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ]","title":"__getitem__()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__init__","text":"Initialize the List Activation Store. Parameters: data ( list [ InputOutputActivationVector ] | None , default: None ) \u2013 Data to initialize the dataset with. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. max_workers ( int | None , default: None ) \u2013 Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled ( bool , default: False ) \u2013 Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). sparse_autoencoder/activation_store/list_store.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device","title":"__init__()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__len__","text":"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/list_store.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data )","title":"__len__()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__sizeof__","text":"Sizeof Dunder Method. Returns the size of the dataset in bytes. sparse_autoencoder/activation_store/list_store.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size","title":"__sizeof__()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.append","text":"Append a single item to the dataset. Note append is blocking . For better performance use extend instead with batches. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. sparse_autoencoder/activation_store/list_store.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device ))","title":"append()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.empty","text":"Empty the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 store.empty() len(store) 0 sparse_autoencoder/activation_store/list_store.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = []","title":"empty()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.extend","text":"Extend the dataset with multiple items (non-blocking). Example import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10 Parameters: batch ( SourceModelActivations ) \u2013 A batch of items to add to the dataset. sparse_autoencoder/activation_store/list_store.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch )","title":"extend()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.shuffle","text":"Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3 sparse_autoencoder/activation_store/list_store.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data )","title":"shuffle()"},{"location":"reference/#sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete","text":"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth: append ) to complete. Example: import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3 sparse_autoencoder/activation_store/list_store.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg )","title":"wait_for_writes_to_complete()"},{"location":"reference/#sparse_autoencoder.LossReducer","text":"Bases: AbstractLoss Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) Source code in sparse_autoencoder/loss/reducer.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @final class LossReducer ( AbstractLoss ): \"\"\"Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example: >>> from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss >>> from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss >>> LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) \"\"\" _modules : dict [ str , \"AbstractLoss\" ] \"\"\"Children loss modules.\"\"\" def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules )","title":"LossReducer"},{"location":"reference/#sparse_autoencoder.LossReducer.__dir__","text":"Dir dunder method. sparse_autoencoder/loss/reducer.py 88 89 90 def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ())","title":"__dir__()"},{"location":"reference/#sparse_autoencoder.LossReducer.__getitem__","text":"Get item dunder method. sparse_autoencoder/loss/reducer.py 92 93 94 def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )]","title":"__getitem__()"},{"location":"reference/#sparse_autoencoder.LossReducer.__init__","text":"Initialize the loss reducer. Parameters: loss_modules ( AbstractLoss , default: () ) \u2013 Loss modules to reduce. Raises: ValueError \u2013 If the loss reducer has no loss modules. sparse_autoencoder/loss/reducer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message )","title":"__init__()"},{"location":"reference/#sparse_autoencoder.LossReducer.__iter__","text":"Iterator dunder method. sparse_autoencoder/loss/reducer.py 96 97 98 def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ())","title":"__iter__()"},{"location":"reference/#sparse_autoencoder.LossReducer.__len__","text":"Length dunder method. sparse_autoencoder/loss/reducer.py 100 101 102 def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules )","title":"__len__()"},{"location":"reference/#sparse_autoencoder.LossReducer.forward","text":"Reduce loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: ItemTensor \u2013 Mean loss across the batch, summed across the loss modules. sparse_autoencoder/loss/reducer.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 )","title":"forward()"},{"location":"reference/#sparse_autoencoder.LossReductionType","text":"Bases: LowercaseStrEnum Loss reduction type (across batch items). Source code in sparse_autoencoder/loss/abstract_loss.py 17 18 19 20 21 22 23 24 class LossReductionType ( LowercaseStrEnum ): \"\"\"Loss reduction type (across batch items).\"\"\" MEAN = \"mean\" \"\"\"Mean loss across batch items.\"\"\" SUM = \"sum\" \"\"\"Sum the loss from all batch items.\"\"\"","title":"LossReductionType"},{"location":"reference/#sparse_autoencoder.LossReductionType.MEAN","text":"Mean loss across batch items.","title":"MEAN"},{"location":"reference/#sparse_autoencoder.LossReductionType.SUM","text":"Sum the loss from all batch items.","title":"SUM"},{"location":"reference/#sparse_autoencoder.MSEReconstructionLoss","text":"Bases: AbstractLoss MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example import torch loss = MSEReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)","title":"MSEReconstructionLoss"},{"location":"reference/#sparse_autoencoder.MSEReconstructionLoss--outputs-both-loss-and-metrics-to-log","text":"loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) Source code in sparse_autoencoder/loss/mse_reconstruction_loss.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @final class MSEReconstructionLoss ( AbstractLoss ): \"\"\"MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example: >>> import torch >>> loss = MSEReconstructionLoss() >>> input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) >>> output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) >>> unused_activations = torch.zeros_like(input_activations) >>> # Outputs both loss and metrics to log >>> loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) \"\"\" def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"Outputs both loss and metrics to log"},{"location":"reference/#sparse_autoencoder.MSEReconstructionLoss.forward","text":"MSE Reconstruction loss (mean across features dimension). Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/mse_reconstruction_loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"forward()"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder","text":"Bases: Module Sparse Autoencoder Model. Source code in sparse_autoencoder/autoencoder/model.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class SparseAutoencoder ( Module ): \"\"\"Sparse Autoencoder Model.\"\"\" geometric_median_dataset : InputOutputActivationVector \"\"\"Estimated Geometric Median of the Dataset. Used for initialising :attr:`tied_bias`. \"\"\" tied_bias : InputOutputActivationBatch \"\"\"Tied Bias Parameter. The same bias is used pre-encoder and post-decoder. \"\"\" n_input_features : int \"\"\"Number of Input Features.\"\"\" n_learned_features : int \"\"\"Number of Learned Features.\"\"\" device : torch . device | None \"\"\"Device to run the model on.\"\"\" dtype : torch . dtype | None \"\"\"Data type to use for the model.\"\"\" encoder : Sequential \"\"\"Encoder Module.\"\"\" decoder : Sequential \"\"\"Decoder Module.\"\"\" def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) ) def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype ) def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters () def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError","title":"SparseAutoencoder"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.decoder","text":"Decoder Module.","title":"decoder"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.device","text":"Device to run the model on.","title":"device"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.dtype","text":"Data type to use for the model.","title":"dtype"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.encoder","text":"Encoder Module.","title":"encoder"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset","text":"Estimated Geometric Median of the Dataset. Used for initialising :attr: tied_bias .","title":"geometric_median_dataset"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_input_features","text":"Number of Input Features.","title":"n_input_features"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_learned_features","text":"Number of Learned Features.","title":"n_learned_features"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.tied_bias","text":"Tied Bias Parameter. The same bias is used pre-encoder and post-decoder.","title":"tied_bias"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.__init__","text":"Initialize the Sparse Autoencoder Model. Parameters: n_input_features ( int ) \u2013 Number of input features (e.g. d_mlp if training on MLP activations from TransformerLens). n_learned_features ( int ) \u2013 Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset ( InputOutputActivationVector ) \u2013 Estimated geometric median of the dataset. device ( device | None , default: None ) \u2013 Device to run the model on. dtype ( dtype | None , default: None ) \u2013 Data type to use for the model. sparse_autoencoder/autoencoder/model.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) )","title":"__init__()"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.forward","text":"Forward Pass. Parameters: x ( InputOutputActivationBatch ) \u2013 Input activations (e.g. activations from an MLP layer in a transformer model). Returns: tuple [ LearnedActivationBatch , InputOutputActivationBatch ] \u2013 Tuple of learned activations and decoded activations. sparse_autoencoder/autoencoder/model.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations","title":"forward()"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters","text":"Initialize the tied parameters. sparse_autoencoder/autoencoder/model.py 127 128 129 130 131 132 def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype )","title":"initialize_tied_parameters()"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.load_from_hf","text":"Load the model from Hugging Face. sparse_autoencoder/autoencoder/model.py 145 146 147 def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError","title":"load_from_hf()"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.reset_parameters","text":"Reset the parameters. sparse_autoencoder/autoencoder/model.py 134 135 136 137 138 139 def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters ()","title":"reset_parameters()"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.save_to_hf","text":"Save the model to Hugging Face. sparse_autoencoder/autoencoder/model.py 141 142 143 def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError","title":"save_to_hf()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore","text":"Bases: ActivationStore Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/tensor_store.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class TensorActivationStore ( ActivationStore ): \"\"\"Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : StoreActivations \"\"\"Underlying Tensor Data Store.\"\"\" items_stored : int = 0 \"\"\"Number of items stored.\"\"\" max_items : int \"\"\"Maximum Number of Items to Store.\"\"\" def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement () def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ] def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0","title":"TensorActivationStore"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.items_stored","text":"Number of items stored.","title":"items_stored"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.max_items","text":"Maximum Number of Items to Store.","title":"max_items"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__getitem__","text":"Get Item Dunder Method. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. Raises: IndexError \u2013 If the index is out of range. sparse_autoencoder/activation_store/tensor_store.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ]","title":"__getitem__()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__init__","text":"Initialise the Tensor Activation Store. Parameters: max_items ( int ) \u2013 Maximum number of items to store (individual activation vectors) num_neurons ( int ) \u2013 Number of neurons in each activation vector. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. sparse_autoencoder/activation_store/tensor_store.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items","title":"__init__()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__len__","text":"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/tensor_store.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored","title":"__len__()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__sizeof__","text":"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=100) store. sizeof () # Pre-allocated tensor of 2x100 800 sparse_autoencoder/activation_store/tensor_store.py 100 101 102 103 104 105 106 107 108 109 110 111 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement ()","title":"__sizeof__()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.append","text":"Add a single item to the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1","title":"append()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.empty","text":"Empty the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0 sparse_autoencoder/activation_store/tensor_store.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0","title":"empty()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.extend","text":"Add a batch to the store. Examples: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9 Parameters: batch ( SourceModelActivations ) \u2013 The batch to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors","title":"extend()"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.shuffle","text":"Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] sparse_autoencoder/activation_store/tensor_store.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ]","title":"shuffle()"},{"location":"reference/#sparse_autoencoder.pipeline","text":"Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Parameters: src_model ( HookedTransformer ) \u2013 The model to get activations from. src_model_activation_hook_point ( str ) \u2013 The hook point to get activations from. src_model_activation_layer ( int ) \u2013 The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset ( SourceDataset ) \u2013 Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store ( ActivationStore ) \u2013 The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training ( int ) \u2013 The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder ( SparseAutoencoder ) \u2013 The autoencoder to train. source_dataset_batch_size ( int , default: 16 ) \u2013 Batch size of tokenized prompts for generating the source data. resample_frequency ( int , default: 25000000 ) \u2013 How often to resample neurons (number of activations learnt on). sweep_parameters ( SweepParametersRuntime , default: SweepParametersRuntime () ) \u2013 Parameter config to use. device ( device | None , default: None ) \u2013 Device to run pipeline on. max_activations ( int , default: 100000000 ) \u2013 Maximum number of activations to train with. May train for less if the source dataset is exhausted. sparse_autoencoder/train/pipeline.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def pipeline ( # noqa: PLR0913 src_model : HookedTransformer , src_model_activation_hook_point : str , src_model_activation_layer : int , source_dataset : SourceDataset , activation_store : ActivationStore , num_activations_before_training : int , autoencoder : SparseAutoencoder , source_dataset_batch_size : int = 16 , resample_frequency : int = 25_000_000 , sweep_parameters : SweepParametersRuntime = SweepParametersRuntime (), # noqa: B008 device : torch . device | None = None , max_activations : int = 100_000_000 , ) -> None : \"\"\"Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Args: src_model: The model to get activations from. src_model_activation_hook_point: The hook point to get activations from. src_model_activation_layer: The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset: Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store: The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training: The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder: The autoencoder to train. source_dataset_batch_size: Batch size of tokenized prompts for generating the source data. resample_frequency: How often to resample neurons (number of activations learnt on). sweep_parameters: Parameter config to use. device: Device to run pipeline on. max_activations: Maximum number of activations to train with. May train for less if the source dataset is exhausted. \"\"\" autoencoder . to ( device ) optimizer = AdamWithReset ( autoencoder . parameters (), lr = sweep_parameters . lr , betas = ( sweep_parameters . adam_beta_1 , sweep_parameters . adam_beta_2 ), eps = sweep_parameters . adam_epsilon , weight_decay = sweep_parameters . adam_weight_decay , named_parameters = autoencoder . named_parameters (), ) source_dataloader = source_dataset . get_dataloader ( source_dataset_batch_size ) source_data_iterator = stateful_dataloader_iterable ( source_dataloader ) total_steps : int = 0 activations_since_resampling : int = 0 neuron_activity : NeuronActivity = torch . zeros ( autoencoder . n_learned_features , dtype = torch . int32 , device = device ) total_activations : int = 0 # Run loop until source data is exhausted: with logging_redirect_tqdm (), tqdm ( desc = \"Total activations trained on\" , dynamic_ncols = True , total = max_activations , postfix = { \"Current mode\" : \"initializing\" }, ) as progress_bar : while total_activations < max_activations : # Add activations to the store activation_store . empty () # In case it was filled by a different run progress_bar . set_postfix ({ \"Current mode\" : \"generating\" }) generate_activations ( src_model , src_model_activation_layer , src_model_activation_hook_point , activation_store , source_data_iterator , device = device , context_size = source_dataset . context_size , num_items = num_activations_before_training , batch_size = source_dataset_batch_size , ) if len ( activation_store ) == 0 : break # Shuffle the store if it has a shuffle method - it is often more efficient to # create a shuffle method ourselves rather than get the DataLoader to shuffle activation_store . shuffle () # Train the autoencoder progress_bar . set_postfix ({ \"Current mode\" : \"training\" }) train_steps , learned_activations_fired_count = train_autoencoder ( activation_store = activation_store , autoencoder = autoencoder , optimizer = optimizer , sweep_parameters = sweep_parameters , device = device , previous_steps = total_steps , ) total_steps += train_steps if activations_since_resampling >= resample_frequency / 2 : neuron_activity . add_ ( learned_activations_fired_count ) activations_since_resampling += len ( activation_store ) total_activations += len ( activation_store ) progress_bar . update ( len ( activation_store )) # Resample neurons if required if len ( activation_store ) < DEFAULT_RESAMPLE_N : warn_str = ( f \"Warning: activation store len { len ( activation_store ) } is less than \" f \"DEFAULT_RESAMPLE_N ( { DEFAULT_RESAMPLE_N } ). Resampling with\" f \"num_resample_inputs as { len ( activation_store ) } .\" ) warnings . warn ( warn_str , stacklevel = 2 , ) num_resample_inputs = len ( activation_store ) else : num_resample_inputs = DEFAULT_RESAMPLE_N if activations_since_resampling >= resample_frequency : progress_bar . set_postfix ({ \"Current mode\" : \"resampling\" }) activations_since_resampling = 0 resample_dead_neurons ( neuron_activity = neuron_activity , store = activation_store , autoencoder = autoencoder , sweep_parameters = sweep_parameters , num_inputs = num_resample_inputs , ) learned_activations_fired_count . zero_ () optimizer . reset_state_all_parameters () activation_store . empty ()","title":"pipeline()"},{"location":"reference/SUMMARY/","text":"Home activation_resampler abstract_activation_resampler activation_store base_store disk_store list_store tensor_store utils extend_resize autoencoder components tied_bias unit_norm_linear model loss abstract_loss learned_activations_l1 mse_reconstruction_loss reducer metrics abstract_metric optimizer abstract_optimizer adam_with_reset source_data abstract_dataset pretokenized_dataset random_int text_dataset src_model store_activations_hook tensor_types train abstract_pipeline generate_activations metrics capacity feature_density pipeline resample_neurons sweep_config train_autoencoder utils wandb_sweep_types","title":"SUMMARY"},{"location":"reference/tensor_types/","text":"Tensor Types. Tensor types with axis labels. Note that this uses the jaxtyping library, which works with PyTorch tensors as well. AliveEncoderWeights : TypeAlias = Float [ Tensor , Axis . dims ( Axis . LEARNT_FEATURE , Axis . ALIVE_FEATURE )] module-attribute Alive encoder weights. BatchTokenizedPrompts : TypeAlias = Int [ Tensor , Axis . dims ( Axis . SOURCE_DATA_BATCH , Axis . POSITION )] module-attribute Batch of tokenized prompts. DeadDecoderNeuronWeightUpdates : TypeAlias = Float [ Tensor , Axis . dims ( Axis . INPUT_OUTPUT_FEATURE , Axis . DEAD_FEATURE )] module-attribute Dead decoder neuron weight updates. DeadEncoderNeuronBiasUpdates : TypeAlias = Float [ Tensor , Axis . DEAD_FEATURE ] module-attribute Dead encoder neuron bias updates. DeadEncoderNeuronWeightUpdates : TypeAlias = Float [ Tensor , Axis . dims ( Axis . DEAD_FEATURE , Axis . INPUT_OUTPUT_FEATURE )] module-attribute Dead encoder neuron weight updates. DeadNeuronIndices : TypeAlias = Int [ Tensor , Axis . LEARNT_FEATURE_IDX ] module-attribute Dead neuron indices. DecoderWeights : TypeAlias = Float [ Tensor , Axis . dims ( Axis . INPUT_OUTPUT_FEATURE , Axis . LEARNT_FEATURE )] module-attribute Decoder weights. These weights form the decoder part of the autoencoder, which aims to reconstruct the original input data from the decompressed representation created by the encoder. The tensor's shape aligns with the training features and the learnt features. In this case, if we view the dictionary vectors in the context of reconstruction, they can be thought of as rows in this weight matrix. EncoderWeights : TypeAlias = Float [ Tensor , Axis . dims ( Axis . LEARNT_FEATURE , Axis . INPUT_OUTPUT_FEATURE )] module-attribute Encoder weights. These weights are part of the encoder module of the autoencoder, responsible for decompressing the input data (activations from a source model) into a higher-dimensional representation. The dictionary vectors (basis vectors in the learnt feature space), they can be thought of as columns of this weight matrix, where each column corresponds to a particular feature in the lower-dimensional space. The sparsity constraint (hopefully) enforces that they respond relatively strongly to only a small portion of possible input vectors. InputOutputActivationBatch : TypeAlias = Float [ Tensor , Axis . dims ( Axis . BATCH , Axis . INPUT_OUTPUT_FEATURE )] module-attribute Input/output activation batch. This is either a batch of input activation vectors from the source model, or a batch of decoded activation vectors from the autoencoder. InputOutputActivationVector : TypeAlias = Float [ Tensor , Axis . INPUT_OUTPUT_FEATURE ] module-attribute Input/output activation vector. This is either a input activation vector from the source model, or a decoded activation vector from the autoencoder. ItemTensor : TypeAlias = Float [ Tensor , Axis . SINGLE_ITEM ] module-attribute Single element item tensor. LearnedActivationBatch : TypeAlias = Float [ Tensor , Axis . dims ( Axis . BATCH , Axis . LEARNT_FEATURE )] module-attribute Learned activation batch. This is a batch of activation vectors from the hidden (learnt) layer of the autoencoder. Typically the feature dimension is larger than the input/output activation vector. LearntActivationVector : TypeAlias = Float [ Tensor , Axis . LEARNT_FEATURE ] module-attribute Learned activation vector. Activation vector from the hidden (learnt) layer of the autoencoder. Typically this is larger than the input/output activation vector. NeuronActivity : TypeAlias = Int [ Tensor , Axis . LEARNT_FEATURE ] module-attribute Neuron activity. Number of times each neuron has fired (since the last reset). SampledDeadNeuronInputs : TypeAlias = Float [ Tensor , Axis . dims ( Axis . DEAD_FEATURE , Axis . INPUT_OUTPUT_FEATURE )] module-attribute Sampled dead neuron inputs. SourceModelActivations : TypeAlias = Float [ Tensor , Axis . dims ( Axis . ANY , Axis . INPUT_OUTPUT_FEATURE )] module-attribute Source model activations. Can have any number of proceeding dimensions (e.g. an attention head may generate activations of shape (batch_size, num_heads, seq_len, feature_dim). StoreActivations : TypeAlias = Float [ Tensor , Axis . dims ( Axis . ITEMS , Axis . INPUT_OUTPUT_FEATURE )] module-attribute Store of activation vectors. This is used to store large numbers of activation vectors from the source model. TrainBatchStatistic : TypeAlias = Float [ Tensor , Axis . BATCH ] module-attribute Train batch statistic. Contains one scalar value per item in the batch. Axis Bases: LowercaseStrEnum Tensor axis names. Used to annotate tensor types. Example When used directly it prints a string: print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature The primary use is to annotate tensor types: from jaxtyping import Float from torch import Tensor from typing import TypeAlias batch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] print(batch) You can also join multiple axis together to represent the dimensions of a tensor: print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Source code in sparse_autoencoder/tensor_types.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class Axis ( LowercaseStrEnum ): \"\"\"Tensor axis names. Used to annotate tensor types. Example: When used directly it prints a string: >>> print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature The primary use is to annotate tensor types: >>> from jaxtyping import Float >>> from torch import Tensor >>> from typing import TypeAlias >>> batch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] >>> print(batch) <class 'jaxtyping.Float[Tensor, 'batch input_output_feature']'> You can also join multiple axis together to represent the dimensions of a tensor: >>> print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature \"\"\" # Batches SOURCE_DATA_BATCH = auto () \"\"\"Batch of prompts used to generate source model activations.\"\"\" BATCH = auto () \"\"\"Batch of items that the SAE is being trained on.\"\"\" ITEMS = auto () \"\"\"Arbitrary number of items.\"\"\" # Features INPUT_OUTPUT_FEATURE = auto () \"\"\"Input or output feature (e.g. feature in activation vector from source model).\"\"\" LEARNT_FEATURE = auto () \"\"\"Learn feature (e.g. feature in learnt activation vector).\"\"\" DEAD_FEATURE = auto () \"\"\"Dead feature.\"\"\" ALIVE_FEATURE = auto () \"\"\"Alive feature.\"\"\" # Feature indices LEARNT_FEATURE_IDX = auto () # Other POSITION = auto () \"\"\"Token position.\"\"\" SINGLE_ITEM = \"\" \"\"\"Single item axis.\"\"\" ANY = \"*any\" \"\"\"Any number of axis.\"\"\" @staticmethod def dims ( * axis : \"Axis\" ) -> str : \"\"\"Join multiple axis together, to represent the dimensions of a tensor. Example: >>> print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Args: *axis: Axis to join. Returns: Joined axis string. \"\"\" return \" \" . join ( axis ) ALIVE_FEATURE = auto () class-attribute instance-attribute Alive feature. ANY = '*any' class-attribute instance-attribute Any number of axis. BATCH = auto () class-attribute instance-attribute Batch of items that the SAE is being trained on. DEAD_FEATURE = auto () class-attribute instance-attribute Dead feature. INPUT_OUTPUT_FEATURE = auto () class-attribute instance-attribute Input or output feature (e.g. feature in activation vector from source model). ITEMS = auto () class-attribute instance-attribute Arbitrary number of items. LEARNT_FEATURE = auto () class-attribute instance-attribute Learn feature (e.g. feature in learnt activation vector). POSITION = auto () class-attribute instance-attribute Token position. SINGLE_ITEM = '' class-attribute instance-attribute Single item axis. SOURCE_DATA_BATCH = auto () class-attribute instance-attribute Batch of prompts used to generate source model activations. dims ( * axis ) staticmethod Join multiple axis together, to represent the dimensions of a tensor. Example print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Parameters: *axis ( Axis , default: () ) \u2013 Axis to join. Returns: str \u2013 Joined axis string. sparse_autoencoder/tensor_types.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @staticmethod def dims ( * axis : \"Axis\" ) -> str : \"\"\"Join multiple axis together, to represent the dimensions of a tensor. Example: >>> print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Args: *axis: Axis to join. Returns: Joined axis string. \"\"\" return \" \" . join ( axis )","title":"tensor_types"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.AliveEncoderWeights","text":"Alive encoder weights.","title":"AliveEncoderWeights"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.BatchTokenizedPrompts","text":"Batch of tokenized prompts.","title":"BatchTokenizedPrompts"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadDecoderNeuronWeightUpdates","text":"Dead decoder neuron weight updates.","title":"DeadDecoderNeuronWeightUpdates"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronBiasUpdates","text":"Dead encoder neuron bias updates.","title":"DeadEncoderNeuronBiasUpdates"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates","text":"Dead encoder neuron weight updates.","title":"DeadEncoderNeuronWeightUpdates"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadNeuronIndices","text":"Dead neuron indices.","title":"DeadNeuronIndices"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DecoderWeights","text":"Decoder weights. These weights form the decoder part of the autoencoder, which aims to reconstruct the original input data from the decompressed representation created by the encoder. The tensor's shape aligns with the training features and the learnt features. In this case, if we view the dictionary vectors in the context of reconstruction, they can be thought of as rows in this weight matrix.","title":"DecoderWeights"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.EncoderWeights","text":"Encoder weights. These weights are part of the encoder module of the autoencoder, responsible for decompressing the input data (activations from a source model) into a higher-dimensional representation. The dictionary vectors (basis vectors in the learnt feature space), they can be thought of as columns of this weight matrix, where each column corresponds to a particular feature in the lower-dimensional space. The sparsity constraint (hopefully) enforces that they respond relatively strongly to only a small portion of possible input vectors.","title":"EncoderWeights"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch","text":"Input/output activation batch. This is either a batch of input activation vectors from the source model, or a batch of decoded activation vectors from the autoencoder.","title":"InputOutputActivationBatch"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector","text":"Input/output activation vector. This is either a input activation vector from the source model, or a decoded activation vector from the autoencoder.","title":"InputOutputActivationVector"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.ItemTensor","text":"Single element item tensor.","title":"ItemTensor"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch","text":"Learned activation batch. This is a batch of activation vectors from the hidden (learnt) layer of the autoencoder. Typically the feature dimension is larger than the input/output activation vector.","title":"LearnedActivationBatch"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.LearntActivationVector","text":"Learned activation vector. Activation vector from the hidden (learnt) layer of the autoencoder. Typically this is larger than the input/output activation vector.","title":"LearntActivationVector"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity","text":"Neuron activity. Number of times each neuron has fired (since the last reset).","title":"NeuronActivity"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.SampledDeadNeuronInputs","text":"Sampled dead neuron inputs.","title":"SampledDeadNeuronInputs"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations","text":"Source model activations. Can have any number of proceeding dimensions (e.g. an attention head may generate activations of shape (batch_size, num_heads, seq_len, feature_dim).","title":"SourceModelActivations"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.StoreActivations","text":"Store of activation vectors. This is used to store large numbers of activation vectors from the source model.","title":"StoreActivations"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic","text":"Train batch statistic. Contains one scalar value per item in the batch.","title":"TrainBatchStatistic"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis","text":"Bases: LowercaseStrEnum Tensor axis names. Used to annotate tensor types. Example When used directly it prints a string: print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature The primary use is to annotate tensor types: from jaxtyping import Float from torch import Tensor from typing import TypeAlias batch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] print(batch) You can also join multiple axis together to represent the dimensions of a tensor: print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Source code in sparse_autoencoder/tensor_types.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class Axis ( LowercaseStrEnum ): \"\"\"Tensor axis names. Used to annotate tensor types. Example: When used directly it prints a string: >>> print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature The primary use is to annotate tensor types: >>> from jaxtyping import Float >>> from torch import Tensor >>> from typing import TypeAlias >>> batch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] >>> print(batch) <class 'jaxtyping.Float[Tensor, 'batch input_output_feature']'> You can also join multiple axis together to represent the dimensions of a tensor: >>> print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature \"\"\" # Batches SOURCE_DATA_BATCH = auto () \"\"\"Batch of prompts used to generate source model activations.\"\"\" BATCH = auto () \"\"\"Batch of items that the SAE is being trained on.\"\"\" ITEMS = auto () \"\"\"Arbitrary number of items.\"\"\" # Features INPUT_OUTPUT_FEATURE = auto () \"\"\"Input or output feature (e.g. feature in activation vector from source model).\"\"\" LEARNT_FEATURE = auto () \"\"\"Learn feature (e.g. feature in learnt activation vector).\"\"\" DEAD_FEATURE = auto () \"\"\"Dead feature.\"\"\" ALIVE_FEATURE = auto () \"\"\"Alive feature.\"\"\" # Feature indices LEARNT_FEATURE_IDX = auto () # Other POSITION = auto () \"\"\"Token position.\"\"\" SINGLE_ITEM = \"\" \"\"\"Single item axis.\"\"\" ANY = \"*any\" \"\"\"Any number of axis.\"\"\" @staticmethod def dims ( * axis : \"Axis\" ) -> str : \"\"\"Join multiple axis together, to represent the dimensions of a tensor. Example: >>> print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Args: *axis: Axis to join. Returns: Joined axis string. \"\"\" return \" \" . join ( axis )","title":"Axis"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ALIVE_FEATURE","text":"Alive feature.","title":"ALIVE_FEATURE"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ANY","text":"Any number of axis.","title":"ANY"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH","text":"Batch of items that the SAE is being trained on.","title":"BATCH"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE","text":"Dead feature.","title":"DEAD_FEATURE"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE","text":"Input or output feature (e.g. feature in activation vector from source model).","title":"INPUT_OUTPUT_FEATURE"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ITEMS","text":"Arbitrary number of items.","title":"ITEMS"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE","text":"Learn feature (e.g. feature in learnt activation vector).","title":"LEARNT_FEATURE"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.POSITION","text":"Token position.","title":"POSITION"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SINGLE_ITEM","text":"Single item axis.","title":"SINGLE_ITEM"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SOURCE_DATA_BATCH","text":"Batch of prompts used to generate source model activations.","title":"SOURCE_DATA_BATCH"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.dims","text":"Join multiple axis together, to represent the dimensions of a tensor. Example print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Parameters: *axis ( Axis , default: () ) \u2013 Axis to join. Returns: str \u2013 Joined axis string. sparse_autoencoder/tensor_types.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @staticmethod def dims ( * axis : \"Axis\" ) -> str : \"\"\"Join multiple axis together, to represent the dimensions of a tensor. Example: >>> print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature Args: *axis: Axis to join. Returns: Joined axis string. \"\"\" return \" \" . join ( axis )","title":"dims()"},{"location":"reference/activation_resampler/","text":"Activation Resampler.","title":"Index"},{"location":"reference/activation_resampler/abstract_activation_resampler/","text":"Abstract activation resampler. AbstractActivationResampler Bases: ABC Abstract activation resampler. Source code in sparse_autoencoder/activation_resampler/abstract_activation_resampler.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class AbstractActivationResampler ( ABC ): \"\"\"Abstract activation resampler.\"\"\" @abstractmethod def resample_dead_neurons ( self , neuron_activity : NeuronActivity , store : TensorActivationStore , num_input_activations : int = 819_200 , ) -> tuple [ DeadEncoderNeuronWeightUpdates , DeadEncoderNeuronBiasUpdates , DeadDecoderNeuronWeightUpdates ]: \"\"\"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. Warning: The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Args: neuron_activity: Number of times each neuron fired. store: Activation store. store: TODO change. num_input_activations: Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. \"\"\" raise NotImplementedError resample_dead_neurons ( neuron_activity , store , num_input_activations = 819200 ) abstractmethod Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. Warning The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Parameters: neuron_activity ( NeuronActivity ) \u2013 Number of times each neuron fired. store: Activation store. store ( TensorActivationStore ) \u2013 TODO change. num_input_activations ( int , default: 819200 ) \u2013 Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. sparse_autoencoder/activation_resampler/abstract_activation_resampler.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @abstractmethod def resample_dead_neurons ( self , neuron_activity : NeuronActivity , store : TensorActivationStore , num_input_activations : int = 819_200 , ) -> tuple [ DeadEncoderNeuronWeightUpdates , DeadEncoderNeuronBiasUpdates , DeadDecoderNeuronWeightUpdates ]: \"\"\"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. Warning: The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Args: neuron_activity: Number of times each neuron fired. store: Activation store. store: TODO change. num_input_activations: Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. \"\"\" raise NotImplementedError","title":"abstract_activation_resampler"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler","text":"Bases: ABC Abstract activation resampler. Source code in sparse_autoencoder/activation_resampler/abstract_activation_resampler.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class AbstractActivationResampler ( ABC ): \"\"\"Abstract activation resampler.\"\"\" @abstractmethod def resample_dead_neurons ( self , neuron_activity : NeuronActivity , store : TensorActivationStore , num_input_activations : int = 819_200 , ) -> tuple [ DeadEncoderNeuronWeightUpdates , DeadEncoderNeuronBiasUpdates , DeadDecoderNeuronWeightUpdates ]: \"\"\"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. Warning: The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Args: neuron_activity: Number of times each neuron fired. store: Activation store. store: TODO change. num_input_activations: Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. \"\"\" raise NotImplementedError","title":"AbstractActivationResampler"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler.resample_dead_neurons","text":"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. Warning The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Parameters: neuron_activity ( NeuronActivity ) \u2013 Number of times each neuron fired. store: Activation store. store ( TensorActivationStore ) \u2013 TODO change. num_input_activations ( int , default: 819200 ) \u2013 Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. sparse_autoencoder/activation_resampler/abstract_activation_resampler.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @abstractmethod def resample_dead_neurons ( self , neuron_activity : NeuronActivity , store : TensorActivationStore , num_input_activations : int = 819_200 , ) -> tuple [ DeadEncoderNeuronWeightUpdates , DeadEncoderNeuronBiasUpdates , DeadDecoderNeuronWeightUpdates ]: \"\"\"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. Warning: The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Args: neuron_activity: Number of times each neuron fired. store: Activation store. store: TODO change. num_input_activations: Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. \"\"\" raise NotImplementedError","title":"resample_dead_neurons()"},{"location":"reference/activation_store/","text":"Activation Stores.","title":"Index"},{"location":"reference/activation_store/base_store/","text":"Activation Store Base Class. ActivationStore Bases: Dataset [ InputOutputActivationVector ] , ABC Activation Store Abstract Class. Extends the torch.utils.data.Dataset class to provide an activation store, with additional :meth: append and :meth: extend methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a torch.utils.data.DataLoader to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create __getitem__ and __len__ methods from the underlying torch.utils.data.Dataset class). Example: import torch class MyActivationStore(ActivationStore): ... def init (self): ... super(). init () ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def getitem (self, index: int): ... return self._data[index] ... ... def len (self) -> int: ... return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1 Source code in sparse_autoencoder/activation_store/base_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class ActivationStore ( Dataset [ InputOutputActivationVector ], ABC ): \"\"\"Activation Store Abstract Class. Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader` to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class). Example: >>> import torch >>> class MyActivationStore(ActivationStore): ... def __init__(self): ... super().__init__() ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def __getitem__(self, index: int): ... return self._data[index] ... ... def __len__(self) -> int: ... return len(self._data) ... >>> store = MyActivationStore() >>> store.append(torch.randn(100)) >>> print(len(store)) 1 \"\"\" @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\" @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample ) __getitem__ ( index ) abstractmethod Get an Item from the Store. sparse_autoencoder/activation_store/base_store.py 71 72 73 74 @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError __len__ () abstractmethod Get the Length of the Store. sparse_autoencoder/activation_store/base_store.py 66 67 68 69 @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError append ( item ) abstractmethod Add a Single Item to the Store. sparse_autoencoder/activation_store/base_store.py 51 52 53 54 @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError empty () abstractmethod Empty the Store. sparse_autoencoder/activation_store/base_store.py 61 62 63 64 @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError extend ( batch ) abstractmethod Add a Batch to the Store. sparse_autoencoder/activation_store/base_store.py 56 57 58 59 @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError fill_with_test_data ( num_batches = 16 , batch_size = 16 , input_features = 256 ) Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning You may want to use torch.seed(0) to make the random data deterministic, if your test requires inspecting the data itself. Example from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=16*16, num_neurons=256) store.fill_with_test_data() len(store) 256 store[0].shape torch.Size([256]) Parameters: num_batches ( int , default: 16 ) \u2013 Number of batches to fill the store with. batch_size ( int , default: 16 ) \u2013 Number of items per batch. input_features ( int , default: 256 ) \u2013 Number of input features per item. sparse_autoencoder/activation_store/base_store.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample ) shuffle () Optional shuffle method. sparse_autoencoder/activation_store/base_store.py 76 77 def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\" StoreFullError Bases: IndexError Exception raised when the activation store is full. Source code in sparse_autoencoder/activation_store/base_store.py 110 111 112 113 114 115 116 117 118 119 class StoreFullError ( IndexError ): \"\"\"Exception raised when the activation store is full.\"\"\" def __init__ ( self , message : str = \"Activation store is full\" ): \"\"\"Initialise the exception. Args: message: Override the default message. \"\"\" super () . __init__ ( message ) __init__ ( message = 'Activation store is full' ) Initialise the exception. Parameters: message ( str , default: 'Activation store is full' ) \u2013 Override the default message. sparse_autoencoder/activation_store/base_store.py 113 114 115 116 117 118 119 def __init__ ( self , message : str = \"Activation store is full\" ): \"\"\"Initialise the exception. Args: message: Override the default message. \"\"\" super () . __init__ ( message )","title":"base_store"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore","text":"Bases: Dataset [ InputOutputActivationVector ] , ABC Activation Store Abstract Class. Extends the torch.utils.data.Dataset class to provide an activation store, with additional :meth: append and :meth: extend methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a torch.utils.data.DataLoader to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create __getitem__ and __len__ methods from the underlying torch.utils.data.Dataset class). Example: import torch class MyActivationStore(ActivationStore): ... def init (self): ... super(). init () ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def getitem (self, index: int): ... return self._data[index] ... ... def len (self) -> int: ... return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1 Source code in sparse_autoencoder/activation_store/base_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class ActivationStore ( Dataset [ InputOutputActivationVector ], ABC ): \"\"\"Activation Store Abstract Class. Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader` to iterate over the dataset. Extend this class if you want to create a new activation store (noting you also need to create `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class). Example: >>> import torch >>> class MyActivationStore(ActivationStore): ... def __init__(self): ... super().__init__() ... self._data = [] # In this example, we just store in a list ... ... def append(self, item) -> None: ... self._data.append(item) ... ... def extend(self, batch): ... self._data.extend(batch) ... ... def empty(self): ... self._data = [] ... ... def __getitem__(self, index: int): ... return self._data[index] ... ... def __len__(self) -> int: ... return len(self._data) ... >>> store = MyActivationStore() >>> store.append(torch.randn(100)) >>> print(len(store)) 1 \"\"\" @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\" @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample )","title":"ActivationStore"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__getitem__","text":"Get an Item from the Store. sparse_autoencoder/activation_store/base_store.py 71 72 73 74 @abstractmethod def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get an Item from the Store.\"\"\" raise NotImplementedError","title":"__getitem__()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__len__","text":"Get the Length of the Store. sparse_autoencoder/activation_store/base_store.py 66 67 68 69 @abstractmethod def __len__ ( self ) -> int : \"\"\"Get the Length of the Store.\"\"\" raise NotImplementedError","title":"__len__()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.append","text":"Add a Single Item to the Store. sparse_autoencoder/activation_store/base_store.py 51 52 53 54 @abstractmethod def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store.\"\"\" raise NotImplementedError","title":"append()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.empty","text":"Empty the Store. sparse_autoencoder/activation_store/base_store.py 61 62 63 64 @abstractmethod def empty ( self ) -> None : \"\"\"Empty the Store.\"\"\" raise NotImplementedError","title":"empty()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.extend","text":"Add a Batch to the Store. sparse_autoencoder/activation_store/base_store.py 56 57 58 59 @abstractmethod def extend ( self , batch : InputOutputActivationBatch ) -> Future | None : \"\"\"Add a Batch to the Store.\"\"\" raise NotImplementedError","title":"extend()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.fill_with_test_data","text":"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning You may want to use torch.seed(0) to make the random data deterministic, if your test requires inspecting the data itself. Example from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=16*16, num_neurons=256) store.fill_with_test_data() len(store) 256 store[0].shape torch.Size([256]) Parameters: num_batches ( int , default: 16 ) \u2013 Number of batches to fill the store with. batch_size ( int , default: 16 ) \u2013 Number of items per batch. input_features ( int , default: 256 ) \u2013 Number of input features per item. sparse_autoencoder/activation_store/base_store.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @final def fill_with_test_data ( self , num_batches : int = 16 , batch_size : int = 16 , input_features : int = 256 ) -> None : \"\"\"Fill the store with test data. For use when testing your code, to ensure it works with a real activation store. Warning: You may want to use `torch.seed(0)` to make the random data deterministic, if your test requires inspecting the data itself. Example: >>> from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore >>> store = TensorActivationStore(max_items=16*16, num_neurons=256) >>> store.fill_with_test_data() >>> len(store) 256 >>> store[0].shape torch.Size([256]) Args: num_batches: Number of batches to fill the store with. batch_size: Number of items per batch. input_features: Number of input features per item. \"\"\" for _ in range ( num_batches ): sample = torch . rand (( batch_size , input_features )) self . extend ( sample )","title":"fill_with_test_data()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.shuffle","text":"Optional shuffle method. sparse_autoencoder/activation_store/base_store.py 76 77 def shuffle ( self ) -> None : \"\"\"Optional shuffle method.\"\"\"","title":"shuffle()"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError","text":"Bases: IndexError Exception raised when the activation store is full. Source code in sparse_autoencoder/activation_store/base_store.py 110 111 112 113 114 115 116 117 118 119 class StoreFullError ( IndexError ): \"\"\"Exception raised when the activation store is full.\"\"\" def __init__ ( self , message : str = \"Activation store is full\" ): \"\"\"Initialise the exception. Args: message: Override the default message. \"\"\" super () . __init__ ( message )","title":"StoreFullError"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError.__init__","text":"Initialise the exception. Parameters: message ( str , default: 'Activation store is full' ) \u2013 Override the default message. sparse_autoencoder/activation_store/base_store.py 113 114 115 116 117 118 119 def __init__ ( self , message : str = \"Activation store is full\" ): \"\"\"Initialise the exception. Args: message: Override the default message. \"\"\" super () . __init__ ( message )","title":"__init__()"},{"location":"reference/activation_store/disk_store/","text":"Disk Activation Store. DiskActivationStore Bases: ActivationStore Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set empty_dir to True . Note also that :meth: close must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. Source code in sparse_autoencoder/activation_store/disk_store.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 class DiskActivationStore ( ActivationStore ): \"\"\"Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set `empty_dir` to `True`. Note also that :meth:`close` must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. \"\"\" _storage_path : Path \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\" _cache : ListProxy \"\"\"Cache for Activation Vectors. Activation vectors are buffered in memory until the cache is full, at which point they are written to disk. \"\"\" _cache_lock : Lock \"\"\"Lock for the Cache.\"\"\" _max_cache_size : int \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\" _thread_pool : ThreadPoolExecutor \"\"\"Threadpool for non-blocking writes to the file system.\"\"\" _disk_n_activation_vectors : ValueProxy [ int ] \"\"\"Length of the Store (on disk). Minus 1 signifies not calculated yet. \"\"\" def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers ) def _write_to_disk ( self , * , wait_for_max : bool = False ) -> None : \"\"\"Write the contents of the queue to disk. Args: wait_for_max: Whether to wait until the cache is full before writing to disk. \"\"\" with self . _cache_lock : # Check we have enough items if len ( self . _cache ) == 0 : return size_to_get = min ( self . _max_cache_size , len ( self . _cache )) if wait_for_max and size_to_get < self . _max_cache_size : return # Get the activations from the cache and delete them activations = self . _cache [ 0 : size_to_get ] del self . _cache [ 0 : size_to_get ] # Update the length cache if self . _disk_n_activation_vectors . value != - 1 : self . _disk_n_activation_vectors . value += len ( activations ) stacked_activations = torch . stack ( activations ) filename = f \" { self . __len__ } .pt\" torch . save ( stacked_activations , self . _storage_path / filename ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk () @property def _all_filenames ( self ) -> list [ Path ]: \"\"\"Return a List of All Activation Vector Filenames.\"\"\" return list ( self . _storage_path . glob ( \"*.pt\" )) def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ] def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete () __del__ () Delete Dunder Method. sparse_autoencoder/activation_store/disk_store.py 267 268 269 270 271 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete () __getitem__ ( index ) Get Item Dunder Method. Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/disk_store.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ] __init__ ( storage_path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size = 10000 , num_workers = 6 , * , empty_dir = False ) Initialize the Disk Activation Store. Parameters: storage_path ( Path , default: DEFAULT_DISK_ACTIVATION_STORE_PATH ) \u2013 Path to the directory where the activation vectors will be stored. max_cache_size ( int , default: 10000 ) \u2013 The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers ( int , default: 6 ) \u2013 Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir ( bool , default: False ) \u2013 Whether to empty the directory before writing. Generally you want to set this to True as otherwise the directory may contain stale activation vectors from previous runs. sparse_autoencoder/activation_store/disk_store.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers ) __len__ () Length Dunder Method. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value append ( item ) Add a Single Item to the Store. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 Parameters: item ( InputOutputActivationVector ) \u2013 Activation vector to add to the store. Returns: Future | None \u2013 Future that completes when the activation vector has queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy empty () Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 store.empty() print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0 extend ( batch ) Add a Batch to the Store. Example: store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10 Parameters: batch ( SourceModelActivations ) \u2013 Batch of activation vectors to add to the store. Returns: Future | None \u2013 Future that completes when the activation vectors have queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy wait_for_writes_to_complete () Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1 sparse_autoencoder/activation_store/disk_store.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk ()","title":"disk_store"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore","text":"Bases: ActivationStore Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set empty_dir to True . Note also that :meth: close must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. Source code in sparse_autoencoder/activation_store/disk_store.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 class DiskActivationStore ( ActivationStore ): \"\"\"Disk Activation Store. Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches. Multiprocess safe (supports writing from multiple GPU workers). Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set `empty_dir` to `True`. Note also that :meth:`close` must be called to ensure all activation vectors are written to disk after the last batch has been added to the store. \"\"\" _storage_path : Path \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\" _cache : ListProxy \"\"\"Cache for Activation Vectors. Activation vectors are buffered in memory until the cache is full, at which point they are written to disk. \"\"\" _cache_lock : Lock \"\"\"Lock for the Cache.\"\"\" _max_cache_size : int \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\" _thread_pool : ThreadPoolExecutor \"\"\"Threadpool for non-blocking writes to the file system.\"\"\" _disk_n_activation_vectors : ValueProxy [ int ] \"\"\"Length of the Store (on disk). Minus 1 signifies not calculated yet. \"\"\" def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers ) def _write_to_disk ( self , * , wait_for_max : bool = False ) -> None : \"\"\"Write the contents of the queue to disk. Args: wait_for_max: Whether to wait until the cache is full before writing to disk. \"\"\" with self . _cache_lock : # Check we have enough items if len ( self . _cache ) == 0 : return size_to_get = min ( self . _max_cache_size , len ( self . _cache )) if wait_for_max and size_to_get < self . _max_cache_size : return # Get the activations from the cache and delete them activations = self . _cache [ 0 : size_to_get ] del self . _cache [ 0 : size_to_get ] # Update the length cache if self . _disk_n_activation_vectors . value != - 1 : self . _disk_n_activation_vectors . value += len ( activations ) stacked_activations = torch . stack ( activations ) filename = f \" { self . __len__ } .pt\" torch . save ( stacked_activations , self . _storage_path / filename ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk () @property def _all_filenames ( self ) -> list [ Path ]: \"\"\"Return a List of All Activation Vector Filenames.\"\"\" return list ( self . _storage_path . glob ( \"*.pt\" )) def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ] def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete ()","title":"DiskActivationStore"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__del__","text":"Delete Dunder Method. sparse_autoencoder/activation_store/disk_store.py 267 268 269 270 271 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" # Shutdown the thread pool after everything is complete self . _thread_pool . shutdown ( wait = True , cancel_futures = False ) self . wait_for_writes_to_complete ()","title":"__del__()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__getitem__","text":"Get Item Dunder Method. Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/disk_store.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" # Find the file containing the activation vector file_index = index // self . _max_cache_size file = self . _storage_path / f \" { file_index } .pt\" # Load the file and return the activation vector activation_vectors = torch . load ( file ) return activation_vectors [ index % self . _max_cache_size ]","title":"__getitem__()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__init__","text":"Initialize the Disk Activation Store. Parameters: storage_path ( Path , default: DEFAULT_DISK_ACTIVATION_STORE_PATH ) \u2013 Path to the directory where the activation vectors will be stored. max_cache_size ( int , default: 10000 ) \u2013 The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers ( int , default: 6 ) \u2013 Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir ( bool , default: False ) \u2013 Whether to empty the directory before writing. Generally you want to set this to True as otherwise the directory may contain stale activation vectors from previous runs. sparse_autoencoder/activation_store/disk_store.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , storage_path : Path = DEFAULT_DISK_ACTIVATION_STORE_PATH , max_cache_size : int = 10_000 , num_workers : int = 6 , * , empty_dir : bool = False , ): \"\"\"Initialize the Disk Activation Store. Args: storage_path: Path to the directory where the activation vectors will be stored. max_cache_size: The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately. num_workers: Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature. empty_dir: Whether to empty the directory before writing. Generally you want to set this to `True` as otherwise the directory may contain stale activation vectors from previous runs. \"\"\" super () . __init__ () # Setup the storage directory self . _storage_path = storage_path self . _storage_path . mkdir ( parents = True , exist_ok = True ) # Setup the Cache manager = Manager () self . _cache = manager . list () self . _max_cache_size = max_cache_size self . _cache_lock = manager . Lock () self . _disk_n_activation_vectors = manager . Value ( \"i\" , - 1 ) # Empty the directory if needed if empty_dir : self . empty () # Create a threadpool for non-blocking writes to the cache self . _thread_pool = ThreadPoolExecutor ( num_workers )","title":"__init__()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__len__","text":"Length Dunder Method. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> print(len(store)) 0 \"\"\" # Calculate the length if not cached if self . _disk_n_activation_vectors . value == - 1 : cache_size : int = 0 for file in self . _all_filenames : cache_size += len ( torch . load ( file )) self . _disk_n_activation_vectors . value = cache_size return self . _disk_n_activation_vectors . value","title":"__len__()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.append","text":"Add a Single Item to the Store. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 Parameters: item ( InputOutputActivationVector ) \u2013 Activation vector to add to the store. Returns: Future | None \u2013 Future that completes when the activation vector has queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Add a Single Item to the Store. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 Args: item: Activation vector to add to the store. Returns: Future that completes when the activation vector has queued to be written to disk, and if needed, written to disk. \"\"\" with self . _cache_lock : self . _cache . append ( item ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy","title":"append()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.empty","text":"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1 store.empty() print(len(store)) 0 sparse_autoencoder/activation_store/disk_store.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def empty ( self ) -> None : \"\"\"Empty the Store. Warning: This will delete all .pt files in the top level of the storage directory. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> future.result() >>> print(len(store)) 1 >>> store.empty() >>> print(len(store)) 0 \"\"\" for file in self . _all_filenames : file . unlink () self . _disk_n_activation_vectors . value = 0","title":"empty()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.extend","text":"Add a Batch to the Store. Example: store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10 Parameters: batch ( SourceModelActivations ) \u2013 Batch of activation vectors to add to the store. Returns: Future | None \u2013 Future that completes when the activation vectors have queued to be written to disk, and Future | None \u2013 if needed, written to disk. sparse_autoencoder/activation_store/disk_store.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Add a Batch to the Store. Example: >>> store = DiskActivationStore(max_cache_size=10, empty_dir=True) >>> future = store.extend(torch.randn(10, 100)) >>> future.result() >>> print(len(store)) 10 Args: batch: Batch of activation vectors to add to the store. Returns: Future that completes when the activation vectors have queued to be written to disk, and if needed, written to disk. \"\"\" items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) with self . _cache_lock : self . _cache . extend ( items ) # Write to disk if needed if len ( self . _cache ) >= self . _max_cache_size : return self . _thread_pool . submit ( self . _write_to_disk , wait_for_max = True ) return None # Keep mypy happy","title":"extend()"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.wait_for_writes_to_complete","text":"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1 sparse_autoencoder/activation_store/disk_store.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk. Example: >>> store = DiskActivationStore(max_cache_size=1, empty_dir=True) >>> future = store.append(torch.randn(100)) >>> store.wait_for_writes_to_complete() >>> print(len(store)) 1 \"\"\" while len ( self . _cache ) > 0 : self . _write_to_disk ()","title":"wait_for_writes_to_complete()"},{"location":"reference/activation_store/list_store/","text":"List Activation Store. ListActivationStore Bases: ActivationStore List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the multiprocessing_enabled argument is set to True . This works in two ways: The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. The extend method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Note that the built-in :meth: shuffle method is much faster than using the shuffle argument on torch.utils.data.DataLoader . You should therefore call this method before passing the dataset to the loader and then set the DataLoader shuffle argument to False . Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/list_store.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class ListActivationStore ( ActivationStore ): \"\"\"List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two ways: 1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. 2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument on `torch.utils.data.DataLoader`. You should therefore call this method before passing the dataset to the loader and then set the DataLoader `shuffle` argument to `False`. Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : list [ InputOutputActivationVector ] | ListProxy \"\"\"Underlying List Data Store.\"\"\" _device : torch . device | None \"\"\"Device to Store the Activation Vectors On.\"\"\" _pool : ProcessPoolExecutor | None = None \"\"\"Multiprocessing Pool.\"\"\" _pool_exceptions : ListProxy | list [ Exception ] \"\"\"Pool Exceptions. Used to keep track of exceptions. \"\"\" _pool_futures : list [ Future ] \"\"\"Pool Futures. Used to keep track of processes running in the pool. \"\"\" def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data ) def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device )) def _extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Extend threadpool method. To be called by :meth:`extend`. Args: batch: A batch of items to add to the dataset. \"\"\" try : # Unstack to a list of tensors items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) self . _data . extend ( items ) except Exception as e : # noqa: BLE001 self . _pool_exceptions . append ( e ) def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch ) def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg ) def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = [] def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True ) __del__ () Delete Dunder Method. sparse_autoencoder/activation_store/list_store.py 312 313 314 315 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True ) __getitem__ ( index ) Get Item Dunder Method. Example: import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/list_store.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ] __init__ ( data = None , device = None , max_workers = None , * , multiprocessing_enabled = False ) Initialize the List Activation Store. Parameters: data ( list [ InputOutputActivationVector ] | None , default: None ) \u2013 Data to initialize the dataset with. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. max_workers ( int | None , default: None ) \u2013 Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled ( bool , default: False ) \u2013 Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). sparse_autoencoder/activation_store/list_store.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device __len__ () Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/list_store.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data ) __sizeof__ () Sizeof Dunder Method. Returns the size of the dataset in bytes. sparse_autoencoder/activation_store/list_store.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size append ( item ) Append a single item to the dataset. Note append is blocking . For better performance use extend instead with batches. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. sparse_autoencoder/activation_store/list_store.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device )) empty () Empty the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 store.empty() len(store) 0 sparse_autoencoder/activation_store/list_store.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = [] extend ( batch ) Extend the dataset with multiple items (non-blocking). Example import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10 Parameters: batch ( SourceModelActivations ) \u2013 A batch of items to add to the dataset. sparse_autoencoder/activation_store/list_store.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch ) shuffle () Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3 sparse_autoencoder/activation_store/list_store.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data ) wait_for_writes_to_complete () Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth: append ) to complete. Example: import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3 sparse_autoencoder/activation_store/list_store.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg )","title":"list_store"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore","text":"Bases: ActivationStore List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the multiprocessing_enabled argument is set to True . This works in two ways: The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. The extend method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Note that the built-in :meth: shuffle method is much faster than using the shuffle argument on torch.utils.data.DataLoader . You should therefore call this method before passing the dataset to the loader and then set the DataLoader shuffle argument to False . Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/list_store.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class ListActivationStore ( ActivationStore ): \"\"\"List Activation Store. Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance. Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two ways: 1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple processes (typically multiple GPUs) to read/write to the list. 2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the background, which allows the main process to continue working even if there is just one GPU. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument on `torch.utils.data.DataLoader`. You should therefore call this method before passing the dataset to the loader and then set the DataLoader `shuffle` argument to `False`. Examples: Create an empty activation dataset: >>> import torch >>> store = ListActivationStore() Add a single activation vector to the dataset (this is blocking): >>> store.append(torch.randn(100)) >>> len(store) 1 Add a batch of activation vectors to the dataset (non-blocking): >>> batch = torch.randn(10, 100) >>> store.extend(batch) >>> len(store) 11 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : list [ InputOutputActivationVector ] | ListProxy \"\"\"Underlying List Data Store.\"\"\" _device : torch . device | None \"\"\"Device to Store the Activation Vectors On.\"\"\" _pool : ProcessPoolExecutor | None = None \"\"\"Multiprocessing Pool.\"\"\" _pool_exceptions : ListProxy | list [ Exception ] \"\"\"Pool Exceptions. Used to keep track of exceptions. \"\"\" _pool_futures : list [ Future ] \"\"\"Pool Futures. Used to keep track of processes running in the pool. \"\"\" def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data ) def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data ) def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device )) def _extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Extend threadpool method. To be called by :meth:`extend`. Args: batch: A batch of items to add to the dataset. \"\"\" try : # Unstack to a list of tensors items : list [ InputOutputActivationVector ] = resize_to_list_vectors ( batch ) self . _data . extend ( items ) except Exception as e : # noqa: BLE001 self . _pool_exceptions . append ( e ) def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch ) def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg ) def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = [] def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True )","title":"ListActivationStore"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__del__","text":"Delete Dunder Method. sparse_autoencoder/activation_store/list_store.py 312 313 314 315 def __del__ ( self ) -> None : \"\"\"Delete Dunder Method.\"\"\" if self . _pool : self . _pool . shutdown ( wait = False , cancel_futures = True )","title":"__del__()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__getitem__","text":"Get Item Dunder Method. Example: import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. sparse_autoencoder/activation_store/list_store.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. \"\"\" return self . _data [ index ]","title":"__getitem__()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__init__","text":"Initialize the List Activation Store. Parameters: data ( list [ InputOutputActivationVector ] | None , default: None ) \u2013 Data to initialize the dataset with. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. max_workers ( int | None , default: None ) \u2013 Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled ( bool , default: False ) \u2013 Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). sparse_autoencoder/activation_store/list_store.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def __init__ ( self , data : list [ InputOutputActivationVector ] | None = None , device : torch . device | None = None , max_workers : int | None = None , * , multiprocessing_enabled : bool = False , ) -> None : \"\"\"Initialize the List Activation Store. Args: data: Data to initialize the dataset with. device: Device to store the activation vectors on. max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have. multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it). \"\"\" # Default to empty if data is None : data = [] # If multiprocessing is enabled, use a multiprocessing manager to create a shared list # between processes. Otherwise, just use a normal list. if multiprocessing_enabled : self . _pool = ProcessPoolExecutor ( max_workers = max_workers ) manager = Manager () self . _data = manager . list ( data ) self . _data . extend ( data ) self . _pool_exceptions = manager . list () else : self . _data = data self . _pool_exceptions = [] self . _pool_futures = [] # Device for storing the activation vectors self . _device = device","title":"__init__()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__len__","text":"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/list_store.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return len ( self . _data )","title":"__len__()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__sizeof__","text":"Sizeof Dunder Method. Returns the size of the dataset in bytes. sparse_autoencoder/activation_store/list_store.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the dataset in bytes. \"\"\" # The list of tensors is really a list of pointers to tensors, so we need to account for # this as well as the size of the tensors themselves. list_of_pointers_size = self . _data . __sizeof__ () # Handle 0 items if len ( self . _data ) == 0 : return list_of_pointers_size # Otherwise, get the size of the first tensor first_tensor = self . _data [ 0 ] first_tensor_size = first_tensor . element_size () * first_tensor . nelement () num_tensors = len ( self . _data ) total_tensors_size = first_tensor_size * num_tensors return total_tensors_size + list_of_pointers_size","title":"__sizeof__()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.append","text":"Append a single item to the dataset. Note append is blocking . For better performance use extend instead with batches. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. sparse_autoencoder/activation_store/list_store.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def append ( self , item : InputOutputActivationVector ) -> Future | None : \"\"\"Append a single item to the dataset. Note **append is blocking**. For better performance use extend instead with batches. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 Args: item: The item to append to the dataset. \"\"\" self . _data . append ( item . to ( self . _device ))","title":"append()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.empty","text":"Empty the dataset. Example: import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 store.empty() len(store) 0 sparse_autoencoder/activation_store/list_store.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def empty ( self ) -> None : \"\"\"Empty the dataset. Example: >>> import torch >>> store = ListActivationStore() >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 >>> store.empty() >>> len(store) 0 \"\"\" self . wait_for_writes_to_complete () # Clearing a list like this works for both standard and multiprocessing lists self . _data [:] = []","title":"empty()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.extend","text":"Extend the dataset with multiple items (non-blocking). Example import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10 Parameters: batch ( SourceModelActivations ) \u2013 A batch of items to add to the dataset. sparse_autoencoder/activation_store/list_store.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def extend ( self , batch : SourceModelActivations ) -> Future | None : \"\"\"Extend the dataset with multiple items (non-blocking). Example: >>> import torch >>> store = ListActivationStore() >>> batch = torch.randn(10, 100) >>> async_result = store.extend(batch) >>> len(store) 10 Args: batch: A batch of items to add to the dataset. \"\"\" # Schedule _extend to run in a separate process if self . _pool : future = self . _pool . submit ( self . _extend , batch ) self . _pool_futures . append ( future ) # Fallback to synchronous execution if not multiprocessing self . _extend ( batch )","title":"extend()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.shuffle","text":"Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3 sparse_autoencoder/activation_store/list_store.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = ListActivationStore() >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.append(torch.tensor([3.])) >>> store.shuffle() >>> len(store) 3 \"\"\" self . wait_for_writes_to_complete () random . shuffle ( self . _data )","title":"shuffle()"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.wait_for_writes_to_complete","text":"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth: append ) to complete. Example: import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3 sparse_autoencoder/activation_store/list_store.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def wait_for_writes_to_complete ( self ) -> None : \"\"\"Wait for Writes to Complete. Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete. Example: >>> import torch >>> store = ListActivationStore(multiprocessing_enabled=True) >>> store.extend(torch.randn(3, 100)) >>> store.wait_for_writes_to_complete() >>> len(store) 3 \"\"\" # Restart the pool if self . _pool : for _future in as_completed ( self . _pool_futures ): pass self . _pool_futures . clear () time . sleep ( 1 ) if self . _pool_exceptions : exceptions_report = \" \\n \" . join ([ str ( e ) for e in self . _pool_exceptions ]) msg = f \"Exceptions occurred in background workers: \\n { exceptions_report } \" raise RuntimeError ( msg )","title":"wait_for_writes_to_complete()"},{"location":"reference/activation_store/tensor_store/","text":"Tensor Activation Store. TensorActivationStore Bases: ActivationStore Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/tensor_store.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class TensorActivationStore ( ActivationStore ): \"\"\"Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : StoreActivations \"\"\"Underlying Tensor Data Store.\"\"\" items_stored : int = 0 \"\"\"Number of items stored.\"\"\" max_items : int \"\"\"Maximum Number of Items to Store.\"\"\" def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement () def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ] def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0 items_stored : int = 0 class-attribute instance-attribute Number of items stored. max_items : int instance-attribute Maximum Number of Items to Store. __getitem__ ( index ) Get Item Dunder Method. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. Raises: IndexError \u2013 If the index is out of range. sparse_autoencoder/activation_store/tensor_store.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ] __init__ ( max_items , num_neurons , device = None ) Initialise the Tensor Activation Store. Parameters: max_items ( int ) \u2013 Maximum number of items to store (individual activation vectors) num_neurons ( int ) \u2013 Number of neurons in each activation vector. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. sparse_autoencoder/activation_store/tensor_store.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items __len__ () Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/tensor_store.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored __sizeof__ () Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=100) store. sizeof () # Pre-allocated tensor of 2x100 800 sparse_autoencoder/activation_store/tensor_store.py 100 101 102 103 104 105 106 107 108 109 110 111 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement () append ( item ) Add a single item to the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1 empty () Empty the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0 sparse_autoencoder/activation_store/tensor_store.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0 extend ( batch ) Add a batch to the store. Examples: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9 Parameters: batch ( SourceModelActivations ) \u2013 The batch to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors shuffle () Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] sparse_autoencoder/activation_store/tensor_store.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ]","title":"tensor_store"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore","text":"Bases: ActivationStore Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the torch.utils.data.Dataset class to provide a list-based activation store, with additional :meth: append and :meth: extend methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset before passing it to the DataLoader : >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) Source code in sparse_autoencoder/activation_store/tensor_store.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class TensorActivationStore ( ActivationStore ): \"\"\"Tensor Activation Store. Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe. Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking). Examples: Create an empty activation dataset: >>> import torch >>> store = TensorActivationStore(max_items=1000, num_neurons=100) Add a single activation vector to the dataset: >>> store.append(torch.randn(100)) >>> len(store) 1 Add a [batch, pos, neurons] activation tensor to the dataset: >>> store.empty() >>> batch = torch.randn(10, 10, 100) >>> store.extend(batch) >>> len(store) 100 Shuffle the dataset **before passing it to the DataLoader**: >>> store.shuffle() # Faster than using the DataLoader shuffle argument >>> loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2) Use the dataloader to iterate over the dataset: >>> next_item = next(iter(loader)) >>> next_item.shape torch.Size([2, 100]) \"\"\" _data : StoreActivations \"\"\"Underlying Tensor Data Store.\"\"\" items_stored : int = 0 \"\"\"Number of items stored.\"\"\" max_items : int \"\"\"Maximum Number of Items to Store.\"\"\" def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement () def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ] def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ] def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0","title":"TensorActivationStore"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.items_stored","text":"Number of items stored.","title":"items_stored"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.max_items","text":"Maximum Number of Items to Store.","title":"max_items"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__getitem__","text":"Get Item Dunder Method. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: index ( int ) \u2013 The index of the tensor to fetch. Returns: InputOutputActivationVector \u2013 The activation store item at the given index. Raises: IndexError \u2013 If the index is out of range. sparse_autoencoder/activation_store/tensor_store.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __getitem__ ( self , index : int ) -> InputOutputActivationVector : \"\"\"Get Item Dunder Method. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: index: The index of the tensor to fetch. Returns: The activation store item at the given index. Raises: IndexError: If the index is out of range. \"\"\" # Check in range if index >= self . items_stored : msg = f \"Index { index } out of range (only { self . items_stored } items stored)\" raise IndexError ( msg ) return self . _data [ index ]","title":"__getitem__()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__init__","text":"Initialise the Tensor Activation Store. Parameters: max_items ( int ) \u2013 Maximum number of items to store (individual activation vectors) num_neurons ( int ) \u2013 Number of neurons in each activation vector. device ( device | None , default: None ) \u2013 Device to store the activation vectors on. sparse_autoencoder/activation_store/tensor_store.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , max_items : int , num_neurons : int , device : torch . device | None = None , ) -> None : \"\"\"Initialise the Tensor Activation Store. Args: max_items: Maximum number of items to store (individual activation vectors) num_neurons: Number of neurons in each activation vector. device: Device to store the activation vectors on. \"\"\" self . _data = torch . empty (( max_items , num_neurons ), device = device ) self . _max_items = max_items","title":"__init__()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__len__","text":"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2 sparse_autoencoder/activation_store/tensor_store.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __len__ ( self ) -> int : \"\"\"Length Dunder Method. Returns the number of activation vectors in the dataset. Example: >>> import torch >>> store = TensorActivationStore(max_items=10_000_000, num_neurons=100) >>> store.append(torch.randn(100)) >>> store.append(torch.randn(100)) >>> len(store) 2 \"\"\" return self . items_stored","title":"__len__()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__sizeof__","text":"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: import torch store = TensorActivationStore(max_items=2, num_neurons=100) store. sizeof () # Pre-allocated tensor of 2x100 800 sparse_autoencoder/activation_store/tensor_store.py 100 101 102 103 104 105 106 107 108 109 110 111 def __sizeof__ ( self ) -> int : \"\"\"Sizeof Dunder Method. Returns the size of the underlying tensor in bytes. Example: >>> import torch >>> store = TensorActivationStore(max_items=2, num_neurons=100) >>> store.__sizeof__() # Pre-allocated tensor of 2x100 800 \"\"\" return self . _data . element_size () * self . _data . nelement ()","title":"__sizeof__()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.append","text":"Add a single item to the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.]) Parameters: item ( InputOutputActivationVector ) \u2013 The item to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def append ( self , item : InputOutputActivationVector ) -> None : \"\"\"Add a single item to the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.append(torch.zeros(5)) >>> store.append(torch.ones(5)) >>> store[1] tensor([1., 1., 1., 1., 1.]) Args: item: The item to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" # Check we have space if self . items_stored + 1 > self . _max_items : raise StoreFullError self . _data [ self . items_stored ] = item . to ( self . _data . device , ) self . items_stored += 1","title":"append()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.empty","text":"Empty the store. Example: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0 sparse_autoencoder/activation_store/tensor_store.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def empty ( self ) -> None : \"\"\"Empty the store. Example: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store.empty() >>> store.items_stored 0 \"\"\" # We don't need to zero the data, just reset the number of items stored self . items_stored = 0","title":"empty()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.extend","text":"Add a batch to the store. Examples: import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9 Parameters: batch ( SourceModelActivations ) \u2013 The batch to append to the dataset. Raises: IndexError \u2013 If there is no space remaining. sparse_autoencoder/activation_store/tensor_store.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def extend ( self , batch : SourceModelActivations ) -> None : \"\"\"Add a batch to the store. Examples: >>> import torch >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(2, 5)) >>> store.items_stored 2 >>> store = TensorActivationStore(max_items=10, num_neurons=5) >>> store.extend(torch.zeros(3, 3, 5)) >>> store.items_stored 9 Args: batch: The batch to append to the dataset. Raises: IndexError: If there is no space remaining. \"\"\" reshaped : InputOutputActivationBatch = resize_to_single_item_dimension ( batch , ) # Check we have space num_activation_tensors : int = reshaped . shape [ 0 ] if self . items_stored + num_activation_tensors > self . _max_items : if reshaped . shape [ 0 ] > self . _max_items : msg = f \"Single batch of { num_activation_tensors } activations is larger than the \\ total maximum in the store of { self . _max_items } .\" raise ValueError ( msg ) raise StoreFullError self . _data [ self . items_stored : self . items_stored + num_activation_tensors ] = reshaped . to ( self . _data . device ) self . items_stored += num_activation_tensors","title":"extend()"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.shuffle","text":"Shuffle the Data In-Place. This is much faster than using the shuffle argument on torch.utils.data.DataLoader . Example: import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] sparse_autoencoder/activation_store/tensor_store.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def shuffle ( self ) -> None : \"\"\"Shuffle the Data In-Place. This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`. Example: >>> import torch >>> _seed = torch.manual_seed(42) >>> store = TensorActivationStore(max_items=10, num_neurons=1) >>> store.append(torch.tensor([0.])) >>> store.append(torch.tensor([1.])) >>> store.append(torch.tensor([2.])) >>> store.shuffle() >>> [store[i].item() for i in range(3)] [0.0, 2.0, 1.0] \"\"\" # Generate a permutation of the indices for the active data perm = torch . randperm ( self . items_stored ) # Use this permutation to shuffle the active data in-place self . _data [: self . items_stored ] = self . _data [ perm ]","title":"shuffle()"},{"location":"reference/activation_store/utils/","text":"Activation Store Utils.","title":"Index"},{"location":"reference/activation_store/utils/extend_resize/","text":"Resize Tensors for Extend Methods. resize_to_list_vectors ( batched_tensor ) Resize Extend List Vectors. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a list of vectors each of size [neurons]. Examples: With 2 axis (e.g. pos neuron): import torch input = torch.rand(3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '3 items of shape torch.Size([100])' With 3 axis (e.g. batch, pos, neuron): input = torch.randn(3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '9 items of shape torch.Size([100])' With 4 axis (e.g. batch, pos, head_idx, neuron) input = torch.rand(3, 3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '27 items of shape torch.Size([100])' Parameters: batched_tensor ( SourceModelActivations ) \u2013 Input Activation Store Batch Returns: list [ InputOutputActivationVector ] \u2013 List of Activation Store Item Vectors sparse_autoencoder/activation_store/utils/extend_resize.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def resize_to_list_vectors ( batched_tensor : SourceModelActivations , ) -> list [ InputOutputActivationVector ]: \"\"\"Resize Extend List Vectors. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a list of vectors each of size [neurons]. Examples: With 2 axis (e.g. pos neuron): >>> import torch >>> input = torch.rand(3, 100) >>> res = resize_to_list_vectors(input) >>> f\"{len(res)} items of shape {res[0].shape}\" '3 items of shape torch.Size([100])' With 3 axis (e.g. batch, pos, neuron): >>> input = torch.randn(3, 3, 100) >>> res = resize_to_list_vectors(input) >>> f\"{len(res)} items of shape {res[0].shape}\" '9 items of shape torch.Size([100])' With 4 axis (e.g. batch, pos, head_idx, neuron) >>> input = torch.rand(3, 3, 3, 100) >>> res = resize_to_list_vectors(input) >>> f\"{len(res)} items of shape {res[0].shape}\" '27 items of shape torch.Size([100])' Args: batched_tensor: Input Activation Store Batch Returns: List of Activation Store Item Vectors \"\"\" rearranged : InputOutputActivationBatch = rearrange ( batched_tensor , \"... neurons -> (...) neurons\" , ) res = rearranged . unbind ( 0 ) return list ( res ) resize_to_single_item_dimension ( batch_activations ) Resize Extend Single Item Dimension. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons]. Examples: With 2 axis (e.g. pos neuron): import torch input = torch.rand(3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([3, 100]) With 3 axis (e.g. batch, pos, neuron): input = torch.randn(3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([9, 100]) With 4 axis (e.g. batch, pos, head_idx, neuron) input = torch.rand(3, 3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([27, 100]) Parameters: batch_activations ( SourceModelActivations ) \u2013 Input Activation Store Batch Returns: InputOutputActivationBatch \u2013 Single Tensor of Activation Store Items sparse_autoencoder/activation_store/utils/extend_resize.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def resize_to_single_item_dimension ( batch_activations : SourceModelActivations , ) -> InputOutputActivationBatch : \"\"\"Resize Extend Single Item Dimension. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons]. Examples: With 2 axis (e.g. pos neuron): >>> import torch >>> input = torch.rand(3, 100) >>> res = resize_to_single_item_dimension(input) >>> res.shape torch.Size([3, 100]) With 3 axis (e.g. batch, pos, neuron): >>> input = torch.randn(3, 3, 100) >>> res = resize_to_single_item_dimension(input) >>> res.shape torch.Size([9, 100]) With 4 axis (e.g. batch, pos, head_idx, neuron) >>> input = torch.rand(3, 3, 3, 100) >>> res = resize_to_single_item_dimension(input) >>> res.shape torch.Size([27, 100]) Args: batch_activations: Input Activation Store Batch Returns: Single Tensor of Activation Store Items \"\"\" return rearrange ( batch_activations , \"... neurons -> (...) neurons\" )","title":"extend_resize"},{"location":"reference/activation_store/utils/extend_resize/#sparse_autoencoder.activation_store.utils.extend_resize.resize_to_list_vectors","text":"Resize Extend List Vectors. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a list of vectors each of size [neurons]. Examples: With 2 axis (e.g. pos neuron): import torch input = torch.rand(3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '3 items of shape torch.Size([100])' With 3 axis (e.g. batch, pos, neuron): input = torch.randn(3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '9 items of shape torch.Size([100])' With 4 axis (e.g. batch, pos, head_idx, neuron) input = torch.rand(3, 3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '27 items of shape torch.Size([100])' Parameters: batched_tensor ( SourceModelActivations ) \u2013 Input Activation Store Batch Returns: list [ InputOutputActivationVector ] \u2013 List of Activation Store Item Vectors sparse_autoencoder/activation_store/utils/extend_resize.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def resize_to_list_vectors ( batched_tensor : SourceModelActivations , ) -> list [ InputOutputActivationVector ]: \"\"\"Resize Extend List Vectors. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a list of vectors each of size [neurons]. Examples: With 2 axis (e.g. pos neuron): >>> import torch >>> input = torch.rand(3, 100) >>> res = resize_to_list_vectors(input) >>> f\"{len(res)} items of shape {res[0].shape}\" '3 items of shape torch.Size([100])' With 3 axis (e.g. batch, pos, neuron): >>> input = torch.randn(3, 3, 100) >>> res = resize_to_list_vectors(input) >>> f\"{len(res)} items of shape {res[0].shape}\" '9 items of shape torch.Size([100])' With 4 axis (e.g. batch, pos, head_idx, neuron) >>> input = torch.rand(3, 3, 3, 100) >>> res = resize_to_list_vectors(input) >>> f\"{len(res)} items of shape {res[0].shape}\" '27 items of shape torch.Size([100])' Args: batched_tensor: Input Activation Store Batch Returns: List of Activation Store Item Vectors \"\"\" rearranged : InputOutputActivationBatch = rearrange ( batched_tensor , \"... neurons -> (...) neurons\" , ) res = rearranged . unbind ( 0 ) return list ( res )","title":"resize_to_list_vectors()"},{"location":"reference/activation_store/utils/extend_resize/#sparse_autoencoder.activation_store.utils.extend_resize.resize_to_single_item_dimension","text":"Resize Extend Single Item Dimension. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons]. Examples: With 2 axis (e.g. pos neuron): import torch input = torch.rand(3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([3, 100]) With 3 axis (e.g. batch, pos, neuron): input = torch.randn(3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([9, 100]) With 4 axis (e.g. batch, pos, head_idx, neuron) input = torch.rand(3, 3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([27, 100]) Parameters: batch_activations ( SourceModelActivations ) \u2013 Input Activation Store Batch Returns: InputOutputActivationBatch \u2013 Single Tensor of Activation Store Items sparse_autoencoder/activation_store/utils/extend_resize.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def resize_to_single_item_dimension ( batch_activations : SourceModelActivations , ) -> InputOutputActivationBatch : \"\"\"Resize Extend Single Item Dimension. Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons]. Examples: With 2 axis (e.g. pos neuron): >>> import torch >>> input = torch.rand(3, 100) >>> res = resize_to_single_item_dimension(input) >>> res.shape torch.Size([3, 100]) With 3 axis (e.g. batch, pos, neuron): >>> input = torch.randn(3, 3, 100) >>> res = resize_to_single_item_dimension(input) >>> res.shape torch.Size([9, 100]) With 4 axis (e.g. batch, pos, head_idx, neuron) >>> input = torch.rand(3, 3, 3, 100) >>> res = resize_to_single_item_dimension(input) >>> res.shape torch.Size([27, 100]) Args: batch_activations: Input Activation Store Batch Returns: Single Tensor of Activation Store Items \"\"\" return rearrange ( batch_activations , \"... neurons -> (...) neurons\" )","title":"resize_to_single_item_dimension()"},{"location":"reference/autoencoder/","text":"Autoencoder.","title":"Index"},{"location":"reference/autoencoder/model/","text":"The Sparse Autoencoder Model. SparseAutoencoder Bases: Module Sparse Autoencoder Model. Source code in sparse_autoencoder/autoencoder/model.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class SparseAutoencoder ( Module ): \"\"\"Sparse Autoencoder Model.\"\"\" geometric_median_dataset : InputOutputActivationVector \"\"\"Estimated Geometric Median of the Dataset. Used for initialising :attr:`tied_bias`. \"\"\" tied_bias : InputOutputActivationBatch \"\"\"Tied Bias Parameter. The same bias is used pre-encoder and post-decoder. \"\"\" n_input_features : int \"\"\"Number of Input Features.\"\"\" n_learned_features : int \"\"\"Number of Learned Features.\"\"\" device : torch . device | None \"\"\"Device to run the model on.\"\"\" dtype : torch . dtype | None \"\"\"Data type to use for the model.\"\"\" encoder : Sequential \"\"\"Encoder Module.\"\"\" decoder : Sequential \"\"\"Decoder Module.\"\"\" def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) ) def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype ) def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters () def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError decoder : Sequential = Sequential ( OrderedDict ({ 'ConstrainedUnitNormLinear' : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), 'TiedBias' : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER )})) instance-attribute Decoder Module. device : torch . device | None = device instance-attribute Device to run the model on. dtype : torch . dtype | None = dtype instance-attribute Data type to use for the model. encoder : Sequential = Sequential ( OrderedDict ({ 'TiedBias' : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), 'Linear' : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), 'ReLU' : ReLU ()})) instance-attribute Encoder Module. geometric_median_dataset : InputOutputActivationVector = geometric_median_dataset . clone () instance-attribute Estimated Geometric Median of the Dataset. Used for initialising :attr: tied_bias . n_input_features : int = n_input_features instance-attribute Number of Input Features. n_learned_features : int = n_learned_features instance-attribute Number of Learned Features. tied_bias : InputOutputActivationBatch = Parameter ( torch . empty ( n_input_features , device = device , dtype = dtype )) instance-attribute Tied Bias Parameter. The same bias is used pre-encoder and post-decoder. __init__ ( n_input_features , n_learned_features , geometric_median_dataset , device = None , dtype = None ) Initialize the Sparse Autoencoder Model. Parameters: n_input_features ( int ) \u2013 Number of input features (e.g. d_mlp if training on MLP activations from TransformerLens). n_learned_features ( int ) \u2013 Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset ( InputOutputActivationVector ) \u2013 Estimated geometric median of the dataset. device ( device | None , default: None ) \u2013 Device to run the model on. dtype ( dtype | None , default: None ) \u2013 Data type to use for the model. sparse_autoencoder/autoencoder/model.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) ) forward ( x ) Forward Pass. Parameters: x ( InputOutputActivationBatch ) \u2013 Input activations (e.g. activations from an MLP layer in a transformer model). Returns: tuple [ LearnedActivationBatch , InputOutputActivationBatch ] \u2013 Tuple of learned activations and decoded activations. sparse_autoencoder/autoencoder/model.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations initialize_tied_parameters () Initialize the tied parameters. sparse_autoencoder/autoencoder/model.py 127 128 129 130 131 132 def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype ) load_from_hf () Load the model from Hugging Face. sparse_autoencoder/autoencoder/model.py 145 146 147 def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError reset_parameters () Reset the parameters. sparse_autoencoder/autoencoder/model.py 134 135 136 137 138 139 def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters () save_to_hf () Save the model to Hugging Face. sparse_autoencoder/autoencoder/model.py 141 142 143 def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError","title":"model"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder","text":"Bases: Module Sparse Autoencoder Model. Source code in sparse_autoencoder/autoencoder/model.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class SparseAutoencoder ( Module ): \"\"\"Sparse Autoencoder Model.\"\"\" geometric_median_dataset : InputOutputActivationVector \"\"\"Estimated Geometric Median of the Dataset. Used for initialising :attr:`tied_bias`. \"\"\" tied_bias : InputOutputActivationBatch \"\"\"Tied Bias Parameter. The same bias is used pre-encoder and post-decoder. \"\"\" n_input_features : int \"\"\"Number of Input Features.\"\"\" n_learned_features : int \"\"\"Number of Learned Features.\"\"\" device : torch . device | None \"\"\"Device to run the model on.\"\"\" dtype : torch . dtype | None \"\"\"Data type to use for the model.\"\"\" encoder : Sequential \"\"\"Encoder Module.\"\"\" decoder : Sequential \"\"\"Decoder Module.\"\"\" def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) ) def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype ) def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters () def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError","title":"SparseAutoencoder"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.decoder","text":"Decoder Module.","title":"decoder"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.device","text":"Device to run the model on.","title":"device"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.dtype","text":"Data type to use for the model.","title":"dtype"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.encoder","text":"Encoder Module.","title":"encoder"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.geometric_median_dataset","text":"Estimated Geometric Median of the Dataset. Used for initialising :attr: tied_bias .","title":"geometric_median_dataset"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_input_features","text":"Number of Input Features.","title":"n_input_features"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_learned_features","text":"Number of Learned Features.","title":"n_learned_features"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.tied_bias","text":"Tied Bias Parameter. The same bias is used pre-encoder and post-decoder.","title":"tied_bias"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.__init__","text":"Initialize the Sparse Autoencoder Model. Parameters: n_input_features ( int ) \u2013 Number of input features (e.g. d_mlp if training on MLP activations from TransformerLens). n_learned_features ( int ) \u2013 Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset ( InputOutputActivationVector ) \u2013 Estimated geometric median of the dataset. device ( device | None , default: None ) \u2013 Device to run the model on. dtype ( dtype | None , default: None ) \u2013 Data type to use for the model. sparse_autoencoder/autoencoder/model.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , n_input_features : int , n_learned_features : int , geometric_median_dataset : InputOutputActivationVector , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the Sparse Autoencoder Model. Args: n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations from TransformerLens). n_learned_features: Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8. geometric_median_dataset: Estimated geometric median of the dataset. device: Device to run the model on. dtype: Data type to use for the model. \"\"\" super () . __init__ () self . n_input_features = n_input_features self . n_learned_features = n_learned_features self . device = device self . dtype = dtype # Store the geometric median of the dataset (so that we can reset parameters). This is not a # parameter itself (the tied bias parameter is used for that), so gradients are disabled. self . geometric_median_dataset = geometric_median_dataset . clone () self . geometric_median_dataset . requires_grad = False # Initialize the tied bias self . tied_bias = Parameter ( torch . empty (( n_input_features ), device = device , dtype = dtype )) self . initialize_tied_parameters () self . encoder = Sequential ( OrderedDict ( { \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . PRE_ENCODER ), \"Linear\" : Linear ( n_input_features , n_learned_features , bias = True , device = device , dtype = dtype ), \"ReLU\" : ReLU (), } ) ) self . decoder = Sequential ( OrderedDict ( { \"ConstrainedUnitNormLinear\" : ConstrainedUnitNormLinear ( n_learned_features , n_input_features , bias = False , device = device , dtype = dtype ), \"TiedBias\" : TiedBias ( self . tied_bias , TiedBiasPosition . POST_DECODER ), } ) )","title":"__init__()"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.forward","text":"Forward Pass. Parameters: x ( InputOutputActivationBatch ) \u2013 Input activations (e.g. activations from an MLP layer in a transformer model). Returns: tuple [ LearnedActivationBatch , InputOutputActivationBatch ] \u2013 Tuple of learned activations and decoded activations. sparse_autoencoder/autoencoder/model.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , x : InputOutputActivationBatch , ) -> tuple [ LearnedActivationBatch , InputOutputActivationBatch , ]: \"\"\"Forward Pass. Args: x: Input activations (e.g. activations from an MLP layer in a transformer model). Returns: Tuple of learned activations and decoded activations. \"\"\" learned_activations = self . encoder ( x ) decoded_activations = self . decoder ( learned_activations ) return learned_activations , decoded_activations","title":"forward()"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.initialize_tied_parameters","text":"Initialize the tied parameters. sparse_autoencoder/autoencoder/model.py 127 128 129 130 131 132 def initialize_tied_parameters ( self ) -> None : \"\"\"Initialize the tied parameters.\"\"\" # The tied bias is initialised as the geometric median of the dataset self . tied_bias . data = self . geometric_median_dataset . clone () . to ( device = self . device , dtype = self . dtype )","title":"initialize_tied_parameters()"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.load_from_hf","text":"Load the model from Hugging Face. sparse_autoencoder/autoencoder/model.py 145 146 147 def load_from_hf ( self ) -> None : \"\"\"Load the model from Hugging Face.\"\"\" raise NotImplementedError","title":"load_from_hf()"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.reset_parameters","text":"Reset the parameters. sparse_autoencoder/autoencoder/model.py 134 135 136 137 138 139 def reset_parameters ( self ) -> None : \"\"\"Reset the parameters.\"\"\" self . initialize_tied_parameters () for module in self . network : if \"reset_parameters\" in dir ( module ): module . reset_parameters ()","title":"reset_parameters()"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.save_to_hf","text":"Save the model to Hugging Face. sparse_autoencoder/autoencoder/model.py 141 142 143 def save_to_hf ( self ) -> None : \"\"\"Save the model to Hugging Face.\"\"\" raise NotImplementedError","title":"save_to_hf()"},{"location":"reference/autoencoder/components/","text":"Autoencoder Components.","title":"Index"},{"location":"reference/autoencoder/components/tied_bias/","text":"Tied Biases (Pre-Encoder and Post-Decoder). TiedBias Bases: Module Tied Bias Layer. The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding. The bias parameter must be initialised in the parent module, and then passed to this layer. https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias Source code in sparse_autoencoder/autoencoder/components/tied_bias.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class TiedBias ( Module ): \"\"\"Tied Bias Layer. The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding. The bias parameter must be initialised in the parent module, and then passed to this layer. https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias \"\"\" _bias_reference : InputOutputActivationVector _bias_position : TiedBiasPosition def __init__ ( self , bias : InputOutputActivationVector , position : TiedBiasPosition , ) -> None : \"\"\"Initialize the bias layer. Args: bias: Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset. position: Whether this is the pre-encoder or post-encoder bias. \"\"\" super () . __init__ () self . _bias_reference = bias # Support string literals as well as enums self . _bias_position = position def forward ( self , x : InputOutputActivationBatch , ) -> InputOutputActivationBatch : \"\"\"Forward Pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # If this is the pre-encoder bias, we subtract the bias from the input. if self . _bias_position == TiedBiasPosition . PRE_ENCODER : return x - self . _bias_reference # If it's the post-encoder bias, we add the bias to the input. return x + self . _bias_reference def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return f \"position= { self . _bias_position . value } \" __init__ ( bias , position ) Initialize the bias layer. Parameters: bias ( InputOutputActivationVector ) \u2013 Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset. position ( TiedBiasPosition ) \u2013 Whether this is the pre-encoder or post-encoder bias. sparse_autoencoder/autoencoder/components/tied_bias.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , bias : InputOutputActivationVector , position : TiedBiasPosition , ) -> None : \"\"\"Initialize the bias layer. Args: bias: Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset. position: Whether this is the pre-encoder or post-encoder bias. \"\"\" super () . __init__ () self . _bias_reference = bias # Support string literals as well as enums self . _bias_position = position extra_repr () String extra representation of the module. sparse_autoencoder/autoencoder/components/tied_bias.py 73 74 75 def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return f \"position= { self . _bias_position . value } \" forward ( x ) Forward Pass. Parameters: x ( InputOutputActivationBatch ) \u2013 Input tensor. Returns: InputOutputActivationBatch \u2013 Output of the forward pass. sparse_autoencoder/autoencoder/components/tied_bias.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def forward ( self , x : InputOutputActivationBatch , ) -> InputOutputActivationBatch : \"\"\"Forward Pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # If this is the pre-encoder bias, we subtract the bias from the input. if self . _bias_position == TiedBiasPosition . PRE_ENCODER : return x - self . _bias_reference # If it's the post-encoder bias, we add the bias to the input. return x + self . _bias_reference TiedBiasPosition Bases: str , Enum Tied Bias Position. Source code in sparse_autoencoder/autoencoder/components/tied_bias.py 12 13 14 15 16 class TiedBiasPosition ( str , Enum ): \"\"\"Tied Bias Position.\"\"\" PRE_ENCODER = \"pre_encoder\" POST_DECODER = \"post_decoder\"","title":"tied_bias"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias","text":"Bases: Module Tied Bias Layer. The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding. The bias parameter must be initialised in the parent module, and then passed to this layer. https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias Source code in sparse_autoencoder/autoencoder/components/tied_bias.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class TiedBias ( Module ): \"\"\"Tied Bias Layer. The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding. The bias parameter must be initialised in the parent module, and then passed to this layer. https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias \"\"\" _bias_reference : InputOutputActivationVector _bias_position : TiedBiasPosition def __init__ ( self , bias : InputOutputActivationVector , position : TiedBiasPosition , ) -> None : \"\"\"Initialize the bias layer. Args: bias: Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset. position: Whether this is the pre-encoder or post-encoder bias. \"\"\" super () . __init__ () self . _bias_reference = bias # Support string literals as well as enums self . _bias_position = position def forward ( self , x : InputOutputActivationBatch , ) -> InputOutputActivationBatch : \"\"\"Forward Pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # If this is the pre-encoder bias, we subtract the bias from the input. if self . _bias_position == TiedBiasPosition . PRE_ENCODER : return x - self . _bias_reference # If it's the post-encoder bias, we add the bias to the input. return x + self . _bias_reference def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return f \"position= { self . _bias_position . value } \"","title":"TiedBias"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.__init__","text":"Initialize the bias layer. Parameters: bias ( InputOutputActivationVector ) \u2013 Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset. position ( TiedBiasPosition ) \u2013 Whether this is the pre-encoder or post-encoder bias. sparse_autoencoder/autoencoder/components/tied_bias.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , bias : InputOutputActivationVector , position : TiedBiasPosition , ) -> None : \"\"\"Initialize the bias layer. Args: bias: Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset. position: Whether this is the pre-encoder or post-encoder bias. \"\"\" super () . __init__ () self . _bias_reference = bias # Support string literals as well as enums self . _bias_position = position","title":"__init__()"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.extra_repr","text":"String extra representation of the module. sparse_autoencoder/autoencoder/components/tied_bias.py 73 74 75 def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return f \"position= { self . _bias_position . value } \"","title":"extra_repr()"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.forward","text":"Forward Pass. Parameters: x ( InputOutputActivationBatch ) \u2013 Input tensor. Returns: InputOutputActivationBatch \u2013 Output of the forward pass. sparse_autoencoder/autoencoder/components/tied_bias.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def forward ( self , x : InputOutputActivationBatch , ) -> InputOutputActivationBatch : \"\"\"Forward Pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # If this is the pre-encoder bias, we subtract the bias from the input. if self . _bias_position == TiedBiasPosition . PRE_ENCODER : return x - self . _bias_reference # If it's the post-encoder bias, we add the bias to the input. return x + self . _bias_reference","title":"forward()"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBiasPosition","text":"Bases: str , Enum Tied Bias Position. Source code in sparse_autoencoder/autoencoder/components/tied_bias.py 12 13 14 15 16 class TiedBiasPosition ( str , Enum ): \"\"\"Tied Bias Position.\"\"\" PRE_ENCODER = \"pre_encoder\" POST_DECODER = \"post_decoder\"","title":"TiedBiasPosition"},{"location":"reference/autoencoder/components/unit_norm_linear/","text":"Linear layer with unit norm weights. ConstrainedUnitNormLinear Bases: Module Constrained unit norm linear decoder layer. Linear layer for autoencoders, where the dictionary vectors (rows of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. Motivation Unit norming the dictionary vectors, which are essentially the rows of the decoding matrices, serves a few purposes: 1. It helps with numerical stability, by preventing the dictionary vectors from growing too large. 2. It acts as a form of regularization, preventing overfitting by not allowing any one feature to dominate the representation. It limits the capacity of the model by forcing the dictionary vectors to live on the hypersphere of radius 1. 3. It encourages sparsity. Since the dictionary vectors have a fixed length, the model must carefully select which features to activate in order to best reconstruct the input. Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization). Source code in sparse_autoencoder/autoencoder/components/unit_norm_linear.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 class ConstrainedUnitNormLinear ( Module ): \"\"\"Constrained unit norm linear decoder layer. Linear layer for autoencoders, where the dictionary vectors (rows of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. Motivation: Unit norming the dictionary vectors, which are essentially the rows of the decoding matrices, serves a few purposes: 1. It helps with numerical stability, by preventing the dictionary vectors from growing too large. 2. It acts as a form of regularization, preventing overfitting by not allowing any one feature to dominate the representation. It limits the capacity of the model by forcing the dictionary vectors to live on the hypersphere of radius 1. 3. It encourages sparsity. Since the dictionary vectors have a fixed length, the model must carefully select which features to activate in order to best reconstruct the input. Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization). \"\"\" learnt_features : int \"\"\"Number of learnt features (inputs to this layer).\"\"\" decoded_features : int \"\"\"Number of decoded features (outputs from this layer).\"\"\" weight : DecoderWeights \"\"\"Weight parameter.\"\"\" bias : InputOutputActivationVector | None \"\"\"Bias parameter.\"\"\" def __init__ ( self , learnt_features : int , decoded_features : int , * , bias : bool = True , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the constrained unit norm linear layer. Args: learnt_features: Number of learnt features in the autoencoder. decoded_features: Number of decoded (output) features in the autoencoder. bias: Whether to include a bias term. device: Device to use. dtype: Data type to use. \"\"\" # Create the linear layer as per the standard PyTorch linear layer super () . __init__ () self . learnt_features = learnt_features self . decoded_features = decoded_features self . weight = Parameter ( torch . empty (( decoded_features , learnt_features ), device = device , dtype = dtype ) ) if bias : self . bias = Parameter ( torch . empty ( decoded_features , device = device , dtype = dtype )) else : self . register_parameter ( \"bias\" , None ) self . reset_parameters () # Register backward hook to remove any gradient information parallel to the dictionary # vectors (rows of the weight matrix) before applying the gradient step. self . weight . register_hook ( self . _weight_backward_hook ) def reset_parameters ( self ) -> None : \"\"\"Initialize or reset the parameters. Example: >>> import torch >>> # Create a layer with 4 columns (learnt features) and 3 rows (decoded features) >>> layer = ConstrainedUnitNormLinear(learnt_features=4, decoded_features=3) >>> layer.reset_parameters() >>> # Get the norm across the rows (by summing across the columns) >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming # normalisation here, since we immediately scale the weights to have unit norm (so the # initial standard deviation doesn't matter). Note also that `init.normal_` is in place. self . weight : EncoderWeights = init . normal_ ( self . weight , mean = 0 , std = 1 ) # Scale so that each row has unit norm with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) # Initialise the bias # This is the standard approach used in `torch.nn.Linear.reset_parameters` if self . bias is not None : fan_in = self . weight . size ( 1 ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) def _weight_backward_hook ( self , grad : EncoderWeights , ) -> EncoderWeights : \"\"\"Unit norm backward hook. By subtracting the projection of the gradient onto the dictionary vectors, we remove the component of the gradient that is parallel to the dictionary vectors and just keep the component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere). The result is that the backward pass does not change the norm of the dictionary vectors. Args: grad: Gradient with respect to the weights. \"\"\" # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can # be thought of as vectors that end on the circumference of a hypersphere. The projection of # the gradient onto the dictionary vectors is the component of the gradient that is parallel # to the dictionary vectors, i.e. the component that moves to or from the center of the # hypersphere. normalized_weight : EncoderWeights = self . weight / torch . norm ( self . weight , dim =- 1 , keepdim = True ) # Calculate the dot product of the gradients with the dictionary vectors. # This represents the component of the gradient parallel to each dictionary vector. # The result will be a tensor of shape [decoded_features]. dot_product = einops . einsum ( grad , normalized_weight , f \" { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } , \\ { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } \\ -> { Axis . LEARNT_FEATURE } \" , ) # Scale the normalized weights by the dot product to get the projection. # The result will be of the same shape as 'grad' and 'self.weight'. projection = einops . einsum ( dot_product , normalized_weight , f \" { Axis . LEARNT_FEATURE } , \\ { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } \\ -> { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } \" , ) # Subtracting the parallel component from the gradient leaves only the component that is # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of # the hypersphere. return grad - projection def constrain_weights_unit_norm ( self ) -> None : \"\"\"Constrain the weights to have unit norm. Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook :meth:`_weight_backward_hook` is applied. Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc). Example: >>> import torch >>> layer = ConstrainedUnitNormLinear(3, 3) >>> layer.weight.data = torch.ones((3, 3)) * 10 >>> layer.constrain_weights_unit_norm() >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) def forward ( self , x : Tensor ) -> Tensor : \"\"\"Forward pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # Prevent the drift of the dictionary vectors away from unit norm. This can happen even # though we remove the gradient information parallel to the dictionary vectors before # applying the gradient step, since optimisers such as Adam don't strictly follow the # gradient, but instead follow a modified gradient that includes momentum. self . constrain_weights_unit_norm () return torch . nn . functional . linear ( x , self . weight , self . bias ) def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return ( f \"in_features= { self . learnt_features } , out_features= { self . decoded_features } , \" f \"bias= { self . bias is not None } \" ) bias : InputOutputActivationVector | None instance-attribute Bias parameter. decoded_features : int = decoded_features instance-attribute Number of decoded features (outputs from this layer). learnt_features : int = learnt_features instance-attribute Number of learnt features (inputs to this layer). weight : DecoderWeights = Parameter ( torch . empty (( decoded_features , learnt_features ), device = device , dtype = dtype )) instance-attribute Weight parameter. __init__ ( learnt_features , decoded_features , * , bias = True , device = None , dtype = None ) Initialize the constrained unit norm linear layer. Parameters: learnt_features ( int ) \u2013 Number of learnt features in the autoencoder. decoded_features ( int ) \u2013 Number of decoded (output) features in the autoencoder. bias ( bool , default: True ) \u2013 Whether to include a bias term. device ( device | None , default: None ) \u2013 Device to use. dtype ( dtype | None , default: None ) \u2013 Data type to use. sparse_autoencoder/autoencoder/components/unit_norm_linear.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , learnt_features : int , decoded_features : int , * , bias : bool = True , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the constrained unit norm linear layer. Args: learnt_features: Number of learnt features in the autoencoder. decoded_features: Number of decoded (output) features in the autoencoder. bias: Whether to include a bias term. device: Device to use. dtype: Data type to use. \"\"\" # Create the linear layer as per the standard PyTorch linear layer super () . __init__ () self . learnt_features = learnt_features self . decoded_features = decoded_features self . weight = Parameter ( torch . empty (( decoded_features , learnt_features ), device = device , dtype = dtype ) ) if bias : self . bias = Parameter ( torch . empty ( decoded_features , device = device , dtype = dtype )) else : self . register_parameter ( \"bias\" , None ) self . reset_parameters () # Register backward hook to remove any gradient information parallel to the dictionary # vectors (rows of the weight matrix) before applying the gradient step. self . weight . register_hook ( self . _weight_backward_hook ) constrain_weights_unit_norm () Constrain the weights to have unit norm. Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook :meth: _weight_backward_hook is applied. Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc). Example import torch layer = ConstrainedUnitNormLinear(3, 3) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() row_norms = torch.sum(layer.weight ** 2, dim=1) row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] sparse_autoencoder/autoencoder/components/unit_norm_linear.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def constrain_weights_unit_norm ( self ) -> None : \"\"\"Constrain the weights to have unit norm. Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook :meth:`_weight_backward_hook` is applied. Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc). Example: >>> import torch >>> layer = ConstrainedUnitNormLinear(3, 3) >>> layer.weight.data = torch.ones((3, 3)) * 10 >>> layer.constrain_weights_unit_norm() >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) extra_repr () String extra representation of the module. sparse_autoencoder/autoencoder/components/unit_norm_linear.py 213 214 215 216 217 218 def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return ( f \"in_features= { self . learnt_features } , out_features= { self . decoded_features } , \" f \"bias= { self . bias is not None } \" ) forward ( x ) Forward pass. Parameters: x ( Tensor ) \u2013 Input tensor. Returns: Tensor \u2013 Output of the forward pass. sparse_autoencoder/autoencoder/components/unit_norm_linear.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def forward ( self , x : Tensor ) -> Tensor : \"\"\"Forward pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # Prevent the drift of the dictionary vectors away from unit norm. This can happen even # though we remove the gradient information parallel to the dictionary vectors before # applying the gradient step, since optimisers such as Adam don't strictly follow the # gradient, but instead follow a modified gradient that includes momentum. self . constrain_weights_unit_norm () return torch . nn . functional . linear ( x , self . weight , self . bias ) reset_parameters () Initialize or reset the parameters. Example import torch Create a layer with 4 columns (learnt features) and 3 rows (decoded features) layer = ConstrainedUnitNormLinear(learnt_features=4, decoded_features=3) layer.reset_parameters() Get the norm across the rows (by summing across the columns) row_norms = torch.sum(layer.weight ** 2, dim=1) row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] sparse_autoencoder/autoencoder/components/unit_norm_linear.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def reset_parameters ( self ) -> None : \"\"\"Initialize or reset the parameters. Example: >>> import torch >>> # Create a layer with 4 columns (learnt features) and 3 rows (decoded features) >>> layer = ConstrainedUnitNormLinear(learnt_features=4, decoded_features=3) >>> layer.reset_parameters() >>> # Get the norm across the rows (by summing across the columns) >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming # normalisation here, since we immediately scale the weights to have unit norm (so the # initial standard deviation doesn't matter). Note also that `init.normal_` is in place. self . weight : EncoderWeights = init . normal_ ( self . weight , mean = 0 , std = 1 ) # Scale so that each row has unit norm with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) # Initialise the bias # This is the standard approach used in `torch.nn.Linear.reset_parameters` if self . bias is not None : fan_in = self . weight . size ( 1 ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound )","title":"unit_norm_linear"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear","text":"Bases: Module Constrained unit norm linear decoder layer. Linear layer for autoencoders, where the dictionary vectors (rows of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. Motivation Unit norming the dictionary vectors, which are essentially the rows of the decoding matrices, serves a few purposes: 1. It helps with numerical stability, by preventing the dictionary vectors from growing too large. 2. It acts as a form of regularization, preventing overfitting by not allowing any one feature to dominate the representation. It limits the capacity of the model by forcing the dictionary vectors to live on the hypersphere of radius 1. 3. It encourages sparsity. Since the dictionary vectors have a fixed length, the model must carefully select which features to activate in order to best reconstruct the input. Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization). Source code in sparse_autoencoder/autoencoder/components/unit_norm_linear.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 class ConstrainedUnitNormLinear ( Module ): \"\"\"Constrained unit norm linear decoder layer. Linear layer for autoencoders, where the dictionary vectors (rows of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. Motivation: Unit norming the dictionary vectors, which are essentially the rows of the decoding matrices, serves a few purposes: 1. It helps with numerical stability, by preventing the dictionary vectors from growing too large. 2. It acts as a form of regularization, preventing overfitting by not allowing any one feature to dominate the representation. It limits the capacity of the model by forcing the dictionary vectors to live on the hypersphere of radius 1. 3. It encourages sparsity. Since the dictionary vectors have a fixed length, the model must carefully select which features to activate in order to best reconstruct the input. Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization). \"\"\" learnt_features : int \"\"\"Number of learnt features (inputs to this layer).\"\"\" decoded_features : int \"\"\"Number of decoded features (outputs from this layer).\"\"\" weight : DecoderWeights \"\"\"Weight parameter.\"\"\" bias : InputOutputActivationVector | None \"\"\"Bias parameter.\"\"\" def __init__ ( self , learnt_features : int , decoded_features : int , * , bias : bool = True , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the constrained unit norm linear layer. Args: learnt_features: Number of learnt features in the autoencoder. decoded_features: Number of decoded (output) features in the autoencoder. bias: Whether to include a bias term. device: Device to use. dtype: Data type to use. \"\"\" # Create the linear layer as per the standard PyTorch linear layer super () . __init__ () self . learnt_features = learnt_features self . decoded_features = decoded_features self . weight = Parameter ( torch . empty (( decoded_features , learnt_features ), device = device , dtype = dtype ) ) if bias : self . bias = Parameter ( torch . empty ( decoded_features , device = device , dtype = dtype )) else : self . register_parameter ( \"bias\" , None ) self . reset_parameters () # Register backward hook to remove any gradient information parallel to the dictionary # vectors (rows of the weight matrix) before applying the gradient step. self . weight . register_hook ( self . _weight_backward_hook ) def reset_parameters ( self ) -> None : \"\"\"Initialize or reset the parameters. Example: >>> import torch >>> # Create a layer with 4 columns (learnt features) and 3 rows (decoded features) >>> layer = ConstrainedUnitNormLinear(learnt_features=4, decoded_features=3) >>> layer.reset_parameters() >>> # Get the norm across the rows (by summing across the columns) >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming # normalisation here, since we immediately scale the weights to have unit norm (so the # initial standard deviation doesn't matter). Note also that `init.normal_` is in place. self . weight : EncoderWeights = init . normal_ ( self . weight , mean = 0 , std = 1 ) # Scale so that each row has unit norm with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) # Initialise the bias # This is the standard approach used in `torch.nn.Linear.reset_parameters` if self . bias is not None : fan_in = self . weight . size ( 1 ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) def _weight_backward_hook ( self , grad : EncoderWeights , ) -> EncoderWeights : \"\"\"Unit norm backward hook. By subtracting the projection of the gradient onto the dictionary vectors, we remove the component of the gradient that is parallel to the dictionary vectors and just keep the component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere). The result is that the backward pass does not change the norm of the dictionary vectors. Args: grad: Gradient with respect to the weights. \"\"\" # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can # be thought of as vectors that end on the circumference of a hypersphere. The projection of # the gradient onto the dictionary vectors is the component of the gradient that is parallel # to the dictionary vectors, i.e. the component that moves to or from the center of the # hypersphere. normalized_weight : EncoderWeights = self . weight / torch . norm ( self . weight , dim =- 1 , keepdim = True ) # Calculate the dot product of the gradients with the dictionary vectors. # This represents the component of the gradient parallel to each dictionary vector. # The result will be a tensor of shape [decoded_features]. dot_product = einops . einsum ( grad , normalized_weight , f \" { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } , \\ { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } \\ -> { Axis . LEARNT_FEATURE } \" , ) # Scale the normalized weights by the dot product to get the projection. # The result will be of the same shape as 'grad' and 'self.weight'. projection = einops . einsum ( dot_product , normalized_weight , f \" { Axis . LEARNT_FEATURE } , \\ { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } \\ -> { Axis . LEARNT_FEATURE } { Axis . INPUT_OUTPUT_FEATURE } \" , ) # Subtracting the parallel component from the gradient leaves only the component that is # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of # the hypersphere. return grad - projection def constrain_weights_unit_norm ( self ) -> None : \"\"\"Constrain the weights to have unit norm. Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook :meth:`_weight_backward_hook` is applied. Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc). Example: >>> import torch >>> layer = ConstrainedUnitNormLinear(3, 3) >>> layer.weight.data = torch.ones((3, 3)) * 10 >>> layer.constrain_weights_unit_norm() >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) def forward ( self , x : Tensor ) -> Tensor : \"\"\"Forward pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # Prevent the drift of the dictionary vectors away from unit norm. This can happen even # though we remove the gradient information parallel to the dictionary vectors before # applying the gradient step, since optimisers such as Adam don't strictly follow the # gradient, but instead follow a modified gradient that includes momentum. self . constrain_weights_unit_norm () return torch . nn . functional . linear ( x , self . weight , self . bias ) def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return ( f \"in_features= { self . learnt_features } , out_features= { self . decoded_features } , \" f \"bias= { self . bias is not None } \" )","title":"ConstrainedUnitNormLinear"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.bias","text":"Bias parameter.","title":"bias"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.decoded_features","text":"Number of decoded features (outputs from this layer).","title":"decoded_features"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.learnt_features","text":"Number of learnt features (inputs to this layer).","title":"learnt_features"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.weight","text":"Weight parameter.","title":"weight"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.__init__","text":"Initialize the constrained unit norm linear layer. Parameters: learnt_features ( int ) \u2013 Number of learnt features in the autoencoder. decoded_features ( int ) \u2013 Number of decoded (output) features in the autoencoder. bias ( bool , default: True ) \u2013 Whether to include a bias term. device ( device | None , default: None ) \u2013 Device to use. dtype ( dtype | None , default: None ) \u2013 Data type to use. sparse_autoencoder/autoencoder/components/unit_norm_linear.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , learnt_features : int , decoded_features : int , * , bias : bool = True , device : torch . device | None = None , dtype : torch . dtype | None = None , ) -> None : \"\"\"Initialize the constrained unit norm linear layer. Args: learnt_features: Number of learnt features in the autoencoder. decoded_features: Number of decoded (output) features in the autoencoder. bias: Whether to include a bias term. device: Device to use. dtype: Data type to use. \"\"\" # Create the linear layer as per the standard PyTorch linear layer super () . __init__ () self . learnt_features = learnt_features self . decoded_features = decoded_features self . weight = Parameter ( torch . empty (( decoded_features , learnt_features ), device = device , dtype = dtype ) ) if bias : self . bias = Parameter ( torch . empty ( decoded_features , device = device , dtype = dtype )) else : self . register_parameter ( \"bias\" , None ) self . reset_parameters () # Register backward hook to remove any gradient information parallel to the dictionary # vectors (rows of the weight matrix) before applying the gradient step. self . weight . register_hook ( self . _weight_backward_hook )","title":"__init__()"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.constrain_weights_unit_norm","text":"Constrain the weights to have unit norm. Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook :meth: _weight_backward_hook is applied. Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc). Example import torch layer = ConstrainedUnitNormLinear(3, 3) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() row_norms = torch.sum(layer.weight ** 2, dim=1) row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] sparse_autoencoder/autoencoder/components/unit_norm_linear.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def constrain_weights_unit_norm ( self ) -> None : \"\"\"Constrain the weights to have unit norm. Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook :meth:`_weight_backward_hook` is applied. Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc). Example: >>> import torch >>> layer = ConstrainedUnitNormLinear(3, 3) >>> layer.weight.data = torch.ones((3, 3)) * 10 >>> layer.constrain_weights_unit_norm() >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight )","title":"constrain_weights_unit_norm()"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.extra_repr","text":"String extra representation of the module. sparse_autoencoder/autoencoder/components/unit_norm_linear.py 213 214 215 216 217 218 def extra_repr ( self ) -> str : \"\"\"String extra representation of the module.\"\"\" return ( f \"in_features= { self . learnt_features } , out_features= { self . decoded_features } , \" f \"bias= { self . bias is not None } \" )","title":"extra_repr()"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.forward","text":"Forward pass. Parameters: x ( Tensor ) \u2013 Input tensor. Returns: Tensor \u2013 Output of the forward pass. sparse_autoencoder/autoencoder/components/unit_norm_linear.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def forward ( self , x : Tensor ) -> Tensor : \"\"\"Forward pass. Args: x: Input tensor. Returns: Output of the forward pass. \"\"\" # Prevent the drift of the dictionary vectors away from unit norm. This can happen even # though we remove the gradient information parallel to the dictionary vectors before # applying the gradient step, since optimisers such as Adam don't strictly follow the # gradient, but instead follow a modified gradient that includes momentum. self . constrain_weights_unit_norm () return torch . nn . functional . linear ( x , self . weight , self . bias )","title":"forward()"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.reset_parameters","text":"Initialize or reset the parameters. Example import torch","title":"reset_parameters()"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features","text":"layer = ConstrainedUnitNormLinear(learnt_features=4, decoded_features=3) layer.reset_parameters()","title":"Create a layer with 4 columns (learnt features) and 3 rows (decoded features)"},{"location":"reference/autoencoder/components/unit_norm_linear/#sparse_autoencoder.autoencoder.components.unit_norm_linear.ConstrainedUnitNormLinear.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns","text":"row_norms = torch.sum(layer.weight ** 2, dim=1) row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] sparse_autoencoder/autoencoder/components/unit_norm_linear.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def reset_parameters ( self ) -> None : \"\"\"Initialize or reset the parameters. Example: >>> import torch >>> # Create a layer with 4 columns (learnt features) and 3 rows (decoded features) >>> layer = ConstrainedUnitNormLinear(learnt_features=4, decoded_features=3) >>> layer.reset_parameters() >>> # Get the norm across the rows (by summing across the columns) >>> row_norms = torch.sum(layer.weight ** 2, dim=1) >>> row_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0] \"\"\" # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming # normalisation here, since we immediately scale the weights to have unit norm (so the # initial standard deviation doesn't matter). Note also that `init.normal_` is in place. self . weight : EncoderWeights = init . normal_ ( self . weight , mean = 0 , std = 1 ) # Scale so that each row has unit norm with torch . no_grad (): torch . nn . functional . normalize ( self . weight , dim =- 1 , out = self . weight ) # Initialise the bias # This is the standard approach used in `torch.nn.Linear.reset_parameters` if self . bias is not None : fan_in = self . weight . size ( 1 ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound )","title":"Get the norm across the rows (by summing across the columns)"},{"location":"reference/loss/","text":"Loss Modules. Loss modules are specialised PyTorch modules that calculate the loss for a Sparse Autoencoder. They all inherit from AbstractLoss, which defines the interface for loss modules and some common methods. If you want to create your own loss function, see :class: AbstractLoss . For combining multiple loss modules into a single loss module, see :class: LossReducer . LossLogType : TypeAlias = dict [ str , int | float | str ] module-attribute Loss log dict. AbstractLoss Bases: Module , ABC Abstract loss interface. Interface for implementing batch itemwise loss functions. Source code in sparse_autoencoder/loss/abstract_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class AbstractLoss ( Module , ABC ): \"\"\"Abstract loss interface. Interface for implementing batch itemwise loss functions. \"\"\" _modules : dict [ str , \"AbstractLoss\" ] # type: ignore[assignment] (narrowing) \"\"\"Children loss modules.\"\"\" @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction ) __call__ ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction ) batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: ItemTensor \u2013 Loss for the batch. sparse_autoencoder/loss/abstract_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics forward ( source_activations , learned_activations , decoded_activations ) abstractmethod Batch itemwise loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/abstract_loss.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError LearnedActivationsL1Loss Bases: AbstractLoss Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations) Returns loss and metrics to log l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) Source code in sparse_autoencoder/loss/learned_activations_l1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @final class LearnedActivationsL1Loss ( AbstractLoss ): \"\"\"Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example: >>> l1_loss = LearnedActivationsL1Loss(0.1) >>> learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) >>> unused_activations = torch.zeros_like(learned_activations) >>> # Returns loss and metrics to log >>> l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) \"\"\" l1_coefficient : float \"\"\"L1 coefficient.\"\"\" def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \" l1_coefficient : float = l1_coefficient instance-attribute L1 coefficient. __init__ ( l1_coefficient ) Initialize the absolute error loss. Parameters: l1_coefficient ( float ) \u2013 L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. sparse_autoencoder/loss/learned_activations_l1.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () extra_repr () Extra representation string. sparse_autoencoder/loss/learned_activations_l1.py 66 67 68 def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \" forward ( source_activations , learned_activations , decoded_activations ) Learned activations L1 (absolute error) loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/learned_activations_l1.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient LossReducer Bases: AbstractLoss Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) Source code in sparse_autoencoder/loss/reducer.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @final class LossReducer ( AbstractLoss ): \"\"\"Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example: >>> from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss >>> from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss >>> LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) \"\"\" _modules : dict [ str , \"AbstractLoss\" ] \"\"\"Children loss modules.\"\"\" def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules ) __dir__ () Dir dunder method. sparse_autoencoder/loss/reducer.py 88 89 90 def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) __getitem__ ( idx ) Get item dunder method. sparse_autoencoder/loss/reducer.py 92 93 94 def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] __init__ ( * loss_modules ) Initialize the loss reducer. Parameters: loss_modules ( AbstractLoss , default: () ) \u2013 Loss modules to reduce. Raises: ValueError \u2013 If the loss reducer has no loss modules. sparse_autoencoder/loss/reducer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) __iter__ () Iterator dunder method. sparse_autoencoder/loss/reducer.py 96 97 98 def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) __len__ () Length dunder method. sparse_autoencoder/loss/reducer.py 100 101 102 def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules ) forward ( source_activations , learned_activations , decoded_activations ) Reduce loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: ItemTensor \u2013 Mean loss across the batch, summed across the loss modules. sparse_autoencoder/loss/reducer.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) LossReductionType Bases: LowercaseStrEnum Loss reduction type (across batch items). Source code in sparse_autoencoder/loss/abstract_loss.py 17 18 19 20 21 22 23 24 class LossReductionType ( LowercaseStrEnum ): \"\"\"Loss reduction type (across batch items).\"\"\" MEAN = \"mean\" \"\"\"Mean loss across batch items.\"\"\" SUM = \"sum\" \"\"\"Sum the loss from all batch items.\"\"\" MEAN = 'mean' class-attribute instance-attribute Mean loss across batch items. SUM = 'sum' class-attribute instance-attribute Sum the loss from all batch items. MSEReconstructionLoss Bases: AbstractLoss MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example import torch loss = MSEReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations) Outputs both loss and metrics to log loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) Source code in sparse_autoencoder/loss/mse_reconstruction_loss.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @final class MSEReconstructionLoss ( AbstractLoss ): \"\"\"MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example: >>> import torch >>> loss = MSEReconstructionLoss() >>> input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) >>> output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) >>> unused_activations = torch.zeros_like(input_activations) >>> # Outputs both loss and metrics to log >>> loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) \"\"\" def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 ) forward ( source_activations , learned_activations , decoded_activations ) MSE Reconstruction loss (mean across features dimension). Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/mse_reconstruction_loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"Index"},{"location":"reference/loss/#sparse_autoencoder.loss.LossLogType","text":"Loss log dict.","title":"LossLogType"},{"location":"reference/loss/#sparse_autoencoder.loss.AbstractLoss","text":"Bases: Module , ABC Abstract loss interface. Interface for implementing batch itemwise loss functions. Source code in sparse_autoencoder/loss/abstract_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class AbstractLoss ( Module , ABC ): \"\"\"Abstract loss interface. Interface for implementing batch itemwise loss functions. \"\"\" _modules : dict [ str , \"AbstractLoss\" ] # type: ignore[assignment] (narrowing) \"\"\"Children loss modules.\"\"\" @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction )","title":"AbstractLoss"},{"location":"reference/loss/#sparse_autoencoder.loss.AbstractLoss.__call__","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction )","title":"__call__()"},{"location":"reference/loss/#sparse_autoencoder.loss.AbstractLoss.batch_scalar_loss","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: ItemTensor \u2013 Loss for the batch. sparse_autoencoder/loss/abstract_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze ()","title":"batch_scalar_loss()"},{"location":"reference/loss/#sparse_autoencoder.loss.AbstractLoss.batch_scalar_loss_with_log","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics","title":"batch_scalar_loss_with_log()"},{"location":"reference/loss/#sparse_autoencoder.loss.AbstractLoss.forward","text":"Batch itemwise loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/abstract_loss.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"reference/loss/#sparse_autoencoder.loss.LearnedActivationsL1Loss","text":"Bases: AbstractLoss Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)","title":"LearnedActivationsL1Loss"},{"location":"reference/loss/#sparse_autoencoder.loss.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","text":"l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) Source code in sparse_autoencoder/loss/learned_activations_l1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @final class LearnedActivationsL1Loss ( AbstractLoss ): \"\"\"Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example: >>> l1_loss = LearnedActivationsL1Loss(0.1) >>> learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) >>> unused_activations = torch.zeros_like(learned_activations) >>> # Returns loss and metrics to log >>> l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) \"\"\" l1_coefficient : float \"\"\"L1 coefficient.\"\"\" def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \"","title":"Returns loss and metrics to log"},{"location":"reference/loss/#sparse_autoencoder.loss.LearnedActivationsL1Loss.l1_coefficient","text":"L1 coefficient.","title":"l1_coefficient"},{"location":"reference/loss/#sparse_autoencoder.loss.LearnedActivationsL1Loss.__init__","text":"Initialize the absolute error loss. Parameters: l1_coefficient ( float ) \u2013 L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. sparse_autoencoder/loss/learned_activations_l1.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ ()","title":"__init__()"},{"location":"reference/loss/#sparse_autoencoder.loss.LearnedActivationsL1Loss.extra_repr","text":"Extra representation string. sparse_autoencoder/loss/learned_activations_l1.py 66 67 68 def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \"","title":"extra_repr()"},{"location":"reference/loss/#sparse_autoencoder.loss.LearnedActivationsL1Loss.forward","text":"Learned activations L1 (absolute error) loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/learned_activations_l1.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient","title":"forward()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer","text":"Bases: AbstractLoss Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) Source code in sparse_autoencoder/loss/reducer.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @final class LossReducer ( AbstractLoss ): \"\"\"Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example: >>> from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss >>> from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss >>> LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) \"\"\" _modules : dict [ str , \"AbstractLoss\" ] \"\"\"Children loss modules.\"\"\" def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules )","title":"LossReducer"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer.__dir__","text":"Dir dunder method. sparse_autoencoder/loss/reducer.py 88 89 90 def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ())","title":"__dir__()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer.__getitem__","text":"Get item dunder method. sparse_autoencoder/loss/reducer.py 92 93 94 def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )]","title":"__getitem__()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer.__init__","text":"Initialize the loss reducer. Parameters: loss_modules ( AbstractLoss , default: () ) \u2013 Loss modules to reduce. Raises: ValueError \u2013 If the loss reducer has no loss modules. sparse_autoencoder/loss/reducer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message )","title":"__init__()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer.__iter__","text":"Iterator dunder method. sparse_autoencoder/loss/reducer.py 96 97 98 def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ())","title":"__iter__()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer.__len__","text":"Length dunder method. sparse_autoencoder/loss/reducer.py 100 101 102 def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules )","title":"__len__()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReducer.forward","text":"Reduce loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: ItemTensor \u2013 Mean loss across the batch, summed across the loss modules. sparse_autoencoder/loss/reducer.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 )","title":"forward()"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReductionType","text":"Bases: LowercaseStrEnum Loss reduction type (across batch items). Source code in sparse_autoencoder/loss/abstract_loss.py 17 18 19 20 21 22 23 24 class LossReductionType ( LowercaseStrEnum ): \"\"\"Loss reduction type (across batch items).\"\"\" MEAN = \"mean\" \"\"\"Mean loss across batch items.\"\"\" SUM = \"sum\" \"\"\"Sum the loss from all batch items.\"\"\"","title":"LossReductionType"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReductionType.MEAN","text":"Mean loss across batch items.","title":"MEAN"},{"location":"reference/loss/#sparse_autoencoder.loss.LossReductionType.SUM","text":"Sum the loss from all batch items.","title":"SUM"},{"location":"reference/loss/#sparse_autoencoder.loss.MSEReconstructionLoss","text":"Bases: AbstractLoss MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example import torch loss = MSEReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)","title":"MSEReconstructionLoss"},{"location":"reference/loss/#sparse_autoencoder.loss.MSEReconstructionLoss--outputs-both-loss-and-metrics-to-log","text":"loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) Source code in sparse_autoencoder/loss/mse_reconstruction_loss.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @final class MSEReconstructionLoss ( AbstractLoss ): \"\"\"MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example: >>> import torch >>> loss = MSEReconstructionLoss() >>> input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) >>> output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) >>> unused_activations = torch.zeros_like(input_activations) >>> # Outputs both loss and metrics to log >>> loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) \"\"\" def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"Outputs both loss and metrics to log"},{"location":"reference/loss/#sparse_autoencoder.loss.MSEReconstructionLoss.forward","text":"MSE Reconstruction loss (mean across features dimension). Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/mse_reconstruction_loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"forward()"},{"location":"reference/loss/abstract_loss/","text":"Abstract loss. LossLogType : TypeAlias = dict [ str , int | float | str ] module-attribute Loss log dict. AbstractLoss Bases: Module , ABC Abstract loss interface. Interface for implementing batch itemwise loss functions. Source code in sparse_autoencoder/loss/abstract_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class AbstractLoss ( Module , ABC ): \"\"\"Abstract loss interface. Interface for implementing batch itemwise loss functions. \"\"\" _modules : dict [ str , \"AbstractLoss\" ] # type: ignore[assignment] (narrowing) \"\"\"Children loss modules.\"\"\" @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction ) __call__ ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction ) batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: ItemTensor \u2013 Loss for the batch. sparse_autoencoder/loss/abstract_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = LossReductionType . MEAN ) Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics forward ( source_activations , learned_activations , decoded_activations ) abstractmethod Batch itemwise loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/abstract_loss.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError LossReductionType Bases: LowercaseStrEnum Loss reduction type (across batch items). Source code in sparse_autoencoder/loss/abstract_loss.py 17 18 19 20 21 22 23 24 class LossReductionType ( LowercaseStrEnum ): \"\"\"Loss reduction type (across batch items).\"\"\" MEAN = \"mean\" \"\"\"Mean loss across batch items.\"\"\" SUM = \"sum\" \"\"\"Sum the loss from all batch items.\"\"\" MEAN = 'mean' class-attribute instance-attribute Mean loss across batch items. SUM = 'sum' class-attribute instance-attribute Sum the loss from all batch items.","title":"abstract_loss"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossLogType","text":"Loss log dict.","title":"LossLogType"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss","text":"Bases: Module , ABC Abstract loss interface. Interface for implementing batch itemwise loss functions. Source code in sparse_autoencoder/loss/abstract_loss.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class AbstractLoss ( Module , ABC ): \"\"\"Abstract loss interface. Interface for implementing batch itemwise loss functions. \"\"\" _modules : dict [ str , \"AbstractLoss\" ] # type: ignore[assignment] (narrowing) \"\"\"Children loss modules.\"\"\" @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze () @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction )","title":"AbstractLoss"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.__call__","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 @final def __call__ ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" return self . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction )","title":"__call__()"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.batch_scalar_loss","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: ItemTensor \u2013 Loss for the batch. sparse_autoencoder/loss/abstract_loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @final def batch_scalar_loss ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> ItemTensor : \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Loss for the batch. \"\"\" itemwise_loss = self . forward ( source_activations , learned_activations , decoded_activations ) match reduction : case LossReductionType . MEAN : return itemwise_loss . mean () . squeeze () case LossReductionType . SUM : return itemwise_loss . sum () . squeeze ()","title":"batch_scalar_loss()"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.batch_scalar_loss_with_log","text":"Batch scalar loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. reduction ( LossReductionType , default: MEAN ) \u2013 Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: tuple [ ItemTensor , LossLogType ] \u2013 Tuple of the batch scalar loss and a dict of any properties to log. sparse_autoencoder/loss/abstract_loss.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @final def batch_scalar_loss_with_log ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , reduction : LossReductionType = LossReductionType . MEAN , ) -> tuple [ ItemTensor , LossLogType ]: \"\"\"Batch scalar loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size. Returns: Tuple of the batch scalar loss and a dict of any properties to log. \"\"\" children_loss_scalars : list [ ItemTensor ] = [] metrics : LossLogType = {} # If the loss module has children (e.g. it is a reducer): if len ( self . _modules ) > 0 : for loss_module in self . _modules . values (): child_loss , child_metrics = loss_module . batch_scalar_loss_with_log ( source_activations , learned_activations , decoded_activations , reduction = reduction , ) children_loss_scalars . append ( child_loss ) metrics . update ( child_metrics ) # Get the total loss & metric current_module_loss = torch . stack ( children_loss_scalars ) . sum () # Otherwise if it is a leaf loss module: else : current_module_loss = self . batch_scalar_loss ( source_activations , learned_activations , decoded_activations , reduction ) # Add in the current loss module's metric class_name = self . __class__ . __name__ metrics [ class_name ] = current_module_loss . detach () . cpu () . item () return current_module_loss , metrics","title":"batch_scalar_loss_with_log()"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.forward","text":"Batch itemwise loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/abstract_loss.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @abstractmethod def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"Batch itemwise loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType","text":"Bases: LowercaseStrEnum Loss reduction type (across batch items). Source code in sparse_autoencoder/loss/abstract_loss.py 17 18 19 20 21 22 23 24 class LossReductionType ( LowercaseStrEnum ): \"\"\"Loss reduction type (across batch items).\"\"\" MEAN = \"mean\" \"\"\"Mean loss across batch items.\"\"\" SUM = \"sum\" \"\"\"Sum the loss from all batch items.\"\"\"","title":"LossReductionType"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN","text":"Mean loss across batch items.","title":"MEAN"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.SUM","text":"Sum the loss from all batch items.","title":"SUM"},{"location":"reference/loss/learned_activations_l1/","text":"Learned activations L1 (absolute error) loss. LearnedActivationsL1Loss Bases: AbstractLoss Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations) Returns loss and metrics to log l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) Source code in sparse_autoencoder/loss/learned_activations_l1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @final class LearnedActivationsL1Loss ( AbstractLoss ): \"\"\"Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example: >>> l1_loss = LearnedActivationsL1Loss(0.1) >>> learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) >>> unused_activations = torch.zeros_like(learned_activations) >>> # Returns loss and metrics to log >>> l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) \"\"\" l1_coefficient : float \"\"\"L1 coefficient.\"\"\" def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \" l1_coefficient : float = l1_coefficient instance-attribute L1 coefficient. __init__ ( l1_coefficient ) Initialize the absolute error loss. Parameters: l1_coefficient ( float ) \u2013 L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. sparse_autoencoder/loss/learned_activations_l1.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () extra_repr () Extra representation string. sparse_autoencoder/loss/learned_activations_l1.py 66 67 68 def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \" forward ( source_activations , learned_activations , decoded_activations ) Learned activations L1 (absolute error) loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/learned_activations_l1.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient","title":"learned_activations_l1"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss","text":"Bases: AbstractLoss Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)","title":"LearnedActivationsL1Loss"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","text":"l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) Source code in sparse_autoencoder/loss/learned_activations_l1.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @final class LearnedActivationsL1Loss ( AbstractLoss ): \"\"\"Learned activations L1 (absolute error) loss. L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity). Example: >>> l1_loss = LearnedActivationsL1Loss(0.1) >>> learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) >>> unused_activations = torch.zeros_like(learned_activations) >>> # Returns loss and metrics to log >>> l1_loss(unused_activations, learned_activations, unused_activations) (tensor(0.5000), {'LearnedActivationsL1Loss': 0.5}) \"\"\" l1_coefficient : float \"\"\"L1 coefficient.\"\"\" def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ () def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \"","title":"Returns loss and metrics to log"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.l1_coefficient","text":"L1 coefficient.","title":"l1_coefficient"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.__init__","text":"Initialize the absolute error loss. Parameters: l1_coefficient ( float ) \u2013 L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. sparse_autoencoder/loss/learned_activations_l1.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , l1_coefficient : float ) -> None : \"\"\"Initialize the absolute error loss. Args: l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient. \"\"\" self . l1_coefficient = l1_coefficient super () . __init__ ()","title":"__init__()"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.extra_repr","text":"Extra representation string. sparse_autoencoder/loss/learned_activations_l1.py 66 67 68 def extra_repr ( self ) -> str : \"\"\"Extra representation string.\"\"\" return f \"l1_coefficient= { self . l1_coefficient } \"","title":"extra_repr()"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.forward","text":"Learned activations L1 (absolute error) loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/learned_activations_l1.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def forward ( self , source_activations : InputOutputActivationBatch , # noqa: ARG002 learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , # noqa: ARG002 ) -> TrainBatchStatistic : \"\"\"Learned activations L1 (absolute error) loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" absolute_loss = torch . abs ( learned_activations ) return absolute_loss . sum ( dim =- 1 ) * self . l1_coefficient","title":"forward()"},{"location":"reference/loss/mse_reconstruction_loss/","text":"MSE Reconstruction loss. MSEReconstructionLoss Bases: AbstractLoss MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example import torch loss = MSEReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations) Outputs both loss and metrics to log loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) Source code in sparse_autoencoder/loss/mse_reconstruction_loss.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @final class MSEReconstructionLoss ( AbstractLoss ): \"\"\"MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example: >>> import torch >>> loss = MSEReconstructionLoss() >>> input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) >>> output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) >>> unused_activations = torch.zeros_like(input_activations) >>> # Outputs both loss and metrics to log >>> loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) \"\"\" def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 ) forward ( source_activations , learned_activations , decoded_activations ) MSE Reconstruction loss (mean across features dimension). Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/mse_reconstruction_loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"mse_reconstruction_loss"},{"location":"reference/loss/mse_reconstruction_loss/#sparse_autoencoder.loss.mse_reconstruction_loss.MSEReconstructionLoss","text":"Bases: AbstractLoss MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example import torch loss = MSEReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)","title":"MSEReconstructionLoss"},{"location":"reference/loss/mse_reconstruction_loss/#sparse_autoencoder.loss.mse_reconstruction_loss.MSEReconstructionLoss--outputs-both-loss-and-metrics-to-log","text":"loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) Source code in sparse_autoencoder/loss/mse_reconstruction_loss.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @final class MSEReconstructionLoss ( AbstractLoss ): \"\"\"MSE Reconstruction loss. MSE reconstruction loss is calculated as the mean squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with MSE may achieve the same loss for both polysemantic and monosemantic representations of true features. Example: >>> import torch >>> loss = MSEReconstructionLoss() >>> input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) >>> output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) >>> unused_activations = torch.zeros_like(input_activations) >>> # Outputs both loss and metrics to log >>> loss(input_activations, unused_activations, output_activations) (tensor(5.5000), {'MSEReconstructionLoss': 5.5}) \"\"\" def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"Outputs both loss and metrics to log"},{"location":"reference/loss/mse_reconstruction_loss/#sparse_autoencoder.loss.mse_reconstruction_loss.MSEReconstructionLoss.forward","text":"MSE Reconstruction loss (mean across features dimension). Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: TrainBatchStatistic \u2013 Loss per batch item. sparse_autoencoder/loss/mse_reconstruction_loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , # noqa: ARG002 decoded_activations : InputOutputActivationBatch , ) -> TrainBatchStatistic : \"\"\"MSE Reconstruction loss (mean across features dimension). Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Loss per batch item. \"\"\" square_error_loss = mse_loss ( source_activations , decoded_activations , reduction = \"none\" ) # Mean over just the features dimension (i.e. batch itemwise loss) return square_error_loss . mean ( dim =- 1 )","title":"forward()"},{"location":"reference/loss/reducer/","text":"Loss reducer. LossReducer Bases: AbstractLoss Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) Source code in sparse_autoencoder/loss/reducer.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @final class LossReducer ( AbstractLoss ): \"\"\"Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example: >>> from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss >>> from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss >>> LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) \"\"\" _modules : dict [ str , \"AbstractLoss\" ] \"\"\"Children loss modules.\"\"\" def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules ) __dir__ () Dir dunder method. sparse_autoencoder/loss/reducer.py 88 89 90 def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) __getitem__ ( idx ) Get item dunder method. sparse_autoencoder/loss/reducer.py 92 93 94 def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] __init__ ( * loss_modules ) Initialize the loss reducer. Parameters: loss_modules ( AbstractLoss , default: () ) \u2013 Loss modules to reduce. Raises: ValueError \u2013 If the loss reducer has no loss modules. sparse_autoencoder/loss/reducer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) __iter__ () Iterator dunder method. sparse_autoencoder/loss/reducer.py 96 97 98 def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) __len__ () Length dunder method. sparse_autoencoder/loss/reducer.py 100 101 102 def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules ) forward ( source_activations , learned_activations , decoded_activations ) Reduce loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: ItemTensor \u2013 Mean loss across the batch, summed across the loss modules. sparse_autoencoder/loss/reducer.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 )","title":"reducer"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer","text":"Bases: AbstractLoss Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) Source code in sparse_autoencoder/loss/reducer.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @final class LossReducer ( AbstractLoss ): \"\"\"Loss reducer. Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential. Example: >>> from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss >>> from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss >>> LossReducer( ... MSEReconstructionLoss(), ... LearnedActivationsL1Loss(0.001), ... ) LossReducer( (0): MSEReconstructionLoss() (1): LearnedActivationsL1Loss(l1_coefficient=0.001) ) \"\"\" _modules : dict [ str , \"AbstractLoss\" ] \"\"\"Children loss modules.\"\"\" def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message ) def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 ) def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ()) def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )] def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ()) def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules )","title":"LossReducer"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__dir__","text":"Dir dunder method. sparse_autoencoder/loss/reducer.py 88 89 90 def __dir__ ( self ) -> list [ str ]: \"\"\"Dir dunder method.\"\"\" return list ( self . _modules . __dir__ ())","title":"__dir__()"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__getitem__","text":"Get item dunder method. sparse_autoencoder/loss/reducer.py 92 93 94 def __getitem__ ( self , idx : int ) -> AbstractLoss : \"\"\"Get item dunder method.\"\"\" return self . _modules [ str ( idx )]","title":"__getitem__()"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__init__","text":"Initialize the loss reducer. Parameters: loss_modules ( AbstractLoss , default: () ) \u2013 Loss modules to reduce. Raises: ValueError \u2013 If the loss reducer has no loss modules. sparse_autoencoder/loss/reducer.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , * loss_modules : AbstractLoss , ): \"\"\"Initialize the loss reducer. Args: loss_modules: Loss modules to reduce. Raises: ValueError: If the loss reducer has no loss modules. \"\"\" super () . __init__ () for idx , loss_module in enumerate ( loss_modules ): self . _modules [ str ( idx )] = loss_module if len ( self ) == 0 : error_message = \"Loss reducer must have at least one loss module.\" raise ValueError ( error_message )","title":"__init__()"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__iter__","text":"Iterator dunder method. sparse_autoencoder/loss/reducer.py 96 97 98 def __iter__ ( self ) -> Iterator [ AbstractLoss ]: \"\"\"Iterator dunder method.\"\"\" return iter ( self . _modules . values ())","title":"__iter__()"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__len__","text":"Length dunder method. sparse_autoencoder/loss/reducer.py 100 101 102 def __len__ ( self ) -> int : \"\"\"Length dunder method.\"\"\" return len ( self . _modules )","title":"__len__()"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.forward","text":"Reduce loss. Parameters: source_activations ( InputOutputActivationBatch ) \u2013 Source activations (input activations to the autoencoder from the source model). learned_activations ( LearnedActivationBatch ) \u2013 Learned activations (intermediate activations in the autoencoder). decoded_activations ( InputOutputActivationBatch ) \u2013 Decoded activations. Returns: ItemTensor \u2013 Mean loss across the batch, summed across the loss modules. sparse_autoencoder/loss/reducer.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , source_activations : InputOutputActivationBatch , learned_activations : LearnedActivationBatch , decoded_activations : InputOutputActivationBatch , ) -> ItemTensor : \"\"\"Reduce loss. Args: source_activations: Source activations (input activations to the autoencoder from the source model). learned_activations: Learned activations (intermediate activations in the autoencoder). decoded_activations: Decoded activations. Returns: Mean loss across the batch, summed across the loss modules. \"\"\" all_modules_loss : Float [ Tensor , \"module train_batch\" ] = torch . stack ( [ loss_module . forward ( source_activations , learned_activations , decoded_activations ) for loss_module in self . _modules . values () ] ) return all_modules_loss . sum ( dim = 0 )","title":"forward()"},{"location":"reference/metrics/","text":"Metrics.","title":"Index"},{"location":"reference/metrics/abstract_metric/","text":"Abstract metric classes. AbstractGenerateMetric Bases: AbstractMetric , ABC Abstract generate metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 54 55 56 57 58 59 60 61 62 63 64 65 class AbstractGenerateMetric ( AbstractMetric , ABC ): \"\"\"Abstract generate metric.\"\"\" @abstractmethod def create_progress_bar_postfix ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError @abstractmethod def create_weights_and_biases_log ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError create_progress_bar_postfix ( data ) abstractmethod Create a progress bar postfix. sparse_autoencoder/metrics/abstract_metric.py 57 58 59 60 @abstractmethod def create_progress_bar_postfix ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError create_weights_and_biases_log ( data ) abstractmethod Create a log item for Weights and Biases. sparse_autoencoder/metrics/abstract_metric.py 62 63 64 65 @abstractmethod def create_weights_and_biases_log ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError AbstractMetric Bases: ABC Abstract metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 40 41 42 43 44 45 46 47 48 49 50 51 class AbstractMetric ( ABC ): \"\"\"Abstract metric.\"\"\" _should_log_progress_bar : bool _should_log_weights_and_biases : bool @final def __init__ ( self , * , log_progress_bar : bool = False , log_weights_and_biases : bool = True ): \"\"\"Initialise the train metric.\"\"\" self . _should_log_progress_bar = log_progress_bar self . _should_log_weights_and_biases = log_weights_and_biases __init__ ( * , log_progress_bar = False , log_weights_and_biases = True ) Initialise the train metric. sparse_autoencoder/metrics/abstract_metric.py 47 48 49 50 51 @final def __init__ ( self , * , log_progress_bar : bool = False , log_weights_and_biases : bool = True ): \"\"\"Initialise the train metric.\"\"\" self . _should_log_progress_bar = log_progress_bar self . _should_log_weights_and_biases = log_weights_and_biases AbstractTrainMetric Bases: AbstractMetric , ABC Abstract train metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 68 69 70 71 72 73 74 75 76 77 78 79 class AbstractTrainMetric ( AbstractMetric , ABC ): \"\"\"Abstract train metric.\"\"\" @abstractmethod def create_progress_bar_postfix ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError @abstractmethod def create_weights_and_biases_log ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError create_progress_bar_postfix ( data ) abstractmethod Create a progress bar postfix. sparse_autoencoder/metrics/abstract_metric.py 71 72 73 74 @abstractmethod def create_progress_bar_postfix ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError create_weights_and_biases_log ( data ) abstractmethod Create a log item for Weights and Biases. sparse_autoencoder/metrics/abstract_metric.py 76 77 78 79 @abstractmethod def create_weights_and_biases_log ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError AbstractValidationMetric Bases: AbstractMetric , ABC Abstract validation metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 82 83 84 85 86 87 88 89 90 91 92 93 class AbstractValidationMetric ( AbstractMetric , ABC ): \"\"\"Abstract validation metric.\"\"\" @abstractmethod def create_progress_bar_postfix ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError @abstractmethod def create_weights_and_biases_log ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError create_progress_bar_postfix ( data ) abstractmethod Create a progress bar postfix. sparse_autoencoder/metrics/abstract_metric.py 85 86 87 88 @abstractmethod def create_progress_bar_postfix ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError create_weights_and_biases_log ( data ) abstractmethod Create a log item for Weights and Biases. sparse_autoencoder/metrics/abstract_metric.py 90 91 92 93 @abstractmethod def create_weights_and_biases_log ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError GenerateMetricData dataclass Generate metric data. Source code in sparse_autoencoder/metrics/abstract_metric.py 13 14 15 16 17 @dataclass class GenerateMetricData : \"\"\"Generate metric data.\"\"\" generated_activations : InputOutputActivationBatch TrainMetricData dataclass Train metric data. Source code in sparse_autoencoder/metrics/abstract_metric.py 20 21 22 23 24 25 26 27 28 @dataclass class TrainMetricData : \"\"\"Train metric data.\"\"\" input_activations : InputOutputActivationBatch learned_activations : LearnedActivationBatch decoded_activations : InputOutputActivationBatch ValidationMetricData dataclass Validation metric data. Source code in sparse_autoencoder/metrics/abstract_metric.py 31 32 33 34 35 36 37 @dataclass class ValidationMetricData : \"\"\"Validation metric data.\"\"\" source_model_loss : float autoencoder_loss : float","title":"abstract_metric"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractGenerateMetric","text":"Bases: AbstractMetric , ABC Abstract generate metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 54 55 56 57 58 59 60 61 62 63 64 65 class AbstractGenerateMetric ( AbstractMetric , ABC ): \"\"\"Abstract generate metric.\"\"\" @abstractmethod def create_progress_bar_postfix ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError @abstractmethod def create_weights_and_biases_log ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError","title":"AbstractGenerateMetric"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractGenerateMetric.create_progress_bar_postfix","text":"Create a progress bar postfix. sparse_autoencoder/metrics/abstract_metric.py 57 58 59 60 @abstractmethod def create_progress_bar_postfix ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError","title":"create_progress_bar_postfix()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractGenerateMetric.create_weights_and_biases_log","text":"Create a log item for Weights and Biases. sparse_autoencoder/metrics/abstract_metric.py 62 63 64 65 @abstractmethod def create_weights_and_biases_log ( self , data : GenerateMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError","title":"create_weights_and_biases_log()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractMetric","text":"Bases: ABC Abstract metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 40 41 42 43 44 45 46 47 48 49 50 51 class AbstractMetric ( ABC ): \"\"\"Abstract metric.\"\"\" _should_log_progress_bar : bool _should_log_weights_and_biases : bool @final def __init__ ( self , * , log_progress_bar : bool = False , log_weights_and_biases : bool = True ): \"\"\"Initialise the train metric.\"\"\" self . _should_log_progress_bar = log_progress_bar self . _should_log_weights_and_biases = log_weights_and_biases","title":"AbstractMetric"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractMetric.__init__","text":"Initialise the train metric. sparse_autoencoder/metrics/abstract_metric.py 47 48 49 50 51 @final def __init__ ( self , * , log_progress_bar : bool = False , log_weights_and_biases : bool = True ): \"\"\"Initialise the train metric.\"\"\" self . _should_log_progress_bar = log_progress_bar self . _should_log_weights_and_biases = log_weights_and_biases","title":"__init__()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractTrainMetric","text":"Bases: AbstractMetric , ABC Abstract train metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 68 69 70 71 72 73 74 75 76 77 78 79 class AbstractTrainMetric ( AbstractMetric , ABC ): \"\"\"Abstract train metric.\"\"\" @abstractmethod def create_progress_bar_postfix ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError @abstractmethod def create_weights_and_biases_log ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError","title":"AbstractTrainMetric"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractTrainMetric.create_progress_bar_postfix","text":"Create a progress bar postfix. sparse_autoencoder/metrics/abstract_metric.py 71 72 73 74 @abstractmethod def create_progress_bar_postfix ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError","title":"create_progress_bar_postfix()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractTrainMetric.create_weights_and_biases_log","text":"Create a log item for Weights and Biases. sparse_autoencoder/metrics/abstract_metric.py 76 77 78 79 @abstractmethod def create_weights_and_biases_log ( self , data : TrainMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError","title":"create_weights_and_biases_log()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractValidationMetric","text":"Bases: AbstractMetric , ABC Abstract validation metric. Source code in sparse_autoencoder/metrics/abstract_metric.py 82 83 84 85 86 87 88 89 90 91 92 93 class AbstractValidationMetric ( AbstractMetric , ABC ): \"\"\"Abstract validation metric.\"\"\" @abstractmethod def create_progress_bar_postfix ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError @abstractmethod def create_weights_and_biases_log ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError","title":"AbstractValidationMetric"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractValidationMetric.create_progress_bar_postfix","text":"Create a progress bar postfix. sparse_autoencoder/metrics/abstract_metric.py 85 86 87 88 @abstractmethod def create_progress_bar_postfix ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a progress bar postfix.\"\"\" raise NotImplementedError","title":"create_progress_bar_postfix()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.AbstractValidationMetric.create_weights_and_biases_log","text":"Create a log item for Weights and Biases. sparse_autoencoder/metrics/abstract_metric.py 90 91 92 93 @abstractmethod def create_weights_and_biases_log ( self , data : ValidationMetricData ) -> OrderedDict [ str , Any ]: \"\"\"Create a log item for Weights and Biases.\"\"\" raise NotImplementedError","title":"create_weights_and_biases_log()"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.GenerateMetricData","text":"Generate metric data. Source code in sparse_autoencoder/metrics/abstract_metric.py 13 14 15 16 17 @dataclass class GenerateMetricData : \"\"\"Generate metric data.\"\"\" generated_activations : InputOutputActivationBatch","title":"GenerateMetricData"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.TrainMetricData","text":"Train metric data. Source code in sparse_autoencoder/metrics/abstract_metric.py 20 21 22 23 24 25 26 27 28 @dataclass class TrainMetricData : \"\"\"Train metric data.\"\"\" input_activations : InputOutputActivationBatch learned_activations : LearnedActivationBatch decoded_activations : InputOutputActivationBatch","title":"TrainMetricData"},{"location":"reference/metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.ValidationMetricData","text":"Validation metric data. Source code in sparse_autoencoder/metrics/abstract_metric.py 31 32 33 34 35 36 37 @dataclass class ValidationMetricData : \"\"\"Validation metric data.\"\"\" source_model_loss : float autoencoder_loss : float","title":"ValidationMetricData"},{"location":"reference/optimizer/","text":"Optimizer.","title":"Index"},{"location":"reference/optimizer/abstract_optimizer/","text":"Abstract optimizer with reset. AbstractOptimizerWithReset Bases: ABC Abstract optimizer with reset. Source code in sparse_autoencoder/optimizer/abstract_optimizer.py 5 6 7 8 9 10 11 12 13 14 15 class AbstractOptimizerWithReset ( ABC ): \"\"\"Abstract optimizer with reset.\"\"\" @abstractmethod def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Resets any optimizer state (e.g. momentum). This is for use after manually editing model parameters (e.g. with activation resampling). \"\"\" raise NotImplementedError reset_state_all_parameters () abstractmethod Reset the state for all parameters. Resets any optimizer state (e.g. momentum). This is for use after manually editing model parameters (e.g. with activation resampling). sparse_autoencoder/optimizer/abstract_optimizer.py 8 9 10 11 12 13 14 15 @abstractmethod def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Resets any optimizer state (e.g. momentum). This is for use after manually editing model parameters (e.g. with activation resampling). \"\"\" raise NotImplementedError","title":"abstract_optimizer"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset","text":"Bases: ABC Abstract optimizer with reset. Source code in sparse_autoencoder/optimizer/abstract_optimizer.py 5 6 7 8 9 10 11 12 13 14 15 class AbstractOptimizerWithReset ( ABC ): \"\"\"Abstract optimizer with reset.\"\"\" @abstractmethod def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Resets any optimizer state (e.g. momentum). This is for use after manually editing model parameters (e.g. with activation resampling). \"\"\" raise NotImplementedError","title":"AbstractOptimizerWithReset"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset.reset_state_all_parameters","text":"Reset the state for all parameters. Resets any optimizer state (e.g. momentum). This is for use after manually editing model parameters (e.g. with activation resampling). sparse_autoencoder/optimizer/abstract_optimizer.py 8 9 10 11 12 13 14 15 @abstractmethod def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Resets any optimizer state (e.g. momentum). This is for use after manually editing model parameters (e.g. with activation resampling). \"\"\" raise NotImplementedError","title":"reset_state_all_parameters()"},{"location":"reference/optimizer/adam_with_reset/","text":"Adam Optimizer with a reset method. This reset method is useful when resampling dead neurons during training. AdamWithReset Bases: Adam , AbstractOptimizerWithReset Adam Optimizer with a reset method. The :meth: reset_state_all_parameters and :meth: reset_neurons_state methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed. Otherwise this is the same as the standard Adam optimizer. Source code in sparse_autoencoder/optimizer/adam_with_reset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @final class AdamWithReset ( Adam , AbstractOptimizerWithReset ): \"\"\"Adam Optimizer with a reset method. The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed. Otherwise this is the same as the standard Adam optimizer. \"\"\" parameter_names : list [ str ] \"\"\"Parameter Names. The names of the parameters, so that we can find them later when resetting the state. \"\"\" def __init__ ( # noqa: PLR0913 , D417 (extending existing implementation) self , params : params_t , lr : float | Tensor = 1e-3 , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 0 , * , amsgrad : bool = False , foreach : bool | None = None , maximize : bool = False , capturable : bool = False , differentiable : bool = False , fused : bool | None = None , named_parameters : Iterator [ tuple [ str , Parameter ]], ) -> None : \"\"\"Initialize the optimizer. Warning: Named parameters must be with default settings (remove duplicates and not recursive). Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> optimizer.reset_state_all_parameters() Args: named_parameters (Iterator[tuple[str, Parameter]]): An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as `model.named_parameters()`. \"\"\" # Initialise the parent class (note we repeat the parameter names so that type hints work). super () . __init__ ( params = params , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad , foreach = foreach , maximize = maximize , capturable = capturable , differentiable = differentiable , fused = fused , ) # Store the names of the parameters, so that we can find them later when resetting the # state. self . parameter_names = [ name for name , _value in named_parameters ] if len ( self . parameter_names ) != len ( self . param_groups [ 0 ][ \"params\" ]): error_message = ( \"The number of parameter names does not match the number of parameters. \" \"If using model.named_parameters() make sure remove_duplicates is True \" \"and recursive is False (the default settings).\" ) raise ValueError ( error_message ) def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Iterates over all parameters and resents both the running averages of the gradients and the squares of gradients. \"\"\" # Iterate over every parameter for group in self . param_groups : for parameter in group [ \"params\" ]: # Get the state state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : continue # Reset running averages exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . zero_ () exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . zero_ () # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . zero_ () def _get_parameter_name_idx ( self , parameter_name : str ) -> int : \"\"\"Get the index of a parameter name. Args: parameter_name (str): The name of the parameter. Returns: int: The index of the parameter name. Raises: ValueError: If the parameter name is not found. \"\"\" if parameter_name not in self . parameter_names : error_message = f \"Parameter name { parameter_name } not found.\" raise ValueError ( error_message ) return self . parameter_names . index ( parameter_name ) def reset_neurons_state ( self , parameter_name : str , neuron_indices : DeadNeuronIndices , axis : int , parameter_group : int = 0 , ) -> None : \"\"\"Reset the state for specific neurons, on a specific parameter. Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> # ... train the model and then resample some dead neurons, then do this ... >>> dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices >>> # Reset the optimizer state for parameters that have been updated >>> optimizer.reset_neurons_state(\"encoder.Linear.weight\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state(\"encoder.Linear.bias\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state( ... \"decoder.ConstrainedUnitNormLinear.weight\", ... dead_neurons_indices, ... axis=1 ... ) Args: parameter_name: The name of the parameter. Examples from the standard sparse autoencoder implementation include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`, `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`. neuron_indices: The indices of the neurons to reset. axis: The axis of the parameter to reset. parameter_group: The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters). Raises: ValueError: If the parameter name is not found. \"\"\" # Get the state of the parameter group = self . param_groups [ parameter_group ] parameter_name_idx = self . _get_parameter_name_idx ( parameter_name ) parameter = group [ \"params\" ][ parameter_name_idx ] state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : return # Reset running averages for the specified neurons if \"exp_avg\" in state : exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . index_fill_ ( axis , neuron_indices , 0 ) if \"exp_avg_sq\" in state : exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 ) # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 ) parameter_names : list [ str ] = [ name for ( name , _value ) in named_parameters ] instance-attribute Parameter Names. The names of the parameters, so that we can find them later when resetting the state. __init__ ( params , lr = 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , * , amsgrad = False , foreach = None , maximize = False , capturable = False , differentiable = False , fused = None , named_parameters ) Initialize the optimizer. Warning Named parameters must be with default settings (remove duplicates and not recursive). Example import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) optimizer.reset_state_all_parameters() Parameters: named_parameters ( Iterator [ tuple [ str , Parameter ]] ) \u2013 An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as model.named_parameters() . sparse_autoencoder/optimizer/adam_with_reset.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( # noqa: PLR0913 , D417 (extending existing implementation) self , params : params_t , lr : float | Tensor = 1e-3 , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 0 , * , amsgrad : bool = False , foreach : bool | None = None , maximize : bool = False , capturable : bool = False , differentiable : bool = False , fused : bool | None = None , named_parameters : Iterator [ tuple [ str , Parameter ]], ) -> None : \"\"\"Initialize the optimizer. Warning: Named parameters must be with default settings (remove duplicates and not recursive). Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> optimizer.reset_state_all_parameters() Args: named_parameters (Iterator[tuple[str, Parameter]]): An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as `model.named_parameters()`. \"\"\" # Initialise the parent class (note we repeat the parameter names so that type hints work). super () . __init__ ( params = params , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad , foreach = foreach , maximize = maximize , capturable = capturable , differentiable = differentiable , fused = fused , ) # Store the names of the parameters, so that we can find them later when resetting the # state. self . parameter_names = [ name for name , _value in named_parameters ] if len ( self . parameter_names ) != len ( self . param_groups [ 0 ][ \"params\" ]): error_message = ( \"The number of parameter names does not match the number of parameters. \" \"If using model.named_parameters() make sure remove_duplicates is True \" \"and recursive is False (the default settings).\" ) raise ValueError ( error_message ) reset_neurons_state ( parameter_name , neuron_indices , axis , parameter_group = 0 ) Reset the state for specific neurons, on a specific parameter. Example import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) ... train the model and then resample some dead neurons, then do this ... dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices Reset the optimizer state for parameters that have been updated optimizer.reset_neurons_state(\"encoder.Linear.weight\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state(\"encoder.Linear.bias\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ... \"decoder.ConstrainedUnitNormLinear.weight\", ... dead_neurons_indices, ... axis=1 ... ) Parameters: parameter_name ( str ) \u2013 The name of the parameter. Examples from the standard sparse autoencoder implementation include tied_bias , encoder.Linear.weight , encoder.Linear.bias , decoder.Linear.weight , and decoder.ConstrainedUnitNormLinear.weight . neuron_indices ( DeadNeuronIndices ) \u2013 The indices of the neurons to reset. axis ( int ) \u2013 The axis of the parameter to reset. parameter_group ( int , default: 0 ) \u2013 The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters). Raises: ValueError \u2013 If the parameter name is not found. sparse_autoencoder/optimizer/adam_with_reset.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def reset_neurons_state ( self , parameter_name : str , neuron_indices : DeadNeuronIndices , axis : int , parameter_group : int = 0 , ) -> None : \"\"\"Reset the state for specific neurons, on a specific parameter. Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> # ... train the model and then resample some dead neurons, then do this ... >>> dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices >>> # Reset the optimizer state for parameters that have been updated >>> optimizer.reset_neurons_state(\"encoder.Linear.weight\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state(\"encoder.Linear.bias\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state( ... \"decoder.ConstrainedUnitNormLinear.weight\", ... dead_neurons_indices, ... axis=1 ... ) Args: parameter_name: The name of the parameter. Examples from the standard sparse autoencoder implementation include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`, `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`. neuron_indices: The indices of the neurons to reset. axis: The axis of the parameter to reset. parameter_group: The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters). Raises: ValueError: If the parameter name is not found. \"\"\" # Get the state of the parameter group = self . param_groups [ parameter_group ] parameter_name_idx = self . _get_parameter_name_idx ( parameter_name ) parameter = group [ \"params\" ][ parameter_name_idx ] state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : return # Reset running averages for the specified neurons if \"exp_avg\" in state : exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . index_fill_ ( axis , neuron_indices , 0 ) if \"exp_avg_sq\" in state : exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 ) # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 ) reset_state_all_parameters () Reset the state for all parameters. Iterates over all parameters and resents both the running averages of the gradients and the squares of gradients. sparse_autoencoder/optimizer/adam_with_reset.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Iterates over all parameters and resents both the running averages of the gradients and the squares of gradients. \"\"\" # Iterate over every parameter for group in self . param_groups : for parameter in group [ \"params\" ]: # Get the state state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : continue # Reset running averages exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . zero_ () exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . zero_ () # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . zero_ ()","title":"adam_with_reset"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset","text":"Bases: Adam , AbstractOptimizerWithReset Adam Optimizer with a reset method. The :meth: reset_state_all_parameters and :meth: reset_neurons_state methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed. Otherwise this is the same as the standard Adam optimizer. Source code in sparse_autoencoder/optimizer/adam_with_reset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @final class AdamWithReset ( Adam , AbstractOptimizerWithReset ): \"\"\"Adam Optimizer with a reset method. The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed. Otherwise this is the same as the standard Adam optimizer. \"\"\" parameter_names : list [ str ] \"\"\"Parameter Names. The names of the parameters, so that we can find them later when resetting the state. \"\"\" def __init__ ( # noqa: PLR0913 , D417 (extending existing implementation) self , params : params_t , lr : float | Tensor = 1e-3 , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 0 , * , amsgrad : bool = False , foreach : bool | None = None , maximize : bool = False , capturable : bool = False , differentiable : bool = False , fused : bool | None = None , named_parameters : Iterator [ tuple [ str , Parameter ]], ) -> None : \"\"\"Initialize the optimizer. Warning: Named parameters must be with default settings (remove duplicates and not recursive). Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> optimizer.reset_state_all_parameters() Args: named_parameters (Iterator[tuple[str, Parameter]]): An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as `model.named_parameters()`. \"\"\" # Initialise the parent class (note we repeat the parameter names so that type hints work). super () . __init__ ( params = params , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad , foreach = foreach , maximize = maximize , capturable = capturable , differentiable = differentiable , fused = fused , ) # Store the names of the parameters, so that we can find them later when resetting the # state. self . parameter_names = [ name for name , _value in named_parameters ] if len ( self . parameter_names ) != len ( self . param_groups [ 0 ][ \"params\" ]): error_message = ( \"The number of parameter names does not match the number of parameters. \" \"If using model.named_parameters() make sure remove_duplicates is True \" \"and recursive is False (the default settings).\" ) raise ValueError ( error_message ) def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Iterates over all parameters and resents both the running averages of the gradients and the squares of gradients. \"\"\" # Iterate over every parameter for group in self . param_groups : for parameter in group [ \"params\" ]: # Get the state state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : continue # Reset running averages exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . zero_ () exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . zero_ () # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . zero_ () def _get_parameter_name_idx ( self , parameter_name : str ) -> int : \"\"\"Get the index of a parameter name. Args: parameter_name (str): The name of the parameter. Returns: int: The index of the parameter name. Raises: ValueError: If the parameter name is not found. \"\"\" if parameter_name not in self . parameter_names : error_message = f \"Parameter name { parameter_name } not found.\" raise ValueError ( error_message ) return self . parameter_names . index ( parameter_name ) def reset_neurons_state ( self , parameter_name : str , neuron_indices : DeadNeuronIndices , axis : int , parameter_group : int = 0 , ) -> None : \"\"\"Reset the state for specific neurons, on a specific parameter. Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> # ... train the model and then resample some dead neurons, then do this ... >>> dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices >>> # Reset the optimizer state for parameters that have been updated >>> optimizer.reset_neurons_state(\"encoder.Linear.weight\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state(\"encoder.Linear.bias\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state( ... \"decoder.ConstrainedUnitNormLinear.weight\", ... dead_neurons_indices, ... axis=1 ... ) Args: parameter_name: The name of the parameter. Examples from the standard sparse autoencoder implementation include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`, `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`. neuron_indices: The indices of the neurons to reset. axis: The axis of the parameter to reset. parameter_group: The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters). Raises: ValueError: If the parameter name is not found. \"\"\" # Get the state of the parameter group = self . param_groups [ parameter_group ] parameter_name_idx = self . _get_parameter_name_idx ( parameter_name ) parameter = group [ \"params\" ][ parameter_name_idx ] state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : return # Reset running averages for the specified neurons if \"exp_avg\" in state : exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . index_fill_ ( axis , neuron_indices , 0 ) if \"exp_avg_sq\" in state : exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 ) # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 )","title":"AdamWithReset"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.parameter_names","text":"Parameter Names. The names of the parameters, so that we can find them later when resetting the state.","title":"parameter_names"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.__init__","text":"Initialize the optimizer. Warning Named parameters must be with default settings (remove duplicates and not recursive). Example import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) optimizer.reset_state_all_parameters() Parameters: named_parameters ( Iterator [ tuple [ str , Parameter ]] ) \u2013 An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as model.named_parameters() . sparse_autoencoder/optimizer/adam_with_reset.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( # noqa: PLR0913 , D417 (extending existing implementation) self , params : params_t , lr : float | Tensor = 1e-3 , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 0 , * , amsgrad : bool = False , foreach : bool | None = None , maximize : bool = False , capturable : bool = False , differentiable : bool = False , fused : bool | None = None , named_parameters : Iterator [ tuple [ str , Parameter ]], ) -> None : \"\"\"Initialize the optimizer. Warning: Named parameters must be with default settings (remove duplicates and not recursive). Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> optimizer.reset_state_all_parameters() Args: named_parameters (Iterator[tuple[str, Parameter]]): An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as `model.named_parameters()`. \"\"\" # Initialise the parent class (note we repeat the parameter names so that type hints work). super () . __init__ ( params = params , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad , foreach = foreach , maximize = maximize , capturable = capturable , differentiable = differentiable , fused = fused , ) # Store the names of the parameters, so that we can find them later when resetting the # state. self . parameter_names = [ name for name , _value in named_parameters ] if len ( self . parameter_names ) != len ( self . param_groups [ 0 ][ \"params\" ]): error_message = ( \"The number of parameter names does not match the number of parameters. \" \"If using model.named_parameters() make sure remove_duplicates is True \" \"and recursive is False (the default settings).\" ) raise ValueError ( error_message )","title":"__init__()"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state","text":"Reset the state for specific neurons, on a specific parameter. Example import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... )","title":"reset_neurons_state()"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","text":"dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices","title":"... train the model and then resample some dead neurons, then do this ..."},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","text":"optimizer.reset_neurons_state(\"encoder.Linear.weight\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state(\"encoder.Linear.bias\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ... \"decoder.ConstrainedUnitNormLinear.weight\", ... dead_neurons_indices, ... axis=1 ... ) Parameters: parameter_name ( str ) \u2013 The name of the parameter. Examples from the standard sparse autoencoder implementation include tied_bias , encoder.Linear.weight , encoder.Linear.bias , decoder.Linear.weight , and decoder.ConstrainedUnitNormLinear.weight . neuron_indices ( DeadNeuronIndices ) \u2013 The indices of the neurons to reset. axis ( int ) \u2013 The axis of the parameter to reset. parameter_group ( int , default: 0 ) \u2013 The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters). Raises: ValueError \u2013 If the parameter name is not found. sparse_autoencoder/optimizer/adam_with_reset.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def reset_neurons_state ( self , parameter_name : str , neuron_indices : DeadNeuronIndices , axis : int , parameter_group : int = 0 , ) -> None : \"\"\"Reset the state for specific neurons, on a specific parameter. Example: >>> import torch >>> from sparse_autoencoder.autoencoder.model import SparseAutoencoder >>> model = SparseAutoencoder(5, 10, torch.zeros(5)) >>> optimizer = AdamWithReset( ... model.parameters(), ... named_parameters=model.named_parameters(), ... ) >>> # ... train the model and then resample some dead neurons, then do this ... >>> dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices >>> # Reset the optimizer state for parameters that have been updated >>> optimizer.reset_neurons_state(\"encoder.Linear.weight\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state(\"encoder.Linear.bias\", dead_neurons_indices, axis=0) >>> optimizer.reset_neurons_state( ... \"decoder.ConstrainedUnitNormLinear.weight\", ... dead_neurons_indices, ... axis=1 ... ) Args: parameter_name: The name of the parameter. Examples from the standard sparse autoencoder implementation include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`, `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`. neuron_indices: The indices of the neurons to reset. axis: The axis of the parameter to reset. parameter_group: The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters). Raises: ValueError: If the parameter name is not found. \"\"\" # Get the state of the parameter group = self . param_groups [ parameter_group ] parameter_name_idx = self . _get_parameter_name_idx ( parameter_name ) parameter = group [ \"params\" ][ parameter_name_idx ] state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : return # Reset running averages for the specified neurons if \"exp_avg\" in state : exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . index_fill_ ( axis , neuron_indices , 0 ) if \"exp_avg_sq\" in state : exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 ) # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . index_fill_ ( axis , neuron_indices , 0 )","title":"Reset the optimizer state for parameters that have been updated"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_state_all_parameters","text":"Reset the state for all parameters. Iterates over all parameters and resents both the running averages of the gradients and the squares of gradients. sparse_autoencoder/optimizer/adam_with_reset.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def reset_state_all_parameters ( self ) -> None : \"\"\"Reset the state for all parameters. Iterates over all parameters and resents both the running averages of the gradients and the squares of gradients. \"\"\" # Iterate over every parameter for group in self . param_groups : for parameter in group [ \"params\" ]: # Get the state state = self . state [ parameter ] # Check if state is initialized if len ( state ) == 0 : continue # Reset running averages exp_avg : Tensor = state [ \"exp_avg\" ] exp_avg . zero_ () exp_avg_sq : Tensor = state [ \"exp_avg_sq\" ] exp_avg_sq . zero_ () # If AdamW is used (weight decay fix), also reset the max exp_avg_sq if \"max_exp_avg_sq\" in state : max_exp_avg_sq : Tensor = state [ \"max_exp_avg_sq\" ] max_exp_avg_sq . zero_ ()","title":"reset_state_all_parameters()"},{"location":"reference/source_data/","text":"Source Data.","title":"Index"},{"location":"reference/source_data/abstract_dataset/","text":"Abstract tokenized prompts dataset class. HuggingFaceDatasetItem = TypeVar ( 'HuggingFaceDatasetItem' , bound = Any ) module-attribute Hugging face dataset item typed dict. When extending :class: SourceDataset you should create a TypedDict that matches the structure of each dataset item in the underlying Hugging Face dataset. Example With the Uncopyrighted Pile this should be a typed dict with text and meta properties. class PileUncopyrightedSourceDataBatch(TypedDict): ... text: list[str] ... meta: list[dict[str, dict[str, str]]] TokenizedPrompt = list [ int ] module-attribute A tokenized prompt. SourceDataset Bases: ABC , Generic [ HuggingFaceDatasetItem ] Abstract source dataset. Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset. Wraps an HuggingFace IterableDataset. Source code in sparse_autoencoder/source_data/abstract_dataset.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 class SourceDataset ( ABC , Generic [ HuggingFaceDatasetItem ]): \"\"\"Abstract source dataset. Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset. Wraps an HuggingFace IterableDataset. \"\"\" context_size : int \"\"\"Number of tokens in the context window. The paper *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. \"\"\" dataset : IterableDataset \"\"\"Underlying HuggingFace IterableDataset. Warning: Hugging Face `Dataset` objects are confusingly not the same as PyTorch `Dataset` objects. \"\"\" @abstractmethod def preprocess ( self , source_batch : HuggingFaceDatasetItem , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess function. Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$. Applied to the dataset with the [Hugging Face Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map) `map` function. Warning: The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token). Args: source_batch: A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. \"\"\" raise NotImplementedError @abstractmethod def __init__ ( self , dataset_path : str , dataset_split : str , context_size : int , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , ): \"\"\"Initialise the dataset. Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face `IterableDataset`. Args: dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). \"\"\" self . context_size = context_size # Load the dataset dataset : IterableDataset = load_dataset ( dataset_path , streaming = True , split = dataset_split ) # type: ignore # Setup preprocessing existing_columns : list [ str ] = list ( next ( iter ( dataset )) . keys ()) mapped_dataset = dataset . map ( self . preprocess , batched = True , batch_size = preprocess_batch_size , fn_kwargs = { \"context_size\" : context_size }, remove_columns = existing_columns , ) # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least # `buffer_size` items and then shuffles just that buffer. # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle self . dataset = mapped_dataset . shuffle ( buffer_size = buffer_size ) @final def __iter__ ( self ) -> Any : # noqa: ANN401 \"\"\"Iterate Dunder Method. Enables direct access to :attr:`dataset` with e.g. `for` loops. \"\"\" return self . dataset . __iter__ () @final def __next__ ( self ) -> Any : # noqa: ANN401 \"\"\"Next Dunder Method. Enables direct access to :attr:`dataset` with e.g. `next` calls. \"\"\" return next ( iter ( self )) @final def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: \"\"\"Get a PyTorch DataLoader. Args: batch_size: The batch size to use. Returns: PyTorch DataLoader. \"\"\" torch_dataset : TorchDataset [ TorchTokenizedPrompts ] = self . dataset . with_format ( \"torch\" ) # type: ignore return DataLoader [ TorchTokenizedPrompts ]( torch_dataset , batch_size = batch_size , # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not # here. shuffle = False , ) context_size : int = context_size instance-attribute Number of tokens in the context window. The paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. dataset : IterableDataset = mapped_dataset . shuffle ( buffer_size = buffer_size ) instance-attribute Underlying HuggingFace IterableDataset. Warning Hugging Face Dataset objects are confusingly not the same as PyTorch Dataset objects. __init__ ( dataset_path , dataset_split , context_size , buffer_size = 1000 , preprocess_batch_size = 1000 ) abstractmethod Initialise the dataset. Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face IterableDataset . Parameters: dataset_path ( str ) \u2013 The path to the dataset on Hugging Face. dataset_split ( str ) \u2013 Dataset split (e.g. train ). context_size ( int ) \u2013 The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. buffer_size ( int , default: 1000 ) \u2013 The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least buffer_size items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size ( int , default: 1000 ) \u2013 The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). sparse_autoencoder/source_data/abstract_dataset.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 @abstractmethod def __init__ ( self , dataset_path : str , dataset_split : str , context_size : int , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , ): \"\"\"Initialise the dataset. Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face `IterableDataset`. Args: dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). \"\"\" self . context_size = context_size # Load the dataset dataset : IterableDataset = load_dataset ( dataset_path , streaming = True , split = dataset_split ) # type: ignore # Setup preprocessing existing_columns : list [ str ] = list ( next ( iter ( dataset )) . keys ()) mapped_dataset = dataset . map ( self . preprocess , batched = True , batch_size = preprocess_batch_size , fn_kwargs = { \"context_size\" : context_size }, remove_columns = existing_columns , ) # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least # `buffer_size` items and then shuffles just that buffer. # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle self . dataset = mapped_dataset . shuffle ( buffer_size = buffer_size ) __iter__ () Iterate Dunder Method. Enables direct access to :attr: dataset with e.g. for loops. sparse_autoencoder/source_data/abstract_dataset.py 150 151 152 153 154 155 156 @final def __iter__ ( self ) -> Any : # noqa: ANN401 \"\"\"Iterate Dunder Method. Enables direct access to :attr:`dataset` with e.g. `for` loops. \"\"\" return self . dataset . __iter__ () __next__ () Next Dunder Method. Enables direct access to :attr: dataset with e.g. next calls. sparse_autoencoder/source_data/abstract_dataset.py 158 159 160 161 162 163 164 @final def __next__ ( self ) -> Any : # noqa: ANN401 \"\"\"Next Dunder Method. Enables direct access to :attr:`dataset` with e.g. `next` calls. \"\"\" return next ( iter ( self )) get_dataloader ( batch_size ) Get a PyTorch DataLoader. Parameters: batch_size ( int ) \u2013 The batch size to use. Returns: DataLoader [ TorchTokenizedPrompts ] \u2013 PyTorch DataLoader. sparse_autoencoder/source_data/abstract_dataset.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @final def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: \"\"\"Get a PyTorch DataLoader. Args: batch_size: The batch size to use. Returns: PyTorch DataLoader. \"\"\" torch_dataset : TorchDataset [ TorchTokenizedPrompts ] = self . dataset . with_format ( \"torch\" ) # type: ignore return DataLoader [ TorchTokenizedPrompts ]( torch_dataset , batch_size = batch_size , # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not # here. shuffle = False , ) preprocess ( source_batch , * , context_size ) abstractmethod Preprocess function. Takes a preprocess_batch_size ($m$) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of input_ids and a value of an arbitrary length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$. Applied to the dataset with the Hugging Face Dataset map function. Warning The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token). Parameters: source_batch ( HuggingFaceDatasetItem ) \u2013 A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized). context_size ( int ) \u2013 The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. sparse_autoencoder/source_data/abstract_dataset.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @abstractmethod def preprocess ( self , source_batch : HuggingFaceDatasetItem , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess function. Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$. Applied to the dataset with the [Hugging Face Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map) `map` function. Warning: The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token). Args: source_batch: A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. \"\"\" raise NotImplementedError TokenizedPrompts Bases: TypedDict Tokenized prompts. Source code in sparse_autoencoder/source_data/abstract_dataset.py 16 17 18 19 class TokenizedPrompts ( TypedDict ): \"\"\"Tokenized prompts.\"\"\" input_ids : list [ TokenizedPrompt ] TorchTokenizedPrompts Bases: TypedDict Tokenized prompts prepared for PyTorch. Source code in sparse_autoencoder/source_data/abstract_dataset.py 22 23 24 25 class TorchTokenizedPrompts ( TypedDict ): \"\"\"Tokenized prompts prepared for PyTorch.\"\"\" input_ids : BatchTokenizedPrompts","title":"abstract_dataset"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem","text":"Hugging face dataset item typed dict. When extending :class: SourceDataset you should create a TypedDict that matches the structure of each dataset item in the underlying Hugging Face dataset. Example With the Uncopyrighted Pile this should be a typed dict with text and meta properties. class PileUncopyrightedSourceDataBatch(TypedDict): ... text: list[str] ... meta: list[dict[str, dict[str, str]]]","title":"HuggingFaceDatasetItem"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompt","text":"A tokenized prompt.","title":"TokenizedPrompt"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset","text":"Bases: ABC , Generic [ HuggingFaceDatasetItem ] Abstract source dataset. Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset. Wraps an HuggingFace IterableDataset. Source code in sparse_autoencoder/source_data/abstract_dataset.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 class SourceDataset ( ABC , Generic [ HuggingFaceDatasetItem ]): \"\"\"Abstract source dataset. Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset. Wraps an HuggingFace IterableDataset. \"\"\" context_size : int \"\"\"Number of tokens in the context window. The paper *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. \"\"\" dataset : IterableDataset \"\"\"Underlying HuggingFace IterableDataset. Warning: Hugging Face `Dataset` objects are confusingly not the same as PyTorch `Dataset` objects. \"\"\" @abstractmethod def preprocess ( self , source_batch : HuggingFaceDatasetItem , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess function. Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$. Applied to the dataset with the [Hugging Face Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map) `map` function. Warning: The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token). Args: source_batch: A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. \"\"\" raise NotImplementedError @abstractmethod def __init__ ( self , dataset_path : str , dataset_split : str , context_size : int , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , ): \"\"\"Initialise the dataset. Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face `IterableDataset`. Args: dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). \"\"\" self . context_size = context_size # Load the dataset dataset : IterableDataset = load_dataset ( dataset_path , streaming = True , split = dataset_split ) # type: ignore # Setup preprocessing existing_columns : list [ str ] = list ( next ( iter ( dataset )) . keys ()) mapped_dataset = dataset . map ( self . preprocess , batched = True , batch_size = preprocess_batch_size , fn_kwargs = { \"context_size\" : context_size }, remove_columns = existing_columns , ) # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least # `buffer_size` items and then shuffles just that buffer. # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle self . dataset = mapped_dataset . shuffle ( buffer_size = buffer_size ) @final def __iter__ ( self ) -> Any : # noqa: ANN401 \"\"\"Iterate Dunder Method. Enables direct access to :attr:`dataset` with e.g. `for` loops. \"\"\" return self . dataset . __iter__ () @final def __next__ ( self ) -> Any : # noqa: ANN401 \"\"\"Next Dunder Method. Enables direct access to :attr:`dataset` with e.g. `next` calls. \"\"\" return next ( iter ( self )) @final def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: \"\"\"Get a PyTorch DataLoader. Args: batch_size: The batch size to use. Returns: PyTorch DataLoader. \"\"\" torch_dataset : TorchDataset [ TorchTokenizedPrompts ] = self . dataset . with_format ( \"torch\" ) # type: ignore return DataLoader [ TorchTokenizedPrompts ]( torch_dataset , batch_size = batch_size , # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not # here. shuffle = False , )","title":"SourceDataset"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.context_size","text":"Number of tokens in the context window. The paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.","title":"context_size"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.dataset","text":"Underlying HuggingFace IterableDataset. Warning Hugging Face Dataset objects are confusingly not the same as PyTorch Dataset objects.","title":"dataset"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__init__","text":"Initialise the dataset. Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face IterableDataset . Parameters: dataset_path ( str ) \u2013 The path to the dataset on Hugging Face. dataset_split ( str ) \u2013 Dataset split (e.g. train ). context_size ( int ) \u2013 The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. buffer_size ( int , default: 1000 ) \u2013 The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least buffer_size items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size ( int , default: 1000 ) \u2013 The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). sparse_autoencoder/source_data/abstract_dataset.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 @abstractmethod def __init__ ( self , dataset_path : str , dataset_split : str , context_size : int , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , ): \"\"\"Initialise the dataset. Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face `IterableDataset`. Args: dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). \"\"\" self . context_size = context_size # Load the dataset dataset : IterableDataset = load_dataset ( dataset_path , streaming = True , split = dataset_split ) # type: ignore # Setup preprocessing existing_columns : list [ str ] = list ( next ( iter ( dataset )) . keys ()) mapped_dataset = dataset . map ( self . preprocess , batched = True , batch_size = preprocess_batch_size , fn_kwargs = { \"context_size\" : context_size }, remove_columns = existing_columns , ) # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least # `buffer_size` items and then shuffles just that buffer. # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle self . dataset = mapped_dataset . shuffle ( buffer_size = buffer_size )","title":"__init__()"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__iter__","text":"Iterate Dunder Method. Enables direct access to :attr: dataset with e.g. for loops. sparse_autoencoder/source_data/abstract_dataset.py 150 151 152 153 154 155 156 @final def __iter__ ( self ) -> Any : # noqa: ANN401 \"\"\"Iterate Dunder Method. Enables direct access to :attr:`dataset` with e.g. `for` loops. \"\"\" return self . dataset . __iter__ ()","title":"__iter__()"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__next__","text":"Next Dunder Method. Enables direct access to :attr: dataset with e.g. next calls. sparse_autoencoder/source_data/abstract_dataset.py 158 159 160 161 162 163 164 @final def __next__ ( self ) -> Any : # noqa: ANN401 \"\"\"Next Dunder Method. Enables direct access to :attr:`dataset` with e.g. `next` calls. \"\"\" return next ( iter ( self ))","title":"__next__()"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.get_dataloader","text":"Get a PyTorch DataLoader. Parameters: batch_size ( int ) \u2013 The batch size to use. Returns: DataLoader [ TorchTokenizedPrompts ] \u2013 PyTorch DataLoader. sparse_autoencoder/source_data/abstract_dataset.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @final def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: \"\"\"Get a PyTorch DataLoader. Args: batch_size: The batch size to use. Returns: PyTorch DataLoader. \"\"\" torch_dataset : TorchDataset [ TorchTokenizedPrompts ] = self . dataset . with_format ( \"torch\" ) # type: ignore return DataLoader [ TorchTokenizedPrompts ]( torch_dataset , batch_size = batch_size , # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not # here. shuffle = False , )","title":"get_dataloader()"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.preprocess","text":"Preprocess function. Takes a preprocess_batch_size ($m$) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of input_ids and a value of an arbitrary length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$. Applied to the dataset with the Hugging Face Dataset map function. Warning The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token). Parameters: source_batch ( HuggingFaceDatasetItem ) \u2013 A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized). context_size ( int ) \u2013 The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. sparse_autoencoder/source_data/abstract_dataset.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @abstractmethod def preprocess ( self , source_batch : HuggingFaceDatasetItem , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess function. Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$. Applied to the dataset with the [Hugging Face Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map) `map` function. Warning: The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token). Args: source_batch: A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized). context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. \"\"\" raise NotImplementedError","title":"preprocess()"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts","text":"Bases: TypedDict Tokenized prompts. Source code in sparse_autoencoder/source_data/abstract_dataset.py 16 17 18 19 class TokenizedPrompts ( TypedDict ): \"\"\"Tokenized prompts.\"\"\" input_ids : list [ TokenizedPrompt ]","title":"TokenizedPrompts"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts","text":"Bases: TypedDict Tokenized prompts prepared for PyTorch. Source code in sparse_autoencoder/source_data/abstract_dataset.py 22 23 24 25 class TorchTokenizedPrompts ( TypedDict ): \"\"\"Tokenized prompts prepared for PyTorch.\"\"\" input_ids : BatchTokenizedPrompts","title":"TorchTokenizedPrompts"},{"location":"reference/source_data/pretokenized_dataset/","text":"Pre-Tokenized Dataset from Hugging Face. PreTokenizedDataset should work with any of the following tokenized datasets: - NeelNanda/pile-small-tokenized-2b - NeelNanda/pile-tokenized-10b - NeelNanda/openwebtext-tokenized-9b - NeelNanda/c4-tokenized-2b - NeelNanda/code-tokenized - NeelNanda/c4-code-tokenized-2b - NeelNanda/pile-old-tokenized-2b PreTokenizedDataBatch Bases: TypedDict General Pre-Tokenized Dataset Item. Structure depends on the specific dataset from Hugging Face. Source code in sparse_autoencoder/source_data/pretokenized_dataset.py 19 20 21 22 23 24 25 26 27 class PreTokenizedDataBatch ( TypedDict ): \"\"\"General Pre-Tokenized Dataset Item. Structure depends on the specific dataset from Hugging Face. \"\"\" tokens : list [ list [ int ] ] # This assumes that the dataset structure is similar to the original Neel Nanda dataset. PreTokenizedDataset Bases: SourceDataset [ PreTokenizedDataBatch ] General Pre-Tokenized Dataset from Hugging Face. Can be used for various datasets available on Hugging Face. Source code in sparse_autoencoder/source_data/pretokenized_dataset.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @final class PreTokenizedDataset ( SourceDataset [ PreTokenizedDataBatch ]): \"\"\"General Pre-Tokenized Dataset from Hugging Face. Can be used for various datasets available on Hugging Face. \"\"\" def preprocess ( self , source_batch : PreTokenizedDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. The method splits each pre-tokenized item based on the context size. Args: source_batch: A batch of source data. context_size: The context size to use for tokenized prompts. \"\"\" tokenized_prompts : list [ list [ int ]] = source_batch [ \"tokens\" ] # Chunk each tokenized prompt into blocks of context_size, # discarding the last block if too small. context_size_prompts = [] for encoding in tokenized_prompts : chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts } def __init__ ( self , dataset_path : str , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_split : str = \"train\" , ): \"\"\"Initialize a pre-tokenized dataset from Hugging Face. Example: >>> data = PreTokenizedDataset(dataset_path=\"NeelNanda/c4-tokenized-2b\") >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: dataset_path: The path to the dataset on Hugging Face. context_size: The context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_split: Dataset split (e.g., `train`). \"\"\" super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , ) __init__ ( dataset_path , context_size = 250 , buffer_size = 1000 , preprocess_batch_size = 1000 , dataset_split = 'train' ) Initialize a pre-tokenized dataset from Hugging Face. Example data = PreTokenizedDataset(dataset_path=\"NeelNanda/c4-tokenized-2b\") first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250 Parameters: dataset_path ( str ) \u2013 The path to the dataset on Hugging Face. context_size ( int , default: 250 ) \u2013 The context size for tokenized prompts. buffer_size ( int , default: 1000 ) \u2013 Buffer size for shuffling the dataset. preprocess_batch_size ( int , default: 1000 ) \u2013 Batch size for preprocessing. dataset_split ( str , default: 'train' ) \u2013 Dataset split (e.g., train ). sparse_autoencoder/source_data/pretokenized_dataset.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , dataset_path : str , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_split : str = \"train\" , ): \"\"\"Initialize a pre-tokenized dataset from Hugging Face. Example: >>> data = PreTokenizedDataset(dataset_path=\"NeelNanda/c4-tokenized-2b\") >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: dataset_path: The path to the dataset on Hugging Face. context_size: The context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_split: Dataset split (e.g., `train`). \"\"\" super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , ) preprocess ( source_batch , * , context_size ) Preprocess a batch of prompts. The method splits each pre-tokenized item based on the context size. Parameters: source_batch ( PreTokenizedDataBatch ) \u2013 A batch of source data. context_size ( int ) \u2013 The context size to use for tokenized prompts. sparse_autoencoder/source_data/pretokenized_dataset.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def preprocess ( self , source_batch : PreTokenizedDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. The method splits each pre-tokenized item based on the context size. Args: source_batch: A batch of source data. context_size: The context size to use for tokenized prompts. \"\"\" tokenized_prompts : list [ list [ int ]] = source_batch [ \"tokens\" ] # Chunk each tokenized prompt into blocks of context_size, # discarding the last block if too small. context_size_prompts = [] for encoding in tokenized_prompts : chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts }","title":"pretokenized_dataset"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch","text":"Bases: TypedDict General Pre-Tokenized Dataset Item. Structure depends on the specific dataset from Hugging Face. Source code in sparse_autoencoder/source_data/pretokenized_dataset.py 19 20 21 22 23 24 25 26 27 class PreTokenizedDataBatch ( TypedDict ): \"\"\"General Pre-Tokenized Dataset Item. Structure depends on the specific dataset from Hugging Face. \"\"\" tokens : list [ list [ int ] ] # This assumes that the dataset structure is similar to the original Neel Nanda dataset.","title":"PreTokenizedDataBatch"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset","text":"Bases: SourceDataset [ PreTokenizedDataBatch ] General Pre-Tokenized Dataset from Hugging Face. Can be used for various datasets available on Hugging Face. Source code in sparse_autoencoder/source_data/pretokenized_dataset.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @final class PreTokenizedDataset ( SourceDataset [ PreTokenizedDataBatch ]): \"\"\"General Pre-Tokenized Dataset from Hugging Face. Can be used for various datasets available on Hugging Face. \"\"\" def preprocess ( self , source_batch : PreTokenizedDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. The method splits each pre-tokenized item based on the context size. Args: source_batch: A batch of source data. context_size: The context size to use for tokenized prompts. \"\"\" tokenized_prompts : list [ list [ int ]] = source_batch [ \"tokens\" ] # Chunk each tokenized prompt into blocks of context_size, # discarding the last block if too small. context_size_prompts = [] for encoding in tokenized_prompts : chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts } def __init__ ( self , dataset_path : str , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_split : str = \"train\" , ): \"\"\"Initialize a pre-tokenized dataset from Hugging Face. Example: >>> data = PreTokenizedDataset(dataset_path=\"NeelNanda/c4-tokenized-2b\") >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: dataset_path: The path to the dataset on Hugging Face. context_size: The context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_split: Dataset split (e.g., `train`). \"\"\" super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , )","title":"PreTokenizedDataset"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.__init__","text":"Initialize a pre-tokenized dataset from Hugging Face. Example data = PreTokenizedDataset(dataset_path=\"NeelNanda/c4-tokenized-2b\") first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250 Parameters: dataset_path ( str ) \u2013 The path to the dataset on Hugging Face. context_size ( int , default: 250 ) \u2013 The context size for tokenized prompts. buffer_size ( int , default: 1000 ) \u2013 Buffer size for shuffling the dataset. preprocess_batch_size ( int , default: 1000 ) \u2013 Batch size for preprocessing. dataset_split ( str , default: 'train' ) \u2013 Dataset split (e.g., train ). sparse_autoencoder/source_data/pretokenized_dataset.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , dataset_path : str , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_split : str = \"train\" , ): \"\"\"Initialize a pre-tokenized dataset from Hugging Face. Example: >>> data = PreTokenizedDataset(dataset_path=\"NeelNanda/c4-tokenized-2b\") >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: dataset_path: The path to the dataset on Hugging Face. context_size: The context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_split: Dataset split (e.g., `train`). \"\"\" super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , )","title":"__init__()"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.preprocess","text":"Preprocess a batch of prompts. The method splits each pre-tokenized item based on the context size. Parameters: source_batch ( PreTokenizedDataBatch ) \u2013 A batch of source data. context_size ( int ) \u2013 The context size to use for tokenized prompts. sparse_autoencoder/source_data/pretokenized_dataset.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def preprocess ( self , source_batch : PreTokenizedDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. The method splits each pre-tokenized item based on the context size. Args: source_batch: A batch of source data. context_size: The context size to use for tokenized prompts. \"\"\" tokenized_prompts : list [ list [ int ]] = source_batch [ \"tokens\" ] # Chunk each tokenized prompt into blocks of context_size, # discarding the last block if too small. context_size_prompts = [] for encoding in tokenized_prompts : chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts }","title":"preprocess()"},{"location":"reference/source_data/random_int/","text":"Random Int Dummy Source Data. For use with tests and simple examples. RandomIntDummyDataset Bases: SourceDataset [ RandomIntSourceData ] Random Int Dummy Dataset. For use with tests and simple examples. Source code in sparse_autoencoder/source_data/random_int.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @final class RandomIntDummyDataset ( SourceDataset [ RandomIntSourceData ]): \"\"\"Random Int Dummy Dataset. For use with tests and simple examples. \"\"\" tokenizer : PreTrainedTokenizerFast def preprocess ( self , source_batch : RandomIntSourceData , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Not implemented for this dummy dataset. \"\"\" raise NotImplementedError def __init__ ( self , context_size : int = 250 , buffer_size : int = 1000 , # noqa: ARG002 preprocess_batch_size : int = 1000 , # noqa: ARG002 dataset_path : str = \"dummy\" , # noqa: ARG002 dataset_split : str = \"train\" , # noqa: ARG002 ): \"\"\"Initialize the Random Int Dummy dataset. Example: >>> data = RandomIntDummyDataset() >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). \"\"\" self . dataset = RandomIntHuggingFaceDataset ( 50000 , context_size = context_size ) # type: ignore def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: # type: ignore \"\"\"Get Dataloader.\"\"\" return DataLoader [ TorchTokenizedPrompts ]( self . dataset , batch_size = batch_size ) # type: ignore __init__ ( context_size = 250 , buffer_size = 1000 , preprocess_batch_size = 1000 , dataset_path = 'dummy' , dataset_split = 'train' ) Initialize the Random Int Dummy dataset. Example data = RandomIntDummyDataset() first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250 Parameters: context_size ( int , default: 250 ) \u2013 The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. buffer_size ( int , default: 1000 ) \u2013 The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least buffer_size items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size ( int , default: 1000 ) \u2013 The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). dataset_path ( str , default: 'dummy' ) \u2013 The path to the dataset on Hugging Face. dataset_split ( str , default: 'train' ) \u2013 Dataset split (e.g. train ). sparse_autoencoder/source_data/random_int.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , context_size : int = 250 , buffer_size : int = 1000 , # noqa: ARG002 preprocess_batch_size : int = 1000 , # noqa: ARG002 dataset_path : str = \"dummy\" , # noqa: ARG002 dataset_split : str = \"train\" , # noqa: ARG002 ): \"\"\"Initialize the Random Int Dummy dataset. Example: >>> data = RandomIntDummyDataset() >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). \"\"\" self . dataset = RandomIntHuggingFaceDataset ( 50000 , context_size = context_size ) # type: ignore get_dataloader ( batch_size ) Get Dataloader. sparse_autoencoder/source_data/random_int.py 110 111 112 def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: # type: ignore \"\"\"Get Dataloader.\"\"\" return DataLoader [ TorchTokenizedPrompts ]( self . dataset , batch_size = batch_size ) # type: ignore preprocess ( source_batch , * , context_size ) Preprocess a batch of prompts. Not implemented for this dummy dataset. sparse_autoencoder/source_data/random_int.py 64 65 66 67 68 69 70 71 72 73 74 def preprocess ( self , source_batch : RandomIntSourceData , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Not implemented for this dummy dataset. \"\"\" raise NotImplementedError RandomIntHuggingFaceDataset Bases: Dataset Dummy Hugging Face Dataset. Source code in sparse_autoencoder/source_data/random_int.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class RandomIntHuggingFaceDataset ( Dataset ): \"\"\"Dummy Hugging Face Dataset.\"\"\" def __init__ ( self , vocab_size : int , context_size : int ): \"\"\"Initialize the Random Int Dummy Hugging Face Dataset. Args: vocab_size: The size of the vocabulary to use. context_size: The number of tokens in the context window \"\"\" self . vocab_size = vocab_size self . context_size = context_size def __iter__ ( self ) -> \"RandomIntHuggingFaceDataset\" : # type: ignore \"\"\"Iter Dunder Method.\"\"\" return self def __next__ ( self ) -> dict [ str , list [ int ]]: \"\"\"Next Dunder Method.\"\"\" data = [ random . randint ( 0 , self . vocab_size ) for _ in range ( self . context_size )] # noqa: S311 return { \"input_ids\" : data } def __len__ ( self ) -> int : \"\"\"Len Dunder Method.\"\"\" return 1000 def __getitem__ ( self , index : int ) -> dict [ str , list [ int ]]: \"\"\"Get Item.\"\"\" return self . __next__ () __getitem__ ( index ) Get Item. sparse_autoencoder/source_data/random_int.py 50 51 52 def __getitem__ ( self , index : int ) -> dict [ str , list [ int ]]: \"\"\"Get Item.\"\"\" return self . __next__ () __init__ ( vocab_size , context_size ) Initialize the Random Int Dummy Hugging Face Dataset. Parameters: vocab_size ( int ) \u2013 The size of the vocabulary to use. context_size ( int ) \u2013 The number of tokens in the context window sparse_autoencoder/source_data/random_int.py 27 28 29 30 31 32 33 34 35 def __init__ ( self , vocab_size : int , context_size : int ): \"\"\"Initialize the Random Int Dummy Hugging Face Dataset. Args: vocab_size: The size of the vocabulary to use. context_size: The number of tokens in the context window \"\"\" self . vocab_size = vocab_size self . context_size = context_size __iter__ () Iter Dunder Method. sparse_autoencoder/source_data/random_int.py 37 38 39 def __iter__ ( self ) -> \"RandomIntHuggingFaceDataset\" : # type: ignore \"\"\"Iter Dunder Method.\"\"\" return self __len__ () Len Dunder Method. sparse_autoencoder/source_data/random_int.py 46 47 48 def __len__ ( self ) -> int : \"\"\"Len Dunder Method.\"\"\" return 1000 __next__ () Next Dunder Method. sparse_autoencoder/source_data/random_int.py 41 42 43 44 def __next__ ( self ) -> dict [ str , list [ int ]]: \"\"\"Next Dunder Method.\"\"\" data = [ random . randint ( 0 , self . vocab_size ) for _ in range ( self . context_size )] # noqa: S311 return { \"input_ids\" : data } RandomIntSourceData Bases: TypedDict Random Int Dummy Source Data. Source code in sparse_autoencoder/source_data/random_int.py 18 19 20 21 class RandomIntSourceData ( TypedDict ): \"\"\"Random Int Dummy Source Data.\"\"\" input_ids : list [ list [ int ]]","title":"random_int"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntDummyDataset","text":"Bases: SourceDataset [ RandomIntSourceData ] Random Int Dummy Dataset. For use with tests and simple examples. Source code in sparse_autoencoder/source_data/random_int.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @final class RandomIntDummyDataset ( SourceDataset [ RandomIntSourceData ]): \"\"\"Random Int Dummy Dataset. For use with tests and simple examples. \"\"\" tokenizer : PreTrainedTokenizerFast def preprocess ( self , source_batch : RandomIntSourceData , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Not implemented for this dummy dataset. \"\"\" raise NotImplementedError def __init__ ( self , context_size : int = 250 , buffer_size : int = 1000 , # noqa: ARG002 preprocess_batch_size : int = 1000 , # noqa: ARG002 dataset_path : str = \"dummy\" , # noqa: ARG002 dataset_split : str = \"train\" , # noqa: ARG002 ): \"\"\"Initialize the Random Int Dummy dataset. Example: >>> data = RandomIntDummyDataset() >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). \"\"\" self . dataset = RandomIntHuggingFaceDataset ( 50000 , context_size = context_size ) # type: ignore def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: # type: ignore \"\"\"Get Dataloader.\"\"\" return DataLoader [ TorchTokenizedPrompts ]( self . dataset , batch_size = batch_size ) # type: ignore","title":"RandomIntDummyDataset"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntDummyDataset.__init__","text":"Initialize the Random Int Dummy dataset. Example data = RandomIntDummyDataset() first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250 Parameters: context_size ( int , default: 250 ) \u2013 The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250. buffer_size ( int , default: 1000 ) \u2013 The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least buffer_size items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size ( int , default: 1000 ) \u2013 The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). dataset_path ( str , default: 'dummy' ) \u2013 The path to the dataset on Hugging Face. dataset_split ( str , default: 'train' ) \u2013 Dataset split (e.g. train ). sparse_autoencoder/source_data/random_int.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , context_size : int = 250 , buffer_size : int = 1000 , # noqa: ARG002 preprocess_batch_size : int = 1000 , # noqa: ARG002 dataset_path : str = \"dummy\" , # noqa: ARG002 dataset_split : str = \"train\" , # noqa: ARG002 ): \"\"\"Initialize the Random Int Dummy dataset. Example: >>> data = RandomIntDummyDataset() >>> first_item = next(iter(data)) >>> len(first_item[\"input_ids\"]) 250 Args: context_size: The context size to use when returning a list of tokenized prompts. *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used a context size of 250. buffer_size: The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least `buffer_size` items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied. preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts). dataset_path: The path to the dataset on Hugging Face. dataset_split: Dataset split (e.g. `train`). \"\"\" self . dataset = RandomIntHuggingFaceDataset ( 50000 , context_size = context_size ) # type: ignore","title":"__init__()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntDummyDataset.get_dataloader","text":"Get Dataloader. sparse_autoencoder/source_data/random_int.py 110 111 112 def get_dataloader ( self , batch_size : int ) -> DataLoader [ TorchTokenizedPrompts ]: # type: ignore \"\"\"Get Dataloader.\"\"\" return DataLoader [ TorchTokenizedPrompts ]( self . dataset , batch_size = batch_size ) # type: ignore","title":"get_dataloader()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntDummyDataset.preprocess","text":"Preprocess a batch of prompts. Not implemented for this dummy dataset. sparse_autoencoder/source_data/random_int.py 64 65 66 67 68 69 70 71 72 73 74 def preprocess ( self , source_batch : RandomIntSourceData , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Not implemented for this dummy dataset. \"\"\" raise NotImplementedError","title":"preprocess()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset","text":"Bases: Dataset Dummy Hugging Face Dataset. Source code in sparse_autoencoder/source_data/random_int.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class RandomIntHuggingFaceDataset ( Dataset ): \"\"\"Dummy Hugging Face Dataset.\"\"\" def __init__ ( self , vocab_size : int , context_size : int ): \"\"\"Initialize the Random Int Dummy Hugging Face Dataset. Args: vocab_size: The size of the vocabulary to use. context_size: The number of tokens in the context window \"\"\" self . vocab_size = vocab_size self . context_size = context_size def __iter__ ( self ) -> \"RandomIntHuggingFaceDataset\" : # type: ignore \"\"\"Iter Dunder Method.\"\"\" return self def __next__ ( self ) -> dict [ str , list [ int ]]: \"\"\"Next Dunder Method.\"\"\" data = [ random . randint ( 0 , self . vocab_size ) for _ in range ( self . context_size )] # noqa: S311 return { \"input_ids\" : data } def __len__ ( self ) -> int : \"\"\"Len Dunder Method.\"\"\" return 1000 def __getitem__ ( self , index : int ) -> dict [ str , list [ int ]]: \"\"\"Get Item.\"\"\" return self . __next__ ()","title":"RandomIntHuggingFaceDataset"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__getitem__","text":"Get Item. sparse_autoencoder/source_data/random_int.py 50 51 52 def __getitem__ ( self , index : int ) -> dict [ str , list [ int ]]: \"\"\"Get Item.\"\"\" return self . __next__ ()","title":"__getitem__()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__init__","text":"Initialize the Random Int Dummy Hugging Face Dataset. Parameters: vocab_size ( int ) \u2013 The size of the vocabulary to use. context_size ( int ) \u2013 The number of tokens in the context window sparse_autoencoder/source_data/random_int.py 27 28 29 30 31 32 33 34 35 def __init__ ( self , vocab_size : int , context_size : int ): \"\"\"Initialize the Random Int Dummy Hugging Face Dataset. Args: vocab_size: The size of the vocabulary to use. context_size: The number of tokens in the context window \"\"\" self . vocab_size = vocab_size self . context_size = context_size","title":"__init__()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__iter__","text":"Iter Dunder Method. sparse_autoencoder/source_data/random_int.py 37 38 39 def __iter__ ( self ) -> \"RandomIntHuggingFaceDataset\" : # type: ignore \"\"\"Iter Dunder Method.\"\"\" return self","title":"__iter__()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__len__","text":"Len Dunder Method. sparse_autoencoder/source_data/random_int.py 46 47 48 def __len__ ( self ) -> int : \"\"\"Len Dunder Method.\"\"\" return 1000","title":"__len__()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__next__","text":"Next Dunder Method. sparse_autoencoder/source_data/random_int.py 41 42 43 44 def __next__ ( self ) -> dict [ str , list [ int ]]: \"\"\"Next Dunder Method.\"\"\" data = [ random . randint ( 0 , self . vocab_size ) for _ in range ( self . context_size )] # noqa: S311 return { \"input_ids\" : data }","title":"__next__()"},{"location":"reference/source_data/random_int/#sparse_autoencoder.source_data.random_int.RandomIntSourceData","text":"Bases: TypedDict Random Int Dummy Source Data. Source code in sparse_autoencoder/source_data/random_int.py 18 19 20 21 class RandomIntSourceData ( TypedDict ): \"\"\"Random Int Dummy Source Data.\"\"\" input_ids : list [ list [ int ]]","title":"RandomIntSourceData"},{"location":"reference/source_data/text_dataset/","text":"Generic Text Dataset Module for Hugging Face Datasets. GenericTextDataset should work with the following datasets: - monology/pile-uncopyrighted - the_pile_openwebtext2 - roneneldan/TinyStories-33M - roneneldan/TinyStories-8M - roneneldan/TinyStories-3M - roneneldan/TinyStories-1Layer-21M - roneneldan/TinyStories-1M - roneneldan/TinyStories-2Layers-33M - roneneldan/TinyStories-Instruct-2Layers-33M - roneneldan/TinyStories-Instruct-28M - roneneldan/TinyStories-Instruct-33M - roneneldan/TinyStories-Instruct-8M - roneneldan/TinyStories-Instruct-3M - roneneldan/TinyStories-Instruct-1M - roneneldan/TinyStories-Instuct-1Layer-21M - roneneldan/TinyStories-28M GenericTextDataBatch Bases: TypedDict Generic Text Dataset Batch. Assumes the dataset provides a 'text' field with a list of strings. Source code in sparse_autoencoder/source_data/text_dataset.py 28 29 30 31 32 33 34 35 class GenericTextDataBatch ( TypedDict ): \"\"\"Generic Text Dataset Batch. Assumes the dataset provides a 'text' field with a list of strings. \"\"\" text : list [ str ] meta : list [ dict [ str , dict [ str , str ]]] # Optional, depending on the dataset structure. GenericTextDataset Bases: SourceDataset [ GenericTextDataBatch ] Generic Text Dataset for any text-based dataset from Hugging Face. Source code in sparse_autoencoder/source_data/text_dataset.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @final class GenericTextDataset ( SourceDataset [ GenericTextDataBatch ]): \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\" tokenizer : PreTrainedTokenizerBase def preprocess ( self , source_batch : GenericTextDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Tokenizes and chunks text data into lists of tokenized prompts with specified context size. Args: source_batch: A batch of source data, including 'text' with a list of strings. context_size: Context size for tokenized prompts. \"\"\" prompts : list [ str ] = source_batch [ \"text\" ] tokenized_prompts = self . tokenizer ( prompts , truncation = True , padding = False ) # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks. context_size_prompts = [] for encoding in list ( tokenized_prompts [ \"input_ids\" ]): # type: ignore chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts } def __init__ ( self , tokenizer : PreTrainedTokenizerBase , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_path : str = \"monology/pile-uncopyrighted\" , dataset_split : str = \"train\" , ): \"\"\"Initialize a generic text dataset from Hugging Face. Args: tokenizer: Tokenizer to process text data. context_size: Context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_path: Path to the dataset on Hugging Face. dataset_split: Dataset split (e.g., 'train'). \"\"\" self . tokenizer = tokenizer super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , ) __init__ ( tokenizer , context_size = 250 , buffer_size = 1000 , preprocess_batch_size = 1000 , dataset_path = 'monology/pile-uncopyrighted' , dataset_split = 'train' ) Initialize a generic text dataset from Hugging Face. Parameters: tokenizer ( PreTrainedTokenizerBase ) \u2013 Tokenizer to process text data. context_size ( int , default: 250 ) \u2013 Context size for tokenized prompts. buffer_size ( int , default: 1000 ) \u2013 Buffer size for shuffling the dataset. preprocess_batch_size ( int , default: 1000 ) \u2013 Batch size for preprocessing. dataset_path ( str , default: 'monology/pile-uncopyrighted' ) \u2013 Path to the dataset on Hugging Face. dataset_split ( str , default: 'train' ) \u2013 Dataset split (e.g., 'train'). sparse_autoencoder/source_data/text_dataset.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , tokenizer : PreTrainedTokenizerBase , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_path : str = \"monology/pile-uncopyrighted\" , dataset_split : str = \"train\" , ): \"\"\"Initialize a generic text dataset from Hugging Face. Args: tokenizer: Tokenizer to process text data. context_size: Context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_path: Path to the dataset on Hugging Face. dataset_split: Dataset split (e.g., 'train'). \"\"\" self . tokenizer = tokenizer super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , ) preprocess ( source_batch , * , context_size ) Preprocess a batch of prompts. Tokenizes and chunks text data into lists of tokenized prompts with specified context size. Parameters: source_batch ( GenericTextDataBatch ) \u2013 A batch of source data, including 'text' with a list of strings. context_size ( int ) \u2013 Context size for tokenized prompts. sparse_autoencoder/source_data/text_dataset.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def preprocess ( self , source_batch : GenericTextDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Tokenizes and chunks text data into lists of tokenized prompts with specified context size. Args: source_batch: A batch of source data, including 'text' with a list of strings. context_size: Context size for tokenized prompts. \"\"\" prompts : list [ str ] = source_batch [ \"text\" ] tokenized_prompts = self . tokenizer ( prompts , truncation = True , padding = False ) # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks. context_size_prompts = [] for encoding in list ( tokenized_prompts [ \"input_ids\" ]): # type: ignore chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts }","title":"text_dataset"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch","text":"Bases: TypedDict Generic Text Dataset Batch. Assumes the dataset provides a 'text' field with a list of strings. Source code in sparse_autoencoder/source_data/text_dataset.py 28 29 30 31 32 33 34 35 class GenericTextDataBatch ( TypedDict ): \"\"\"Generic Text Dataset Batch. Assumes the dataset provides a 'text' field with a list of strings. \"\"\" text : list [ str ] meta : list [ dict [ str , dict [ str , str ]]] # Optional, depending on the dataset structure.","title":"GenericTextDataBatch"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataset","text":"Bases: SourceDataset [ GenericTextDataBatch ] Generic Text Dataset for any text-based dataset from Hugging Face. Source code in sparse_autoencoder/source_data/text_dataset.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @final class GenericTextDataset ( SourceDataset [ GenericTextDataBatch ]): \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\" tokenizer : PreTrainedTokenizerBase def preprocess ( self , source_batch : GenericTextDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Tokenizes and chunks text data into lists of tokenized prompts with specified context size. Args: source_batch: A batch of source data, including 'text' with a list of strings. context_size: Context size for tokenized prompts. \"\"\" prompts : list [ str ] = source_batch [ \"text\" ] tokenized_prompts = self . tokenizer ( prompts , truncation = True , padding = False ) # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks. context_size_prompts = [] for encoding in list ( tokenized_prompts [ \"input_ids\" ]): # type: ignore chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts } def __init__ ( self , tokenizer : PreTrainedTokenizerBase , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_path : str = \"monology/pile-uncopyrighted\" , dataset_split : str = \"train\" , ): \"\"\"Initialize a generic text dataset from Hugging Face. Args: tokenizer: Tokenizer to process text data. context_size: Context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_path: Path to the dataset on Hugging Face. dataset_split: Dataset split (e.g., 'train'). \"\"\" self . tokenizer = tokenizer super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , )","title":"GenericTextDataset"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataset.__init__","text":"Initialize a generic text dataset from Hugging Face. Parameters: tokenizer ( PreTrainedTokenizerBase ) \u2013 Tokenizer to process text data. context_size ( int , default: 250 ) \u2013 Context size for tokenized prompts. buffer_size ( int , default: 1000 ) \u2013 Buffer size for shuffling the dataset. preprocess_batch_size ( int , default: 1000 ) \u2013 Batch size for preprocessing. dataset_path ( str , default: 'monology/pile-uncopyrighted' ) \u2013 Path to the dataset on Hugging Face. dataset_split ( str , default: 'train' ) \u2013 Dataset split (e.g., 'train'). sparse_autoencoder/source_data/text_dataset.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , tokenizer : PreTrainedTokenizerBase , context_size : int = 250 , buffer_size : int = 1000 , preprocess_batch_size : int = 1000 , dataset_path : str = \"monology/pile-uncopyrighted\" , dataset_split : str = \"train\" , ): \"\"\"Initialize a generic text dataset from Hugging Face. Args: tokenizer: Tokenizer to process text data. context_size: Context size for tokenized prompts. buffer_size: Buffer size for shuffling the dataset. preprocess_batch_size: Batch size for preprocessing. dataset_path: Path to the dataset on Hugging Face. dataset_split: Dataset split (e.g., 'train'). \"\"\" self . tokenizer = tokenizer super () . __init__ ( dataset_path = dataset_path , dataset_split = dataset_split , context_size = context_size , buffer_size = buffer_size , preprocess_batch_size = preprocess_batch_size , )","title":"__init__()"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataset.preprocess","text":"Preprocess a batch of prompts. Tokenizes and chunks text data into lists of tokenized prompts with specified context size. Parameters: source_batch ( GenericTextDataBatch ) \u2013 A batch of source data, including 'text' with a list of strings. context_size ( int ) \u2013 Context size for tokenized prompts. sparse_autoencoder/source_data/text_dataset.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def preprocess ( self , source_batch : GenericTextDataBatch , * , context_size : int , ) -> TokenizedPrompts : \"\"\"Preprocess a batch of prompts. Tokenizes and chunks text data into lists of tokenized prompts with specified context size. Args: source_batch: A batch of source data, including 'text' with a list of strings. context_size: Context size for tokenized prompts. \"\"\" prompts : list [ str ] = source_batch [ \"text\" ] tokenized_prompts = self . tokenizer ( prompts , truncation = True , padding = False ) # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks. context_size_prompts = [] for encoding in list ( tokenized_prompts [ \"input_ids\" ]): # type: ignore chunks = [ encoding [ i : i + context_size ] for i in range ( 0 , len ( encoding ), context_size ) if len ( encoding [ i : i + context_size ]) == context_size ] context_size_prompts . extend ( chunks ) return { \"input_ids\" : context_size_prompts }","title":"preprocess()"},{"location":"reference/src_model/","text":"Source Model.","title":"Index"},{"location":"reference/src_model/store_activations_hook/","text":"TransformerLens Hook for storing activations. store_activations_hook ( value , hook , store ) Store Activations Hook. Useful for getting just the specific activations wanted, rather than the full cache. Example: First we'll need a source model from TransformerLens and an activation store. from functools import partial from transformer_lens import HookedTransformer from sparse_autoencoder.activation_store.list_store import ListActivationStore store = ListActivationStore() model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass. model.add_hook( ... \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store) ... ) tokens = model.to_tokens(\"Hello world\") tokens.shape torch.Size([1, 3]) Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set stop_at_layer=1 as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer). _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required len(store) 3 Parameters: value ( SourceModelActivations ) \u2013 The activations to store. hook ( HookPoint ) \u2013 The hook point. store ( ActivationStore ) \u2013 The activation store. This should be pre-initialised with functools.partial . sparse_autoencoder/src_model/store_activations_hook.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def store_activations_hook ( value : SourceModelActivations , hook : HookPoint , # noqa: ARG001 store : ActivationStore , ) -> SourceModelActivations : \"\"\"Store Activations Hook. Useful for getting just the specific activations wanted, rather than the full cache. Example: First we'll need a source model from TransformerLens and an activation store. >>> from functools import partial >>> from transformer_lens import HookedTransformer >>> from sparse_autoencoder.activation_store.list_store import ListActivationStore >>> store = ListActivationStore() >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass. >>> model.add_hook( ... \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store) ... ) >>> tokens = model.to_tokens(\"Hello world\") >>> tokens.shape torch.Size([1, 3]) Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set `stop_at_layer=1` as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer). >>> _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required >>> len(store) 3 Args: value: The activations to store. hook: The hook point. store: The activation store. This should be pre-initialised with `functools.partial`. \"\"\" store . extend ( value ) # Return the unmodified value return value","title":"store_activations_hook"},{"location":"reference/src_model/store_activations_hook/#sparse_autoencoder.src_model.store_activations_hook.store_activations_hook","text":"Store Activations Hook. Useful for getting just the specific activations wanted, rather than the full cache. Example: First we'll need a source model from TransformerLens and an activation store. from functools import partial from transformer_lens import HookedTransformer from sparse_autoencoder.activation_store.list_store import ListActivationStore store = ListActivationStore() model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass. model.add_hook( ... \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store) ... ) tokens = model.to_tokens(\"Hello world\") tokens.shape torch.Size([1, 3]) Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set stop_at_layer=1 as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer). _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required len(store) 3 Parameters: value ( SourceModelActivations ) \u2013 The activations to store. hook ( HookPoint ) \u2013 The hook point. store ( ActivationStore ) \u2013 The activation store. This should be pre-initialised with functools.partial . sparse_autoencoder/src_model/store_activations_hook.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def store_activations_hook ( value : SourceModelActivations , hook : HookPoint , # noqa: ARG001 store : ActivationStore , ) -> SourceModelActivations : \"\"\"Store Activations Hook. Useful for getting just the specific activations wanted, rather than the full cache. Example: First we'll need a source model from TransformerLens and an activation store. >>> from functools import partial >>> from transformer_lens import HookedTransformer >>> from sparse_autoencoder.activation_store.list_store import ListActivationStore >>> store = ListActivationStore() >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass. >>> model.add_hook( ... \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store) ... ) >>> tokens = model.to_tokens(\"Hello world\") >>> tokens.shape torch.Size([1, 3]) Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set `stop_at_layer=1` as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer). >>> _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required >>> len(store) 3 Args: value: The activations to store. hook: The hook point. store: The activation store. This should be pre-initialised with `functools.partial`. \"\"\" store . extend ( value ) # Return the unmodified value return value","title":"store_activations_hook()"},{"location":"reference/train/","text":"Train.","title":"Index"},{"location":"reference/train/abstract_pipeline/","text":"Abstract pipeline. AbstractPipeline Bases: ABC Pipeline for training a Sparse Autoencoder on TransformerLens activations. Includes all the key functionality to train a sparse autoencoder, with a specific set of hyperparameters. Source code in sparse_autoencoder/train/abstract_pipeline.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class AbstractPipeline ( ABC ): \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations. Includes all the key functionality to train a sparse autoencoder, with a specific set of hyperparameters. \"\"\" generate_metrics : list [ AbstractGenerateMetric ] train_metrics : list [ AbstractTrainMetric ] validation_metric : list [ AbstractValidationMetric ] source_model : HookedTransformer source_dataset : SourceDataset autoencoder : SparseAutoencoder loss : AbstractLoss optimizer : AbstractOptimizerWithReset activation_resampler : AbstractActivationResampler | None progress_bar : tqdm | None @final def __init__ ( self , generate_metrics : list [ AbstractGenerateMetric ], train_metrics : list [ AbstractTrainMetric ], validation_metric : list [ AbstractValidationMetric ], source_model : HookedTransformer , autoencoder : SparseAutoencoder , source_dataset : SourceDataset , activation_resampler : AbstractActivationResampler | None , optimizer : AbstractOptimizerWithReset , loss : AbstractLoss , ): \"\"\"Initialize the pipeline.\"\"\" self . generate_metrics = generate_metrics self . train_metrics = train_metrics self . validation_metric = validation_metric self . source_model = source_model self . autoencoder = autoencoder self . source_dataset = source_dataset self . activation_resampler = activation_resampler self . optimizer = optimizer self . loss = loss @abstractmethod def generate_activations ( self ) -> TensorActivationStore : \"\"\"Generate activations.\"\"\" raise NotImplementedError @abstractmethod def train_autoencoder ( self , activations : TensorActivationStore ) -> NeuronActivity : \"\"\"Train the sparse autoencoder.\"\"\" raise NotImplementedError @abstractmethod def resample_neurons ( self , neuron_activity : NeuronActivity ) -> None : \"\"\"Resample dead neurons.\"\"\" raise NotImplementedError def validate_sae ( self ) -> None : \"\"\"Get validation metrics.\"\"\" raise NotImplementedError @final def run_pipeline ( self , source_batch_size : int , resample_frequency : int , validate_frequency : int , checkpoint_frequency : int , max_activations : int , ) -> None : \"\"\"Run the full training pipeline.\"\"\" last_resampled : int = 0 last_validated : int = 0 last_checkpoint : int = 0 neuron_activity : NeuronActivity | None = None for _ in tqdm ( range ( 0 , max_activations , source_batch_size ), title = \"Activations trained on\" ): # Generate activations : TensorActivationStore = self . generate_activations () # Train batch_neuron_activity : NeuronActivity = self . train_autoencoder ( activations ) detached_neuron_activity = batch_neuron_activity . detach () . cpu () if neuron_activity : neuron_activity . add_ ( detached_neuron_activity ) else : neuron_activity = detached_neuron_activity # Resample dead neurons (if needed) if last_resampled > resample_frequency : self . resample_neurons ( neuron_activity ) self . last_resampled = 0 # Get validation metrics (if needed) if last_validated > validate_frequency : self . validate_sae () self . last_validated = 0 # Checkpoint (if needed) if last_checkpoint > checkpoint_frequency : self . autoencoder . save_to_hf () self . last_checkpoint = 0 __init__ ( generate_metrics , train_metrics , validation_metric , source_model , autoencoder , source_dataset , activation_resampler , optimizer , loss ) Initialize the pipeline. sparse_autoencoder/train/abstract_pipeline.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @final def __init__ ( self , generate_metrics : list [ AbstractGenerateMetric ], train_metrics : list [ AbstractTrainMetric ], validation_metric : list [ AbstractValidationMetric ], source_model : HookedTransformer , autoencoder : SparseAutoencoder , source_dataset : SourceDataset , activation_resampler : AbstractActivationResampler | None , optimizer : AbstractOptimizerWithReset , loss : AbstractLoss , ): \"\"\"Initialize the pipeline.\"\"\" self . generate_metrics = generate_metrics self . train_metrics = train_metrics self . validation_metric = validation_metric self . source_model = source_model self . autoencoder = autoencoder self . source_dataset = source_dataset self . activation_resampler = activation_resampler self . optimizer = optimizer self . loss = loss generate_activations () abstractmethod Generate activations. sparse_autoencoder/train/abstract_pipeline.py 75 76 77 78 @abstractmethod def generate_activations ( self ) -> TensorActivationStore : \"\"\"Generate activations.\"\"\" raise NotImplementedError resample_neurons ( neuron_activity ) abstractmethod Resample dead neurons. sparse_autoencoder/train/abstract_pipeline.py 85 86 87 88 @abstractmethod def resample_neurons ( self , neuron_activity : NeuronActivity ) -> None : \"\"\"Resample dead neurons.\"\"\" raise NotImplementedError run_pipeline ( source_batch_size , resample_frequency , validate_frequency , checkpoint_frequency , max_activations ) Run the full training pipeline. sparse_autoencoder/train/abstract_pipeline.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @final def run_pipeline ( self , source_batch_size : int , resample_frequency : int , validate_frequency : int , checkpoint_frequency : int , max_activations : int , ) -> None : \"\"\"Run the full training pipeline.\"\"\" last_resampled : int = 0 last_validated : int = 0 last_checkpoint : int = 0 neuron_activity : NeuronActivity | None = None for _ in tqdm ( range ( 0 , max_activations , source_batch_size ), title = \"Activations trained on\" ): # Generate activations : TensorActivationStore = self . generate_activations () # Train batch_neuron_activity : NeuronActivity = self . train_autoencoder ( activations ) detached_neuron_activity = batch_neuron_activity . detach () . cpu () if neuron_activity : neuron_activity . add_ ( detached_neuron_activity ) else : neuron_activity = detached_neuron_activity # Resample dead neurons (if needed) if last_resampled > resample_frequency : self . resample_neurons ( neuron_activity ) self . last_resampled = 0 # Get validation metrics (if needed) if last_validated > validate_frequency : self . validate_sae () self . last_validated = 0 # Checkpoint (if needed) if last_checkpoint > checkpoint_frequency : self . autoencoder . save_to_hf () self . last_checkpoint = 0 train_autoencoder ( activations ) abstractmethod Train the sparse autoencoder. sparse_autoencoder/train/abstract_pipeline.py 80 81 82 83 @abstractmethod def train_autoencoder ( self , activations : TensorActivationStore ) -> NeuronActivity : \"\"\"Train the sparse autoencoder.\"\"\" raise NotImplementedError validate_sae () Get validation metrics. sparse_autoencoder/train/abstract_pipeline.py 90 91 92 def validate_sae ( self ) -> None : \"\"\"Get validation metrics.\"\"\" raise NotImplementedError","title":"abstract_pipeline"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline","text":"Bases: ABC Pipeline for training a Sparse Autoencoder on TransformerLens activations. Includes all the key functionality to train a sparse autoencoder, with a specific set of hyperparameters. Source code in sparse_autoencoder/train/abstract_pipeline.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class AbstractPipeline ( ABC ): \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations. Includes all the key functionality to train a sparse autoencoder, with a specific set of hyperparameters. \"\"\" generate_metrics : list [ AbstractGenerateMetric ] train_metrics : list [ AbstractTrainMetric ] validation_metric : list [ AbstractValidationMetric ] source_model : HookedTransformer source_dataset : SourceDataset autoencoder : SparseAutoencoder loss : AbstractLoss optimizer : AbstractOptimizerWithReset activation_resampler : AbstractActivationResampler | None progress_bar : tqdm | None @final def __init__ ( self , generate_metrics : list [ AbstractGenerateMetric ], train_metrics : list [ AbstractTrainMetric ], validation_metric : list [ AbstractValidationMetric ], source_model : HookedTransformer , autoencoder : SparseAutoencoder , source_dataset : SourceDataset , activation_resampler : AbstractActivationResampler | None , optimizer : AbstractOptimizerWithReset , loss : AbstractLoss , ): \"\"\"Initialize the pipeline.\"\"\" self . generate_metrics = generate_metrics self . train_metrics = train_metrics self . validation_metric = validation_metric self . source_model = source_model self . autoencoder = autoencoder self . source_dataset = source_dataset self . activation_resampler = activation_resampler self . optimizer = optimizer self . loss = loss @abstractmethod def generate_activations ( self ) -> TensorActivationStore : \"\"\"Generate activations.\"\"\" raise NotImplementedError @abstractmethod def train_autoencoder ( self , activations : TensorActivationStore ) -> NeuronActivity : \"\"\"Train the sparse autoencoder.\"\"\" raise NotImplementedError @abstractmethod def resample_neurons ( self , neuron_activity : NeuronActivity ) -> None : \"\"\"Resample dead neurons.\"\"\" raise NotImplementedError def validate_sae ( self ) -> None : \"\"\"Get validation metrics.\"\"\" raise NotImplementedError @final def run_pipeline ( self , source_batch_size : int , resample_frequency : int , validate_frequency : int , checkpoint_frequency : int , max_activations : int , ) -> None : \"\"\"Run the full training pipeline.\"\"\" last_resampled : int = 0 last_validated : int = 0 last_checkpoint : int = 0 neuron_activity : NeuronActivity | None = None for _ in tqdm ( range ( 0 , max_activations , source_batch_size ), title = \"Activations trained on\" ): # Generate activations : TensorActivationStore = self . generate_activations () # Train batch_neuron_activity : NeuronActivity = self . train_autoencoder ( activations ) detached_neuron_activity = batch_neuron_activity . detach () . cpu () if neuron_activity : neuron_activity . add_ ( detached_neuron_activity ) else : neuron_activity = detached_neuron_activity # Resample dead neurons (if needed) if last_resampled > resample_frequency : self . resample_neurons ( neuron_activity ) self . last_resampled = 0 # Get validation metrics (if needed) if last_validated > validate_frequency : self . validate_sae () self . last_validated = 0 # Checkpoint (if needed) if last_checkpoint > checkpoint_frequency : self . autoencoder . save_to_hf () self . last_checkpoint = 0","title":"AbstractPipeline"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline.__init__","text":"Initialize the pipeline. sparse_autoencoder/train/abstract_pipeline.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @final def __init__ ( self , generate_metrics : list [ AbstractGenerateMetric ], train_metrics : list [ AbstractTrainMetric ], validation_metric : list [ AbstractValidationMetric ], source_model : HookedTransformer , autoencoder : SparseAutoencoder , source_dataset : SourceDataset , activation_resampler : AbstractActivationResampler | None , optimizer : AbstractOptimizerWithReset , loss : AbstractLoss , ): \"\"\"Initialize the pipeline.\"\"\" self . generate_metrics = generate_metrics self . train_metrics = train_metrics self . validation_metric = validation_metric self . source_model = source_model self . autoencoder = autoencoder self . source_dataset = source_dataset self . activation_resampler = activation_resampler self . optimizer = optimizer self . loss = loss","title":"__init__()"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline.generate_activations","text":"Generate activations. sparse_autoencoder/train/abstract_pipeline.py 75 76 77 78 @abstractmethod def generate_activations ( self ) -> TensorActivationStore : \"\"\"Generate activations.\"\"\" raise NotImplementedError","title":"generate_activations()"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline.resample_neurons","text":"Resample dead neurons. sparse_autoencoder/train/abstract_pipeline.py 85 86 87 88 @abstractmethod def resample_neurons ( self , neuron_activity : NeuronActivity ) -> None : \"\"\"Resample dead neurons.\"\"\" raise NotImplementedError","title":"resample_neurons()"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline.run_pipeline","text":"Run the full training pipeline. sparse_autoencoder/train/abstract_pipeline.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @final def run_pipeline ( self , source_batch_size : int , resample_frequency : int , validate_frequency : int , checkpoint_frequency : int , max_activations : int , ) -> None : \"\"\"Run the full training pipeline.\"\"\" last_resampled : int = 0 last_validated : int = 0 last_checkpoint : int = 0 neuron_activity : NeuronActivity | None = None for _ in tqdm ( range ( 0 , max_activations , source_batch_size ), title = \"Activations trained on\" ): # Generate activations : TensorActivationStore = self . generate_activations () # Train batch_neuron_activity : NeuronActivity = self . train_autoencoder ( activations ) detached_neuron_activity = batch_neuron_activity . detach () . cpu () if neuron_activity : neuron_activity . add_ ( detached_neuron_activity ) else : neuron_activity = detached_neuron_activity # Resample dead neurons (if needed) if last_resampled > resample_frequency : self . resample_neurons ( neuron_activity ) self . last_resampled = 0 # Get validation metrics (if needed) if last_validated > validate_frequency : self . validate_sae () self . last_validated = 0 # Checkpoint (if needed) if last_checkpoint > checkpoint_frequency : self . autoencoder . save_to_hf () self . last_checkpoint = 0","title":"run_pipeline()"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline.train_autoencoder","text":"Train the sparse autoencoder. sparse_autoencoder/train/abstract_pipeline.py 80 81 82 83 @abstractmethod def train_autoencoder ( self , activations : TensorActivationStore ) -> NeuronActivity : \"\"\"Train the sparse autoencoder.\"\"\" raise NotImplementedError","title":"train_autoencoder()"},{"location":"reference/train/abstract_pipeline/#sparse_autoencoder.train.abstract_pipeline.AbstractPipeline.validate_sae","text":"Get validation metrics. sparse_autoencoder/train/abstract_pipeline.py 90 91 92 def validate_sae ( self ) -> None : \"\"\"Get validation metrics.\"\"\" raise NotImplementedError","title":"validate_sae()"},{"location":"reference/train/generate_activations/","text":"Generate activations for training a Sparse Autoencoder. generate_activations ( model , layer , cache_name , store , source_data , context_size , batch_size , num_items , device = None ) Generate activations for training a Sparse Autoencoder. Generates activations and updates the activation store in place. Warning: This function is a little confusing as it uses side effects. The way it works is to add a hook to the model, which will automatically store activations every time the model runs. When it has filled up the store to the desired size, it will return None . Your activations will then be ready in the store object that you passed to this function (i.e. it is updated in place). This approach is used as it depends on TransformerLens which uses side effects to get the activations. Parameters: model ( HookedTransformer ) \u2013 The model to generate activations for. layer ( int ) \u2013 The layer that you are hooking into to get activations. This is used to stop the model at this point rather than generating all remaining activations and logits. cache_name ( str ) \u2013 The name of the cache hook point to get activations from. Examples include [ hook_embed hook_pos_embed , blocks.0.hook_resid_pre , blocks.0.ln1.hook_scale , blocks.0.ln1.hook_normalized , blocks.0.attn.hook_q , blocks.0.attn.hook_k , blocks.0.attn.hook_v , blocks.0.attn.hook_attn_scores , blocks.0.attn.hook_pattern , blocks.0.attn.hook_z , blocks.0.hook_attn_out , blocks.0.hook_resid_mid , blocks.0.ln2.hook_scale , blocks.0.ln2.hook_normalized , blocks.0.mlp.hook_pre , blocks.0.mlp.hook_post , blocks.0.hook_mlp_out , blocks.0.hook_resid_post ]. store ( ActivationStore ) \u2013 The activation store to use. source_data ( Iterable [ TorchTokenizedPrompts ] ) \u2013 Stateful iterator that yields batches of data to generate activations. context_size ( int ) \u2013 Number of tokens in each prompt. batch_size ( int ) \u2013 Size of each batch. num_items ( int ) \u2013 Number of activation vectors to generate. This is an approximate rather than strict limit. device ( device | None , default: None ) \u2013 Device to run the model on. sparse_autoencoder/train/generate_activations.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def generate_activations ( model : HookedTransformer , layer : int , cache_name : str , store : ActivationStore , source_data : Iterable [ TorchTokenizedPrompts ], context_size : int , batch_size : int , num_items : int , device : torch . device | None = None , ) -> None : \"\"\"Generate activations for training a Sparse Autoencoder. Generates activations and updates the activation store in place. Warning: This function is a little confusing as it uses side effects. The way it works is to add a hook to the model, which will automatically store activations every time the model runs. When it has filled up the store to the desired size, it will return `None`. Your activations will then be ready in the `store` object that you passed to this function (i.e. it is updated in place). This approach is used as it depends on TransformerLens which uses side effects to get the activations. Args: model: The model to generate activations for. layer: The layer that you are hooking into to get activations. This is used to stop the model at this point rather than generating all remaining activations and logits. cache_name: The name of the cache hook point to get activations from. Examples include [`hook_embed` `hook_pos_embed`, `blocks.0.hook_resid_pre`, `blocks.0.ln1.hook_scale`, `blocks.0.ln1.hook_normalized`, `blocks.0.attn.hook_q`, `blocks.0.attn.hook_k`, `blocks.0.attn.hook_v`, `blocks.0.attn.hook_attn_scores`, `blocks.0.attn.hook_pattern`, `blocks.0.attn.hook_z`, `blocks.0.hook_attn_out`, `blocks.0.hook_resid_mid`, `blocks.0.ln2.hook_scale`, `blocks.0.ln2.hook_normalized`, `blocks.0.mlp.hook_pre`, `blocks.0.mlp.hook_post`, `blocks.0.hook_mlp_out`, `blocks.0.hook_resid_post`]. store: The activation store to use. source_data: Stateful iterator that yields batches of data to generate activations. context_size: Number of tokens in each prompt. batch_size: Size of each batch. num_items: Number of activation vectors to generate. This is an approximate rather than strict limit. device: Device to run the model on. \"\"\" # Set model to evaluation (inference) mode model . eval () if isinstance ( device , torch . device ): model . to ( device , print_details = False ) # Add the hook to the model (will automatically store the activations every time the model runs) model . remove_all_hook_fns () hook = partial ( store_activations_hook , store = store ) model . add_hook ( cache_name , hook ) # Get the input dimensions for logging activations_per_batch : int = context_size * batch_size total : int = num_items - num_items % activations_per_batch # Loop through the dataloader until the store reaches the desired size with torch . no_grad (): for batch in source_data : if len ( store ) + activations_per_batch > total : break input_ids : BatchTokenizedPrompts = batch [ \"input_ids\" ] . to ( device ) model . forward ( input_ids , stop_at_layer = layer + 1 ) # type: ignore (TLens is typed incorrectly)","title":"generate_activations"},{"location":"reference/train/generate_activations/#sparse_autoencoder.train.generate_activations.generate_activations","text":"Generate activations for training a Sparse Autoencoder. Generates activations and updates the activation store in place. Warning: This function is a little confusing as it uses side effects. The way it works is to add a hook to the model, which will automatically store activations every time the model runs. When it has filled up the store to the desired size, it will return None . Your activations will then be ready in the store object that you passed to this function (i.e. it is updated in place). This approach is used as it depends on TransformerLens which uses side effects to get the activations. Parameters: model ( HookedTransformer ) \u2013 The model to generate activations for. layer ( int ) \u2013 The layer that you are hooking into to get activations. This is used to stop the model at this point rather than generating all remaining activations and logits. cache_name ( str ) \u2013 The name of the cache hook point to get activations from. Examples include [ hook_embed hook_pos_embed , blocks.0.hook_resid_pre , blocks.0.ln1.hook_scale , blocks.0.ln1.hook_normalized , blocks.0.attn.hook_q , blocks.0.attn.hook_k , blocks.0.attn.hook_v , blocks.0.attn.hook_attn_scores , blocks.0.attn.hook_pattern , blocks.0.attn.hook_z , blocks.0.hook_attn_out , blocks.0.hook_resid_mid , blocks.0.ln2.hook_scale , blocks.0.ln2.hook_normalized , blocks.0.mlp.hook_pre , blocks.0.mlp.hook_post , blocks.0.hook_mlp_out , blocks.0.hook_resid_post ]. store ( ActivationStore ) \u2013 The activation store to use. source_data ( Iterable [ TorchTokenizedPrompts ] ) \u2013 Stateful iterator that yields batches of data to generate activations. context_size ( int ) \u2013 Number of tokens in each prompt. batch_size ( int ) \u2013 Size of each batch. num_items ( int ) \u2013 Number of activation vectors to generate. This is an approximate rather than strict limit. device ( device | None , default: None ) \u2013 Device to run the model on. sparse_autoencoder/train/generate_activations.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def generate_activations ( model : HookedTransformer , layer : int , cache_name : str , store : ActivationStore , source_data : Iterable [ TorchTokenizedPrompts ], context_size : int , batch_size : int , num_items : int , device : torch . device | None = None , ) -> None : \"\"\"Generate activations for training a Sparse Autoencoder. Generates activations and updates the activation store in place. Warning: This function is a little confusing as it uses side effects. The way it works is to add a hook to the model, which will automatically store activations every time the model runs. When it has filled up the store to the desired size, it will return `None`. Your activations will then be ready in the `store` object that you passed to this function (i.e. it is updated in place). This approach is used as it depends on TransformerLens which uses side effects to get the activations. Args: model: The model to generate activations for. layer: The layer that you are hooking into to get activations. This is used to stop the model at this point rather than generating all remaining activations and logits. cache_name: The name of the cache hook point to get activations from. Examples include [`hook_embed` `hook_pos_embed`, `blocks.0.hook_resid_pre`, `blocks.0.ln1.hook_scale`, `blocks.0.ln1.hook_normalized`, `blocks.0.attn.hook_q`, `blocks.0.attn.hook_k`, `blocks.0.attn.hook_v`, `blocks.0.attn.hook_attn_scores`, `blocks.0.attn.hook_pattern`, `blocks.0.attn.hook_z`, `blocks.0.hook_attn_out`, `blocks.0.hook_resid_mid`, `blocks.0.ln2.hook_scale`, `blocks.0.ln2.hook_normalized`, `blocks.0.mlp.hook_pre`, `blocks.0.mlp.hook_post`, `blocks.0.hook_mlp_out`, `blocks.0.hook_resid_post`]. store: The activation store to use. source_data: Stateful iterator that yields batches of data to generate activations. context_size: Number of tokens in each prompt. batch_size: Size of each batch. num_items: Number of activation vectors to generate. This is an approximate rather than strict limit. device: Device to run the model on. \"\"\" # Set model to evaluation (inference) mode model . eval () if isinstance ( device , torch . device ): model . to ( device , print_details = False ) # Add the hook to the model (will automatically store the activations every time the model runs) model . remove_all_hook_fns () hook = partial ( store_activations_hook , store = store ) model . add_hook ( cache_name , hook ) # Get the input dimensions for logging activations_per_batch : int = context_size * batch_size total : int = num_items - num_items % activations_per_batch # Loop through the dataloader until the store reaches the desired size with torch . no_grad (): for batch in source_data : if len ( store ) + activations_per_batch > total : break input_ids : BatchTokenizedPrompts = batch [ \"input_ids\" ] . to ( device ) model . forward ( input_ids , stop_at_layer = layer + 1 ) # type: ignore (TLens is typed incorrectly)","title":"generate_activations()"},{"location":"reference/train/pipeline/","text":"Training Pipeline. pipeline ( src_model , src_model_activation_hook_point , src_model_activation_layer , source_dataset , activation_store , num_activations_before_training , autoencoder , source_dataset_batch_size = 16 , resample_frequency = 25000000 , sweep_parameters = SweepParametersRuntime (), device = None , max_activations = 100000000 ) Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Parameters: src_model ( HookedTransformer ) \u2013 The model to get activations from. src_model_activation_hook_point ( str ) \u2013 The hook point to get activations from. src_model_activation_layer ( int ) \u2013 The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset ( SourceDataset ) \u2013 Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store ( ActivationStore ) \u2013 The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training ( int ) \u2013 The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder ( SparseAutoencoder ) \u2013 The autoencoder to train. source_dataset_batch_size ( int , default: 16 ) \u2013 Batch size of tokenized prompts for generating the source data. resample_frequency ( int , default: 25000000 ) \u2013 How often to resample neurons (number of activations learnt on). sweep_parameters ( SweepParametersRuntime , default: SweepParametersRuntime () ) \u2013 Parameter config to use. device ( device | None , default: None ) \u2013 Device to run pipeline on. max_activations ( int , default: 100000000 ) \u2013 Maximum number of activations to train with. May train for less if the source dataset is exhausted. sparse_autoencoder/train/pipeline.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def pipeline ( # noqa: PLR0913 src_model : HookedTransformer , src_model_activation_hook_point : str , src_model_activation_layer : int , source_dataset : SourceDataset , activation_store : ActivationStore , num_activations_before_training : int , autoencoder : SparseAutoencoder , source_dataset_batch_size : int = 16 , resample_frequency : int = 25_000_000 , sweep_parameters : SweepParametersRuntime = SweepParametersRuntime (), # noqa: B008 device : torch . device | None = None , max_activations : int = 100_000_000 , ) -> None : \"\"\"Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Args: src_model: The model to get activations from. src_model_activation_hook_point: The hook point to get activations from. src_model_activation_layer: The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset: Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store: The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training: The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder: The autoencoder to train. source_dataset_batch_size: Batch size of tokenized prompts for generating the source data. resample_frequency: How often to resample neurons (number of activations learnt on). sweep_parameters: Parameter config to use. device: Device to run pipeline on. max_activations: Maximum number of activations to train with. May train for less if the source dataset is exhausted. \"\"\" autoencoder . to ( device ) optimizer = AdamWithReset ( autoencoder . parameters (), lr = sweep_parameters . lr , betas = ( sweep_parameters . adam_beta_1 , sweep_parameters . adam_beta_2 ), eps = sweep_parameters . adam_epsilon , weight_decay = sweep_parameters . adam_weight_decay , named_parameters = autoencoder . named_parameters (), ) source_dataloader = source_dataset . get_dataloader ( source_dataset_batch_size ) source_data_iterator = stateful_dataloader_iterable ( source_dataloader ) total_steps : int = 0 activations_since_resampling : int = 0 neuron_activity : NeuronActivity = torch . zeros ( autoencoder . n_learned_features , dtype = torch . int32 , device = device ) total_activations : int = 0 # Run loop until source data is exhausted: with logging_redirect_tqdm (), tqdm ( desc = \"Total activations trained on\" , dynamic_ncols = True , total = max_activations , postfix = { \"Current mode\" : \"initializing\" }, ) as progress_bar : while total_activations < max_activations : # Add activations to the store activation_store . empty () # In case it was filled by a different run progress_bar . set_postfix ({ \"Current mode\" : \"generating\" }) generate_activations ( src_model , src_model_activation_layer , src_model_activation_hook_point , activation_store , source_data_iterator , device = device , context_size = source_dataset . context_size , num_items = num_activations_before_training , batch_size = source_dataset_batch_size , ) if len ( activation_store ) == 0 : break # Shuffle the store if it has a shuffle method - it is often more efficient to # create a shuffle method ourselves rather than get the DataLoader to shuffle activation_store . shuffle () # Train the autoencoder progress_bar . set_postfix ({ \"Current mode\" : \"training\" }) train_steps , learned_activations_fired_count = train_autoencoder ( activation_store = activation_store , autoencoder = autoencoder , optimizer = optimizer , sweep_parameters = sweep_parameters , device = device , previous_steps = total_steps , ) total_steps += train_steps if activations_since_resampling >= resample_frequency / 2 : neuron_activity . add_ ( learned_activations_fired_count ) activations_since_resampling += len ( activation_store ) total_activations += len ( activation_store ) progress_bar . update ( len ( activation_store )) # Resample neurons if required if len ( activation_store ) < DEFAULT_RESAMPLE_N : warn_str = ( f \"Warning: activation store len { len ( activation_store ) } is less than \" f \"DEFAULT_RESAMPLE_N ( { DEFAULT_RESAMPLE_N } ). Resampling with\" f \"num_resample_inputs as { len ( activation_store ) } .\" ) warnings . warn ( warn_str , stacklevel = 2 , ) num_resample_inputs = len ( activation_store ) else : num_resample_inputs = DEFAULT_RESAMPLE_N if activations_since_resampling >= resample_frequency : progress_bar . set_postfix ({ \"Current mode\" : \"resampling\" }) activations_since_resampling = 0 resample_dead_neurons ( neuron_activity = neuron_activity , store = activation_store , autoencoder = autoencoder , sweep_parameters = sweep_parameters , num_inputs = num_resample_inputs , ) learned_activations_fired_count . zero_ () optimizer . reset_state_all_parameters () activation_store . empty () stateful_dataloader_iterable ( dataloader ) Create a stateful dataloader iterable. Create an iterable that maintains it's position in the dataloader between loops. Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off. >>> from datasets import Dataset >>> from torch.utils.data import DataLoader >>> def gen (): ... yield { \"int\" : 0 } ... yield { \"int\" : 1 } >>> data = DataLoader ( Dataset . from_generator ( gen )) >>> next ( iter ( data ))[ \"int\" ], next ( iter ( data ))[ \"int\" ] (tensor([0]), tensor([0])) By contrast if you create a stateful iterable from the dataloader, each loop will get different data. >>> iterator = stateful_dataloader_iterable ( data ) >>> next ( iterator )[ \"int\" ], next ( iterator )[ \"int\" ] (tensor([0]), tensor([1])) Parameters: dataloader ( DataLoader [ TorchTokenizedPrompts ] ) \u2013 PyTorch DataLoader. Returns: Iterable [ TorchTokenizedPrompts ] \u2013 Stateful iterable over the data in the dataloader. sparse_autoencoder/train/pipeline.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def stateful_dataloader_iterable ( dataloader : DataLoader [ TorchTokenizedPrompts ] ) -> Iterable [ TorchTokenizedPrompts ]: \"\"\"Create a stateful dataloader iterable. Create an iterable that maintains it's position in the dataloader between loops. Examples: Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off. >>> from datasets import Dataset >>> from torch.utils.data import DataLoader >>> def gen(): ... yield {\"int\": 0} ... yield {\"int\": 1} >>> data = DataLoader(Dataset.from_generator(gen)) >>> next(iter(data))[\"int\"], next(iter(data))[\"int\"] (tensor([0]), tensor([0])) By contrast if you create a stateful iterable from the dataloader, each loop will get different data. >>> iterator = stateful_dataloader_iterable(data) >>> next(iterator)[\"int\"], next(iterator)[\"int\"] (tensor([0]), tensor([1])) Args: dataloader: PyTorch DataLoader. Returns: Stateful iterable over the data in the dataloader. \"\"\" yield from dataloader","title":"pipeline"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.pipeline","text":"Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Parameters: src_model ( HookedTransformer ) \u2013 The model to get activations from. src_model_activation_hook_point ( str ) \u2013 The hook point to get activations from. src_model_activation_layer ( int ) \u2013 The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset ( SourceDataset ) \u2013 Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store ( ActivationStore ) \u2013 The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training ( int ) \u2013 The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder ( SparseAutoencoder ) \u2013 The autoencoder to train. source_dataset_batch_size ( int , default: 16 ) \u2013 Batch size of tokenized prompts for generating the source data. resample_frequency ( int , default: 25000000 ) \u2013 How often to resample neurons (number of activations learnt on). sweep_parameters ( SweepParametersRuntime , default: SweepParametersRuntime () ) \u2013 Parameter config to use. device ( device | None , default: None ) \u2013 Device to run pipeline on. max_activations ( int , default: 100000000 ) \u2013 Maximum number of activations to train with. May train for less if the source dataset is exhausted. sparse_autoencoder/train/pipeline.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def pipeline ( # noqa: PLR0913 src_model : HookedTransformer , src_model_activation_hook_point : str , src_model_activation_layer : int , source_dataset : SourceDataset , activation_store : ActivationStore , num_activations_before_training : int , autoencoder : SparseAutoencoder , source_dataset_batch_size : int = 16 , resample_frequency : int = 25_000_000 , sweep_parameters : SweepParametersRuntime = SweepParametersRuntime (), # noqa: B008 device : torch . device | None = None , max_activations : int = 100_000_000 , ) -> None : \"\"\"Full pipeline for training the sparse autoEncoder. The pipeline alternates between generating activations and training the autoencoder. Args: src_model: The model to get activations from. src_model_activation_hook_point: The hook point to get activations from. src_model_activation_layer: The layer to get activations from. This is used to stop the model after this layer, as we don't need the final logits. source_dataset: Source dataset containing source model inputs (typically batches of prompts) that are used to generate the activations data. activation_store: The store to buffer activations in once generated, before training the autoencoder. num_activations_before_training: The number of activations to generate before training the autoencoder. As a guide, 1 million activations, each of size 1024, will take up about 2GB of memory (assuming float16/bfloat16). autoencoder: The autoencoder to train. source_dataset_batch_size: Batch size of tokenized prompts for generating the source data. resample_frequency: How often to resample neurons (number of activations learnt on). sweep_parameters: Parameter config to use. device: Device to run pipeline on. max_activations: Maximum number of activations to train with. May train for less if the source dataset is exhausted. \"\"\" autoencoder . to ( device ) optimizer = AdamWithReset ( autoencoder . parameters (), lr = sweep_parameters . lr , betas = ( sweep_parameters . adam_beta_1 , sweep_parameters . adam_beta_2 ), eps = sweep_parameters . adam_epsilon , weight_decay = sweep_parameters . adam_weight_decay , named_parameters = autoencoder . named_parameters (), ) source_dataloader = source_dataset . get_dataloader ( source_dataset_batch_size ) source_data_iterator = stateful_dataloader_iterable ( source_dataloader ) total_steps : int = 0 activations_since_resampling : int = 0 neuron_activity : NeuronActivity = torch . zeros ( autoencoder . n_learned_features , dtype = torch . int32 , device = device ) total_activations : int = 0 # Run loop until source data is exhausted: with logging_redirect_tqdm (), tqdm ( desc = \"Total activations trained on\" , dynamic_ncols = True , total = max_activations , postfix = { \"Current mode\" : \"initializing\" }, ) as progress_bar : while total_activations < max_activations : # Add activations to the store activation_store . empty () # In case it was filled by a different run progress_bar . set_postfix ({ \"Current mode\" : \"generating\" }) generate_activations ( src_model , src_model_activation_layer , src_model_activation_hook_point , activation_store , source_data_iterator , device = device , context_size = source_dataset . context_size , num_items = num_activations_before_training , batch_size = source_dataset_batch_size , ) if len ( activation_store ) == 0 : break # Shuffle the store if it has a shuffle method - it is often more efficient to # create a shuffle method ourselves rather than get the DataLoader to shuffle activation_store . shuffle () # Train the autoencoder progress_bar . set_postfix ({ \"Current mode\" : \"training\" }) train_steps , learned_activations_fired_count = train_autoencoder ( activation_store = activation_store , autoencoder = autoencoder , optimizer = optimizer , sweep_parameters = sweep_parameters , device = device , previous_steps = total_steps , ) total_steps += train_steps if activations_since_resampling >= resample_frequency / 2 : neuron_activity . add_ ( learned_activations_fired_count ) activations_since_resampling += len ( activation_store ) total_activations += len ( activation_store ) progress_bar . update ( len ( activation_store )) # Resample neurons if required if len ( activation_store ) < DEFAULT_RESAMPLE_N : warn_str = ( f \"Warning: activation store len { len ( activation_store ) } is less than \" f \"DEFAULT_RESAMPLE_N ( { DEFAULT_RESAMPLE_N } ). Resampling with\" f \"num_resample_inputs as { len ( activation_store ) } .\" ) warnings . warn ( warn_str , stacklevel = 2 , ) num_resample_inputs = len ( activation_store ) else : num_resample_inputs = DEFAULT_RESAMPLE_N if activations_since_resampling >= resample_frequency : progress_bar . set_postfix ({ \"Current mode\" : \"resampling\" }) activations_since_resampling = 0 resample_dead_neurons ( neuron_activity = neuron_activity , store = activation_store , autoencoder = autoencoder , sweep_parameters = sweep_parameters , num_inputs = num_resample_inputs , ) learned_activations_fired_count . zero_ () optimizer . reset_state_all_parameters () activation_store . empty ()","title":"pipeline()"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.stateful_dataloader_iterable","text":"Create a stateful dataloader iterable. Create an iterable that maintains it's position in the dataloader between loops. Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off. >>> from datasets import Dataset >>> from torch.utils.data import DataLoader >>> def gen (): ... yield { \"int\" : 0 } ... yield { \"int\" : 1 } >>> data = DataLoader ( Dataset . from_generator ( gen )) >>> next ( iter ( data ))[ \"int\" ], next ( iter ( data ))[ \"int\" ] (tensor([0]), tensor([0])) By contrast if you create a stateful iterable from the dataloader, each loop will get different data. >>> iterator = stateful_dataloader_iterable ( data ) >>> next ( iterator )[ \"int\" ], next ( iterator )[ \"int\" ] (tensor([0]), tensor([1])) Parameters: dataloader ( DataLoader [ TorchTokenizedPrompts ] ) \u2013 PyTorch DataLoader. Returns: Iterable [ TorchTokenizedPrompts ] \u2013 Stateful iterable over the data in the dataloader. sparse_autoencoder/train/pipeline.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def stateful_dataloader_iterable ( dataloader : DataLoader [ TorchTokenizedPrompts ] ) -> Iterable [ TorchTokenizedPrompts ]: \"\"\"Create a stateful dataloader iterable. Create an iterable that maintains it's position in the dataloader between loops. Examples: Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off. >>> from datasets import Dataset >>> from torch.utils.data import DataLoader >>> def gen(): ... yield {\"int\": 0} ... yield {\"int\": 1} >>> data = DataLoader(Dataset.from_generator(gen)) >>> next(iter(data))[\"int\"], next(iter(data))[\"int\"] (tensor([0]), tensor([0])) By contrast if you create a stateful iterable from the dataloader, each loop will get different data. >>> iterator = stateful_dataloader_iterable(data) >>> next(iterator)[\"int\"], next(iterator)[\"int\"] (tensor([0]), tensor([1])) Args: dataloader: PyTorch DataLoader. Returns: Stateful iterable over the data in the dataloader. \"\"\" yield from dataloader","title":"stateful_dataloader_iterable()"},{"location":"reference/train/resample_neurons/","text":"Neuron Resampling. assign_sampling_probabilities ( loss ) Assign the sampling probabilities for each input activations vector. Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input. Example loss = torch.tensor([1.0, 2.0, 3.0]) assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000]) Parameters: loss ( TrainBatchStatistic ) \u2013 Loss per item. Returns: Tensor \u2013 A tensor of probabilities for each item. sparse_autoencoder/train/resample_neurons.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def assign_sampling_probabilities ( loss : TrainBatchStatistic ) -> Tensor : \"\"\"Assign the sampling probabilities for each input activations vector. Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input. Example: >>> loss = torch.tensor([1.0, 2.0, 3.0]) >>> assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000]) Args: loss: Loss per item. Returns: A tensor of probabilities for each item. \"\"\" square_loss = loss . pow ( 2 ) return square_loss / square_loss . sum () compute_loss_and_get_activations ( store , autoencoder , sweep_parameters , num_inputs ) Compute the loss on a random subset of inputs. Computes the loss and also stores the input activations (for use in resampling neurons). Parameters: store ( ActivationStore ) \u2013 Activation store. autoencoder ( SparseAutoencoder ) \u2013 Sparse autoencoder model. sweep_parameters ( SweepParametersRuntime ) \u2013 Current training sweep parameters. num_inputs ( int ) \u2013 Number of input activations to use. Returns: tuple [ TrainBatchStatistic , InputOutputActivationBatch ] \u2013 A tuple containing the loss per item, and all input activations. sparse_autoencoder/train/resample_neurons.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def compute_loss_and_get_activations ( store : ActivationStore , autoencoder : SparseAutoencoder , sweep_parameters : SweepParametersRuntime , num_inputs : int , ) -> tuple [ TrainBatchStatistic , InputOutputActivationBatch ]: \"\"\"Compute the loss on a random subset of inputs. Computes the loss and also stores the input activations (for use in resampling neurons). Args: store: Activation store. autoencoder: Sparse autoencoder model. sweep_parameters: Current training sweep parameters. num_inputs: Number of input activations to use. Returns: A tuple containing the loss per item, and all input activations. \"\"\" with torch . no_grad (): loss , _metrics = LossReducer ( MSEReconstructionLoss (), LearnedActivationsL1Loss ( sweep_parameters . l1_coefficient ), ) loss_batches : list [ TrainBatchStatistic ] = [] input_activations_batches : list [ InputOutputActivationBatch ] = [] batch_size : int = sweep_parameters . batch_size dataloader = DataLoader ( store , batch_size = batch_size ) batches : int = num_inputs // batch_size for batch_idx , batch in enumerate ( iter ( dataloader )): input_activations_batches . append ( batch ) learned_activations , reconstructed_activations = autoencoder ( batch ) loss_batches . append ( loss . forward ( batch , learned_activations , reconstructed_activations )) if batch_idx >= batches : break loss = torch . cat ( loss_batches ) input_activations = torch . cat ( input_activations_batches ) # Check we generated enough data if len ( loss ) < num_inputs : error_message = ( f \"Cannot get { num_inputs } items from the store, as only { len ( loss ) } were available.\" ) raise ValueError ( error_message ) return loss , input_activations get_dead_neuron_indices ( neuron_activity , threshold = 0 ) Identify the indices of neurons that have zero activity. Example neuron_activity = torch.tensor([0, 0, 3, 10, 0]) dead_neuron_indices = get_dead_neuron_indices(neuron_activity) dead_neuron_indices.tolist() [0, 1, 4] Parameters: neuron_activity ( NeuronActivity ) \u2013 Tensor representing the number of times each neuron fired. threshold ( int , default: 0 ) \u2013 Threshold for determining if a neuron is dead (has fired less than this number of times. Returns: DeadNeuronIndices \u2013 A tensor containing the indices of neurons that are 'dead' (zero activity). sparse_autoencoder/train/resample_neurons.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def get_dead_neuron_indices ( neuron_activity : NeuronActivity , threshold : int = 0 ) -> DeadNeuronIndices : \"\"\"Identify the indices of neurons that have zero activity. Example: >>> neuron_activity = torch.tensor([0, 0, 3, 10, 0]) >>> dead_neuron_indices = get_dead_neuron_indices(neuron_activity) >>> dead_neuron_indices.tolist() [0, 1, 4] Args: neuron_activity: Tensor representing the number of times each neuron fired. threshold: Threshold for determining if a neuron is dead (has fired less than this number of times. Returns: A tensor containing the indices of neurons that are 'dead' (zero activity). \"\"\" return torch . where ( neuron_activity <= threshold )[ 0 ] renormalize_and_scale ( sampled_input , neuron_activity , encoder_weight ) Renormalize and scale the resampled dictionary vectors. Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2. Example _seed = torch.manual_seed(0) # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) encoder_weight = torch.ones((6, 2)) rescaled_input = renormalize_and_scale(sampled_input, neuron_activity, encoder_weight) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]]) Parameters: sampled_input ( SampledDeadNeuronInputs ) \u2013 Tensor of the sampled input activation. neuron_activity ( NeuronActivity ) \u2013 Tensor representing the number of times each neuron fired. encoder_weight ( EncoderWeights ) \u2013 Tensor of encoder weights. Returns: DeadEncoderNeuronWeightUpdates \u2013 Rescaled sampled input. Raises: ValueError \u2013 If there are no alive neurons. sparse_autoencoder/train/resample_neurons.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def renormalize_and_scale ( sampled_input : SampledDeadNeuronInputs , neuron_activity : NeuronActivity , encoder_weight : EncoderWeights , ) -> DeadEncoderNeuronWeightUpdates : \"\"\"Renormalize and scale the resampled dictionary vectors. Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2. Example: >>> _seed = torch.manual_seed(0) # For reproducibility in example >>> sampled_input = torch.tensor([[3.0, 4.0]]) >>> neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) >>> encoder_weight = torch.ones((6, 2)) >>> rescaled_input = renormalize_and_scale(sampled_input, neuron_activity, encoder_weight) >>> rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]]) Args: sampled_input: Tensor of the sampled input activation. neuron_activity: Tensor representing the number of times each neuron fired. encoder_weight: Tensor of encoder weights. Returns: Rescaled sampled input. Raises: ValueError: If there are no alive neurons. \"\"\" alive_neuron_mask : Bool [ Tensor , \" learned_features\" ] = neuron_activity > 0 # Check there is at least one alive neuron if not torch . any ( alive_neuron_mask ): error_message = \"No alive neurons found.\" raise ValueError ( error_message ) # Handle all alive neurons if torch . all ( alive_neuron_mask ): return torch . empty ( ( 0 , sampled_input . shape [ - 1 ]), dtype = sampled_input . dtype , device = sampled_input . device ) # Calculate the average norm of the encoder weights for alive neurons. alive_encoder_weights : AliveEncoderWeights = encoder_weight [ alive_neuron_mask , :] average_alive_norm : ItemTensor = alive_encoder_weights . norm ( dim =- 1 ) . mean () # Renormalize the input vector to equal the average norm of the encoder weights for alive # neurons times 0.2. renormalized_input : SampledDeadNeuronInputs = torch . nn . functional . normalize ( sampled_input , dim =- 1 ) return renormalized_input * ( average_alive_norm * 0.2 ) resample_dead_neurons ( neuron_activity , store , autoencoder , sweep_parameters , num_inputs = 819200 ) Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone. This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network. Warning The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Note this approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge. Parameters: neuron_activity ( NeuronActivity ) \u2013 Number of times each neuron fired. store ( ActivationStore ) \u2013 Activation store. autoencoder ( SparseAutoencoder ) \u2013 Sparse autoencoder model. sweep_parameters ( SweepParametersRuntime ) \u2013 Current training sweep parameters. num_inputs ( int , default: 819200 ) \u2013 Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. sparse_autoencoder/train/resample_neurons.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def resample_dead_neurons ( neuron_activity : NeuronActivity , store : ActivationStore , autoencoder : SparseAutoencoder , sweep_parameters : SweepParametersRuntime , num_inputs : int = 819_200 , ) -> None : \"\"\"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone. This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network. Warning: The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Note this approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge. Args: neuron_activity: Number of times each neuron fired. store: Activation store. autoencoder: Sparse autoencoder model. sweep_parameters: Current training sweep parameters. num_inputs: Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. \"\"\" with torch . no_grad (): dead_neuron_indices = get_dead_neuron_indices ( neuron_activity ) # Compute the loss for the current model on a random subset of inputs and get the # activations. loss , input_activations = compute_loss_and_get_activations ( store , autoencoder , sweep_parameters , num_inputs ) # Assign each input vector a probability of being picked that is proportional to the square # of the autoencoder's loss on that input. sample_probabilities : TrainBatchStatistic = assign_sampling_probabilities ( loss ) # Get references to the encoder and decoder parameters encoder_linear : torch . nn . Linear = autoencoder . encoder . get_submodule ( \"Linear\" ) # type: ignore decoder_linear : ConstrainedUnitNormLinear = autoencoder . decoder . get_submodule ( \"ConstrainedUnitNormLinear\" ) # type: ignore encoder_weight : EncoderWeights = encoder_linear . weight encoder_bias : LearntActivationVector = encoder_linear . bias decoder_weight : DecoderWeights = decoder_linear . weight # For each dead neuron sample an input according to these probabilities. sampled_input : SampledDeadNeuronInputs = sample_input ( sample_probabilities , input_activations , len ( dead_neuron_indices ) ) # Renormalize the input vector to have unit L2 norm and set this to be the dictionary # vector for the dead autoencoder neuron. renormalized_input : SampledDeadNeuronInputs = torch . nn . functional . normalize ( sampled_input , dim =- 1 ) decoder_weight [:, dead_neuron_indices ] = rearrange ( renormalized_input , \"dead_neuron input_feature -> input_feature dead_neuron\" ) # For the corresponding encoder vector, renormalize the input vector to equal the # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding # encoder bias element to zero. rescaled_sampled_input = renormalize_and_scale ( sampled_input , neuron_activity , encoder_weight ) encoder_weight . data [ dead_neuron_indices , :] = rescaled_sampled_input encoder_bias . data [ dead_neuron_indices ] = 0.0 sample_input ( probabilities , input_activations , num_samples ) Sample an input vector based on the provided probabilities. Example probabilities = torch.tensor([0.1, 0.2, 0.7]) input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) _seed = torch.manual_seed(0) # For reproducibility in example sampled_input = sample_input(probabilities, input_activations, 2) sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]] Parameters: probabilities ( TrainBatchStatistic ) \u2013 Probabilities for each input. input_activations ( InputOutputActivationBatch ) \u2013 Input activation vectors. num_samples ( int ) \u2013 Number of samples to take (number of dead neurons). Returns: SampledDeadNeuronInputs \u2013 Sampled input activation vector. Raises: ValueError \u2013 If the number of samples is greater than the number of input activations. sparse_autoencoder/train/resample_neurons.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def sample_input ( probabilities : TrainBatchStatistic , input_activations : InputOutputActivationBatch , num_samples : int , ) -> SampledDeadNeuronInputs : \"\"\"Sample an input vector based on the provided probabilities. Example: >>> probabilities = torch.tensor([0.1, 0.2, 0.7]) >>> input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> _seed = torch.manual_seed(0) # For reproducibility in example >>> sampled_input = sample_input(probabilities, input_activations, 2) >>> sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]] Args: probabilities: Probabilities for each input. input_activations: Input activation vectors. num_samples: Number of samples to take (number of dead neurons). Returns: Sampled input activation vector. Raises: ValueError: If the number of samples is greater than the number of input activations. \"\"\" if num_samples > len ( input_activations ): exception_message = ( f \"Cannot sample { num_samples } inputs from { len ( input_activations ) } input activations.\" ) raise ValueError ( exception_message ) if num_samples == 0 : return torch . empty ( ( 0 , input_activations . shape [ - 1 ]), dtype = input_activations . dtype , device = input_activations . device , ) sample_indices : DeadNeuronIndices = torch . multinomial ( probabilities , num_samples = num_samples ) return input_activations [ sample_indices , :]","title":"resample_neurons"},{"location":"reference/train/resample_neurons/#sparse_autoencoder.train.resample_neurons.assign_sampling_probabilities","text":"Assign the sampling probabilities for each input activations vector. Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input. Example loss = torch.tensor([1.0, 2.0, 3.0]) assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000]) Parameters: loss ( TrainBatchStatistic ) \u2013 Loss per item. Returns: Tensor \u2013 A tensor of probabilities for each item. sparse_autoencoder/train/resample_neurons.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def assign_sampling_probabilities ( loss : TrainBatchStatistic ) -> Tensor : \"\"\"Assign the sampling probabilities for each input activations vector. Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input. Example: >>> loss = torch.tensor([1.0, 2.0, 3.0]) >>> assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000]) Args: loss: Loss per item. Returns: A tensor of probabilities for each item. \"\"\" square_loss = loss . pow ( 2 ) return square_loss / square_loss . sum ()","title":"assign_sampling_probabilities()"},{"location":"reference/train/resample_neurons/#sparse_autoencoder.train.resample_neurons.compute_loss_and_get_activations","text":"Compute the loss on a random subset of inputs. Computes the loss and also stores the input activations (for use in resampling neurons). Parameters: store ( ActivationStore ) \u2013 Activation store. autoencoder ( SparseAutoencoder ) \u2013 Sparse autoencoder model. sweep_parameters ( SweepParametersRuntime ) \u2013 Current training sweep parameters. num_inputs ( int ) \u2013 Number of input activations to use. Returns: tuple [ TrainBatchStatistic , InputOutputActivationBatch ] \u2013 A tuple containing the loss per item, and all input activations. sparse_autoencoder/train/resample_neurons.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def compute_loss_and_get_activations ( store : ActivationStore , autoencoder : SparseAutoencoder , sweep_parameters : SweepParametersRuntime , num_inputs : int , ) -> tuple [ TrainBatchStatistic , InputOutputActivationBatch ]: \"\"\"Compute the loss on a random subset of inputs. Computes the loss and also stores the input activations (for use in resampling neurons). Args: store: Activation store. autoencoder: Sparse autoencoder model. sweep_parameters: Current training sweep parameters. num_inputs: Number of input activations to use. Returns: A tuple containing the loss per item, and all input activations. \"\"\" with torch . no_grad (): loss , _metrics = LossReducer ( MSEReconstructionLoss (), LearnedActivationsL1Loss ( sweep_parameters . l1_coefficient ), ) loss_batches : list [ TrainBatchStatistic ] = [] input_activations_batches : list [ InputOutputActivationBatch ] = [] batch_size : int = sweep_parameters . batch_size dataloader = DataLoader ( store , batch_size = batch_size ) batches : int = num_inputs // batch_size for batch_idx , batch in enumerate ( iter ( dataloader )): input_activations_batches . append ( batch ) learned_activations , reconstructed_activations = autoencoder ( batch ) loss_batches . append ( loss . forward ( batch , learned_activations , reconstructed_activations )) if batch_idx >= batches : break loss = torch . cat ( loss_batches ) input_activations = torch . cat ( input_activations_batches ) # Check we generated enough data if len ( loss ) < num_inputs : error_message = ( f \"Cannot get { num_inputs } items from the store, as only { len ( loss ) } were available.\" ) raise ValueError ( error_message ) return loss , input_activations","title":"compute_loss_and_get_activations()"},{"location":"reference/train/resample_neurons/#sparse_autoencoder.train.resample_neurons.get_dead_neuron_indices","text":"Identify the indices of neurons that have zero activity. Example neuron_activity = torch.tensor([0, 0, 3, 10, 0]) dead_neuron_indices = get_dead_neuron_indices(neuron_activity) dead_neuron_indices.tolist() [0, 1, 4] Parameters: neuron_activity ( NeuronActivity ) \u2013 Tensor representing the number of times each neuron fired. threshold ( int , default: 0 ) \u2013 Threshold for determining if a neuron is dead (has fired less than this number of times. Returns: DeadNeuronIndices \u2013 A tensor containing the indices of neurons that are 'dead' (zero activity). sparse_autoencoder/train/resample_neurons.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def get_dead_neuron_indices ( neuron_activity : NeuronActivity , threshold : int = 0 ) -> DeadNeuronIndices : \"\"\"Identify the indices of neurons that have zero activity. Example: >>> neuron_activity = torch.tensor([0, 0, 3, 10, 0]) >>> dead_neuron_indices = get_dead_neuron_indices(neuron_activity) >>> dead_neuron_indices.tolist() [0, 1, 4] Args: neuron_activity: Tensor representing the number of times each neuron fired. threshold: Threshold for determining if a neuron is dead (has fired less than this number of times. Returns: A tensor containing the indices of neurons that are 'dead' (zero activity). \"\"\" return torch . where ( neuron_activity <= threshold )[ 0 ]","title":"get_dead_neuron_indices()"},{"location":"reference/train/resample_neurons/#sparse_autoencoder.train.resample_neurons.renormalize_and_scale","text":"Renormalize and scale the resampled dictionary vectors. Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2. Example _seed = torch.manual_seed(0) # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) encoder_weight = torch.ones((6, 2)) rescaled_input = renormalize_and_scale(sampled_input, neuron_activity, encoder_weight) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]]) Parameters: sampled_input ( SampledDeadNeuronInputs ) \u2013 Tensor of the sampled input activation. neuron_activity ( NeuronActivity ) \u2013 Tensor representing the number of times each neuron fired. encoder_weight ( EncoderWeights ) \u2013 Tensor of encoder weights. Returns: DeadEncoderNeuronWeightUpdates \u2013 Rescaled sampled input. Raises: ValueError \u2013 If there are no alive neurons. sparse_autoencoder/train/resample_neurons.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def renormalize_and_scale ( sampled_input : SampledDeadNeuronInputs , neuron_activity : NeuronActivity , encoder_weight : EncoderWeights , ) -> DeadEncoderNeuronWeightUpdates : \"\"\"Renormalize and scale the resampled dictionary vectors. Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2. Example: >>> _seed = torch.manual_seed(0) # For reproducibility in example >>> sampled_input = torch.tensor([[3.0, 4.0]]) >>> neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) >>> encoder_weight = torch.ones((6, 2)) >>> rescaled_input = renormalize_and_scale(sampled_input, neuron_activity, encoder_weight) >>> rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]]) Args: sampled_input: Tensor of the sampled input activation. neuron_activity: Tensor representing the number of times each neuron fired. encoder_weight: Tensor of encoder weights. Returns: Rescaled sampled input. Raises: ValueError: If there are no alive neurons. \"\"\" alive_neuron_mask : Bool [ Tensor , \" learned_features\" ] = neuron_activity > 0 # Check there is at least one alive neuron if not torch . any ( alive_neuron_mask ): error_message = \"No alive neurons found.\" raise ValueError ( error_message ) # Handle all alive neurons if torch . all ( alive_neuron_mask ): return torch . empty ( ( 0 , sampled_input . shape [ - 1 ]), dtype = sampled_input . dtype , device = sampled_input . device ) # Calculate the average norm of the encoder weights for alive neurons. alive_encoder_weights : AliveEncoderWeights = encoder_weight [ alive_neuron_mask , :] average_alive_norm : ItemTensor = alive_encoder_weights . norm ( dim =- 1 ) . mean () # Renormalize the input vector to equal the average norm of the encoder weights for alive # neurons times 0.2. renormalized_input : SampledDeadNeuronInputs = torch . nn . functional . normalize ( sampled_input , dim =- 1 ) return renormalized_input * ( average_alive_norm * 0.2 )","title":"renormalize_and_scale()"},{"location":"reference/train/resample_neurons/#sparse_autoencoder.train.resample_neurons.resample_dead_neurons","text":"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone. This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network. Warning The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Note this approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge. Parameters: neuron_activity ( NeuronActivity ) \u2013 Number of times each neuron fired. store ( ActivationStore ) \u2013 Activation store. autoencoder ( SparseAutoencoder ) \u2013 Sparse autoencoder model. sweep_parameters ( SweepParametersRuntime ) \u2013 Current training sweep parameters. num_inputs ( int , default: 819200 ) \u2013 Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. sparse_autoencoder/train/resample_neurons.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def resample_dead_neurons ( neuron_activity : NeuronActivity , store : ActivationStore , autoencoder : SparseAutoencoder , sweep_parameters : SweepParametersRuntime , num_inputs : int = 819_200 , ) -> None : \"\"\"Resample dead neurons. Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions. An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone. This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network. Warning: The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases. Note this approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge. Args: neuron_activity: Number of times each neuron fired. store: Activation store. autoencoder: Sparse autoencoder model. sweep_parameters: Current training sweep parameters. num_inputs: Number of input activations to use when resampling. Will be rounded down to be divisible by the batch size, and cannot be larger than the number of items currently in the store. \"\"\" with torch . no_grad (): dead_neuron_indices = get_dead_neuron_indices ( neuron_activity ) # Compute the loss for the current model on a random subset of inputs and get the # activations. loss , input_activations = compute_loss_and_get_activations ( store , autoencoder , sweep_parameters , num_inputs ) # Assign each input vector a probability of being picked that is proportional to the square # of the autoencoder's loss on that input. sample_probabilities : TrainBatchStatistic = assign_sampling_probabilities ( loss ) # Get references to the encoder and decoder parameters encoder_linear : torch . nn . Linear = autoencoder . encoder . get_submodule ( \"Linear\" ) # type: ignore decoder_linear : ConstrainedUnitNormLinear = autoencoder . decoder . get_submodule ( \"ConstrainedUnitNormLinear\" ) # type: ignore encoder_weight : EncoderWeights = encoder_linear . weight encoder_bias : LearntActivationVector = encoder_linear . bias decoder_weight : DecoderWeights = decoder_linear . weight # For each dead neuron sample an input according to these probabilities. sampled_input : SampledDeadNeuronInputs = sample_input ( sample_probabilities , input_activations , len ( dead_neuron_indices ) ) # Renormalize the input vector to have unit L2 norm and set this to be the dictionary # vector for the dead autoencoder neuron. renormalized_input : SampledDeadNeuronInputs = torch . nn . functional . normalize ( sampled_input , dim =- 1 ) decoder_weight [:, dead_neuron_indices ] = rearrange ( renormalized_input , \"dead_neuron input_feature -> input_feature dead_neuron\" ) # For the corresponding encoder vector, renormalize the input vector to equal the # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding # encoder bias element to zero. rescaled_sampled_input = renormalize_and_scale ( sampled_input , neuron_activity , encoder_weight ) encoder_weight . data [ dead_neuron_indices , :] = rescaled_sampled_input encoder_bias . data [ dead_neuron_indices ] = 0.0","title":"resample_dead_neurons()"},{"location":"reference/train/resample_neurons/#sparse_autoencoder.train.resample_neurons.sample_input","text":"Sample an input vector based on the provided probabilities. Example probabilities = torch.tensor([0.1, 0.2, 0.7]) input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) _seed = torch.manual_seed(0) # For reproducibility in example sampled_input = sample_input(probabilities, input_activations, 2) sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]] Parameters: probabilities ( TrainBatchStatistic ) \u2013 Probabilities for each input. input_activations ( InputOutputActivationBatch ) \u2013 Input activation vectors. num_samples ( int ) \u2013 Number of samples to take (number of dead neurons). Returns: SampledDeadNeuronInputs \u2013 Sampled input activation vector. Raises: ValueError \u2013 If the number of samples is greater than the number of input activations. sparse_autoencoder/train/resample_neurons.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def sample_input ( probabilities : TrainBatchStatistic , input_activations : InputOutputActivationBatch , num_samples : int , ) -> SampledDeadNeuronInputs : \"\"\"Sample an input vector based on the provided probabilities. Example: >>> probabilities = torch.tensor([0.1, 0.2, 0.7]) >>> input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> _seed = torch.manual_seed(0) # For reproducibility in example >>> sampled_input = sample_input(probabilities, input_activations, 2) >>> sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]] Args: probabilities: Probabilities for each input. input_activations: Input activation vectors. num_samples: Number of samples to take (number of dead neurons). Returns: Sampled input activation vector. Raises: ValueError: If the number of samples is greater than the number of input activations. \"\"\" if num_samples > len ( input_activations ): exception_message = ( f \"Cannot sample { num_samples } inputs from { len ( input_activations ) } input activations.\" ) raise ValueError ( exception_message ) if num_samples == 0 : return torch . empty ( ( 0 , input_activations . shape [ - 1 ]), dtype = input_activations . dtype , device = input_activations . device , ) sample_indices : DeadNeuronIndices = torch . multinomial ( probabilities , num_samples = num_samples ) return input_activations [ sample_indices , :]","title":"sample_input()"},{"location":"reference/train/sweep_config/","text":"Sweep Config. SweepConfig dataclass Bases: WandbSweepConfig Sweep Config. Source code in sparse_autoencoder/train/sweep_config.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @dataclass ( frozen = True ) class SweepConfig ( WandbSweepConfig ): \"\"\"Sweep Config.\"\"\" parameters : SweepParameterConfig method : Method = Method . grid metric : Metric = field ( default_factory = lambda : Metric ( name = \"loss\" )) def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" dict_representation = asdict ( self ) # Convert StrEnums to strings dict_representation [ \"method\" ] = dict_representation [ \"method\" ] . value return dict_representation to_dict () Return dict representation of this object. sparse_autoencoder/train/sweep_config.py 97 98 99 100 101 102 103 104 def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" dict_representation = asdict ( self ) # Convert StrEnums to strings dict_representation [ \"method\" ] = dict_representation [ \"method\" ] . value return dict_representation SweepParameterConfig dataclass Bases: Parameters Sweep Parameter Config. Source code in sparse_autoencoder/train/sweep_config.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @dataclass ( frozen = True ) class SweepParameterConfig ( Parameters ): \"\"\"Sweep Parameter Config.\"\"\" lr : Parameter [ float ] | None \"\"\"Adam Learning Rate.\"\"\" adam_beta_1 : Parameter [ float ] | None \"\"\"Adam Beta 1. The exponential decay rate for the first moment estimates (mean) of the gradient. \"\"\" adam_beta_2 : Parameter [ float ] | None \"\"\"Adam Beta 2. The exponential decay rate for the second moment estimates (variance) of the gradient. \"\"\" adam_epsilon : Parameter [ float ] | None \"\"\"Adam Epsilon. A small constant for numerical stability. \"\"\" adam_weight_decay : Parameter [ float ] | None \"\"\"Adam Weight Decay. Weight decay (L2 penalty). \"\"\" l1_coefficient : Parameter [ float ] | None \"\"\"L1 Penalty Coefficient. The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. Default values from the [original paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html). \"\"\" batch_size : Parameter [ int ] | None \"\"\"Batch size. Used in SAE Forward Pass.\"\"\" adam_beta_1 : Parameter [ float ] | None instance-attribute Adam Beta 1. The exponential decay rate for the first moment estimates (mean) of the gradient. adam_beta_2 : Parameter [ float ] | None instance-attribute Adam Beta 2. The exponential decay rate for the second moment estimates (variance) of the gradient. adam_epsilon : Parameter [ float ] | None instance-attribute Adam Epsilon. A small constant for numerical stability. adam_weight_decay : Parameter [ float ] | None instance-attribute Adam Weight Decay. Weight decay (L2 penalty). batch_size : Parameter [ int ] | None instance-attribute Batch size. Used in SAE Forward Pass. l1_coefficient : Parameter [ float ] | None instance-attribute L1 Penalty Coefficient. The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. Default values from the original paper . lr : Parameter [ float ] | None instance-attribute Adam Learning Rate. SweepParametersRuntime dataclass Bases: dict [ str , Any ] Sweep parameter runtime values. Source code in sparse_autoencoder/train/sweep_config.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass ( frozen = True ) class SweepParametersRuntime ( dict [ str , Any ]): \"\"\"Sweep parameter runtime values.\"\"\" lr : float = 0.001 adam_beta_1 : float = 0.9 adam_beta_2 : float = 0.999 adam_epsilon : float = 1e-8 adam_weight_decay : float = 0.0 l1_coefficient : float = 0.001 batch_size : int = 4096 def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" return asdict ( self ) to_dict () Return dict representation of this object. sparse_autoencoder/train/sweep_config.py 82 83 84 def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" return asdict ( self )","title":"sweep_config"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig","text":"Bases: WandbSweepConfig Sweep Config. Source code in sparse_autoencoder/train/sweep_config.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @dataclass ( frozen = True ) class SweepConfig ( WandbSweepConfig ): \"\"\"Sweep Config.\"\"\" parameters : SweepParameterConfig method : Method = Method . grid metric : Metric = field ( default_factory = lambda : Metric ( name = \"loss\" )) def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" dict_representation = asdict ( self ) # Convert StrEnums to strings dict_representation [ \"method\" ] = dict_representation [ \"method\" ] . value return dict_representation","title":"SweepConfig"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig.to_dict","text":"Return dict representation of this object. sparse_autoencoder/train/sweep_config.py 97 98 99 100 101 102 103 104 def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" dict_representation = asdict ( self ) # Convert StrEnums to strings dict_representation [ \"method\" ] = dict_representation [ \"method\" ] . value return dict_representation","title":"to_dict()"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig","text":"Bases: Parameters Sweep Parameter Config. Source code in sparse_autoencoder/train/sweep_config.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @dataclass ( frozen = True ) class SweepParameterConfig ( Parameters ): \"\"\"Sweep Parameter Config.\"\"\" lr : Parameter [ float ] | None \"\"\"Adam Learning Rate.\"\"\" adam_beta_1 : Parameter [ float ] | None \"\"\"Adam Beta 1. The exponential decay rate for the first moment estimates (mean) of the gradient. \"\"\" adam_beta_2 : Parameter [ float ] | None \"\"\"Adam Beta 2. The exponential decay rate for the second moment estimates (variance) of the gradient. \"\"\" adam_epsilon : Parameter [ float ] | None \"\"\"Adam Epsilon. A small constant for numerical stability. \"\"\" adam_weight_decay : Parameter [ float ] | None \"\"\"Adam Weight Decay. Weight decay (L2 penalty). \"\"\" l1_coefficient : Parameter [ float ] | None \"\"\"L1 Penalty Coefficient. The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. Default values from the [original paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html). \"\"\" batch_size : Parameter [ int ] | None \"\"\"Batch size. Used in SAE Forward Pass.\"\"\"","title":"SweepParameterConfig"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_beta_1","text":"Adam Beta 1. The exponential decay rate for the first moment estimates (mean) of the gradient.","title":"adam_beta_1"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_beta_2","text":"Adam Beta 2. The exponential decay rate for the second moment estimates (variance) of the gradient.","title":"adam_beta_2"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_epsilon","text":"Adam Epsilon. A small constant for numerical stability.","title":"adam_epsilon"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_weight_decay","text":"Adam Weight Decay. Weight decay (L2 penalty).","title":"adam_weight_decay"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.batch_size","text":"Batch size. Used in SAE Forward Pass.","title":"batch_size"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.l1_coefficient","text":"L1 Penalty Coefficient. The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. Default values from the original paper .","title":"l1_coefficient"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.lr","text":"Adam Learning Rate.","title":"lr"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParametersRuntime","text":"Bases: dict [ str , Any ] Sweep parameter runtime values. Source code in sparse_autoencoder/train/sweep_config.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass ( frozen = True ) class SweepParametersRuntime ( dict [ str , Any ]): \"\"\"Sweep parameter runtime values.\"\"\" lr : float = 0.001 adam_beta_1 : float = 0.9 adam_beta_2 : float = 0.999 adam_epsilon : float = 1e-8 adam_weight_decay : float = 0.0 l1_coefficient : float = 0.001 batch_size : int = 4096 def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" return asdict ( self )","title":"SweepParametersRuntime"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParametersRuntime.to_dict","text":"Return dict representation of this object. sparse_autoencoder/train/sweep_config.py 82 83 84 def to_dict ( self ) -> dict [ str , Any ]: \"\"\"Return dict representation of this object.\"\"\" return asdict ( self )","title":"to_dict()"},{"location":"reference/train/train_autoencoder/","text":"Training Pipeline. train_autoencoder ( activation_store , autoencoder , optimizer , sweep_parameters , previous_steps , log_interval = 10 , device = None ) Sparse Autoencoder Training Loop. Parameters: activation_store ( ActivationStore ) \u2013 Activation store to train on. autoencoder ( SparseAutoencoder ) \u2013 Sparse autoencoder model. optimizer ( Optimizer ) \u2013 The optimizer to use. sweep_parameters ( SweepParametersRuntime ) \u2013 The sweep parameters to use. previous_steps ( int ) \u2013 Training steps from previous generate/train iterations. log_interval ( int , default: 10 ) \u2013 How often to log progress. device ( device | None , default: None ) \u2013 Decide to use. Returns: tuple [ int , LearntActivationVector ] \u2013 Number of steps taken. sparse_autoencoder/train/train_autoencoder.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def train_autoencoder ( activation_store : ActivationStore , autoencoder : SparseAutoencoder , optimizer : Optimizer , sweep_parameters : SweepParametersRuntime , previous_steps : int , log_interval : int = 10 , device : device | None = None , ) -> tuple [ int , LearntActivationVector ]: \"\"\"Sparse Autoencoder Training Loop. Args: activation_store: Activation store to train on. autoencoder: Sparse autoencoder model. optimizer: The optimizer to use. sweep_parameters: The sweep parameters to use. previous_steps: Training steps from previous generate/train iterations. log_interval: How often to log progress. device: Decide to use. Returns: Number of steps taken. \"\"\" # Create a dataloader from the store activations_dataloader = DataLoader ( activation_store , batch_size = sweep_parameters . batch_size , ) learned_activations_fired_count : NeuronActivity = torch . zeros ( autoencoder . n_learned_features , dtype = torch . int32 , device = device ) loss = LossReducer ( MSEReconstructionLoss (), LearnedActivationsL1Loss ( sweep_parameters . l1_coefficient ), ) step : int = 0 # Initialize step for step , store_batch in enumerate ( activations_dataloader ): # Zero the gradients optimizer . zero_grad () # Move the batch to the device (in place) batch = store_batch . detach () . to ( device ) # Forward pass learned_activations , reconstructed_activations = autoencoder ( batch ) # Get metrics total_loss , metrics = loss . batch_scalar_loss_with_log ( batch , learned_activations , reconstructed_activations ) # Store count of how many neurons have fired with torch . no_grad (): fired = learned_activations > 0 learned_activations_fired_count . add_ ( fired . sum ( dim = 0 )) # Backwards pass total_loss . backward () optimizer . step () # Log if step % log_interval == 0 and wandb . run is not None : wandb . log ( metrics ) current_step = previous_steps + step + 1 return current_step , learned_activations_fired_count","title":"train_autoencoder"},{"location":"reference/train/train_autoencoder/#sparse_autoencoder.train.train_autoencoder.train_autoencoder","text":"Sparse Autoencoder Training Loop. Parameters: activation_store ( ActivationStore ) \u2013 Activation store to train on. autoencoder ( SparseAutoencoder ) \u2013 Sparse autoencoder model. optimizer ( Optimizer ) \u2013 The optimizer to use. sweep_parameters ( SweepParametersRuntime ) \u2013 The sweep parameters to use. previous_steps ( int ) \u2013 Training steps from previous generate/train iterations. log_interval ( int , default: 10 ) \u2013 How often to log progress. device ( device | None , default: None ) \u2013 Decide to use. Returns: tuple [ int , LearntActivationVector ] \u2013 Number of steps taken. sparse_autoencoder/train/train_autoencoder.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def train_autoencoder ( activation_store : ActivationStore , autoencoder : SparseAutoencoder , optimizer : Optimizer , sweep_parameters : SweepParametersRuntime , previous_steps : int , log_interval : int = 10 , device : device | None = None , ) -> tuple [ int , LearntActivationVector ]: \"\"\"Sparse Autoencoder Training Loop. Args: activation_store: Activation store to train on. autoencoder: Sparse autoencoder model. optimizer: The optimizer to use. sweep_parameters: The sweep parameters to use. previous_steps: Training steps from previous generate/train iterations. log_interval: How often to log progress. device: Decide to use. Returns: Number of steps taken. \"\"\" # Create a dataloader from the store activations_dataloader = DataLoader ( activation_store , batch_size = sweep_parameters . batch_size , ) learned_activations_fired_count : NeuronActivity = torch . zeros ( autoencoder . n_learned_features , dtype = torch . int32 , device = device ) loss = LossReducer ( MSEReconstructionLoss (), LearnedActivationsL1Loss ( sweep_parameters . l1_coefficient ), ) step : int = 0 # Initialize step for step , store_batch in enumerate ( activations_dataloader ): # Zero the gradients optimizer . zero_grad () # Move the batch to the device (in place) batch = store_batch . detach () . to ( device ) # Forward pass learned_activations , reconstructed_activations = autoencoder ( batch ) # Get metrics total_loss , metrics = loss . batch_scalar_loss_with_log ( batch , learned_activations , reconstructed_activations ) # Store count of how many neurons have fired with torch . no_grad (): fired = learned_activations > 0 learned_activations_fired_count . add_ ( fired . sum ( dim = 0 )) # Backwards pass total_loss . backward () optimizer . step () # Log if step % log_interval == 0 and wandb . run is not None : wandb . log ( metrics ) current_step = previous_steps + step + 1 return current_step , learned_activations_fired_count","title":"train_autoencoder()"},{"location":"reference/train/metrics/","text":"Train Metrics.","title":"Index"},{"location":"reference/train/metrics/capacity/","text":"Capacity metrics for sets of learned features. calc_capacities ( features ) Calculate capacities. Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks . Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features. If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n. Example import torch orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) orthogonal_caps = calc_capacities(orthogonal_features) orthogonal_caps tensor([1., 1., 1.]) Parameters: features ( LearnedActivationBatch ) \u2013 A collection of features. Returns: TrainBatchStatistic \u2013 A 1D tensor of capacities, where each element is the capacity of the corresponding feature. sparse_autoencoder/train/metrics/capacity.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def calc_capacities ( features : LearnedActivationBatch ) -> TrainBatchStatistic : \"\"\"Calculate capacities. Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf). Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features. If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n. Example: >>> import torch >>> orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) >>> orthogonal_caps = calc_capacities(orthogonal_features) >>> orthogonal_caps tensor([1., 1., 1.]) Args: features: A collection of features. Returns: A 1D tensor of capacities, where each element is the capacity of the corresponding feature. \"\"\" squared_dot_products = ( einops . einsum ( features , features , \"n_feats1 feat_dim, n_feats2 feat_dim -> n_feats1 n_feats2\" ) ** 2 ) sum_of_sq_dot = squared_dot_products . sum ( dim =- 1 ) return torch . diag ( squared_dot_products ) / sum_of_sq_dot wandb_capacities_histogram ( capacities ) Create a W&B histogram of the capacities. This can be logged with Weights & Biases using e.g. wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)}) . Parameters: capacities ( TrainBatchStatistic ) \u2013 Capacity of each feature. Can be calculated using :func: calc_capacities . Returns: Histogram \u2013 Weights & Biases histogram for logging with wandb.log . sparse_autoencoder/train/metrics/capacity.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def wandb_capacities_histogram ( capacities : TrainBatchStatistic , ) -> wandb . Histogram : \"\"\"Create a W&B histogram of the capacities. This can be logged with Weights & Biases using e.g. `wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)})`. Args: capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`. Returns: Weights & Biases histogram for logging with `wandb.log`. \"\"\" numpy_capacities : NDArray [ np . float_ ] = capacities . detach () . cpu () . numpy () bins , values = histogram ( numpy_capacities , bins = 20 , range = ( 0 , 1 )) return wandb . Histogram ( np_histogram = ( bins , values ))","title":"capacity"},{"location":"reference/train/metrics/capacity/#sparse_autoencoder.train.metrics.capacity.calc_capacities","text":"Calculate capacities. Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks . Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features. If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n. Example import torch orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) orthogonal_caps = calc_capacities(orthogonal_features) orthogonal_caps tensor([1., 1., 1.]) Parameters: features ( LearnedActivationBatch ) \u2013 A collection of features. Returns: TrainBatchStatistic \u2013 A 1D tensor of capacities, where each element is the capacity of the corresponding feature. sparse_autoencoder/train/metrics/capacity.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def calc_capacities ( features : LearnedActivationBatch ) -> TrainBatchStatistic : \"\"\"Calculate capacities. Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf). Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features. If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n. Example: >>> import torch >>> orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) >>> orthogonal_caps = calc_capacities(orthogonal_features) >>> orthogonal_caps tensor([1., 1., 1.]) Args: features: A collection of features. Returns: A 1D tensor of capacities, where each element is the capacity of the corresponding feature. \"\"\" squared_dot_products = ( einops . einsum ( features , features , \"n_feats1 feat_dim, n_feats2 feat_dim -> n_feats1 n_feats2\" ) ** 2 ) sum_of_sq_dot = squared_dot_products . sum ( dim =- 1 ) return torch . diag ( squared_dot_products ) / sum_of_sq_dot","title":"calc_capacities()"},{"location":"reference/train/metrics/capacity/#sparse_autoencoder.train.metrics.capacity.wandb_capacities_histogram","text":"Create a W&B histogram of the capacities. This can be logged with Weights & Biases using e.g. wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)}) . Parameters: capacities ( TrainBatchStatistic ) \u2013 Capacity of each feature. Can be calculated using :func: calc_capacities . Returns: Histogram \u2013 Weights & Biases histogram for logging with wandb.log . sparse_autoencoder/train/metrics/capacity.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def wandb_capacities_histogram ( capacities : TrainBatchStatistic , ) -> wandb . Histogram : \"\"\"Create a W&B histogram of the capacities. This can be logged with Weights & Biases using e.g. `wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)})`. Args: capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`. Returns: Weights & Biases histogram for logging with `wandb.log`. \"\"\" numpy_capacities : NDArray [ np . float_ ] = capacities . detach () . cpu () . numpy () bins , values = histogram ( numpy_capacities , bins = 20 , range = ( 0 , 1 )) return wandb . Histogram ( np_histogram = ( bins , values ))","title":"wandb_capacities_histogram()"},{"location":"reference/train/metrics/feature_density/","text":"Feature density metrics & histogram. calc_feature_density ( activations , threshold = 0.001 ) Count how many times each feature was active. Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"). Example import torch activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) calc_feature_density(activations).tolist() [1.0, 0.5, 0.0] Parameters: activations ( LearnedActivationBatch ) \u2013 Sample of cached activations (the Autoencoder's learned features). threshold ( float , default: 0.001 ) \u2013 Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero. Returns: LearntActivationVector \u2013 Number of times each feature was active in a sample. sparse_autoencoder/train/metrics/feature_density.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def calc_feature_density ( activations : LearnedActivationBatch , threshold : float = 0.001 ) -> LearntActivationVector : \"\"\"Count how many times each feature was active. Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"). Example: >>> import torch >>> activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) >>> calc_feature_density(activations).tolist() [1.0, 0.5, 0.0] Args: activations: Sample of cached activations (the Autoencoder's learned features). threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero. Returns: Number of times each feature was active in a sample. \"\"\" has_fired : LearnedActivationBatch = torch . gt ( activations , threshold ) . to ( # Use float as einops requires this (64 as some features are very sparse) dtype = torch . float64 ) return einops . reduce ( has_fired , \"sample activation -> activation\" , \"mean\" ) wandb_feature_density_histogram ( feature_density ) Create a W&B histogram of the feature density. This can be logged with Weights & Biases using e.g. wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)}) . Parameters: feature_density ( LearntActivationVector ) \u2013 Number of times each feature was active in a sample. Can be calculated using :func: feature_activity_count . Returns: Histogram \u2013 Weights & Biases histogram for logging with wandb.log . sparse_autoencoder/train/metrics/feature_density.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def wandb_feature_density_histogram ( feature_density : LearntActivationVector , ) -> wandb . Histogram : \"\"\"Create a W&B histogram of the feature density. This can be logged with Weights & Biases using e.g. `wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)})`. Args: feature_density: Number of times each feature was active in a sample. Can be calculated using :func:`feature_activity_count`. Returns: Weights & Biases histogram for logging with `wandb.log`. \"\"\" numpy_feature_density : NDArray [ np . float_ ] = feature_density . detach () . cpu () . numpy () bins , values = histogram ( numpy_feature_density , bins = \"auto\" ) return wandb . Histogram ( np_histogram = ( bins , values ))","title":"feature_density"},{"location":"reference/train/metrics/feature_density/#sparse_autoencoder.train.metrics.feature_density.calc_feature_density","text":"Count how many times each feature was active. Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"). Example import torch activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) calc_feature_density(activations).tolist() [1.0, 0.5, 0.0] Parameters: activations ( LearnedActivationBatch ) \u2013 Sample of cached activations (the Autoencoder's learned features). threshold ( float , default: 0.001 ) \u2013 Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero. Returns: LearntActivationVector \u2013 Number of times each feature was active in a sample. sparse_autoencoder/train/metrics/feature_density.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def calc_feature_density ( activations : LearnedActivationBatch , threshold : float = 0.001 ) -> LearntActivationVector : \"\"\"Count how many times each feature was active. Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"). Example: >>> import torch >>> activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) >>> calc_feature_density(activations).tolist() [1.0, 0.5, 0.0] Args: activations: Sample of cached activations (the Autoencoder's learned features). threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero. Returns: Number of times each feature was active in a sample. \"\"\" has_fired : LearnedActivationBatch = torch . gt ( activations , threshold ) . to ( # Use float as einops requires this (64 as some features are very sparse) dtype = torch . float64 ) return einops . reduce ( has_fired , \"sample activation -> activation\" , \"mean\" )","title":"calc_feature_density()"},{"location":"reference/train/metrics/feature_density/#sparse_autoencoder.train.metrics.feature_density.wandb_feature_density_histogram","text":"Create a W&B histogram of the feature density. This can be logged with Weights & Biases using e.g. wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)}) . Parameters: feature_density ( LearntActivationVector ) \u2013 Number of times each feature was active in a sample. Can be calculated using :func: feature_activity_count . Returns: Histogram \u2013 Weights & Biases histogram for logging with wandb.log . sparse_autoencoder/train/metrics/feature_density.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def wandb_feature_density_histogram ( feature_density : LearntActivationVector , ) -> wandb . Histogram : \"\"\"Create a W&B histogram of the feature density. This can be logged with Weights & Biases using e.g. `wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)})`. Args: feature_density: Number of times each feature was active in a sample. Can be calculated using :func:`feature_activity_count`. Returns: Weights & Biases histogram for logging with `wandb.log`. \"\"\" numpy_feature_density : NDArray [ np . float_ ] = feature_density . detach () . cpu () . numpy () bins , values = histogram ( numpy_feature_density , bins = \"auto\" ) return wandb . Histogram ( np_histogram = ( bins , values ))","title":"wandb_feature_density_histogram()"},{"location":"reference/train/utils/","text":"Train Utils.","title":"Index"},{"location":"reference/train/utils/wandb_sweep_types/","text":"Wandb Sweep Config Dataclasses. Weights & Biases just provide a JSON Schema, so we've converted here to dataclasses. Controller dataclass Controller. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 18 19 20 21 22 @dataclass ( frozen = True ) class Controller : \"\"\"Controller.\"\"\" type : ControllerType # noqa: A003 ControllerType Bases: Enum Controller Type. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 11 12 13 14 15 class ControllerType ( Enum ): \"\"\"Controller Type.\"\"\" cloud = \"cloud\" local = \"local\" Distribution Bases: Enum Sweep Distribution. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Distribution ( Enum ): \"\"\"Sweep Distribution.\"\"\" beta = \"beta\" categorical = \"categorical\" categoricalwprobabilities = \"categorical_w_probabilities\" constant = \"constant\" intuniform = \"int_uniform\" invloguniform = \"inv_log_uniform\" invloguniformvalues = \"inv_log_uniform_values\" lognormal = \"log_normal\" loguniform = \"log_uniform\" loguniformvalues = \"log_uniform_values\" normal = \"normal\" qbeta = \"q_beta\" qlognormal = \"q_log_normal\" qloguniform = \"q_log_uniform\" qloguniformvalues = \"q_log_uniform_values\" qnormal = \"q_normal\" quniform = \"q_uniform\" uniform = \"uniform\" Goal Bases: Enum Goal. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 84 85 86 87 88 class Goal ( Enum ): \"\"\"Goal.\"\"\" maximize = \"maximize\" minimize = \"minimize\" HyperbandStopping dataclass Hyperband Stopping Config. Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @dataclass ( frozen = True ) class HyperbandStopping : \"\"\"Hyperband Stopping Config. Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs. \"\"\" type : HyperbandStoppingType # noqa: A003 eta : float | None = None \"\"\"ETA. At every eta^n steps, hyperband continues running the top 1/eta runs and stops all other runs. \"\"\" maxiter : int | None = None \"\"\"Max Iterations. Set the last epoch to finish trimming runs, and hyperband will automatically calculate the prior epochs to trim runs. \"\"\" miniter : int | None = None \"\"\"Min Iterations. Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs. \"\"\" s : float | None = None \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\" strict : bool | None = None \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\" eta : float | None = None class-attribute instance-attribute ETA. At every eta^n steps, hyperband continues running the top 1/eta runs and stops all other runs. maxiter : int | None = None class-attribute instance-attribute Max Iterations. Set the last epoch to finish trimming runs, and hyperband will automatically calculate the prior epochs to trim runs. miniter : int | None = None class-attribute instance-attribute Min Iterations. Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs. s : float | None = None class-attribute instance-attribute Set the number of steps you trim runs at, working backwards from the max_iter. strict : bool | None = None class-attribute instance-attribute Use a more aggressive condition for termination, stops more runs. HyperbandStoppingType Bases: Enum Hyperband Stopping Type. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 25 26 27 28 class HyperbandStoppingType ( Enum ): \"\"\"Hyperband Stopping Type.\"\"\" hyperband = \"hyperband\" Impute Bases: Enum Metric value to use in bayes search for runs that fail, crash, or are killed. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 91 92 93 94 95 96 class Impute ( Enum ): \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\" best = \"best\" latest = \"latest\" worst = \"worst\" ImputeWhileRunning Bases: Enum Appends a calculated metric even when epochs are in a running state. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 99 100 101 102 103 104 105 class ImputeWhileRunning ( Enum ): \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\" best = \"best\" false = \"false\" latest = \"latest\" worst = \"worst\" Kind Bases: Enum Kind. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 69 70 71 72 class Kind ( Enum ): \"\"\"Kind.\"\"\" sweep = \"sweep\" Method Bases: Enum Method. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 75 76 77 78 79 80 81 class Method ( Enum ): \"\"\"Method.\"\"\" bayes = \"bayes\" custom = \"custom\" grid = \"grid\" random = \"random\" Metric dataclass Metric to optimize. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @dataclass ( frozen = True ) class Metric : \"\"\"Metric to optimize.\"\"\" name : str \"\"\"Name of metric.\"\"\" goal : Goal | None = None impute : Impute | None = None \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\" imputewhilerunning : ImputeWhileRunning | None = None \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\" target : float | None = None \"\"\"The sweep will finish once any run achieves this value.\"\"\" impute : Impute | None = None class-attribute instance-attribute Metric value to use in bayes search for runs that fail, crash, or are killed imputewhilerunning : ImputeWhileRunning | None = None class-attribute instance-attribute Appends a calculated metric even when epochs are in a running state. name : str instance-attribute Name of metric. target : float | None = None class-attribute instance-attribute The sweep will finish once any run achieves this value. Parameter dataclass Bases: Generic [ ParamType ] Sweep Parameter. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @dataclass ( frozen = True ) class Parameter ( Generic [ ParamType ]): \"\"\"Sweep Parameter.\"\"\" value : ParamType | list [ ParamType ] max : ParamType | None = None # noqa: A003 min : ParamType | None = None # noqa: A003 a : float | None = None b : float | None = None distribution : Distribution | None = None q : float | None = None \"\"\"Quantization parameter for quantized distributions\"\"\" values : list [ ParamType ] | None = None \"\"\"Discrete values\"\"\" probabilities : list [ float ] | None = None \"\"\"Probability of each value\"\"\" mu : float | None = None \"\"\"Mean for normal or lognormal distributions\"\"\" sigma : float | None = None \"\"\"Std Dev for normal or lognormal distributions\"\"\" parameters : dict [ str , \"Parameter[ParamType]\" ] | None = None mu : float | None = None class-attribute instance-attribute Mean for normal or lognormal distributions probabilities : list [ float ] | None = None class-attribute instance-attribute Probability of each value q : float | None = None class-attribute instance-attribute Quantization parameter for quantized distributions sigma : float | None = None class-attribute instance-attribute Std Dev for normal or lognormal distributions values : list [ ParamType ] | None = None class-attribute instance-attribute Discrete values WandbSweepConfig dataclass Weights & Biases Sweep Configuration. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 @dataclass ( frozen = True ) class WandbSweepConfig : \"\"\"Weights & Biases Sweep Configuration.\"\"\" parameters : Parameters | Any method : Method metric : Metric \"\"\"Metric to optimize\"\"\" apiVersion : str | None = None command : list [ Any ] | None = None \"\"\"Command used to launch the training script\"\"\" controller : Controller | None = None description : str | None = None \"\"\"Short package description\"\"\" earlyterminate : HyperbandStopping | None = None entity : str | None = None \"\"\"The entity for this sweep\"\"\" imageuri : str | None = None \"\"\"Sweeps on Launch will use this uri instead of a job.\"\"\" job : str | None = None \"\"\"Launch Job to run.\"\"\" kind : Kind | None = None name : str | None = None \"\"\"The name of the sweep, displayed in the W&B UI.\"\"\" program : str | None = None \"\"\"Training script to run.\"\"\" project : str | None = None \"\"\"The project for this sweep.\"\"\" runcap : int | None = None \"\"\"Run Cap. Sweep will run no more than this number of runs, across any number of agents. \"\"\" command : list [ Any ] | None = None class-attribute instance-attribute Command used to launch the training script description : str | None = None class-attribute instance-attribute Short package description entity : str | None = None class-attribute instance-attribute The entity for this sweep imageuri : str | None = None class-attribute instance-attribute Sweeps on Launch will use this uri instead of a job. job : str | None = None class-attribute instance-attribute Launch Job to run. metric : Metric instance-attribute Metric to optimize name : str | None = None class-attribute instance-attribute The name of the sweep, displayed in the W&B UI. program : str | None = None class-attribute instance-attribute Training script to run. project : str | None = None class-attribute instance-attribute The project for this sweep. runcap : int | None = None class-attribute instance-attribute Run Cap. Sweep will run no more than this number of runs, across any number of agents.","title":"wandb_sweep_types"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Controller","text":"Controller. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 18 19 20 21 22 @dataclass ( frozen = True ) class Controller : \"\"\"Controller.\"\"\" type : ControllerType # noqa: A003","title":"Controller"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType","text":"Bases: Enum Controller Type. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 11 12 13 14 15 class ControllerType ( Enum ): \"\"\"Controller Type.\"\"\" cloud = \"cloud\" local = \"local\"","title":"ControllerType"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution","text":"Bases: Enum Sweep Distribution. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Distribution ( Enum ): \"\"\"Sweep Distribution.\"\"\" beta = \"beta\" categorical = \"categorical\" categoricalwprobabilities = \"categorical_w_probabilities\" constant = \"constant\" intuniform = \"int_uniform\" invloguniform = \"inv_log_uniform\" invloguniformvalues = \"inv_log_uniform_values\" lognormal = \"log_normal\" loguniform = \"log_uniform\" loguniformvalues = \"log_uniform_values\" normal = \"normal\" qbeta = \"q_beta\" qlognormal = \"q_log_normal\" qloguniform = \"q_log_uniform\" qloguniformvalues = \"q_log_uniform_values\" qnormal = \"q_normal\" quniform = \"q_uniform\" uniform = \"uniform\"","title":"Distribution"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal","text":"Bases: Enum Goal. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 84 85 86 87 88 class Goal ( Enum ): \"\"\"Goal.\"\"\" maximize = \"maximize\" minimize = \"minimize\"","title":"Goal"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping","text":"Hyperband Stopping Config. Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @dataclass ( frozen = True ) class HyperbandStopping : \"\"\"Hyperband Stopping Config. Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs. \"\"\" type : HyperbandStoppingType # noqa: A003 eta : float | None = None \"\"\"ETA. At every eta^n steps, hyperband continues running the top 1/eta runs and stops all other runs. \"\"\" maxiter : int | None = None \"\"\"Max Iterations. Set the last epoch to finish trimming runs, and hyperband will automatically calculate the prior epochs to trim runs. \"\"\" miniter : int | None = None \"\"\"Min Iterations. Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs. \"\"\" s : float | None = None \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\" strict : bool | None = None \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\"","title":"HyperbandStopping"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.eta","text":"ETA. At every eta^n steps, hyperband continues running the top 1/eta runs and stops all other runs.","title":"eta"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.maxiter","text":"Max Iterations. Set the last epoch to finish trimming runs, and hyperband will automatically calculate the prior epochs to trim runs.","title":"maxiter"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.miniter","text":"Min Iterations. Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs.","title":"miniter"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.s","text":"Set the number of steps you trim runs at, working backwards from the max_iter.","title":"s"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.strict","text":"Use a more aggressive condition for termination, stops more runs.","title":"strict"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStoppingType","text":"Bases: Enum Hyperband Stopping Type. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 25 26 27 28 class HyperbandStoppingType ( Enum ): \"\"\"Hyperband Stopping Type.\"\"\" hyperband = \"hyperband\"","title":"HyperbandStoppingType"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Impute","text":"Bases: Enum Metric value to use in bayes search for runs that fail, crash, or are killed. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 91 92 93 94 95 96 class Impute ( Enum ): \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\" best = \"best\" latest = \"latest\" worst = \"worst\"","title":"Impute"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ImputeWhileRunning","text":"Bases: Enum Appends a calculated metric even when epochs are in a running state. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 99 100 101 102 103 104 105 class ImputeWhileRunning ( Enum ): \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\" best = \"best\" false = \"false\" latest = \"latest\" worst = \"worst\"","title":"ImputeWhileRunning"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Kind","text":"Bases: Enum Kind. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 69 70 71 72 class Kind ( Enum ): \"\"\"Kind.\"\"\" sweep = \"sweep\"","title":"Kind"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method","text":"Bases: Enum Method. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 75 76 77 78 79 80 81 class Method ( Enum ): \"\"\"Method.\"\"\" bayes = \"bayes\" custom = \"custom\" grid = \"grid\" random = \"random\"","title":"Method"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric","text":"Metric to optimize. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @dataclass ( frozen = True ) class Metric : \"\"\"Metric to optimize.\"\"\" name : str \"\"\"Name of metric.\"\"\" goal : Goal | None = None impute : Impute | None = None \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\" imputewhilerunning : ImputeWhileRunning | None = None \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\" target : float | None = None \"\"\"The sweep will finish once any run achieves this value.\"\"\"","title":"Metric"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.impute","text":"Metric value to use in bayes search for runs that fail, crash, or are killed","title":"impute"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.imputewhilerunning","text":"Appends a calculated metric even when epochs are in a running state.","title":"imputewhilerunning"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.name","text":"Name of metric.","title":"name"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.target","text":"The sweep will finish once any run achieves this value.","title":"target"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter","text":"Bases: Generic [ ParamType ] Sweep Parameter. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @dataclass ( frozen = True ) class Parameter ( Generic [ ParamType ]): \"\"\"Sweep Parameter.\"\"\" value : ParamType | list [ ParamType ] max : ParamType | None = None # noqa: A003 min : ParamType | None = None # noqa: A003 a : float | None = None b : float | None = None distribution : Distribution | None = None q : float | None = None \"\"\"Quantization parameter for quantized distributions\"\"\" values : list [ ParamType ] | None = None \"\"\"Discrete values\"\"\" probabilities : list [ float ] | None = None \"\"\"Probability of each value\"\"\" mu : float | None = None \"\"\"Mean for normal or lognormal distributions\"\"\" sigma : float | None = None \"\"\"Std Dev for normal or lognormal distributions\"\"\" parameters : dict [ str , \"Parameter[ParamType]\" ] | None = None","title":"Parameter"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.mu","text":"Mean for normal or lognormal distributions","title":"mu"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.probabilities","text":"Probability of each value","title":"probabilities"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.q","text":"Quantization parameter for quantized distributions","title":"q"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.sigma","text":"Std Dev for normal or lognormal distributions","title":"sigma"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.values","text":"Discrete values","title":"values"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig","text":"Weights & Biases Sweep Configuration. Source code in sparse_autoencoder/train/utils/wandb_sweep_types.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 @dataclass ( frozen = True ) class WandbSweepConfig : \"\"\"Weights & Biases Sweep Configuration.\"\"\" parameters : Parameters | Any method : Method metric : Metric \"\"\"Metric to optimize\"\"\" apiVersion : str | None = None command : list [ Any ] | None = None \"\"\"Command used to launch the training script\"\"\" controller : Controller | None = None description : str | None = None \"\"\"Short package description\"\"\" earlyterminate : HyperbandStopping | None = None entity : str | None = None \"\"\"The entity for this sweep\"\"\" imageuri : str | None = None \"\"\"Sweeps on Launch will use this uri instead of a job.\"\"\" job : str | None = None \"\"\"Launch Job to run.\"\"\" kind : Kind | None = None name : str | None = None \"\"\"The name of the sweep, displayed in the W&B UI.\"\"\" program : str | None = None \"\"\"Training script to run.\"\"\" project : str | None = None \"\"\"The project for this sweep.\"\"\" runcap : int | None = None \"\"\"Run Cap. Sweep will run no more than this number of runs, across any number of agents. \"\"\"","title":"WandbSweepConfig"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.command","text":"Command used to launch the training script","title":"command"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.description","text":"Short package description","title":"description"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.entity","text":"The entity for this sweep","title":"entity"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.imageuri","text":"Sweeps on Launch will use this uri instead of a job.","title":"imageuri"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.job","text":"Launch Job to run.","title":"job"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.metric","text":"Metric to optimize","title":"metric"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.name","text":"The name of the sweep, displayed in the W&B UI.","title":"name"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.program","text":"Training script to run.","title":"program"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.project","text":"The project for this sweep.","title":"project"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.runcap","text":"Run Cap. Sweep will run no more than this number of runs, across any number of agents.","title":"runcap"}]}