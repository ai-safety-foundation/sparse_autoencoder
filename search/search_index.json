{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sparse Autoencoder","text":"<p>A sparse autoencoder for mechanistic interpretability research.</p> <pre><code>pip install sparse_autoencoder\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Check out the demo notebook for a guide to using this library.</p> <p>We also highly recommend skimming the reference docs to see all the features that are available.</p>"},{"location":"#features","title":"Features","text":"<p>This library contains:</p> <ol> <li>A sparse autoencoder model, along with all the underlying PyTorch components you need to       customise and/or build your own:<ul> <li>Encoder, constrained unit norm decoder and tied bias PyTorch modules in     sparse_autoencoder.autoencoder.</li> <li>Adam module with helper method to reset state in sparse_autoencoder.optimizer.</li> </ul> </li> <li>Activations data generator using TransformerLens, with the underlying steps in case you       want to customise the approach:<ul> <li>Activation store options (in-memory or on disk) in sparse_autoencoder.activation_store.</li> <li>Hook to get the activations from TransformerLens in an efficient way in     sparse_autoencoder.source_model.</li> <li>Source dataset (i.e. prompts to generate these activations) utils in     sparse_autoencoder.source_data, that stream data from HuggingFace and pre-process     (tokenize &amp; shuffle).</li> </ul> </li> <li>Activation resampler to help reduce the number of dead neurons.</li> <li>Metrics that log at various stages of training (loss, train metrics and validation metrics)       , based on torchmetrics.</li> <li>Training pipeline that combines everything together, allowing you to run hyperparameter       sweeps and view progress on wandb.</li> </ol>"},{"location":"#designed-for-research","title":"Designed for Research","text":"<p>The library is designed to be modular. By default it takes the approach from Towards Monosemanticity: Decomposing Language Models With Dictionary Learning , so you can pip install the library and get started quickly. Then when you need to customise something, you can just extend the abstract class for that component (every component is documented so that it's easy to do this).</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Demo</li> <li>Source dataset pre-processing</li> <li>Reference</li> <li>Contributing</li> <li>Citation</li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>Please cite this library as:</p> <pre><code>@misc{cooney2023SparseAutoencoder,\n    title = {Sparse Autoencoder Library},\n    author = {Alan Cooney},\n    year = {2023},\n    howpublished = {\\url{https://github.com/ai-safety-foundation/sparse_autoencoder}},\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#setup","title":"Setup","text":"<p>This project uses Poetry for dependency management, and PoeThePoet for scripts. After checking out the repo, we recommend setting poetry's config to create the <code>.venv</code> in the root directory (note this is a global setting) and then installing with the dev and demos dependencies.</p> <pre><code>poetry config virtualenvs.in-project true\npoetry install --with dev,demos\n</code></pre> <p>If you are using VSCode we highly recommend installing the recommended extensions as well (it will prompt you to do this when you checkout the repo).</p>"},{"location":"contributing/#checks","title":"Checks","text":"<p>For a full list of available commands (e.g. <code>test</code> or <code>typecheck</code>), run this in your terminal (assumes the venv is active already).</p> <pre><code>poe\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Please make sure to add thorough documentation for any features you add. You should do this directly in the docstring, and this will then automatically generate the API docs when merged into <code>main</code>. They will also be automatically checked with pytest (via doctest).</p> <p>If you want to view your documentation changes, run <code>poe docs-hot-reload</code>. This will give you hot-reloading docs (they change in real time as you edit docstrings).</p>"},{"location":"contributing/#docstring-style-guide","title":"Docstring Style Guide","text":"<p>We follow the Google Python Docstring Style for writing docstrings. Some important details below:</p>"},{"location":"contributing/#sections-and-order","title":"Sections and Order","text":"<p>You should follow this order:</p> <pre><code>\"\"\"Title In Title Case.\n\nA description of what the function/class does, including as much detail as is necessary to fully understand it.\n\nWarning:\n\nAny warnings to the user (e.g. common pitfalls).\n\nExamples:\n\nInclude any examples here. They will be checked with doctest.\n\n  &gt;&gt;&gt; print(1 + 2)\n  3\n\nArgs:\n    param_without_type_signature:\n        Each description should be indented once more.\n    param_2:\n        Another example parameter.\n\nReturns:\n    Returns description without type signature.\n\nRaises:\n    Information about the error it may raise (if any).\n\"\"\"\n</code></pre>"},{"location":"contributing/#latex-support","title":"LaTeX support","text":"<p>You can use LaTeX, inside <code>$$</code> for blocks or <code>$</code> for inline</p> <pre><code>Some text $(a + b)^2 = a^2 + 2ab + b^2$\n</code></pre> <pre><code>Some text:\n\n$$\ny    &amp; = &amp; ax^2 + bx + c \\\\\nf(x) &amp; = &amp; x^2 + 2xy + y^2\n$$\n</code></pre>"},{"location":"contributing/#markup","title":"Markup","text":"<ul> <li>Italics - <code>*text*</code></li> <li>Bold - <code>**text**</code></li> <li>Code - <code>``code``</code></li> <li>List items - <code>*item</code></li> <li>Numbered items - <code>1. Item</code></li> <li>Quotes - indent one level</li> <li>External links = <code>`Link text &lt;https://domain.invalid/&gt;`</code></li> </ul>"},{"location":"demo/","title":"Demo","text":"<pre><code># Check if we're in Colab\ntry:\n    import google.colab  # noqa: F401 # type: ignore\n\n    in_colab = True\nexcept ImportError:\n    in_colab = False\n\n#  Install if in Colab\nif in_colab:\n    %pip install sparse_autoencoder transformer_lens transformers wandb\n\n# Otherwise enable hot reloading in dev mode\nif not in_colab:\n    %load_ext autoreload\n    %autoreload 2\n</code></pre> <pre><code>import os\n\nfrom sparse_autoencoder import (\n    ActivationResamplerHyperparameters,\n    AutoencoderHyperparameters,\n    Hyperparameters,\n    LossHyperparameters,\n    Method,\n    OptimizerHyperparameters,\n    Parameter,\n    PipelineHyperparameters,\n    SourceDataHyperparameters,\n    SourceModelHyperparameters,\n    SweepConfig,\n    sweep,\n)\n\nos.environ[\"WANDB_NOTEBOOK_NAME\"] = \"demo.ipynb\"\n</code></pre> <p>Customize any hyperparameters you want below (by default we're sweeping over l1 coefficient and learning rate).</p> <p>Note we are using the RANDOM sweep approach (try random combinations of hyperparameters), which works surprisingly well but will need to be stopped at some point (as otherwise it will continue forever). If you want to run pre-defined runs consider using <code>Parameter(values=[0.01, 0.05...])</code> for example rather than <code>Parameter(max=0.03, min=0.008)</code> for each parameter you are sweeping over. You can then set the strategy to <code>Method.GRID</code>.</p> <pre><code>def train_gpt_small_mlp_layers(\n    expansion_factor: int = 4,\n    n_layers: int = 12,\n) -&amp;gt; None:\n    \"\"\"Run a new sweep experiment on GPT 2 Small's MLP layers.\n\n    Args:\n        expansion_factor: Expansion factor for the autoencoder.\n        n_layers: Number of layers to train on. Max is 12.\n\n    \"\"\"\n    sweep_config = SweepConfig(\n        parameters=Hyperparameters(\n            loss=LossHyperparameters(\n                l1_coefficient=Parameter(max=0.03, min=0.008),\n            ),\n            optimizer=OptimizerHyperparameters(\n                lr=Parameter(max=0.001, min=0.00001),\n            ),\n            source_model=SourceModelHyperparameters(\n                name=Parameter(\"gpt2\"),\n                cache_names=Parameter(\n                    [f\"blocks.{layer}.hook_mlp_out\" for layer in range(n_layers)]\n                ),\n                hook_dimension=Parameter(768),\n            ),\n            source_data=SourceDataHyperparameters(\n                dataset_path=Parameter(\"alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2\"),\n                context_size=Parameter(256),\n                pre_tokenized=Parameter(value=True),\n                pre_download=Parameter(value=False),  # Default to streaming the dataset\n            ),\n            autoencoder=AutoencoderHyperparameters(\n                expansion_factor=Parameter(value=expansion_factor)\n            ),\n            pipeline=PipelineHyperparameters(\n                max_activations=Parameter(1_000_000_000),\n                checkpoint_frequency=Parameter(100_000_000),\n                validation_frequency=Parameter(100_000_000),\n                max_store_size=Parameter(1_000_000),\n            ),\n            activation_resampler=ActivationResamplerHyperparameters(\n                resample_interval=Parameter(200_000_000),\n                n_activations_activity_collate=Parameter(100_000_000),\n                threshold_is_dead_portion_fires=Parameter(1e-6),\n                max_n_resamples=Parameter(4),\n            ),\n        ),\n        method=Method.RANDOM,\n    )\n\n    sweep(sweep_config=sweep_config)\n</code></pre> <p>This will start a sweep with just one agent (the current machine). If you have multiple GPUs, it will use them automatically. Similarly it will work on Apple silicon devices by automatically using MPS.</p> <pre><code>train_gpt_small_mlp_layers()\n</code></pre> <p>Want to speed things up? You can trivially add extra machines to the sweep, each of which will peel of some runs from the sweep agent (stored on Wandb). To do this, on another machine simply run:</p> <pre><code>pip install sparse_autoencoder\njoin-sae-sweep --id=SWEEP_ID_SHOWN_ON_WANDB\n</code></pre>"},{"location":"demo/#training-demo","title":"Training Demo","text":"<p>This is a quick start demo to get training a SAE right away. All you need to do is choose a few hyperparameters (like the model to train on), and then set it off.</p> <p>In this demo we'll train a sparse autoencoder on all MLP layer outputs in GPT-2 small (effectively training an SAE on each layer in parallel).</p>"},{"location":"demo/#setup","title":"Setup","text":""},{"location":"demo/#imports","title":"Imports","text":""},{"location":"demo/#hyperparameters","title":"Hyperparameters","text":""},{"location":"demo/#run-the-sweep","title":"Run the sweep","text":""},{"location":"pre-process-datasets/","title":"Source dataset pre-processing","text":"<p>Note you will also need to login to HuggingFace via the CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <pre><code># Check if we're in Colab\ntry:\n    import google.colab  # noqa: F401 # type: ignore\n\n    in_colab = True\nexcept ImportError:\n    in_colab = False\n\n#  Install if in Colab\nif in_colab:\n    %pip install sparse_autoencoder transformer_lens transformers wandb datasets\n\n# Otherwise enable hot reloading in dev mode\nif not in_colab:\n    %load_ext autoreload\n    %autoreload 2\n</code></pre> <pre><code>from dataclasses import dataclass\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom sparse_autoencoder import TextDataset\n</code></pre> <pre><code>@dataclass\nclass DatasetToPreprocess:\n    \"\"\"Dataset to preprocess info.\"\"\"\n\n    source_path: str\n    \"\"\"Source path from HF (e.g. `roneneldan/TinyStories`).\"\"\"\n\n    tokenizer_name: str\n    \"\"\"HF tokenizer name (e.g. `gpt2`).\"\"\"\n\n    data_dir: str | None = None\n    \"\"\"Data directory to download from the source dataset.\"\"\"\n\n    data_files: list[str] | None = None\n    \"\"\"Data files to download from the source dataset.\"\"\"\n\n    hugging_face_username: str = \"alancooney\"\n    \"\"\"HF username for the upload.\"\"\"\n\n    @property\n    def source_alias(self) -&amp;gt; str:\n        \"\"\"Create a source alias for the destination dataset name.\n\n        Returns:\n            The modified source path as source alias.\n        \"\"\"\n        return self.source_path.replace(\"/\", \"-\")\n\n    @property\n    def tokenizer_alias(self) -&amp;gt; str:\n        \"\"\"Create a tokenizer alias for the destination dataset name.\n\n        Returns:\n            The modified tokenizer name as tokenizer alias.\n        \"\"\"\n        return self.tokenizer_name.replace(\"/\", \"-\")\n\n    @property\n    def destination_repo_name(self) -&amp;gt; str:\n        \"\"\"Destination repo name.\n\n        Returns:\n            The destination repo name.\n        \"\"\"\n        return f\"sae-{self.source_alias}-tokenizer-{self.tokenizer_alias}\"\n\n    @property\n    def destination_repo_id(self) -&amp;gt; str:\n        \"\"\"Destination repo ID.\n\n        Returns:\n            The destination repo ID.\n        \"\"\"\n        return f\"{self.hugging_face_username}/{self.destination_repo_name}\"\n\n\ndef upload_datasets(datasets_to_preprocess: list[DatasetToPreprocess]) -&amp;gt; None:\n    \"\"\"Upload datasets to HF.\n\n    Warning:\n        Assumes you have already created the corresponding repos on HF.\n\n    Args:\n        datasets_to_preprocess: List of datasets to preprocess.\n\n    Raises:\n        ValueError: If the repo doesn't exist.\n    \"\"\"\n    repositories_updating = [dataset.destination_repo_id for dataset in datasets_to_preprocess]\n    print(\"Updating repositories:\\n\" \"\\n\".join(repositories_updating))\n\n    for dataset in datasets_to_preprocess:\n        print(\"Processing dataset: \", dataset.source_path)\n\n        # Preprocess\n        tokenizer = AutoTokenizer.from_pretrained(dataset.tokenizer_name)\n        text_dataset = TextDataset(\n            dataset_path=dataset.source_path,\n            tokenizer=tokenizer,\n            pre_download=True,  # Must be true to upload after pre-processing, to the hub.\n            dataset_files=dataset.data_files,\n            dataset_dir=dataset.data_dir,\n        )\n        print(\"Size: \", text_dataset.dataset.size_in_bytes)\n        print(\"Info: \", text_dataset.dataset.info)\n\n        # Upload\n        text_dataset.push_to_hugging_face_hub(repo_id=dataset.destination_repo_id)\n</code></pre> <pre><code>datasets: list[DatasetToPreprocess] = [\n    DatasetToPreprocess(\n        source_path=\"roneneldan/TinyStories\",\n        tokenizer_name=\"gpt2\",\n        # Get the newer versions (Generated with GPT-4 only)\n        data_files=[\"TinyStoriesV2-GPT4-train.txt\", \"TinyStoriesV2-GPT4-valid.txt\"],\n    ),\n    DatasetToPreprocess(\n        source_path=\"monology/pile-uncopyrighted\",\n        tokenizer_name=\"gpt2\",\n        # Get just the first few (each file is 11GB so this should be enough for a large dataset)\n        data_files=[\n            \"00.jsonl.zst\",\n            \"01.jsonl.zst\",\n            \"02.jsonl.zst\",\n            \"03.jsonl.zst\",\n            \"04.jsonl.zst\",\n            \"05.jsonl.zst\",\n        ],\n        data_dir=\"train\",\n    ),\n    DatasetToPreprocess(\n        source_path=\"monology/pile-uncopyrighted\",\n        tokenizer_name=\"EleutherAI/gpt-neox-20b\",\n        data_files=[\n            \"00.jsonl.zst\",\n            \"01.jsonl.zst\",\n            \"02.jsonl.zst\",\n            \"03.jsonl.zst\",\n            \"04.jsonl.zst\",\n            \"05.jsonl.zst\",\n        ],\n        data_dir=\"train\",\n    ),\n]\n\nupload_datasets(datasets)\n</code></pre> <pre><code>downloaded_dataset = load_dataset(\n    \"alancooney/sae-roneneldan-TinyStories-tokenizer-gpt2\", streaming=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ni = 0\nfirst_k = 3\nfor data_item in iter(downloaded_dataset[\"train\"]):  # type:ignore\n    # Get just the first few\n    i += 1\n    if i &amp;gt;= first_k:\n        break\n\n    # Print the decoded items\n    input_ids = data_item[\"input_ids\"]\n    decoded = tokenizer.decode(input_ids)\n    print(f\"{len(input_ids)} tokens: {decoded}\")\n</code></pre>"},{"location":"pre-process-datasets/#pre-process-datasets","title":"Pre-process datasets","text":"<p>When training a sparse autoencoder (SAE) often you want to use a text dataset such as The Pile. </p> <p>The <code>TextDataset</code> class can pre-process this for you on the fly (i.e. tokenize and split into <code>context_size</code> chunks of tokens), so that you can get started right away. However, if you're experimenting a lot, it can be nicer to run this once and then save the resulting dataset to HuggingFace. You can then use <code>PreTokenizedDataset</code> to load this directly, saving you from running this pre-processing every time you use it.</p> <p>The following code shows you how to do this, and is also used to upload a set of commonly used datasets for SAE training to Alan Cooney's HuggingFace hub.</p>"},{"location":"pre-process-datasets/#setup","title":"Setup","text":""},{"location":"pre-process-datasets/#upload-helper","title":"Upload helper","text":"<p>Here we define a helper function to upload multiple datasets.</p>"},{"location":"pre-process-datasets/#upload-to-hugging-face","title":"Upload to Hugging Face","text":""},{"location":"pre-process-datasets/#check-a-dataset-is-as-expected","title":"Check a dataset is as expected","text":""},{"location":"reference/","title":"Sparse Autoencoder Library","text":"<p>Sparse Autoencoder Library.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResampler","title":"<code>ActivationResampler</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Activation resampler.</p> <p>Collates the number of times each neuron fires over a set number of learned activation vectors, and then provides the parameters necessary to reset any dead neurons.</p> Motivation <p>Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.</p> <p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.</p> <p>This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network.</p> Warning <p>The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases.</p> Warning <p>This approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>class ActivationResampler(Metric):\n    \"\"\"Activation resampler.\n\n    Collates the number of times each neuron fires over a set number of learned activation vectors,\n    and then provides the parameters necessary to reset any dead neurons.\n\n    Motivation:\n        Over the course of training, a subset of autoencoder neurons will have zero activity across\n        a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language\n        Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training\n        improves the number of likely-interpretable features (i.e., those in the high density\n        cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket\n        Hypothesis and increase the number of chances the network has to find promising feature\n        directions.\n\n        An interesting nuance around dead neurons involves the ultralow density cluster. They found\n        that if we increase the number of training steps then networks will kill off more of these\n        ultralow density neurons. This reinforces the use of the high density cluster as a useful\n        metric because there can exist neurons that are de facto dead but will not appear to be when\n        looking at the number of dead neurons alone.\n\n        This approach is designed to seed new features to fit inputs where the current autoencoder\n        performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled\n        neuron will only fire weakly for inputs similar to the one used for its reinitialization.\n        This was done to minimize interference with the rest of the network.\n\n    Warning:\n        The optimizer should be reset after applying this function, as the Adam state will be\n        incorrect for the modified weights and biases.\n\n    Warning:\n        This approach is also known to create sudden loss spikes, and resampling too frequently\n        causes training to diverge.\n    \"\"\"\n\n    # Collated data from the train loop\n    _neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT, Axis.LEARNT_FEATURE)]\n    _loss: list[Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL)]] | Float[\n        Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL)\n    ]\n    _input_activations: list[\n        Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]\n    ] | Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]\n\n    # Tracking\n    _n_activations_seen_process: int\n    _n_times_resampled: int\n\n    # Settings\n    _n_components: int\n    _threshold_is_dead_portion_fires: float\n    _max_n_resamples: int\n    resample_interval: int\n    resample_interval_process: int\n    start_collecting_neuron_activity_process: int\n    start_collecting_loss_process: int\n\n    @validate_call\n    def __init__(\n        self,\n        n_learned_features: PositiveInt,\n        n_components: NonNegativeInt = 1,\n        resample_interval: PositiveInt = 200_000_000,\n        max_n_resamples: NonNegativeInt = 4,\n        n_activations_activity_collate: PositiveInt = 100_000_000,\n        resample_dataset_size: PositiveInt = 819_200,\n        threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n    ) -&gt; None:\n        r\"\"\"Initialize the activation resampler.\n\n        Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n        Args:\n            n_learned_features: Number of learned features\n            n_components: Number of components that the SAE is being trained on.\n            resample_interval: Interval in number of autoencoder input activation vectors trained\n                on, before resampling.\n            max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n                Set to inf if you want to have no limit.\n            n_activations_activity_collate: Number of autoencoder learned activation vectors to\n                collate before resampling (the activation resampler will start collecting on vector\n                $\\text{resample_interval} - \\text{n_steps_collate}$).\n            resample_dataset_size: Number of autoencoder input activations to use for calculating\n                the loss, as part of the resampling process to create the reset neuron weights.\n            threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n                \"fired\" in less than this portion of the collated sample).\n\n        Raises:\n            ValueError: If any of the arguments are invalid (e.g. negative integers).\n        \"\"\"\n        super().__init__(\n            sync_on_compute=False  # Manually sync instead in compute, where needed\n        )\n\n        # Error handling\n        if n_activations_activity_collate &gt; resample_interval:\n            error_message = \"Must collate less activation activity than the resample interval.\"\n            raise ValueError(error_message)\n\n        # Number of processes\n        world_size = (\n            get_world_size(group.WORLD)\n            if distributed.is_available() and distributed.is_initialized()\n            else 1\n        )\n        process_resample_dataset_size = resample_dataset_size // world_size\n\n        # State setup (note half precision is used as it's sufficient for resampling purposes)\n        self.add_state(\n            \"_neuron_fired_count\",\n            torch.zeros((n_components, n_learned_features)),\n            \"sum\",\n        )\n        self.add_state(\"_loss\", [], \"cat\")\n        self.add_state(\"_input_activations\", [], \"cat\")\n\n        # Tracking\n        self._n_activations_seen_process = 0\n        self._n_times_resampled = 0\n\n        # Settings\n        self._n_components = n_components\n        self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n        self._max_n_resamples = max_n_resamples\n        self.resample_interval = resample_interval\n        self.resample_interval_process = resample_interval // world_size\n        self.start_collecting_neuron_activity_process = (\n            self.resample_interval_process - n_activations_activity_collate // world_size\n        )\n        self.start_collecting_loss_process = (\n            self.resample_interval_process - process_resample_dataset_size\n        )\n\n    def update(\n        self,\n        input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n        encoder_weight_reference: Parameter,\n    ) -&gt; None:\n        \"\"\"Update the collated data from forward passes.\n\n        Args:\n            input_activations: Input activations to the SAE.\n            learned_activations: Learned activations from the SAE.\n            loss: Loss per input activation.\n            encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n        Raises:\n            TypeError: If the loss or input activations are not lists (e.g. from unsync having not\n                been called).\n        \"\"\"\n        if self._n_activations_seen_process &gt;= self.start_collecting_neuron_activity_process:\n            neuron_has_fired: Bool[\n                Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n            ] = torch.gt(learned_activations, 0)\n            self._neuron_fired_count += neuron_has_fired.sum(dim=0)\n\n        if self._n_activations_seen_process &gt;= self.start_collecting_loss_process:\n            # Typecast\n            if not isinstance(self._loss, list) or not isinstance(self._input_activations, list):\n                raise TypeError\n\n            # Append\n            self._loss.append(loss)\n            self._input_activations.append(input_activations)\n\n        self._n_activations_seen_process += len(learned_activations)\n        self._encoder_weight = encoder_weight_reference\n\n    def _get_dead_neuron_indices(\n        self,\n    ) -&gt; list[Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)]]:\n        \"\"\"Identify the indices of neurons that are dead.\n\n        Identifies any neurons that have fired less than the threshold portion of the collated\n        sample size.\n\n        Returns:\n            List of dead neuron indices for each component.\n\n        Raises:\n            ValueError: If no neuron activity has been collated yet.\n        \"\"\"\n        # Check we have already collated some neuron activity\n        if torch.all(self._neuron_fired_count == 0):\n            error_message = \"Cannot get dead neuron indices without neuron activity.\"\n            raise ValueError(error_message)\n\n        # Find any neurons that fire less than the threshold portion of times\n        threshold_is_dead_n_fires: int = int(\n            self.resample_interval * self._threshold_is_dead_portion_fires\n        )\n\n        return [\n            torch.where(self._neuron_fired_count[component_idx] &lt;= threshold_is_dead_n_fires)[0].to(\n                dtype=torch.int\n            )\n            for component_idx in range(self._n_components)\n        ]\n\n    @staticmethod\n    def assign_sampling_probabilities(\n        loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Assign the sampling probabilities for each input activations vector.\n\n        Assign each input vector a probability of being picked that is proportional to the square of\n        the autoencoder's loss on that input.\n\n        Examples:\n            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n            tensor([0.0700, 0.2900, 0.6400])\n\n            &gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n            tensor([[0.0700, 0.0700],\n                    [0.2900, 0.2900],\n                    [0.6400, 0.6400]])\n\n        Args:\n            loss: Loss per item.\n\n        Returns:\n            A tensor of probabilities for each item.\n        \"\"\"\n        square_loss = loss.pow(2)\n        return square_loss / square_loss.sum(0)\n\n    @staticmethod\n    def sample_input(\n        probabilities: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n        input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        n_samples: list[int],\n    ) -&gt; list[Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]]:\n        \"\"\"Sample an input vector based on the provided probabilities.\n\n        Example:\n            &gt;&gt;&gt; probabilities = torch.tensor([[0.1], [0.2], [0.7]])\n            &gt;&gt;&gt; input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n            ...     probabilities, input_activations, [2]\n            ... )\n            &gt;&gt;&gt; sampled_input[0].tolist()\n            [[5.0, 6.0], [3.0, 4.0]]\n\n        Args:\n            probabilities: Probabilities for each input.\n            input_activations: Input activation vectors.\n            n_samples: Number of samples to take (number of dead neurons).\n\n        Returns:\n            Sampled input activation vector.\n\n        Raises:\n            ValueError: If the number of samples is greater than the number of input activations.\n        \"\"\"\n        sampled_inputs: list[\n            Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n        ] = []\n\n        for component_idx, component_n_samples in enumerate(n_samples):\n            component_probabilities: Float[Tensor, Axis.BATCH] = get_component_slice_tensor(\n                input_tensor=probabilities,\n                n_dim_with_component=2,\n                component_dim=1,\n                component_idx=component_idx,\n            )\n\n            component_input_activations: Float[\n                Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n            ] = get_component_slice_tensor(\n                input_tensor=input_activations,\n                n_dim_with_component=3,\n                component_dim=1,\n                component_idx=component_idx,\n            )\n\n            if component_n_samples &gt; len(component_input_activations):\n                exception_message = (\n                    f\"Cannot sample {component_n_samples} inputs from \"\n                    f\"{len(component_input_activations)} input activations.\"\n                )\n                raise ValueError(exception_message)\n\n            # Handle the 0 dead neurons case\n            if component_n_samples == 0:\n                sampled_inputs.append(\n                    torch.empty(\n                        (0, component_input_activations.shape[-1]),\n                        dtype=component_input_activations.dtype,\n                        device=component_input_activations.device,\n                    )\n                )\n                continue\n\n            # Handle the 1+ dead neuron case\n            component_sample_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n                component_probabilities, num_samples=component_n_samples\n            )\n            sampled_inputs.append(component_input_activations[component_sample_indices, :])\n\n        return sampled_inputs\n\n    @staticmethod\n    def renormalize_and_scale(\n        sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n        neuron_activity: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE)],\n        encoder_weight: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n        Renormalize the input vector to equal the average norm of the encoder weights for alive\n        neurons times 0.2.\n\n        Example:\n            &gt;&gt;&gt; from torch.nn import Parameter\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n            &gt;&gt;&gt; neuron_activity = torch.tensor([3.0, 0, 5, 0, 1, 3])\n            &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n            ...     sampled_input,\n            ...     neuron_activity,\n            ...     encoder_weight\n            ... )\n            &gt;&gt;&gt; rescaled_input.round(decimals=1)\n            tensor([[0.2000, 0.2000]])\n\n        Args:\n            sampled_input: Tensor of the sampled input activation.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            encoder_weight: Tensor of encoder weights.\n\n        Returns:\n            Rescaled sampled input.\n\n        Raises:\n            ValueError: If there are no alive neurons.\n        \"\"\"\n        alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n        # Check there is at least one alive neuron\n        if not torch.any(alive_neuron_mask):\n            error_message = \"No alive neurons found.\"\n            raise ValueError(error_message)\n\n        # Handle no dead neurons\n        n_dead_neurons = len(sampled_input)\n        if n_dead_neurons == 0:\n            return torch.empty(\n                (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n            )\n\n        # Calculate the average norm of the encoder weights for alive neurons.\n        detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n        alive_encoder_weights: Float[\n            Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = detached_encoder_weight[alive_neuron_mask, :]\n        average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n            dim=-1\n        ).mean()\n\n        # Renormalize the input vector to equal the average norm of the encoder weights for alive\n        # neurons times 0.2.\n        renormalized_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n        return renormalized_input * (average_alive_norm * 0.2)\n\n    def compute(self) -&gt; list[ParameterUpdateResults] | None:\n        \"\"\"Compute the parameters that need to be updated.\n\n        Returns:\n            A list of parameter update results (for each component that the SAE is being trained\n            on), if an update is needed.\n        \"\"\"\n        # Resample if needed\n        if self._n_activations_seen_process &gt;= self.resample_interval_process:\n            with torch.no_grad():\n                # Initialise results\n                parameter_update_results: list[ParameterUpdateResults] = []\n\n                # Sync &amp; typecast\n                self.sync()\n                loss = dim_zero_cat(self._loss)\n                input_activations = dim_zero_cat(self._input_activations)\n\n                dead_neuron_indices: list[\n                    Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)]\n                ] = self._get_dead_neuron_indices()\n\n                # Assign each input vector a probability of being picked that is proportional to the\n                # square of the autoencoder's loss on that input.\n                sample_probabilities: Float[\n                    Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n                ] = self.assign_sampling_probabilities(loss)\n\n                # For each dead neuron sample an input according to these probabilities.\n                sampled_input: list[\n                    Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n                ] = self.sample_input(\n                    sample_probabilities,\n                    input_activations,\n                    [len(dead) for dead in dead_neuron_indices],\n                )\n\n                for component_idx in range(self._n_components):\n                    # Renormalize each input vector to have unit L2 norm and set this to be the\n                    # dictionary vector for the dead autoencoder neuron.\n                    renormalized_input: Float[\n                        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                    ] = torch.nn.functional.normalize(sampled_input[component_idx], dim=-1)\n\n                    dead_decoder_weight_updates = rearrange(\n                        renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n                    )\n\n                    # For the corresponding encoder vector, renormalize the input vector to equal\n                    # the average norm of the encoder weights for alive neurons times 0.2. Set the\n                    # corresponding encoder bias element to zero.\n                    encoder_weight: Float[\n                        Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                    ] = get_component_slice_tensor(self._encoder_weight, 3, 0, component_idx)\n\n                    rescaled_sampled_input = self.renormalize_and_scale(\n                        sampled_input=sampled_input[component_idx],\n                        neuron_activity=self._neuron_fired_count[component_idx],\n                        encoder_weight=encoder_weight,\n                    )\n\n                    dead_encoder_bias_updates = torch.zeros_like(\n                        dead_neuron_indices[component_idx],\n                        dtype=dead_decoder_weight_updates.dtype,\n                        device=dead_decoder_weight_updates.device,\n                    )\n\n                    parameter_update_results.append(\n                        ParameterUpdateResults(\n                            dead_neuron_indices=dead_neuron_indices[component_idx],\n                            dead_encoder_weight_updates=rescaled_sampled_input,\n                            dead_encoder_bias_updates=dead_encoder_bias_updates,\n                            dead_decoder_weight_updates=dead_decoder_weight_updates,\n                        )\n                    )\n\n                # Reset\n                self.unsync(should_unsync=self._is_synced)\n                self.reset()\n\n                return parameter_update_results\n\n        return None\n\n    def forward(  # type: ignore[override]\n        self,\n        input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n        encoder_weight_reference: Parameter,\n    ) -&gt; list[ParameterUpdateResults] | None:\n        \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n        Args:\n            input_activations: Input activations to the SAE.\n            learned_activations: Learned activations from the SAE.\n            loss: Loss per input activation.\n            encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n        Returns:\n            Parameter update results (for each component that the SAE is being trained on) if\n            resampling is due. Otherwise None.\n        \"\"\"\n        # Don't do anything if we have already completed all resamples\n        if self._n_times_resampled &gt;= self._max_n_resamples:\n            return None\n\n        self.update(\n            input_activations=input_activations,\n            learned_activations=learned_activations,\n            loss=loss,\n            encoder_weight_reference=encoder_weight_reference,\n        )\n\n        return self.compute()\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the activation resampler.\n\n        Warning:\n            This is only called when forward/compute has returned parameters to update (i.e.\n            resampling is due).\n        \"\"\"\n        self._n_activations_seen_process = 0\n        self._neuron_fired_count = torch.zeros_like(self._neuron_fired_count)\n        self._loss = []\n        self._input_activations = []\n        self._n_times_resampled += 1\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.__init__","title":"<code>__init__(n_learned_features, n_components=1, resample_interval=200000000, max_n_resamples=4, n_activations_activity_collate=100000000, resample_dataset_size=819200, threshold_is_dead_portion_fires=0.0)</code>","text":"<p>Initialize the activation resampler.</p> <p>Defaults to values used in the Anthropic Towards Monosemanticity paper.</p> <p>Parameters:</p> Name Type Description Default <code>n_learned_features</code> <code>PositiveInt</code> <p>Number of learned features</p> required <code>n_components</code> <code>NonNegativeInt</code> <p>Number of components that the SAE is being trained on.</p> <code>1</code> <code>resample_interval</code> <code>PositiveInt</code> <p>Interval in number of autoencoder input activation vectors trained on, before resampling.</p> <code>200000000</code> <code>max_n_resamples</code> <code>NonNegativeInt</code> <p>Maximum number of resamples to perform throughout the entire pipeline. Set to inf if you want to have no limit.</p> <code>4</code> <code>n_activations_activity_collate</code> <code>PositiveInt</code> <p>Number of autoencoder learned activation vectors to collate before resampling (the activation resampler will start collecting on vector \\(\\text{resample_interval} - \\text{n_steps_collate}\\)).</p> <code>100000000</code> <code>resample_dataset_size</code> <code>PositiveInt</code> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p> <code>819200</code> <code>threshold_is_dead_portion_fires</code> <code>Annotated[float, Field(strict=True, ge=0, le=1)]</code> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the arguments are invalid (e.g. negative integers).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    n_learned_features: PositiveInt,\n    n_components: NonNegativeInt = 1,\n    resample_interval: PositiveInt = 200_000_000,\n    max_n_resamples: NonNegativeInt = 4,\n    n_activations_activity_collate: PositiveInt = 100_000_000,\n    resample_dataset_size: PositiveInt = 819_200,\n    threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n) -&gt; None:\n    r\"\"\"Initialize the activation resampler.\n\n    Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n    Args:\n        n_learned_features: Number of learned features\n        n_components: Number of components that the SAE is being trained on.\n        resample_interval: Interval in number of autoencoder input activation vectors trained\n            on, before resampling.\n        max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n            Set to inf if you want to have no limit.\n        n_activations_activity_collate: Number of autoencoder learned activation vectors to\n            collate before resampling (the activation resampler will start collecting on vector\n            $\\text{resample_interval} - \\text{n_steps_collate}$).\n        resample_dataset_size: Number of autoencoder input activations to use for calculating\n            the loss, as part of the resampling process to create the reset neuron weights.\n        threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n            \"fired\" in less than this portion of the collated sample).\n\n    Raises:\n        ValueError: If any of the arguments are invalid (e.g. negative integers).\n    \"\"\"\n    super().__init__(\n        sync_on_compute=False  # Manually sync instead in compute, where needed\n    )\n\n    # Error handling\n    if n_activations_activity_collate &gt; resample_interval:\n        error_message = \"Must collate less activation activity than the resample interval.\"\n        raise ValueError(error_message)\n\n    # Number of processes\n    world_size = (\n        get_world_size(group.WORLD)\n        if distributed.is_available() and distributed.is_initialized()\n        else 1\n    )\n    process_resample_dataset_size = resample_dataset_size // world_size\n\n    # State setup (note half precision is used as it's sufficient for resampling purposes)\n    self.add_state(\n        \"_neuron_fired_count\",\n        torch.zeros((n_components, n_learned_features)),\n        \"sum\",\n    )\n    self.add_state(\"_loss\", [], \"cat\")\n    self.add_state(\"_input_activations\", [], \"cat\")\n\n    # Tracking\n    self._n_activations_seen_process = 0\n    self._n_times_resampled = 0\n\n    # Settings\n    self._n_components = n_components\n    self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n    self._max_n_resamples = max_n_resamples\n    self.resample_interval = resample_interval\n    self.resample_interval_process = resample_interval // world_size\n    self.start_collecting_neuron_activity_process = (\n        self.resample_interval_process - n_activations_activity_collate // world_size\n    )\n    self.start_collecting_loss_process = (\n        self.resample_interval_process - process_resample_dataset_size\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities","title":"<code>assign_sampling_probabilities(loss)</code>  <code>staticmethod</code>","text":"<p>Assign the sampling probabilities for each input activations vector.</p> <p>Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n&gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\ntensor([0.0700, 0.2900, 0.6400])\n</code></pre> <pre><code>&gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])\n&gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\ntensor([[0.0700, 0.0700],\n        [0.2900, 0.2900],\n        [0.6400, 0.6400]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Loss per item.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>A tensor of probabilities for each item.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef assign_sampling_probabilities(\n    loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Assign the sampling probabilities for each input activations vector.\n\n    Assign each input vector a probability of being picked that is proportional to the square of\n    the autoencoder's loss on that input.\n\n    Examples:\n        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n        tensor([0.0700, 0.2900, 0.6400])\n\n        &gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n        tensor([[0.0700, 0.0700],\n                [0.2900, 0.2900],\n                [0.6400, 0.6400]])\n\n    Args:\n        loss: Loss per item.\n\n    Returns:\n        A tensor of probabilities for each item.\n    \"\"\"\n    square_loss = loss.pow(2)\n    return square_loss / square_loss.sum(0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.compute","title":"<code>compute()</code>","text":"<p>Compute the parameters that need to be updated.</p> <p>Returns:</p> Type Description <code>list[ParameterUpdateResults] | None</code> <p>A list of parameter update results (for each component that the SAE is being trained</p> <code>list[ParameterUpdateResults] | None</code> <p>on), if an update is needed.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def compute(self) -&gt; list[ParameterUpdateResults] | None:\n    \"\"\"Compute the parameters that need to be updated.\n\n    Returns:\n        A list of parameter update results (for each component that the SAE is being trained\n        on), if an update is needed.\n    \"\"\"\n    # Resample if needed\n    if self._n_activations_seen_process &gt;= self.resample_interval_process:\n        with torch.no_grad():\n            # Initialise results\n            parameter_update_results: list[ParameterUpdateResults] = []\n\n            # Sync &amp; typecast\n            self.sync()\n            loss = dim_zero_cat(self._loss)\n            input_activations = dim_zero_cat(self._input_activations)\n\n            dead_neuron_indices: list[\n                Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)]\n            ] = self._get_dead_neuron_indices()\n\n            # Assign each input vector a probability of being picked that is proportional to the\n            # square of the autoencoder's loss on that input.\n            sample_probabilities: Float[\n                Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n            ] = self.assign_sampling_probabilities(loss)\n\n            # For each dead neuron sample an input according to these probabilities.\n            sampled_input: list[\n                Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n            ] = self.sample_input(\n                sample_probabilities,\n                input_activations,\n                [len(dead) for dead in dead_neuron_indices],\n            )\n\n            for component_idx in range(self._n_components):\n                # Renormalize each input vector to have unit L2 norm and set this to be the\n                # dictionary vector for the dead autoencoder neuron.\n                renormalized_input: Float[\n                    Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                ] = torch.nn.functional.normalize(sampled_input[component_idx], dim=-1)\n\n                dead_decoder_weight_updates = rearrange(\n                    renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n                )\n\n                # For the corresponding encoder vector, renormalize the input vector to equal\n                # the average norm of the encoder weights for alive neurons times 0.2. Set the\n                # corresponding encoder bias element to zero.\n                encoder_weight: Float[\n                    Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                ] = get_component_slice_tensor(self._encoder_weight, 3, 0, component_idx)\n\n                rescaled_sampled_input = self.renormalize_and_scale(\n                    sampled_input=sampled_input[component_idx],\n                    neuron_activity=self._neuron_fired_count[component_idx],\n                    encoder_weight=encoder_weight,\n                )\n\n                dead_encoder_bias_updates = torch.zeros_like(\n                    dead_neuron_indices[component_idx],\n                    dtype=dead_decoder_weight_updates.dtype,\n                    device=dead_decoder_weight_updates.device,\n                )\n\n                parameter_update_results.append(\n                    ParameterUpdateResults(\n                        dead_neuron_indices=dead_neuron_indices[component_idx],\n                        dead_encoder_weight_updates=rescaled_sampled_input,\n                        dead_encoder_bias_updates=dead_encoder_bias_updates,\n                        dead_decoder_weight_updates=dead_decoder_weight_updates,\n                    )\n                )\n\n            # Reset\n            self.unsync(should_unsync=self._is_synced)\n            self.reset()\n\n            return parameter_update_results\n\n    return None\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.forward","title":"<code>forward(input_activations, learned_activations, loss, encoder_weight_reference)</code>","text":"<p>Step the resampler, collating neuron activity and resampling if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>input_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations to the SAE.</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations from the SAE.</p> required <code>loss</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Loss per input activation.</p> required <code>encoder_weight_reference</code> <code>Parameter</code> <p>Reference to the SAE encoder weight tensor.</p> required <p>Returns:</p> Type Description <code>list[ParameterUpdateResults] | None</code> <p>Parameter update results (for each component that the SAE is being trained on) if</p> <code>list[ParameterUpdateResults] | None</code> <p>resampling is due. Otherwise None.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def forward(  # type: ignore[override]\n    self,\n    input_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    encoder_weight_reference: Parameter,\n) -&gt; list[ParameterUpdateResults] | None:\n    \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n    Args:\n        input_activations: Input activations to the SAE.\n        learned_activations: Learned activations from the SAE.\n        loss: Loss per input activation.\n        encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n    Returns:\n        Parameter update results (for each component that the SAE is being trained on) if\n        resampling is due. Otherwise None.\n    \"\"\"\n    # Don't do anything if we have already completed all resamples\n    if self._n_times_resampled &gt;= self._max_n_resamples:\n        return None\n\n    self.update(\n        input_activations=input_activations,\n        learned_activations=learned_activations,\n        loss=loss,\n        encoder_weight_reference=encoder_weight_reference,\n    )\n\n    return self.compute()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.renormalize_and_scale","title":"<code>renormalize_and_scale(sampled_input, neuron_activity, encoder_weight)</code>  <code>staticmethod</code>","text":"<p>Renormalize and scale the resampled dictionary vectors.</p> <p>Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2.</p> Example <p>from torch.nn import Parameter _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3.0, 0, 5, 0, 1, 3]) encoder_weight = Parameter(torch.ones((6, 2))) rescaled_input = ActivationResampler.renormalize_and_scale( ...     sampled_input, ...     neuron_activity, ...     encoder_weight ... ) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]])</p> <p>Parameters:</p> Name Type Description Default <code>sampled_input</code> <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of the sampled input activation.</p> required <code>neuron_activity</code> <code>Float[Tensor, names(LEARNT_FEATURE)]</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>encoder_weight</code> <code>Float[Tensor, names(LEARNT_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of encoder weights.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Rescaled sampled input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no alive neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef renormalize_and_scale(\n    sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n    neuron_activity: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE)],\n    encoder_weight: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n    Renormalize the input vector to equal the average norm of the encoder weights for alive\n    neurons times 0.2.\n\n    Example:\n        &gt;&gt;&gt; from torch.nn import Parameter\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n        &gt;&gt;&gt; neuron_activity = torch.tensor([3.0, 0, 5, 0, 1, 3])\n        &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n        ...     sampled_input,\n        ...     neuron_activity,\n        ...     encoder_weight\n        ... )\n        &gt;&gt;&gt; rescaled_input.round(decimals=1)\n        tensor([[0.2000, 0.2000]])\n\n    Args:\n        sampled_input: Tensor of the sampled input activation.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        encoder_weight: Tensor of encoder weights.\n\n    Returns:\n        Rescaled sampled input.\n\n    Raises:\n        ValueError: If there are no alive neurons.\n    \"\"\"\n    alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n    # Check there is at least one alive neuron\n    if not torch.any(alive_neuron_mask):\n        error_message = \"No alive neurons found.\"\n        raise ValueError(error_message)\n\n    # Handle no dead neurons\n    n_dead_neurons = len(sampled_input)\n    if n_dead_neurons == 0:\n        return torch.empty(\n            (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n        )\n\n    # Calculate the average norm of the encoder weights for alive neurons.\n    detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n    alive_encoder_weights: Float[\n        Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = detached_encoder_weight[alive_neuron_mask, :]\n    average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n        dim=-1\n    ).mean()\n\n    # Renormalize the input vector to equal the average norm of the encoder weights for alive\n    # neurons times 0.2.\n    renormalized_input: Float[\n        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n    return renormalized_input * (average_alive_norm * 0.2)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.reset","title":"<code>reset()</code>","text":"<p>Reset the activation resampler.</p> Warning <p>This is only called when forward/compute has returned parameters to update (i.e. resampling is due).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the activation resampler.\n\n    Warning:\n        This is only called when forward/compute has returned parameters to update (i.e.\n        resampling is due).\n    \"\"\"\n    self._n_activations_seen_process = 0\n    self._neuron_fired_count = torch.zeros_like(self._neuron_fired_count)\n    self._loss = []\n    self._input_activations = []\n    self._n_times_resampled += 1\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.sample_input","title":"<code>sample_input(probabilities, input_activations, n_samples)</code>  <code>staticmethod</code>","text":"<p>Sample an input vector based on the provided probabilities.</p> Example <p>probabilities = torch.tensor([[0.1], [0.2], [0.7]]) input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]]) _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = ActivationResampler.sample_input( ...     probabilities, input_activations, [2] ... ) sampled_input[0].tolist() [[5.0, 6.0], [3.0, 4.0]]</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Probabilities for each input.</p> required <code>input_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activation vectors.</p> required <code>n_samples</code> <code>list[int]</code> <p>Number of samples to take (number of dead neurons).</p> required <p>Returns:</p> Type Description <code>list[Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]]</code> <p>Sampled input activation vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of samples is greater than the number of input activations.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef sample_input(\n    probabilities: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    input_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    n_samples: list[int],\n) -&gt; list[Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]]:\n    \"\"\"Sample an input vector based on the provided probabilities.\n\n    Example:\n        &gt;&gt;&gt; probabilities = torch.tensor([[0.1], [0.2], [0.7]])\n        &gt;&gt;&gt; input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n        ...     probabilities, input_activations, [2]\n        ... )\n        &gt;&gt;&gt; sampled_input[0].tolist()\n        [[5.0, 6.0], [3.0, 4.0]]\n\n    Args:\n        probabilities: Probabilities for each input.\n        input_activations: Input activation vectors.\n        n_samples: Number of samples to take (number of dead neurons).\n\n    Returns:\n        Sampled input activation vector.\n\n    Raises:\n        ValueError: If the number of samples is greater than the number of input activations.\n    \"\"\"\n    sampled_inputs: list[\n        Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n    ] = []\n\n    for component_idx, component_n_samples in enumerate(n_samples):\n        component_probabilities: Float[Tensor, Axis.BATCH] = get_component_slice_tensor(\n            input_tensor=probabilities,\n            n_dim_with_component=2,\n            component_dim=1,\n            component_idx=component_idx,\n        )\n\n        component_input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n        ] = get_component_slice_tensor(\n            input_tensor=input_activations,\n            n_dim_with_component=3,\n            component_dim=1,\n            component_idx=component_idx,\n        )\n\n        if component_n_samples &gt; len(component_input_activations):\n            exception_message = (\n                f\"Cannot sample {component_n_samples} inputs from \"\n                f\"{len(component_input_activations)} input activations.\"\n            )\n            raise ValueError(exception_message)\n\n        # Handle the 0 dead neurons case\n        if component_n_samples == 0:\n            sampled_inputs.append(\n                torch.empty(\n                    (0, component_input_activations.shape[-1]),\n                    dtype=component_input_activations.dtype,\n                    device=component_input_activations.device,\n                )\n            )\n            continue\n\n        # Handle the 1+ dead neuron case\n        component_sample_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n            component_probabilities, num_samples=component_n_samples\n        )\n        sampled_inputs.append(component_input_activations[component_sample_indices, :])\n\n    return sampled_inputs\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.update","title":"<code>update(input_activations, learned_activations, loss, encoder_weight_reference)</code>","text":"<p>Update the collated data from forward passes.</p> <p>Parameters:</p> Name Type Description Default <code>input_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations to the SAE.</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations from the SAE.</p> required <code>loss</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Loss per input activation.</p> required <code>encoder_weight_reference</code> <code>Parameter</code> <p>Reference to the SAE encoder weight tensor.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the loss or input activations are not lists (e.g. from unsync having not been called).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def update(\n    self,\n    input_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    encoder_weight_reference: Parameter,\n) -&gt; None:\n    \"\"\"Update the collated data from forward passes.\n\n    Args:\n        input_activations: Input activations to the SAE.\n        learned_activations: Learned activations from the SAE.\n        loss: Loss per input activation.\n        encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n    Raises:\n        TypeError: If the loss or input activations are not lists (e.g. from unsync having not\n            been called).\n    \"\"\"\n    if self._n_activations_seen_process &gt;= self.start_collecting_neuron_activity_process:\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n        self._neuron_fired_count += neuron_has_fired.sum(dim=0)\n\n    if self._n_activations_seen_process &gt;= self.start_collecting_loss_process:\n        # Typecast\n        if not isinstance(self._loss, list) or not isinstance(self._input_activations, list):\n            raise TypeError\n\n        # Append\n        self._loss.append(loss)\n        self._input_activations.append(input_activations)\n\n    self._n_activations_seen_process += len(learned_activations)\n    self._encoder_weight = encoder_weight_reference\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters","title":"<code>ActivationResamplerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Activation resampler hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass ActivationResamplerHyperparameters(NestedParameter):\n    \"\"\"Activation resampler hyperparameters.\"\"\"\n\n    resample_interval: Parameter[int] = field(\n        default=Parameter(round_to_multiple(200_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Resample interval.\"\"\"\n\n    max_n_resamples: Parameter[int] = field(default=Parameter(4))\n    \"\"\"Maximum number of resamples.\"\"\"\n\n    n_activations_activity_collate: Parameter[int] = field(\n        default=Parameter(round_to_multiple(100_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Number of steps to collate before resampling.\n\n    Number of autoencoder learned activation vectors to collate before resampling.\n    \"\"\"\n\n    resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))\n    \"\"\"Resample dataset size.\n\n    Number of autoencoder input activations to use for calculating the loss, as part of the\n    resampling process to create the reset neuron weights.\n    \"\"\"\n\n    threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Dead neuron threshold.\n\n    Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the\n    collated sample).\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.max_n_resamples","title":"<code>max_n_resamples: Parameter[int] = field(default=Parameter(4))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of resamples.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.n_activations_activity_collate","title":"<code>n_activations_activity_collate: Parameter[int] = field(default=Parameter(round_to_multiple(100000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of steps to collate before resampling.</p> <p>Number of autoencoder learned activation vectors to collate before resampling.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.resample_dataset_size","title":"<code>resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample dataset size.</p> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.resample_interval","title":"<code>resample_interval: Parameter[int] = field(default=Parameter(round_to_multiple(200000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample interval.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires","title":"<code>threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead neuron threshold.</p> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset","title":"<code>AdamWithReset</code>","text":"<p>             Bases: <code>Adam</code></p> <p>Adam Optimizer with a reset method.</p> <p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed.</p> <p>Otherwise this is the same as the standard Adam optimizer.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>class AdamWithReset(Adam):\n    \"\"\"Adam Optimizer with a reset method.\n\n    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when\n    manually editing the model parameters during training (e.g. when resampling dead neurons). This\n    is because Adam maintains running averages of the gradients and the squares of gradients, which\n    will be incorrect if the parameters are changed.\n\n    Otherwise this is the same as the standard Adam optimizer.\n    \"\"\"\n\n    parameter_names: list[str]\n    \"\"\"Parameter Names.\n\n    The names of the parameters, so that we can find them later when resetting the state.\n    \"\"\"\n\n    _has_components_dim: bool\n    \"\"\"Whether the parameters have a components dimension.\"\"\"\n\n    def __init__(  # (extending existing implementation)\n        self,\n        params: params_t,\n        lr: float | Float[Tensor, Axis.names(Axis.SINGLE_ITEM)] = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0.0,\n        *,\n        amsgrad: bool = False,\n        foreach: bool | None = None,\n        maximize: bool = False,\n        capturable: bool = False,\n        differentiable: bool = False,\n        fused: bool | None = None,\n        named_parameters: Iterator[tuple[str, Parameter]],\n        has_components_dim: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer.\n\n        Warning:\n            Named parameters must be with default settings (remove duplicates and not recursive).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n            ...     SparseAutoencoder, SparseAutoencoderConfig\n            ... )\n            &gt;&gt;&gt; model = SparseAutoencoder(\n            ...        SparseAutoencoderConfig(\n            ...             n_input_features=5,\n            ...             n_learned_features=10,\n            ...             n_components=2\n            ...         )\n            ...    )\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ...     has_components_dim=True,\n            ... )\n            &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n        Args:\n            params: Iterable of parameters to optimize or dicts defining parameter groups.\n            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n                float LR unless specifying fused=True or capturable=True.\n            betas: Coefficients used for computing running averages of gradient and its square.\n            eps: Term added to the denominator to improve numerical stability.\n            weight_decay: Weight decay (L2 penalty).\n            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n                Convergence of Adam and Beyond\".\n            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n                over the for-loop implementation on CUDA if more performant. Note that foreach uses\n                more peak memory.\n            maximize: If True, maximizes the parameters based on the objective, instead of\n                minimizing.\n            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n                ungraphed performance.\n            differentiable: Whether autograd should occur through the optimizer step in training.\n                Setting to True can impair performance.\n            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n                torch.float32, torch.float16, and torch.bfloat16.\n            named_parameters: An iterator over the named parameters of the model. This is used to\n                find the parameters when resetting their state. You should set this as\n                `model.named_parameters()`.\n            has_components_dim: If the parameters have a components dimension (i.e. if you are\n                training an SAE on more than one component).\n\n        Raises:\n            ValueError: If the number of parameter names does not match the number of parameters.\n        \"\"\"\n        # Initialise the parent class (note we repeat the parameter names so that type hints work).\n        super().__init__(\n            params=params,\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n            differentiable=differentiable,\n            fused=fused,\n        )\n\n        self._has_components_dim = has_components_dim\n\n        # Store the names of the parameters, so that we can find them later when resetting the\n        # state.\n        self.parameter_names = [name for name, _value in named_parameters]\n\n        if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n            error_message = (\n                \"The number of parameter names does not match the number of parameters. \"\n                \"If using model.named_parameters() make sure remove_duplicates is True \"\n                \"and recursive is False (the default settings).\"\n            )\n            raise ValueError(error_message)\n\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Iterates over all parameters and resets both the running averages of the gradients and the\n        squares of gradients.\n        \"\"\"\n        # Iterate over every parameter\n        for group in self.param_groups:\n            for parameter in group[\"params\"]:\n                # Get the state\n                state = self.state[parameter]\n\n                # Check if state is initialized\n                if len(state) == 0:\n                    continue\n\n                # Reset running averages\n                exp_avg: Tensor = state[\"exp_avg\"]\n                exp_avg.zero_()\n                exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n                exp_avg_sq.zero_()\n\n                # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n                if \"max_exp_avg_sq\" in state:\n                    max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                    max_exp_avg_sq.zero_()\n\n    def reset_neurons_state(\n        self,\n        parameter: Parameter,\n        neuron_indices: Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)],\n        axis: int,\n        component_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n            ...     SparseAutoencoder, SparseAutoencoderConfig\n            ... )\n            &gt;&gt;&gt; model = SparseAutoencoder(\n            ...        SparseAutoencoderConfig(\n            ...             n_input_features=5,\n            ...             n_learned_features=10,\n            ...             n_components=2\n            ...         )\n            ...    )\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ...     has_components_dim=True,\n            ... )\n            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\n            ...     model.decoder.weight,\n            ...     dead_neurons_indices,\n            ...     axis=1\n            ... )\n\n        Args:\n            parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the state values to reset (i.e. the input/output features axis, as\n                we're resetting all input/output features for a specific dead neuron).\n            component_idx: The component index of the state values to reset.\n\n        Raises:\n            ValueError: If the parameter has a components dimension, but has_components_dim is\n                False.\n        \"\"\"\n        # Get the state of the parameter\n        state = self.state[parameter]\n\n        # If the number of dimensions is 3, we definitely have a components dimension. If 2, we may\n        # do (as the bias has 2 dimensions with components, but the weight has 2 dimensions without\n        # components).\n        definitely_has_components_dimension = 3\n        if (\n            not self._has_components_dim\n            and state[\"exp_avg\"].ndim == definitely_has_components_dimension\n        ):\n            error_message = (\n                \"The parameter has a components dimension, but has_components_dim is False. \"\n                \"This should not happen.\"\n            )\n            raise ValueError(error_message)\n\n        # Check if state is initialized\n        if len(state) == 0:\n            return\n\n        # Check there are any neurons to reset\n        if neuron_indices.numel() == 0:\n            return\n\n        # Move the neuron indices to the correct device\n        neuron_indices = neuron_indices.to(device=state[\"exp_avg\"].device)\n\n        # Reset running averages for the specified neurons\n        if \"exp_avg\" in state:\n            if self._has_components_dim:\n                state[\"exp_avg\"][component_idx].index_fill_(axis, neuron_indices, 0)\n            else:\n                state[\"exp_avg\"].index_fill_(axis, neuron_indices, 0)\n\n        if \"exp_avg_sq\" in state:\n            if self._has_components_dim:\n                state[\"exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n            else:\n                state[\"exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n\n        # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n        if \"max_exp_avg_sq\" in state:\n            if self._has_components_dim:\n                state[\"max_exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n            else:\n                state[\"max_exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.parameter_names","title":"<code>parameter_names: list[str] = [name for (name, _value) in named_parameters]</code>  <code>instance-attribute</code>","text":"<p>Parameter Names.</p> <p>The names of the parameters, so that we can find them later when resetting the state.</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.__init__","title":"<code>__init__(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0, *, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, named_parameters, has_components_dim)</code>","text":"<p>Initialize the optimizer.</p> Warning <p>Named parameters must be with default settings (remove duplicates and not recursive).</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import ( ...     SparseAutoencoder, SparseAutoencoderConfig ... ) model = SparseAutoencoder( ...        SparseAutoencoderConfig( ...             n_input_features=5, ...             n_learned_features=10, ...             n_components=2 ...         ) ...    ) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ...     has_components_dim=True, ... ) optimizer.reset_state_all_parameters()</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>params_t</code> <p>Iterable of parameters to optimize or dicts defining parameter groups.</p> required <code>lr</code> <code>float | Float[Tensor, names(SINGLE_ITEM)]</code> <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a float LR unless specifying fused=True or capturable=True.</p> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients used for computing running averages of gradient and its square.</p> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <p>Term added to the denominator to improve numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (L2 penalty).</p> <code>0.0</code> <code>amsgrad</code> <code>bool</code> <p>Whether to use the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Whether foreach implementation of optimizer is used. If None, foreach is used over the for-loop implementation on CUDA if more performant. Note that foreach uses more peak memory.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, maximizes the parameters based on the objective, instead of minimizing.</p> <code>False</code> <code>capturable</code> <code>bool</code> <p>Whether this instance is safe to capture in a CUDA graph. True can impair ungraphed performance.</p> <code>False</code> <code>differentiable</code> <code>bool</code> <p>Whether autograd should occur through the optimizer step in training. Setting to True can impair performance.</p> <code>False</code> <code>fused</code> <code>bool | None</code> <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64, torch.float32, torch.float16, and torch.bfloat16.</p> <code>None</code> <code>named_parameters</code> <code>Iterator[tuple[str, Parameter]]</code> <p>An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as <code>model.named_parameters()</code>.</p> required <code>has_components_dim</code> <code>bool</code> <p>If the parameters have a components dimension (i.e. if you are training an SAE on more than one component).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of parameter names does not match the number of parameters.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def __init__(  # (extending existing implementation)\n    self,\n    params: params_t,\n    lr: float | Float[Tensor, Axis.names(Axis.SINGLE_ITEM)] = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    *,\n    amsgrad: bool = False,\n    foreach: bool | None = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: bool | None = None,\n    named_parameters: Iterator[tuple[str, Parameter]],\n    has_components_dim: bool,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Warning:\n        Named parameters must be with default settings (remove duplicates and not recursive).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n        ...     SparseAutoencoder, SparseAutoencoderConfig\n        ... )\n        &gt;&gt;&gt; model = SparseAutoencoder(\n        ...        SparseAutoencoderConfig(\n        ...             n_input_features=5,\n        ...             n_learned_features=10,\n        ...             n_components=2\n        ...         )\n        ...    )\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ...     has_components_dim=True,\n        ... )\n        &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n    Args:\n        params: Iterable of parameters to optimize or dicts defining parameter groups.\n        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n            float LR unless specifying fused=True or capturable=True.\n        betas: Coefficients used for computing running averages of gradient and its square.\n        eps: Term added to the denominator to improve numerical stability.\n        weight_decay: Weight decay (L2 penalty).\n        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n            Convergence of Adam and Beyond\".\n        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n            over the for-loop implementation on CUDA if more performant. Note that foreach uses\n            more peak memory.\n        maximize: If True, maximizes the parameters based on the objective, instead of\n            minimizing.\n        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n            ungraphed performance.\n        differentiable: Whether autograd should occur through the optimizer step in training.\n            Setting to True can impair performance.\n        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n            torch.float32, torch.float16, and torch.bfloat16.\n        named_parameters: An iterator over the named parameters of the model. This is used to\n            find the parameters when resetting their state. You should set this as\n            `model.named_parameters()`.\n        has_components_dim: If the parameters have a components dimension (i.e. if you are\n            training an SAE on more than one component).\n\n    Raises:\n        ValueError: If the number of parameter names does not match the number of parameters.\n    \"\"\"\n    # Initialise the parent class (note we repeat the parameter names so that type hints work).\n    super().__init__(\n        params=params,\n        lr=lr,\n        betas=betas,\n        eps=eps,\n        weight_decay=weight_decay,\n        amsgrad=amsgrad,\n        foreach=foreach,\n        maximize=maximize,\n        capturable=capturable,\n        differentiable=differentiable,\n        fused=fused,\n    )\n\n    self._has_components_dim = has_components_dim\n\n    # Store the names of the parameters, so that we can find them later when resetting the\n    # state.\n    self.parameter_names = [name for name, _value in named_parameters]\n\n    if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n        error_message = (\n            \"The number of parameter names does not match the number of parameters. \"\n            \"If using model.named_parameters() make sure remove_duplicates is True \"\n            \"and recursive is False (the default settings).\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter, neuron_indices, axis, component_idx=0)</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import ( ...     SparseAutoencoder, SparseAutoencoderConfig ... ) model = SparseAutoencoder( ...        SparseAutoencoderConfig( ...             n_input_features=5, ...             n_learned_features=10, ...             n_components=2 ...         ) ...    ) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ...     has_components_dim=True, ... )</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>Parameter</code> <p>The parameter to be reset. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>,</p> required <code>neuron_indices</code> <code>Int[Tensor, names(LEARNT_FEATURE_IDX)]</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the state values to reset (i.e. the input/output features axis, as we're resetting all input/output features for a specific dead neuron).</p> required <code>component_idx</code> <code>int</code> <p>The component index of the state values to reset.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the parameter has a components dimension, but has_components_dim is False.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_neurons_state(\n    self,\n    parameter: Parameter,\n    neuron_indices: Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)],\n    axis: int,\n    component_idx: int = 0,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n        ...     SparseAutoencoder, SparseAutoencoderConfig\n        ... )\n        &gt;&gt;&gt; model = SparseAutoencoder(\n        ...        SparseAutoencoderConfig(\n        ...             n_input_features=5,\n        ...             n_learned_features=10,\n        ...             n_components=2\n        ...         )\n        ...    )\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ...     has_components_dim=True,\n        ... )\n        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\n        ...     model.decoder.weight,\n        ...     dead_neurons_indices,\n        ...     axis=1\n        ... )\n\n    Args:\n        parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the state values to reset (i.e. the input/output features axis, as\n            we're resetting all input/output features for a specific dead neuron).\n        component_idx: The component index of the state values to reset.\n\n    Raises:\n        ValueError: If the parameter has a components dimension, but has_components_dim is\n            False.\n    \"\"\"\n    # Get the state of the parameter\n    state = self.state[parameter]\n\n    # If the number of dimensions is 3, we definitely have a components dimension. If 2, we may\n    # do (as the bias has 2 dimensions with components, but the weight has 2 dimensions without\n    # components).\n    definitely_has_components_dimension = 3\n    if (\n        not self._has_components_dim\n        and state[\"exp_avg\"].ndim == definitely_has_components_dimension\n    ):\n        error_message = (\n            \"The parameter has a components dimension, but has_components_dim is False. \"\n            \"This should not happen.\"\n        )\n        raise ValueError(error_message)\n\n    # Check if state is initialized\n    if len(state) == 0:\n        return\n\n    # Check there are any neurons to reset\n    if neuron_indices.numel() == 0:\n        return\n\n    # Move the neuron indices to the correct device\n    neuron_indices = neuron_indices.to(device=state[\"exp_avg\"].device)\n\n    # Reset running averages for the specified neurons\n    if \"exp_avg\" in state:\n        if self._has_components_dim:\n            state[\"exp_avg\"][component_idx].index_fill_(axis, neuron_indices, 0)\n        else:\n            state[\"exp_avg\"].index_fill_(axis, neuron_indices, 0)\n\n    if \"exp_avg_sq\" in state:\n        if self._has_components_dim:\n            state[\"exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n        else:\n            state[\"exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n\n    # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n    if \"max_exp_avg_sq\" in state:\n        if self._has_components_dim:\n            state[\"max_exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n        else:\n            state[\"max_exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","title":"... train the model and then resample some dead neurons, then do this ...","text":"<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","title":"Reset the optimizer state for parameters that have been updated","text":"<p>optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0) optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ...     model.decoder.weight, ...     dead_neurons_indices, ...     axis=1 ... )</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>","text":"<p>Reset the state for all parameters.</p> <p>Iterates over all parameters and resets both the running averages of the gradients and the squares of gradients.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Iterates over all parameters and resets both the running averages of the gradients and the\n    squares of gradients.\n    \"\"\"\n    # Iterate over every parameter\n    for group in self.param_groups:\n        for parameter in group[\"params\"]:\n            # Get the state\n            state = self.state[parameter]\n\n            # Check if state is initialized\n            if len(state) == 0:\n                continue\n\n            # Reset running averages\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.zero_()\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.zero_()\n\n            # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n            if \"max_exp_avg_sq\" in state:\n                max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                max_exp_avg_sq.zero_()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AutoencoderHyperparameters","title":"<code>AutoencoderHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Sparse autoencoder hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass AutoencoderHyperparameters(NestedParameter):\n    \"\"\"Sparse autoencoder hyperparameters.\"\"\"\n\n    expansion_factor: Parameter[int] = field(default=Parameter(2))\n    \"\"\"Expansion Factor.\n\n    Size of the learned features relative to the input features. A good expansion factor to start\n    with is typically 2-4.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AutoencoderHyperparameters.expansion_factor","title":"<code>expansion_factor: Parameter[int] = field(default=Parameter(2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion Factor.</p> <p>Size of the learned features relative to the input features. A good expansion factor to start with is typically 2-4.</p>"},{"location":"reference/#sparse_autoencoder.CapacityMetric","title":"<code>CapacityMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Capacities metric.</p> <p>Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks.</p> <p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features.</p> Warning <p>This is memory intensive as it requires caching all learned activations for a batch.</p> <p>Examples:</p> <p>If the features are orthogonal, the capacity is 1.</p> <pre><code>&gt;&gt;&gt; metric = CapacityMetric()\n&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [ # Batch 1\n...         [1., 0., 1.] # Component 1: learned features\n...     ],\n...     [ # Batch 2\n...         [0., 1., 0.] # Component 1: learned features (orthogonal)\n...     ]\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor([[1., 1.]])\n</code></pre> <p>If they are all the same, the capacity is 1/n.</p> <pre><code>&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [ # Batch 1\n...         [1., 1., 1.] # Component 1: learned features\n...     ],\n...     [ # Batch 2\n...         [1., 1., 1.] # Component 1: learned features (same)\n...     ]\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor([[0.5000, 0.5000]])\n</code></pre> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>class CapacityMetric(Metric):\n    \"\"\"Capacities metric.\n\n    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural\n    Networks](https://arxiv.org/pdf/2210.01892.pdf).\n\n    Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.\n    Formally it's the ratio of the squared dot product of a feature with itself to the sum of its\n    squared dot products of all features.\n\n    Warning:\n        This is memory intensive as it requires caching all learned activations for a batch.\n\n    Examples:\n        If the features are orthogonal, the capacity is 1.\n\n        &gt;&gt;&gt; metric = CapacityMetric()\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.] # Component 1: learned features\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 1., 0.] # Component 1: learned features (orthogonal)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([[1., 1.]])\n\n        If they are all the same, the capacity is 1/n.\n\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 1., 1.] # Component 1: learned features\n        ...     ],\n        ...     [ # Batch 2\n        ...         [1., 1., 1.] # Component 1: learned features (same)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([[0.5000, 0.5000]])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n    plot_upper_bound: float | None = 1.0\n\n    # State\n    learned_activations: list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    ]\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__()\n        self.add_state(\"learned_activations\", default=[])\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        self.learned_activations.append(learned_activations)\n\n    @staticmethod\n    def capacities(\n        features: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n        r\"\"\"Calculate capacities.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])\n            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n            &gt;&gt;&gt; orthogonal_caps\n            tensor([[1., 1., 1.]])\n\n        Args:\n            features: A collection of features.\n\n        Returns:\n            A 1D tensor of capacities, where each element is the capacity of the corresponding\n            feature.\n        \"\"\"\n        squared_dot_products: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n        ] = (\n            einops.einsum(\n                features,\n                features,\n                f\"batch_1 ... {Axis.LEARNT_FEATURE}, \\\n                    batch_2 ... {Axis.LEARNT_FEATURE} \\\n                    -&gt; ... batch_1 batch_2\",\n            )\n            ** 2\n        )\n\n        sum_of_sq_dot: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)\n        ] = squared_dot_products.sum(dim=-1)\n\n        diagonal: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)] = torch.diagonal(\n            squared_dot_products, dim1=1, dim2=2\n        )\n\n        return diagonal / sum_of_sq_dot\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n        \"\"\"Compute the metric.\"\"\"\n        batch_learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.cat(self.learned_activations)\n\n        return self.capacities(batch_learned_activations)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__()\n    self.add_state(\"learned_activations\", default=[])\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.capacities","title":"<code>capacities(features)</code>  <code>staticmethod</code>","text":"<p>Calculate capacities.</p> Example <p>import torch orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]]) orthogonal_caps = CapacityMetric.capacities(orthogonal_features) orthogonal_caps tensor([[1., 1., 1.]])</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>A collection of features.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(COMPONENT_OPTIONAL, BATCH)]</code> <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p> <code>Float[Tensor, names(COMPONENT_OPTIONAL, BATCH)]</code> <p>feature.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef capacities(\n    features: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n    r\"\"\"Calculate capacities.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])\n        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n        &gt;&gt;&gt; orthogonal_caps\n        tensor([[1., 1., 1.]])\n\n    Args:\n        features: A collection of features.\n\n    Returns:\n        A 1D tensor of capacities, where each element is the capacity of the corresponding\n        feature.\n    \"\"\"\n    squared_dot_products: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n    ] = (\n        einops.einsum(\n            features,\n            features,\n            f\"batch_1 ... {Axis.LEARNT_FEATURE}, \\\n                batch_2 ... {Axis.LEARNT_FEATURE} \\\n                -&gt; ... batch_1 batch_2\",\n        )\n        ** 2\n    )\n\n    sum_of_sq_dot: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)\n    ] = squared_dot_products.sum(dim=-1)\n\n    diagonal: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)] = torch.diagonal(\n        squared_dot_products, dim1=1, dim2=2\n    )\n\n    return diagonal / sum_of_sq_dot\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n    \"\"\"Compute the metric.\"\"\"\n    batch_learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.cat(self.learned_activations)\n\n    return self.capacities(batch_learned_activations)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    self.learned_activations.append(learned_activations)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Controller","title":"<code>Controller</code>  <code>dataclass</code>","text":"<p>Controller.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass Controller:\n    \"\"\"Controller.\"\"\"\n\n    type: ControllerType\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ControllerType","title":"<code>ControllerType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Controller Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ControllerType(LowercaseStrEnum):\n    \"\"\"Controller Type.\"\"\"\n\n    CLOUD = auto()\n    \"\"\"Weights &amp; Biases cloud controller.\n\n    Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all\n    communicate with the Weights &amp; Biases cloud service to coordinate the sweep.\n    \"\"\"\n\n    LOCAL = auto()\n    \"\"\"Local controller.\n\n    Manages the sweep operation locally, without the need for cloud-based coordination or external\n    services.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ControllerType.CLOUD","title":"<code>CLOUD = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weights &amp; Biases cloud controller.</p> <p>Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all communicate with the Weights &amp; Biases cloud service to coordinate the sweep.</p>"},{"location":"reference/#sparse_autoencoder.ControllerType.LOCAL","title":"<code>LOCAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Local controller.</p> <p>Manages the sweep operation locally, without the need for cloud-based coordination or external services.</p>"},{"location":"reference/#sparse_autoencoder.Distribution","title":"<code>Distribution</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Sweep Distribution.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Distribution(LowercaseStrEnum):\n    \"\"\"Sweep Distribution.\"\"\"\n\n    BETA = auto()\n    \"\"\"Beta distribution.\n\n    Utilizes the Beta distribution, a family of continuous probability distributions defined on the\n    interval [0, 1], for parameter sampling.\n    \"\"\"\n\n    CATEGORICAL = auto()\n    \"\"\"Categorical distribution.\n\n    Employs a categorical distribution for discrete variable sampling, where each category has an\n    equal probability of being selected.\n    \"\"\"\n\n    CATEGORICAL_W_PROBABILITIES = auto()\n    \"\"\"Categorical distribution with probabilities.\n\n    Similar to categorical distribution but allows assigning different probabilities to each\n    category.\n    \"\"\"\n\n    CONSTANT = auto()\n    \"\"\"Constant distribution.\n\n    Uses a constant value for the parameter, ensuring it remains the same across all runs.\n    \"\"\"\n\n    INT_UNIFORM = auto()\n    \"\"\"Integer uniform distribution.\n\n    Samples integer values uniformly across a specified range.\n    \"\"\"\n\n    INV_LOG_UNIFORM = auto()\n    \"\"\"Inverse log-uniform distribution.\n\n    Samples values according to an inverse log-uniform distribution, useful for parameters that span\n    several orders of magnitude.\n    \"\"\"\n\n    INV_LOG_UNIFORM_VALUES = auto()\n    \"\"\"Inverse log-uniform values distribution.\n\n    Similar to the inverse log-uniform distribution but allows specifying exact values to be\n    sampled.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Distribution.BETA","title":"<code>BETA = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Beta distribution.</p> <p>Utilizes the Beta distribution, a family of continuous probability distributions defined on the interval [0, 1], for parameter sampling.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.CATEGORICAL","title":"<code>CATEGORICAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution.</p> <p>Employs a categorical distribution for discrete variable sampling, where each category has an equal probability of being selected.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.CATEGORICAL_W_PROBABILITIES","title":"<code>CATEGORICAL_W_PROBABILITIES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution with probabilities.</p> <p>Similar to categorical distribution but allows assigning different probabilities to each category.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.CONSTANT","title":"<code>CONSTANT = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant distribution.</p> <p>Uses a constant value for the parameter, ensuring it remains the same across all runs.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.INT_UNIFORM","title":"<code>INT_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Integer uniform distribution.</p> <p>Samples integer values uniformly across a specified range.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.INV_LOG_UNIFORM","title":"<code>INV_LOG_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform distribution.</p> <p>Samples values according to an inverse log-uniform distribution, useful for parameters that span several orders of magnitude.</p>"},{"location":"reference/#sparse_autoencoder.Distribution.INV_LOG_UNIFORM_VALUES","title":"<code>INV_LOG_UNIFORM_VALUES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform values distribution.</p> <p>Similar to the inverse log-uniform distribution but allows specifying exact values to be sampled.</p>"},{"location":"reference/#sparse_autoencoder.FeatureDensityMetric","title":"<code>FeatureDensityMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Feature density metric.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a training batch.</p> <p>Generally we want a small number of features to be active in each batch, so average feature density should be low. By contrast if the average feature density is high, it means that the features are not sparse enough.</p> Example <p>metric = FeatureDensityMetric(num_learned_features=3, num_components=1) learned_activations = torch.tensor([ ...     [ # Batch 1 ...         [1., 0., 1.] # Component 1: learned features (2 active neurons) ...     ], ...     [ # Batch 2 ...         [0., 0., 0.] # Component 1: learned features (0 active neuron) ...     ] ... ]) metric.forward(learned_activations) tensor([[0.5000, 0.0000, 0.5000]])</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>class FeatureDensityMetric(Metric):\n    \"\"\"Feature density metric.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a\n    training batch.\n\n    Generally we want a small number of features to be active in each batch, so average feature\n    density should be low. By contrast if the average feature density is high, it means that the\n    features are not sparse enough.\n\n    Example:\n        &gt;&gt;&gt; metric = FeatureDensityMetric(num_learned_features=3, num_components=1)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.] # Component 1: learned features (2 active neurons)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 0., 0.] # Component 1: learned features (0 active neuron)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([[0.5000, 0.0000, 0.5000]])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = True\n    plot_lower_bound: float | None = 0.0\n    plot_upper_bound: float | None = 1.0\n\n    # State\n    neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self, num_learned_features: PositiveInt, num_components: PositiveInt | None = None\n    ) -&gt; None:\n        \"\"\"Initialise the metric.\"\"\"\n        super().__init__()\n\n        self.add_state(\n            \"neuron_fired_count\",\n            default=torch.zeros(\n                size=shape_with_optional_dimensions(num_components, num_learned_features),\n                dtype=torch.float,  # Float is needed for dist reduce to work\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        # Increment the counter of activations seen since the last compute step\n        self.num_activation_vectors += learned_activations.shape[0]\n\n        # Count the number of active neurons in the batch\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n\n        self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.int64)\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Compute the metric.\"\"\"\n        return self.neuron_fired_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.FeatureDensityMetric.__init__","title":"<code>__init__(num_learned_features, num_components=None)</code>","text":"<p>Initialise the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>@validate_call\ndef __init__(\n    self, num_learned_features: PositiveInt, num_components: PositiveInt | None = None\n) -&gt; None:\n    \"\"\"Initialise the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\n        \"neuron_fired_count\",\n        default=torch.zeros(\n            size=shape_with_optional_dimensions(num_components, num_learned_features),\n            dtype=torch.float,  # Float is needed for dist reduce to work\n        ),\n        dist_reduce_fx=\"sum\",\n    )\n\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.FeatureDensityMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Compute the metric.\"\"\"\n    return self.neuron_fired_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.FeatureDensityMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    # Increment the counter of activations seen since the last compute step\n    self.num_activation_vectors += learned_activations.shape[0]\n\n    # Count the number of active neurons in the batch\n    neuron_has_fired: Bool[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.gt(learned_activations, 0)\n\n    self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.int64)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Goal","title":"<code>Goal</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Goal.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Goal(LowercaseStrEnum):\n    \"\"\"Goal.\"\"\"\n\n    MAXIMIZE = auto()\n    \"\"\"Maximization goal.\n\n    Sets the objective of the hyperparameter tuning process to maximize a specified metric.\n    \"\"\"\n\n    MINIMIZE = auto()\n    \"\"\"Minimization goal.\n\n    Aims to minimize a specified metric during the hyperparameter tuning process.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Goal.MAXIMIZE","title":"<code>MAXIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximization goal.</p> <p>Sets the objective of the hyperparameter tuning process to maximize a specified metric.</p>"},{"location":"reference/#sparse_autoencoder.Goal.MINIMIZE","title":"<code>MINIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimization goal.</p> <p>Aims to minimize a specified metric during the hyperparameter tuning process.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping","title":"<code>HyperbandStopping</code>  <code>dataclass</code>","text":"<p>Hyperband Stopping Config.</p> <p>Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs.</p> Example <p>HyperbandStopping(type=HyperbandStoppingType.HYPERBAND) HyperbandStopping(type=hyperband)</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass HyperbandStopping:\n    \"\"\"Hyperband Stopping Config.\n\n    Speed up hyperparameter search by killing off runs that appear to have lower performance\n    than successful training runs.\n\n    Example:\n        &gt;&gt;&gt; HyperbandStopping(type=HyperbandStoppingType.HYPERBAND)\n        HyperbandStopping(type=hyperband)\n    \"\"\"\n\n    type: HyperbandStoppingType | None = HyperbandStoppingType.HYPERBAND\n\n    eta: float | None = None\n    \"\"\"ETA.\n\n    Specify the bracket multiplier schedule (default: 3).\n    \"\"\"\n\n    maxiter: int | None = None\n    \"\"\"Max Iterations.\n\n    Specify the maximum number of iterations. Note this is number of times the metric is logged, not\n    the number of activations.\n    \"\"\"\n\n    miniter: int | None = None\n    \"\"\"Min Iterations.\n\n    Set the first epoch to start trimming runs, and hyperband will automatically calculate\n    the subsequent epochs to trim runs.\n    \"\"\"\n\n    s: float | None = None\n    \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\"\n\n    strict: bool | None = None\n    \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.eta","title":"<code>eta: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ETA.</p> <p>Specify the bracket multiplier schedule (default: 3).</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.maxiter","title":"<code>maxiter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max Iterations.</p> <p>Specify the maximum number of iterations. Note this is number of times the metric is logged, not the number of activations.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.miniter","title":"<code>miniter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Min Iterations.</p> <p>Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.s","title":"<code>s: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Set the number of steps you trim runs at, working backwards from the max_iter.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.strict","title":"<code>strict: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use a more aggressive condition for termination, stops more runs.</p>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStopping.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStoppingType","title":"<code>HyperbandStoppingType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Hyperband Stopping Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class HyperbandStoppingType(LowercaseStrEnum):\n    \"\"\"Hyperband Stopping Type.\"\"\"\n\n    HYPERBAND = auto()\n    \"\"\"Hyperband algorithm.\n\n    Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping\n    method to efficiently tune hyperparameters.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.HyperbandStoppingType.HYPERBAND","title":"<code>HYPERBAND = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hyperband algorithm.</p> <p>Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping method to efficiently tune hyperparameters.</p>"},{"location":"reference/#sparse_autoencoder.Hyperparameters","title":"<code>Hyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Parameters</code></p> <p>Sweep Hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass Hyperparameters(Parameters):\n    \"\"\"Sweep Hyperparameters.\"\"\"\n\n    # Required parameters\n    source_data: SourceDataHyperparameters\n\n    source_model: SourceModelHyperparameters\n\n    # Optional parameters\n    activation_resampler: ActivationResamplerHyperparameters = field(\n        default=ActivationResamplerHyperparameters()\n    )\n\n    autoencoder: AutoencoderHyperparameters = field(default=AutoencoderHyperparameters())\n\n    loss: LossHyperparameters = field(default=LossHyperparameters())\n\n    optimizer: OptimizerHyperparameters = field(default=OptimizerHyperparameters())\n\n    pipeline: PipelineHyperparameters = field(default=PipelineHyperparameters())\n\n    random_seed: Parameter[int] = field(default=Parameter(49))\n    \"\"\"Random seed.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\"\"\"\n        # Check the resample dataset size &lt;= the store size (currently only works if value is used\n        # for both).\n        if (\n            self.activation_resampler.resample_dataset_size.value is not None\n            and self.pipeline.max_store_size.value is not None\n            and self.activation_resampler.resample_dataset_size.value\n            &gt; int(self.pipeline.max_store_size.value)\n        ):\n            error_message = (\n                \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n                f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n                f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n            )\n            raise ValueError(error_message)\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \"\\n    \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}(\\n    {joined_items}\\n)\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.random_seed","title":"<code>random_seed: Parameter[int] = field(default=Parameter(49))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\"\"\"\n    # Check the resample dataset size &lt;= the store size (currently only works if value is used\n    # for both).\n    if (\n        self.activation_resampler.resample_dataset_size.value is not None\n        and self.pipeline.max_store_size.value is not None\n        and self.activation_resampler.resample_dataset_size.value\n        &gt; int(self.pipeline.max_store_size.value)\n    ):\n        error_message = (\n            \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n            f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n            f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Hyperparameters.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \"\\n    \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}(\\n    {joined_items}\\n)\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Impute","title":"<code>Impute</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Metric value to use in bayes search for runs that fail, crash, or are killed.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Impute(LowercaseStrEnum):\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\"\n\n    BEST = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ImputeWhileRunning","title":"<code>ImputeWhileRunning</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Appends a calculated metric even when epochs are in a running state.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ImputeWhileRunning(LowercaseStrEnum):\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    BEST = auto()\n    FALSE = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Kind","title":"<code>Kind</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Kind.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Kind(LowercaseStrEnum):\n    \"\"\"Kind.\"\"\"\n\n    SWEEP = auto()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L0NormMetric","title":"<code>L0NormMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Learned activations L0 norm metric.</p> <p>The L0 norm is the number of non-zero elements in a learned activation vector, averaged over the number of activation vectors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metric = L0NormMetric()\n&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n...     [0., 1., 0.]  # Batch 2 (single component): learned features (1 active neuron)\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor(1.5000)\n</code></pre> <p>With 2 components, the metric will return the average number of active (non-zero) neurons as a 1d tensor.</p> <pre><code>&gt;&gt;&gt; metric = L0NormMetric(num_components=2)\n&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [ # Batch 1\n...         [1., 0., 1.], # Component 1: learned features (2 active neurons)\n...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n...     ],\n...     [ # Batch 2\n...         [0., 1., 0.], # Component 1: learned features (1 active neuron)\n...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n...     ]\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor([1.5000, 2.0000])\n</code></pre> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>class L0NormMetric(Metric):\n    \"\"\"Learned activations L0 norm metric.\n\n    The L0 norm is the number of non-zero elements in a learned activation vector, averaged over the\n    number of activation vectors.\n\n    Examples:\n        &gt;&gt;&gt; metric = L0NormMetric()\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n        ...     [0., 1., 0.]  # Batch 2 (single component): learned features (1 active neuron)\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor(1.5000)\n\n        With 2 components, the metric will return the average number of active (non-zero)\n        neurons as a 1d tensor.\n\n        &gt;&gt;&gt; metric = L0NormMetric(num_components=2)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.], # Component 1: learned features (2 active neurons)\n        ...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 1., 0.], # Component 1: learned features (1 active neuron)\n        ...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([1.5000, 2.0000])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # State\n    active_neurons_count: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(self, num_components: PositiveInt | None = None) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__()\n\n        self.add_state(\n            \"active_neurons_count\",\n            default=torch.zeros(shape_with_optional_dimensions(num_components), dtype=torch.float),\n            dist_reduce_fx=\"sum\",  # Float is needed for dist reduce to work\n        )\n\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        self.num_activation_vectors += learned_activations.shape[0]\n\n        self.active_neurons_count += torch.count_nonzero(learned_activations, dim=-1).sum(\n            dim=0, dtype=torch.int64\n        )\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Compute the metric.\n\n        Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n        \"\"\"\n        return self.active_neurons_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L0NormMetric.__init__","title":"<code>__init__(num_components=None)</code>","text":"<p>Initialize the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>@validate_call\ndef __init__(self, num_components: PositiveInt | None = None) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\n        \"active_neurons_count\",\n        default=torch.zeros(shape_with_optional_dimensions(num_components), dtype=torch.float),\n        dist_reduce_fx=\"sum\",  # Float is needed for dist reduce to work\n    )\n\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L0NormMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> <p>Note that torchmetrics converts shape <code>[0]</code> tensors into scalars (shape <code>0</code>).</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Compute the metric.\n\n    Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n    \"\"\"\n    return self.active_neurons_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L0NormMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    self.num_activation_vectors += learned_activations.shape[0]\n\n    self.active_neurons_count += torch.count_nonzero(learned_activations, dim=-1).sum(\n        dim=0, dtype=torch.int64\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L1AbsoluteLoss","title":"<code>L1AbsoluteLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>L1 (absolute error) loss.</p> <p>L1 loss penalty is the absolute sum of the learned activations, averaged over the number of activation vectors.</p> Example <p>l1_loss = L1AbsoluteLoss() learned_activations = torch.tensor([ ...     [ # Batch 1 ...         [1., 0., 1.] # Component 1: learned features (L1 of 2) ...     ], ...     [ # Batch 2 ...         [0., 1., 0.] # Component 1: learned features (L1 of 1) ...     ] ... ]) l1_loss.forward(learned_activations=learned_activations) tensor(1.5000)</p> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>class L1AbsoluteLoss(Metric):\n    \"\"\"L1 (absolute error) loss.\n\n    L1 loss penalty is the absolute sum of the learned activations, averaged over the number of\n    activation vectors.\n\n    Example:\n        &gt;&gt;&gt; l1_loss = L1AbsoluteLoss()\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.] # Component 1: learned features (L1 of 2)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 1., 0.] # Component 1: learned features (L1 of 1)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; l1_loss.forward(learned_activations=learned_activations)\n        tensor(1.5000)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # Settings\n    _num_components: int\n    _keep_batch_dim: bool\n\n    @property\n    def keep_batch_dim(self) -&gt; bool:\n        \"\"\"Whether to keep the batch dimension in the loss output.\"\"\"\n        return self._keep_batch_dim\n\n    @keep_batch_dim.setter\n    def keep_batch_dim(self, keep_batch_dim: bool) -&gt; None:\n        \"\"\"Set whether to keep the batch dimension in the loss output.\n\n        When setting this we need to change the state to either a list if keeping the batch\n        dimension (so we can accumulate all the losses and concatenate them at the end along this\n        dimension). Alternatively it should be a tensor if not keeping the batch dimension (so we\n        can sum the losses over the batch dimension during update and then take the mean).\n\n        By doing this in a setter we allow changing of this setting after the metric is initialised.\n        \"\"\"\n        self._keep_batch_dim = keep_batch_dim\n        self.reset()  # Reset the metric to update the state\n        if keep_batch_dim and not isinstance(self.absolute_loss, list):\n            self.add_state(\n                \"absolute_loss\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n        elif not isinstance(self.absolute_loss, Tensor):\n            self.add_state(\n                \"absolute_loss\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n\n    # State\n    absolute_loss: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_components: PositiveInt = 1,\n        *,\n        keep_batch_dim: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\n\n        Args:\n            num_components: Number of components.\n            keep_batch_dim: Whether to keep the batch dimension in the loss output.\n        \"\"\"\n        super().__init__()\n        self._num_components = num_components\n        self.keep_batch_dim = keep_batch_dim\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    @staticmethod\n    def calculate_abs_sum(\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Calculate the absolute sum of the learned activations.\n\n        Args:\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n\n        Returns:\n            Absolute sum of the learned activations (keeping the batch and component axis).\n        \"\"\"\n        return torch.abs(learned_activations).sum(dim=-1)\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        If we're keeping the batch dimension, we simply take the absolute sum of the activations\n        (over the features dimension) and then append this tensor to a list. Then during compute we\n        just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n        when resampling neurons (see the neuron resampler for details).\n\n        By contrast if we're averaging over the batch dimension, we sum the activations over the\n        batch dimension during update (on each process), and then divide by the number of activation\n        vectors on compute to get the mean.\n\n        Args:\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        absolute_loss = self.calculate_abs_sum(learned_activations)\n\n        if self.keep_batch_dim:\n            self.absolute_loss.append(absolute_loss)  # type: ignore\n        else:\n            self.absolute_loss += absolute_loss.sum(dim=0)\n            self.num_activation_vectors += learned_activations.shape[0]\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute the metric.\"\"\"\n        return (\n            torch.cat(self.absolute_loss)  # type: ignore\n            if self.keep_batch_dim\n            else self.absolute_loss / self.num_activation_vectors\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L1AbsoluteLoss.keep_batch_dim","title":"<code>keep_batch_dim: bool = keep_batch_dim</code>  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<p>Whether to keep the batch dimension in the loss output.</p>"},{"location":"reference/#sparse_autoencoder.L1AbsoluteLoss.__init__","title":"<code>__init__(num_components=1, *, keep_batch_dim=False)</code>","text":"<p>Initialize the metric.</p> <p>Parameters:</p> Name Type Description Default <code>num_components</code> <code>PositiveInt</code> <p>Number of components.</p> <code>1</code> <code>keep_batch_dim</code> <code>bool</code> <p>Whether to keep the batch dimension in the loss output.</p> <code>False</code> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_components: PositiveInt = 1,\n    *,\n    keep_batch_dim: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the metric.\n\n    Args:\n        num_components: Number of components.\n        keep_batch_dim: Whether to keep the batch dimension in the loss output.\n    \"\"\"\n    super().__init__()\n    self._num_components = num_components\n    self.keep_batch_dim = keep_batch_dim\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L1AbsoluteLoss.calculate_abs_sum","title":"<code>calculate_abs_sum(learned_activations)</code>  <code>staticmethod</code>","text":"<p>Calculate the absolute sum of the learned activations.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Absolute sum of the learned activations (keeping the batch and component axis).</p> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>@staticmethod\ndef calculate_abs_sum(\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Calculate the absolute sum of the learned activations.\n\n    Args:\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n\n    Returns:\n        Absolute sum of the learned activations (keeping the batch and component axis).\n    \"\"\"\n    return torch.abs(learned_activations).sum(dim=-1)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L1AbsoluteLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\"\"\"\n    return (\n        torch.cat(self.absolute_loss)  # type: ignore\n        if self.keep_batch_dim\n        else self.absolute_loss / self.num_activation_vectors\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L1AbsoluteLoss.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>If we're keeping the batch dimension, we simply take the absolute sum of the activations (over the features dimension) and then append this tensor to a list. Then during compute we just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item when resampling neurons (see the neuron resampler for details).</p> <p>By contrast if we're averaging over the batch dimension, we sum the activations over the batch dimension during update (on each process), and then divide by the number of activation vectors on compute to get the mean.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If we're keeping the batch dimension, we simply take the absolute sum of the activations\n    (over the features dimension) and then append this tensor to a list. Then during compute we\n    just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n    when resampling neurons (see the neuron resampler for details).\n\n    By contrast if we're averaging over the batch dimension, we sum the activations over the\n    batch dimension during update (on each process), and then divide by the number of activation\n    vectors on compute to get the mean.\n\n    Args:\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    absolute_loss = self.calculate_abs_sum(learned_activations)\n\n    if self.keep_batch_dim:\n        self.absolute_loss.append(absolute_loss)  # type: ignore\n    else:\n        self.absolute_loss += absolute_loss.sum(dim=0)\n        self.num_activation_vectors += learned_activations.shape[0]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss","title":"<code>L2ReconstructionLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>L2 Reconstruction loss (MSE).</p> <p>L2 reconstruction loss is calculated as the sum squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with L2 may achieve the same loss for both polysemantic and monosemantic representations of true features.</p> Example <p>import torch loss = L2ReconstructionLoss(num_components=1) source_activations = torch.tensor([ ...     [ # Batch 1 ...         [4., 2.] # Component 1 ...     ], ...     [ # Batch 2 ...         [2., 0.] # Component 1 ...     ] ... ]) decoded_activations = torch.tensor([ ...     [ # Batch 1 ...         [2., 0.] # Component 1 (MSE of 4) ...     ], ...     [ # Batch 2 ...         [0., 0.] # Component 1 (MSE of 2) ...     ] ... ]) loss.forward( ...     decoded_activations=decoded_activations, source_activations=source_activations ... ) tensor(3.)</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>class L2ReconstructionLoss(Metric):\n    \"\"\"L2 Reconstruction loss (MSE).\n\n    L2 reconstruction loss is calculated as the sum squared error between each each input vector\n    and it's corresponding decoded vector. The original paper found that models trained with some\n    loss functions such as cross-entropy loss generally prefer to represent features\n    polysemantically, whereas models trained with L2 may achieve the same loss for both\n    polysemantic and monosemantic representations of true features.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; loss = L2ReconstructionLoss(num_components=1)\n        &gt;&gt;&gt; source_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [4., 2.] # Component 1\n        ...     ],\n        ...     [ # Batch 2\n        ...         [2., 0.] # Component 1\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; decoded_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [2., 0.] # Component 1 (MSE of 4)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 0.] # Component 1 (MSE of 2)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; loss.forward(\n        ...     decoded_activations=decoded_activations, source_activations=source_activations\n        ... )\n        tensor(3.)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    higher_is_better = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # Settings\n    _num_components: int\n    _keep_batch_dim: bool\n\n    @property\n    def keep_batch_dim(self) -&gt; bool:\n        \"\"\"Whether to keep the batch dimension in the loss output.\"\"\"\n        return self._keep_batch_dim\n\n    @keep_batch_dim.setter\n    def keep_batch_dim(self, keep_batch_dim: bool) -&gt; None:\n        \"\"\"Set whether to keep the batch dimension in the loss output.\n\n        When setting this we need to change the state to either a list if keeping the batch\n        dimension (so we can accumulate all the losses and concatenate them at the end along this\n        dimension). Alternatively it should be a tensor if not keeping the batch dimension (so we\n        can sum the losses over the batch dimension during update and then take the mean).\n\n        By doing this in a setter we allow changing of this setting after the metric is initialised.\n        \"\"\"\n        self._keep_batch_dim = keep_batch_dim\n        self.reset()  # Reset the metric to update the state\n        if keep_batch_dim and not isinstance(self.mse, list):\n            self.add_state(\n                \"mse\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n        elif not isinstance(self.mse, Tensor):\n            self.add_state(\n                \"mse\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n\n    # State\n    mse: Float[Tensor, Axis.COMPONENT_OPTIONAL] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_components: PositiveInt = 1,\n        *,\n        keep_batch_dim: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the L2 reconstruction loss.\"\"\"\n        super().__init__()\n        self._num_components = num_components\n        self.keep_batch_dim = keep_batch_dim\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    @staticmethod\n    def calculate_mse(\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Calculate the MSE.\"\"\"\n        return (decoded_activations - source_activations).pow(2).mean(dim=-1)\n\n    def update(\n        self,\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        If we're keeping the batch dimension, we simply take the mse of the activations\n        (over the features dimension) and then append this tensor to a list. Then during compute we\n        just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n        when resampling neurons (see the neuron resampler for details).\n\n        By contrast if we're averaging over the batch dimension, we sum the activations over the\n        batch dimension during update (on each process), and then divide by the number of activation\n        vectors on compute to get the mean.\n\n        Args:\n            decoded_activations: The decoded activations from the autoencoder.\n            source_activations: The source activations from the autoencoder.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        mse = self.calculate_mse(decoded_activations, source_activations)\n\n        if self.keep_batch_dim:\n            self.mse.append(mse)  # type: ignore\n        else:\n            self.mse += mse.sum(dim=0)\n            self.num_activation_vectors += source_activations.shape[0]\n\n    def compute(self) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n        \"\"\"Compute the metric.\"\"\"\n        return (\n            torch.cat(self.mse)  # type: ignore\n            if self.keep_batch_dim\n            else self.mse / self.num_activation_vectors\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.keep_batch_dim","title":"<code>keep_batch_dim: bool = keep_batch_dim</code>  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<p>Whether to keep the batch dimension in the loss output.</p>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.__init__","title":"<code>__init__(num_components=1, *, keep_batch_dim=False)</code>","text":"<p>Initialise the L2 reconstruction loss.</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_components: PositiveInt = 1,\n    *,\n    keep_batch_dim: bool = False,\n) -&gt; None:\n    \"\"\"Initialise the L2 reconstruction loss.\"\"\"\n    super().__init__()\n    self._num_components = num_components\n    self.keep_batch_dim = keep_batch_dim\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.calculate_mse","title":"<code>calculate_mse(decoded_activations, source_activations)</code>  <code>staticmethod</code>","text":"<p>Calculate the MSE.</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>@staticmethod\ndef calculate_mse(\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Calculate the MSE.\"\"\"\n    return (decoded_activations - source_activations).pow(2).mean(dim=-1)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>def compute(self) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n    \"\"\"Compute the metric.\"\"\"\n    return (\n        torch.cat(self.mse)  # type: ignore\n        if self.keep_batch_dim\n        else self.mse / self.num_activation_vectors\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.update","title":"<code>update(decoded_activations, source_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>If we're keeping the batch dimension, we simply take the mse of the activations (over the features dimension) and then append this tensor to a list. Then during compute we just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item when resampling neurons (see the neuron resampler for details).</p> <p>By contrast if we're averaging over the batch dimension, we sum the activations over the batch dimension during update (on each process), and then divide by the number of activation vectors on compute to get the mean.</p> <p>Parameters:</p> Name Type Description Default <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>The decoded activations from the autoencoder.</p> required <code>source_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>The source activations from the autoencoder.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>def update(\n    self,\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If we're keeping the batch dimension, we simply take the mse of the activations\n    (over the features dimension) and then append this tensor to a list. Then during compute we\n    just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n    when resampling neurons (see the neuron resampler for details).\n\n    By contrast if we're averaging over the batch dimension, we sum the activations over the\n    batch dimension during update (on each process), and then divide by the number of activation\n    vectors on compute to get the mean.\n\n    Args:\n        decoded_activations: The decoded activations from the autoencoder.\n        source_activations: The source activations from the autoencoder.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    mse = self.calculate_mse(decoded_activations, source_activations)\n\n    if self.keep_batch_dim:\n        self.mse.append(mse)  # type: ignore\n    else:\n        self.mse += mse.sum(dim=0)\n        self.num_activation_vectors += source_activations.shape[0]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossHyperparameters","title":"<code>LossHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Loss hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass LossHyperparameters(NestedParameter):\n    \"\"\"Loss hyperparameters.\"\"\"\n\n    l1_coefficient: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"L1 Penalty Coefficient.\n\n    The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.\n    The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by\n    using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good\n    starting point for the L1 coefficient is 1e-3.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossHyperparameters.l1_coefficient","title":"<code>l1_coefficient: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>L1 Penalty Coefficient.</p> <p>The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good starting point for the L1 coefficient is 1e-3.</p>"},{"location":"reference/#sparse_autoencoder.Method","title":"<code>Method</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Method.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Method(LowercaseStrEnum):\n    \"\"\"Method.\"\"\"\n\n    BAYES = auto()\n    \"\"\"Bayesian optimization.\n\n    Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach\n    for finding the optimal set of parameters.\n    \"\"\"\n\n    CUSTOM = auto()\n    \"\"\"Custom method.\n\n    Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the\n    sweep process.\n    \"\"\"\n\n    GRID = auto()\n    \"\"\"Grid search.\n\n    Utilizes a grid search approach for hyperparameter tuning, systematically working through\n    multiple combinations of parameter values.\n    \"\"\"\n\n    RANDOM = auto()\n    \"\"\"Random search.\n\n    Implements a random search strategy for hyperparameter tuning, exploring the parameter space\n    randomly.\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Method.BAYES","title":"<code>BAYES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bayesian optimization.</p> <p>Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach for finding the optimal set of parameters.</p>"},{"location":"reference/#sparse_autoencoder.Method.CUSTOM","title":"<code>CUSTOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom method.</p> <p>Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the sweep process.</p>"},{"location":"reference/#sparse_autoencoder.Method.GRID","title":"<code>GRID = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Grid search.</p> <p>Utilizes a grid search approach for hyperparameter tuning, systematically working through multiple combinations of parameter values.</p>"},{"location":"reference/#sparse_autoencoder.Method.RANDOM","title":"<code>RANDOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random search.</p> <p>Implements a random search strategy for hyperparameter tuning, exploring the parameter space randomly.</p>"},{"location":"reference/#sparse_autoencoder.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>Metric to optimize.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Metric:\n    \"\"\"Metric to optimize.\"\"\"\n\n    name: str\n    \"\"\"Name of metric.\"\"\"\n\n    goal: Goal | None = Goal.MINIMIZE\n\n    impute: Impute | None = None\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\"\n\n    imputewhilerunning: ImputeWhileRunning | None = None\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    target: float | None = None\n    \"\"\"The sweep will finish once any run achieves this value.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Metric.impute","title":"<code>impute: Impute | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metric value to use in bayes search for runs that fail, crash, or are killed</p>"},{"location":"reference/#sparse_autoencoder.Metric.imputewhilerunning","title":"<code>imputewhilerunning: ImputeWhileRunning | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Appends a calculated metric even when epochs are in a running state.</p>"},{"location":"reference/#sparse_autoencoder.Metric.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of metric.</p>"},{"location":"reference/#sparse_autoencoder.Metric.target","title":"<code>target: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will finish once any run achieves this value.</p>"},{"location":"reference/#sparse_autoencoder.Metric.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Metric.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NestedParameter","title":"<code>NestedParameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Nested Parameter.</p> Example <p>from dataclasses import field @dataclass(frozen=True) ... class MyNestedParameter(NestedParameter): ...     a: int = field(default=Parameter(1)) ...     b: int = field(default=Parameter(2)) MyNestedParameter().to_dict() {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass NestedParameter(ABC):  # noqa: B024 (abstract so that we can check against its type)\n    \"\"\"Nested Parameter.\n\n    Example:\n        &gt;&gt;&gt; from dataclasses import field\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class MyNestedParameter(NestedParameter):\n        ...     a: int = field(default=Parameter(1))\n        ...     b: int = field(default=Parameter(2))\n        &gt;&gt;&gt; MyNestedParameter().to_dict()\n        {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}\n    \"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\"\"\"\n\n        def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n            \"\"\"Return dict without None values.\n\n            Args:\n                obj: The object to convert to a dict.\n\n            Returns:\n                The dict representation of the object.\n            \"\"\"\n            dict_none_removed = {}\n            dict_with_none = dict(obj)\n            for key, value in dict_with_none.items():\n                if value is not None:\n                    dict_none_removed[key] = value\n            return dict_none_removed\n\n        return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n\n    def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n        \"\"\"Return dict representation of this object.\"\"\"\n        return self.to_dict()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NestedParameter.__dict__","title":"<code>__dict__()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n    \"\"\"Return dict representation of this object.\"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NestedParameter.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\"\"\"\n\n    def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n        \"\"\"Return dict without None values.\n\n        Args:\n            obj: The object to convert to a dict.\n\n        Returns:\n            The dict representation of the object.\n        \"\"\"\n        dict_none_removed = {}\n        dict_with_none = dict(obj)\n        for key, value in dict_with_none.items():\n            if value is not None:\n                dict_none_removed[key] = value\n        return dict_none_removed\n\n    return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronActivityMetric","title":"<code>NeuronActivityMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Neuron activity metric.</p> Example <p>With a single component and a horizon of 2 activations, the metric will return nothing after the first activation is added and then computed, and then return the number of dead neurons after the second activation is added (with update). The breakdown by component isn't shown here as there is just one component.</p> <p>metric = NeuronActivityMetric(num_learned_features=3) learned_activations = torch.tensor([ ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons) ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron) ... ]) metric.forward(learned_activations) tensor(1)</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>class NeuronActivityMetric(Metric):\n    \"\"\"Neuron activity metric.\n\n    Example:\n        With a single component and a horizon of 2 activations, the metric will return nothing\n        after the first activation is added and then computed, and then return the number of dead\n        neurons after the second activation is added (with update). The breakdown by component isn't\n        shown here as there is just one component.\n\n        &gt;&gt;&gt; metric = NeuronActivityMetric(num_learned_features=3)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n        ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron)\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor(1)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = True\n    plot_lower_bound: float | None = 0.0\n\n    # Metric settings\n    _threshold_is_dead_portion_fires: NonNegativeFloat\n\n    # State\n    neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_learned_features: PositiveInt,\n        num_components: PositiveInt | None = None,\n        threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n    ) -&gt; None:\n        \"\"\"Initialise the metric.\n\n        Args:\n            num_learned_features: Number of learned features.\n            num_components: Number of components.\n            threshold_is_dead_portion_fires: Thresholds for counting a neuron as dead (portion of\n                activation vectors that it fires for must be less than or equal to this number).\n                Commonly used values are 0.0, 1e-5 and 1e-6.\n        \"\"\"\n        super().__init__()\n        self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n\n        self.add_state(\n            \"neuron_fired_count\",\n            default=torch.zeros(\n                shape_with_optional_dimensions(num_components, num_learned_features),\n                dtype=torch.float,  # Float is needed for dist reduce to work\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        # Increment the counter of activations seen since the last compute step\n        self.num_activation_vectors += learned_activations.shape[0]\n\n        # Count the number of active neurons in the batch\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n\n        self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n\n    def compute(self) -&gt; Int64[Tensor, Axis.COMPONENT_OPTIONAL]:\n        \"\"\"Compute the metric.\n\n        Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n        \"\"\"\n        threshold_activations: Float[Tensor, Axis.SINGLE_ITEM] = (\n            self._threshold_is_dead_portion_fires * self.num_activation_vectors\n        )\n\n        return torch.sum(\n            self.neuron_fired_count &lt;= threshold_activations, dim=-1, dtype=torch.int64\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronActivityMetric.__init__","title":"<code>__init__(num_learned_features, num_components=None, threshold_is_dead_portion_fires=0.0)</code>","text":"<p>Initialise the metric.</p> <p>Parameters:</p> Name Type Description Default <code>num_learned_features</code> <code>PositiveInt</code> <p>Number of learned features.</p> required <code>num_components</code> <code>PositiveInt | None</code> <p>Number of components.</p> <code>None</code> <code>threshold_is_dead_portion_fires</code> <code>Annotated[float, Field(strict=True, ge=0, le=1)]</code> <p>Thresholds for counting a neuron as dead (portion of activation vectors that it fires for must be less than or equal to this number). Commonly used values are 0.0, 1e-5 and 1e-6.</p> <code>0.0</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_learned_features: PositiveInt,\n    num_components: PositiveInt | None = None,\n    threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n) -&gt; None:\n    \"\"\"Initialise the metric.\n\n    Args:\n        num_learned_features: Number of learned features.\n        num_components: Number of components.\n        threshold_is_dead_portion_fires: Thresholds for counting a neuron as dead (portion of\n            activation vectors that it fires for must be less than or equal to this number).\n            Commonly used values are 0.0, 1e-5 and 1e-6.\n    \"\"\"\n    super().__init__()\n    self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n\n    self.add_state(\n        \"neuron_fired_count\",\n        default=torch.zeros(\n            shape_with_optional_dimensions(num_components, num_learned_features),\n            dtype=torch.float,  # Float is needed for dist reduce to work\n        ),\n        dist_reduce_fx=\"sum\",\n    )\n\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronActivityMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> <p>Note that torchmetrics converts shape <code>[0]</code> tensors into scalars (shape <code>0</code>).</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>def compute(self) -&gt; Int64[Tensor, Axis.COMPONENT_OPTIONAL]:\n    \"\"\"Compute the metric.\n\n    Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n    \"\"\"\n    threshold_activations: Float[Tensor, Axis.SINGLE_ITEM] = (\n        self._threshold_is_dead_portion_fires * self.num_activation_vectors\n    )\n\n    return torch.sum(\n        self.neuron_fired_count &lt;= threshold_activations, dim=-1, dtype=torch.int64\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronActivityMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    # Increment the counter of activations seen since the last compute step\n    self.num_activation_vectors += learned_activations.shape[0]\n\n    # Count the number of active neurons in the batch\n    neuron_has_fired: Bool[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.gt(learned_activations, 0)\n\n    self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronFiredCountMetric","title":"<code>NeuronFiredCountMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Neuron activity metric.</p> Example <p>metric = NeuronFiredCountMetric(num_learned_features=3) learned_activations = torch.tensor([ ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons) ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron) ... ]) metric.forward(learned_activations) tensor([1, 0, 1])</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>class NeuronFiredCountMetric(Metric):\n    \"\"\"Neuron activity metric.\n\n    Example:\n        &gt;&gt;&gt; metric = NeuronFiredCountMetric(num_learned_features=3)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n        ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron)\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([1, 0, 1])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    full_state_update: bool | None = True\n    plot_lower_bound: float | None = 0.0\n\n    # State\n    neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n\n    @validate_call\n    def __init__(\n        self,\n        num_learned_features: PositiveInt,\n        num_components: PositiveInt | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the metric.\n\n        Args:\n            num_learned_features: Number of learned features.\n            num_components: Number of components.\n        \"\"\"\n        super().__init__()\n        self.add_state(\n            \"neuron_fired_count\",\n            default=torch.zeros(\n                shape_with_optional_dimensions(num_components, num_learned_features),\n                dtype=torch.float,  # Float is needed for dist reduce to work\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n\n        self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n\n    def compute(self) -&gt; Int[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Compute the metric.\"\"\"\n        return self.neuron_fired_count.to(dtype=torch.int64)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronFiredCountMetric.__init__","title":"<code>__init__(num_learned_features, num_components=None)</code>","text":"<p>Initialise the metric.</p> <p>Parameters:</p> Name Type Description Default <code>num_learned_features</code> <code>PositiveInt</code> <p>Number of learned features.</p> required <code>num_components</code> <code>PositiveInt | None</code> <p>Number of components.</p> <code>None</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_learned_features: PositiveInt,\n    num_components: PositiveInt | None = None,\n) -&gt; None:\n    \"\"\"Initialise the metric.\n\n    Args:\n        num_learned_features: Number of learned features.\n        num_components: Number of components.\n    \"\"\"\n    super().__init__()\n    self.add_state(\n        \"neuron_fired_count\",\n        default=torch.zeros(\n            shape_with_optional_dimensions(num_components, num_learned_features),\n            dtype=torch.float,  # Float is needed for dist reduce to work\n        ),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronFiredCountMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>def compute(self) -&gt; Int[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Compute the metric.\"\"\"\n    return self.neuron_fired_count.to(dtype=torch.int64)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronFiredCountMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    neuron_has_fired: Bool[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.gt(learned_activations, 0)\n\n    self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters","title":"<code>OptimizerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Optimizer hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass OptimizerHyperparameters(NestedParameter):\n    \"\"\"Optimizer hyperparameters.\"\"\"\n\n    lr: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"Learning rate.\n\n    A good starting point for the learning rate is 1e-3, but this is one of the key parameters so\n    you should probably tune it.\n    \"\"\"\n\n    adam_beta_1: Parameter[float] = field(default=Parameter(0.9))\n    \"\"\"Adam Beta 1.\n\n    The exponential decay rate for the first moment estimates (mean) of the gradient.\n    \"\"\"\n\n    adam_beta_2: Parameter[float] = field(default=Parameter(0.99))\n    \"\"\"Adam Beta 2.\n\n    The exponential decay rate for the second moment estimates (variance) of the gradient.\n    \"\"\"\n\n    adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Adam Weight Decay.\n\n    Weight decay (L2 penalty).\n    \"\"\"\n\n    amsgrad: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"AMSGrad.\n\n    Whether to use the AMSGrad variant of this algorithm from the paper [On the Convergence of Adam\n    and Beyond](https://arxiv.org/abs/1904.09237).\n    \"\"\"\n\n    fused: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Fused.\n\n    Whether to use a fused implementation of the optimizer (may be faster on CUDA).\n    \"\"\"\n\n    lr_scheduler: Parameter[Literal[\"reduce_on_plateau\", \"cosine_annealing\"]] | None = field(\n        default=None\n    )\n    \"\"\"Learning rate scheduler.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.adam_beta_1","title":"<code>adam_beta_1: Parameter[float] = field(default=Parameter(0.9))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 1.</p> <p>The exponential decay rate for the first moment estimates (mean) of the gradient.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.adam_beta_2","title":"<code>adam_beta_2: Parameter[float] = field(default=Parameter(0.99))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 2.</p> <p>The exponential decay rate for the second moment estimates (variance) of the gradient.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.adam_weight_decay","title":"<code>adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Weight Decay.</p> <p>Weight decay (L2 penalty).</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.amsgrad","title":"<code>amsgrad: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AMSGrad.</p> <p>Whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.fused","title":"<code>fused: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fused.</p> <p>Whether to use a fused implementation of the optimizer (may be faster on CUDA).</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.lr","title":"<code>lr: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate.</p> <p>A good starting point for the learning rate is 1e-3, but this is one of the key parameters so you should probably tune it.</p>"},{"location":"reference/#sparse_autoencoder.OptimizerHyperparameters.lr_scheduler","title":"<code>lr_scheduler: Parameter[Literal['reduce_on_plateau', 'cosine_annealing']] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate scheduler.</p>"},{"location":"reference/#sparse_autoencoder.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[ParamType]</code></p> <p>Sweep Parameter.</p> <p>https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Parameter(Generic[ParamType]):\n    \"\"\"Sweep Parameter.\n\n    https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters\n    \"\"\"\n\n    value: ParamType | None = None\n    \"\"\"Single value.\n\n    Specifies the single valid value for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    max: ParamType | None = None\n    \"\"\"Maximum value.\"\"\"\n\n    min: ParamType | None = None\n    \"\"\"Minimum value.\"\"\"\n\n    distribution: Distribution | None = None\n    \"\"\"Distribution\n\n    If not specified, will default to categorical if values is set, to int_uniform if max and min\n    are set to integers, to uniform if max and min are set to floats, or to constant if value is\n    set.\n    \"\"\"\n\n    q: float | None = None\n    \"\"\"Quantization parameter.\n\n    Quantization step size for quantized hyperparameters.\n    \"\"\"\n\n    values: list[ParamType] | None = None\n    \"\"\"Discrete values.\n\n    Specifies all valid values for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    probabilities: list[float] | None = None\n    \"\"\"Probability of each value\"\"\"\n\n    mu: float | None = None\n    \"\"\"Mean for normal or lognormal distributions\"\"\"\n\n    sigma: float | None = None\n    \"\"\"Std Dev for normal or lognormal distributions\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Parameter.distribution","title":"<code>distribution: Distribution | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Distribution</p> <p>If not specified, will default to categorical if values is set, to int_uniform if max and min are set to integers, to uniform if max and min are set to floats, or to constant if value is set.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.max","title":"<code>max: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum value.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.min","title":"<code>min: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.mu","title":"<code>mu: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean for normal or lognormal distributions</p>"},{"location":"reference/#sparse_autoencoder.Parameter.probabilities","title":"<code>probabilities: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Probability of each value</p>"},{"location":"reference/#sparse_autoencoder.Parameter.q","title":"<code>q: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantization parameter.</p> <p>Quantization step size for quantized hyperparameters.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.sigma","title":"<code>sigma: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Std Dev for normal or lognormal distributions</p>"},{"location":"reference/#sparse_autoencoder.Parameter.value","title":"<code>value: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single value.</p> <p>Specifies the single valid value for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.values","title":"<code>values: list[ParamType] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Discrete values.</p> <p>Specifies all valid values for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/#sparse_autoencoder.Parameter.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Parameter.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p> <p>Includes all the key functionality to train a sparse autoencoder, with a specific set of     hyperparameters.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations.\n\n    Includes all the key functionality to train a sparse autoencoder, with a specific set of\n        hyperparameters.\n    \"\"\"\n\n    autoencoder: LitSparseAutoencoder\n    \"\"\"Sparse autoencoder to train.\"\"\"\n\n    n_input_features: int\n    \"\"\"Number of input features in the sparse autoencoder.\"\"\"\n\n    n_learned_features: int\n    \"\"\"Number of learned features in the sparse autoencoder.\"\"\"\n\n    cache_names: list[str]\n    \"\"\"Names of the cache hook points to use in the source model.\"\"\"\n\n    layer: int\n    \"\"\"Layer to stope the source model at (if we don't need activations after this layer).\"\"\"\n\n    log_frequency: int\n    \"\"\"Frequency at which to log metrics (in steps).\"\"\"\n\n    progress_bar: tqdm | None\n    \"\"\"Progress bar for the pipeline.\"\"\"\n\n    source_data: Iterator[TorchTokenizedPrompts]\n    \"\"\"Iterable over the source data.\"\"\"\n\n    source_dataset: SourceDataset\n    \"\"\"Source dataset to generate activation data from (tokenized prompts).\"\"\"\n\n    source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]\n    \"\"\"Source model to get activations from.\"\"\"\n\n    total_activations_trained_on: int = 0\n    \"\"\"Total number of activations trained on state.\"\"\"\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of source model components the SAE is trained on.\"\"\"\n        return len(self.cache_names)\n\n    @final\n    @validate_call(config={\"arbitrary_types_allowed\": True})\n    def __init__(\n        self,\n        autoencoder: LitSparseAutoencoder,\n        cache_names: list[str],\n        layer: NonNegativeInt,\n        source_dataset: SourceDataset,\n        source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer],\n        n_input_features: int,\n        n_learned_features: int,\n        run_name: str = \"sparse_autoencoder\",\n        checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n        log_frequency: PositiveInt = 100,\n        num_workers_data_loading: NonNegativeInt = 0,\n        source_data_batch_size: PositiveInt = 12,\n    ) -&gt; None:\n        \"\"\"Initialize the pipeline.\n\n        Args:\n            autoencoder: Sparse autoencoder to train.\n            cache_names: Names of the cache hook points to use in the source model.\n            layer: Layer to stope the source model at (if we don't need activations after this\n                layer).\n            source_dataset: Source dataset to get data from.\n            source_model: Source model to get activations from.\n            n_input_features: Number of input features in the sparse autoencoder.\n            n_learned_features: Number of learned features in the sparse autoencoder.\n            run_name: Name of the run for saving checkpoints.\n            checkpoint_directory: Directory to save checkpoints to.\n            log_frequency: Frequency at which to log metrics (in steps)\n            num_workers_data_loading: Number of CPU workers for the dataloader.\n            source_data_batch_size: Batch size for the source data.\n        \"\"\"\n        self.autoencoder = autoencoder\n        self.cache_names = cache_names\n        self.checkpoint_directory = checkpoint_directory\n        self.layer = layer\n        self.log_frequency = log_frequency\n        self.run_name = run_name\n        self.source_data_batch_size = source_data_batch_size\n        self.source_dataset = source_dataset\n        self.source_model = source_model\n        self.n_input_features = n_input_features\n        self.n_learned_features = n_learned_features\n\n        # Add validate metric\n        self.reconstruction_score = ClasswiseWrapperWithMean(\n            ReconstructionScoreMetric(len(cache_names)),\n            component_names=cache_names,\n            prefix=\"validation/reconstruction_score\",\n        )\n        self.reconstruction_score.to(get_model_device(self.autoencoder))\n\n        # Create a stateful iterator\n        source_dataloader = source_dataset.get_dataloader(\n            source_data_batch_size, num_workers=num_workers_data_loading\n        )\n        self.source_data = iter(source_dataloader)\n\n    @validate_call\n    def generate_activations(self, store_size: PositiveInt) -&gt; TensorActivationStore:\n        \"\"\"Generate activations.\n\n        Args:\n            store_size: Number of activations to generate.\n\n        Returns:\n            Activation store for the train section.\n\n        Raises:\n            ValueError: If the store size is not divisible by the batch size.\n        \"\"\"\n        # Check the store size is divisible by the batch size\n        if store_size % (self.source_data_batch_size * self.source_dataset.context_size) != 0:\n            error_message = (\n                f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n                f\"got {store_size}\"\n            )\n            raise ValueError(error_message)\n\n        # Setup the store\n        source_model_device = get_model_device(self.source_model)\n        store = TensorActivationStore(\n            store_size, self.n_input_features, n_components=self.n_components\n        )\n\n        # Add the hook to the model (will automatically store the activations every time the model\n        # runs)\n        self.source_model.remove_all_hook_fns()\n        for component_idx, cache_name in enumerate(self.cache_names):\n            hook = partial(store_activations_hook, store=store, component_idx=component_idx)\n            self.source_model.add_hook(cache_name, hook)\n\n        # Loop through the dataloader until the store reaches the desired size\n        with torch.no_grad():\n            while len(store) &lt; store_size:\n                batch = next(self.source_data)\n                input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                    \"input_ids\"\n                ].to(source_model_device)\n                self.source_model.forward(\n                    input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n                )  # type: ignore (TLens is typed incorrectly)\n\n        self.source_model.remove_all_hook_fns()\n        store.shuffle()\n\n        return store\n\n    def train_autoencoder(\n        self,\n        activation_store: TensorActivationStore,\n        train_batch_size: PositiveInt,\n    ) -&gt; None:\n        \"\"\"Train the sparse autoencoder.\n\n        Args:\n            activation_store: Activation store from the generate section.\n            train_batch_size: Train batch size.\n\n        Returns:\n            Number of times each neuron fired, for each component.\n        \"\"\"\n        activations_dataloader = DataLoader(\n            activation_store, batch_size=train_batch_size, num_workers=4, persistent_workers=False\n        )\n\n        # Setup the trainer with no console logging\n        logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n        trainer = Trainer(\n            logger=WandbLogger() if wandb.run is not None else None,\n            max_epochs=1,\n            enable_progress_bar=False,\n            enable_model_summary=False,\n            enable_checkpointing=False,\n            precision=\"16-mixed\",\n        )\n        trainer.fit(self.autoencoder, activations_dataloader)\n\n    @validate_call\n    def validate_sae(self, validation_n_activations: PositiveInt) -&gt; None:\n        \"\"\"Get validation metrics.\n\n        Args:\n            validation_n_activations: Number of activations to use for validation.\n        \"\"\"\n        n_batches = validation_n_activations // (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n        source_model_device = get_model_device(self.source_model)\n\n        # Create the metric data stores\n        losses: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n            self.n_components, device=source_model_device\n        )\n        losses_with_reconstruction: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n            self.n_components, device=source_model_device\n        )\n        losses_with_zero_ablation: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n            self.n_components, device=source_model_device\n        )\n\n        sae_model = self.autoencoder.sparse_autoencoder.clone()\n        sae_model.to(source_model_device)\n\n        for component_idx, cache_name in enumerate(self.cache_names):\n            for _batch_idx in range(n_batches):\n                batch = next(self.source_data)\n\n                input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                    \"input_ids\"\n                ].to(source_model_device)\n\n                # Run a forward pass with and without the replaced activations\n                self.source_model.remove_all_hook_fns()\n                replacement_hook = partial(\n                    replace_activations_hook,\n                    sparse_autoencoder=sae_model,\n                    component_idx=component_idx,\n                    n_components=self.n_components,\n                )\n\n                with torch.no_grad():\n                    loss: Float[\n                        Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)\n                    ] = self.source_model.forward(input_ids, return_type=\"loss\")\n                    loss_with_reconstruction = self.source_model.run_with_hooks(\n                        input_ids,\n                        return_type=\"loss\",\n                        fwd_hooks=[\n                            (\n                                cache_name,\n                                replacement_hook,\n                            )\n                        ],\n                    )\n                    loss_with_zero_ablation = self.source_model.run_with_hooks(\n                        input_ids, return_type=\"loss\", fwd_hooks=[(cache_name, zero_ablate_hook)]\n                    )\n\n                    self.reconstruction_score.update(\n                        source_model_loss=loss,\n                        source_model_loss_with_reconstruction=loss_with_reconstruction,\n                        source_model_loss_with_zero_ablation=loss_with_zero_ablation,\n                        component_idx=component_idx,\n                    )\n\n                    losses[component_idx] += loss.sum()\n                    losses_with_reconstruction[component_idx] += loss_with_reconstruction.sum()\n                    losses_with_zero_ablation[component_idx] += loss_with_zero_ablation.sum()\n\n        # Log\n        if wandb.run is not None:\n            log = {\n                f\"validation/source_model_losses/{c}\": val\n                for c, val in zip(self.cache_names, losses / n_batches)\n            }\n            log.update(\n                {\n                    f\"validation/source_model_losses_with_reconstruction/{c}\": val\n                    for c, val in zip(self.cache_names, losses_with_reconstruction / n_batches)\n                }\n            )\n            log.update(\n                {\n                    f\"validation/source_model_losses_with_zero_ablation/{c}\": val\n                    for c, val in zip(self.cache_names, losses_with_zero_ablation / n_batches)\n                }\n            )\n            log.update(self.reconstruction_score.compute())\n            wandb.log(log)\n\n    @final\n    def save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n        \"\"\"Save the model as a checkpoint.\n\n        Args:\n            is_final: Whether this is the final checkpoint.\n\n        Returns:\n            Path to the saved checkpoint.\n        \"\"\"\n        name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n\n        # Wandb\n        if wandb.run is not None:\n            self.autoencoder.sparse_autoencoder.save_to_wandb(name)\n\n        # Local\n        local_path = self.checkpoint_directory / f\"{name}.pt\"\n        self.autoencoder.sparse_autoencoder.save(local_path)\n        return local_path\n\n    @validate_call\n    def run_pipeline(\n        self,\n        train_batch_size: PositiveInt,\n        max_store_size: PositiveInt,\n        max_activations: PositiveInt,\n        validation_n_activations: PositiveInt = 1024,\n        validate_frequency: PositiveInt | None = None,\n        checkpoint_frequency: PositiveInt | None = None,\n    ) -&gt; None:\n        \"\"\"Run the full training pipeline.\n\n        Args:\n            train_batch_size: Train batch size.\n            max_store_size: Maximum size of the activation store.\n            max_activations: Maximum total number of activations to train on (the original paper\n                used 8bn, although others have had success with 100m+).\n            validation_n_activations: Number of activations to use for validation.\n            validate_frequency: Frequency at which to get validation metrics.\n            checkpoint_frequency: Frequency at which to save a checkpoint.\n        \"\"\"\n        last_validated: int = 0\n        last_checkpoint: int = 0\n\n        self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n        # Get the store size\n        store_size: int = max_store_size - max_store_size % (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n\n        # Get the loss fn\n        loss_fn = self.autoencoder.loss_fn.clone()\n        loss_fn.keep_batch_dim = True\n\n        with tqdm(\n            desc=\"Activations trained on\",\n            total=max_activations,\n        ) as progress_bar:\n            for _ in range(0, max_activations, store_size):\n                # Generate\n                progress_bar.set_postfix({\"stage\": \"generate\"})\n                activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n                # Update the counters\n                n_activation_vectors_in_store = len(activation_store)\n                last_validated += n_activation_vectors_in_store\n                last_checkpoint += n_activation_vectors_in_store\n\n                # Train &amp; resample if needed\n                progress_bar.set_postfix({\"stage\": \"train\"})\n                self.train_autoencoder(activation_store, train_batch_size=train_batch_size)\n\n                # Get validation metrics (if needed)\n                progress_bar.set_postfix({\"stage\": \"validate\"})\n                if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                    self.validate_sae(validation_n_activations)\n                    last_validated = 0\n\n                # Checkpoint (if needed)\n                progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n                if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                    last_checkpoint = 0\n                    self.save_checkpoint()\n\n                # Update the progress bar\n                progress_bar.update(store_size)\n\n        # Save the final checkpoint\n        self.save_checkpoint(is_final=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.autoencoder","title":"<code>autoencoder: LitSparseAutoencoder = autoencoder</code>  <code>instance-attribute</code>","text":"<p>Sparse autoencoder to train.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.cache_names","title":"<code>cache_names: list[str] = cache_names</code>  <code>instance-attribute</code>","text":"<p>Names of the cache hook points to use in the source model.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.layer","title":"<code>layer: int = layer</code>  <code>instance-attribute</code>","text":"<p>Layer to stope the source model at (if we don't need activations after this layer).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.log_frequency","title":"<code>log_frequency: int = log_frequency</code>  <code>instance-attribute</code>","text":"<p>Frequency at which to log metrics (in steps).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of source model components the SAE is trained on.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.n_input_features","title":"<code>n_input_features: int = n_input_features</code>  <code>instance-attribute</code>","text":"<p>Number of input features in the sparse autoencoder.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.n_learned_features","title":"<code>n_learned_features: int = n_learned_features</code>  <code>instance-attribute</code>","text":"<p>Number of learned features in the sparse autoencoder.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.progress_bar","title":"<code>progress_bar: tqdm | None</code>  <code>instance-attribute</code>","text":"<p>Progress bar for the pipeline.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_data","title":"<code>source_data: Iterator[TorchTokenizedPrompts] = iter(source_dataloader)</code>  <code>instance-attribute</code>","text":"<p>Iterable over the source data.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_dataset","title":"<code>source_dataset: SourceDataset = source_dataset</code>  <code>instance-attribute</code>","text":"<p>Source dataset to generate activation data from (tokenized prompts).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_model","title":"<code>source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer] = source_model</code>  <code>instance-attribute</code>","text":"<p>Source model to get activations from.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.total_activations_trained_on","title":"<code>total_activations_trained_on: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of activations trained on state.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.__init__","title":"<code>__init__(autoencoder, cache_names, layer, source_dataset, source_model, n_input_features, n_learned_features, run_name='sparse_autoencoder', checkpoint_directory=DEFAULT_CHECKPOINT_DIRECTORY, log_frequency=100, num_workers_data_loading=0, source_data_batch_size=12)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>autoencoder</code> <code>LitSparseAutoencoder</code> <p>Sparse autoencoder to train.</p> required <code>cache_names</code> <code>list[str]</code> <p>Names of the cache hook points to use in the source model.</p> required <code>layer</code> <code>NonNegativeInt</code> <p>Layer to stope the source model at (if we don't need activations after this layer).</p> required <code>source_dataset</code> <code>SourceDataset</code> <p>Source dataset to get data from.</p> required <code>source_model</code> <code>HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]</code> <p>Source model to get activations from.</p> required <code>n_input_features</code> <code>int</code> <p>Number of input features in the sparse autoencoder.</p> required <code>n_learned_features</code> <code>int</code> <p>Number of learned features in the sparse autoencoder.</p> required <code>run_name</code> <code>str</code> <p>Name of the run for saving checkpoints.</p> <code>'sparse_autoencoder'</code> <code>checkpoint_directory</code> <code>Path</code> <p>Directory to save checkpoints to.</p> <code>DEFAULT_CHECKPOINT_DIRECTORY</code> <code>log_frequency</code> <code>PositiveInt</code> <p>Frequency at which to log metrics (in steps)</p> <code>100</code> <code>num_workers_data_loading</code> <code>NonNegativeInt</code> <p>Number of CPU workers for the dataloader.</p> <code>0</code> <code>source_data_batch_size</code> <code>PositiveInt</code> <p>Batch size for the source data.</p> <code>12</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\n@validate_call(config={\"arbitrary_types_allowed\": True})\ndef __init__(\n    self,\n    autoencoder: LitSparseAutoencoder,\n    cache_names: list[str],\n    layer: NonNegativeInt,\n    source_dataset: SourceDataset,\n    source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer],\n    n_input_features: int,\n    n_learned_features: int,\n    run_name: str = \"sparse_autoencoder\",\n    checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n    log_frequency: PositiveInt = 100,\n    num_workers_data_loading: NonNegativeInt = 0,\n    source_data_batch_size: PositiveInt = 12,\n) -&gt; None:\n    \"\"\"Initialize the pipeline.\n\n    Args:\n        autoencoder: Sparse autoencoder to train.\n        cache_names: Names of the cache hook points to use in the source model.\n        layer: Layer to stope the source model at (if we don't need activations after this\n            layer).\n        source_dataset: Source dataset to get data from.\n        source_model: Source model to get activations from.\n        n_input_features: Number of input features in the sparse autoencoder.\n        n_learned_features: Number of learned features in the sparse autoencoder.\n        run_name: Name of the run for saving checkpoints.\n        checkpoint_directory: Directory to save checkpoints to.\n        log_frequency: Frequency at which to log metrics (in steps)\n        num_workers_data_loading: Number of CPU workers for the dataloader.\n        source_data_batch_size: Batch size for the source data.\n    \"\"\"\n    self.autoencoder = autoencoder\n    self.cache_names = cache_names\n    self.checkpoint_directory = checkpoint_directory\n    self.layer = layer\n    self.log_frequency = log_frequency\n    self.run_name = run_name\n    self.source_data_batch_size = source_data_batch_size\n    self.source_dataset = source_dataset\n    self.source_model = source_model\n    self.n_input_features = n_input_features\n    self.n_learned_features = n_learned_features\n\n    # Add validate metric\n    self.reconstruction_score = ClasswiseWrapperWithMean(\n        ReconstructionScoreMetric(len(cache_names)),\n        component_names=cache_names,\n        prefix=\"validation/reconstruction_score\",\n    )\n    self.reconstruction_score.to(get_model_device(self.autoencoder))\n\n    # Create a stateful iterator\n    source_dataloader = source_dataset.get_dataloader(\n        source_data_batch_size, num_workers=num_workers_data_loading\n    )\n    self.source_data = iter(source_dataloader)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.generate_activations","title":"<code>generate_activations(store_size)</code>","text":"<p>Generate activations.</p> <p>Parameters:</p> Name Type Description Default <code>store_size</code> <code>PositiveInt</code> <p>Number of activations to generate.</p> required <p>Returns:</p> Type Description <code>TensorActivationStore</code> <p>Activation store for the train section.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store size is not divisible by the batch size.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@validate_call\ndef generate_activations(self, store_size: PositiveInt) -&gt; TensorActivationStore:\n    \"\"\"Generate activations.\n\n    Args:\n        store_size: Number of activations to generate.\n\n    Returns:\n        Activation store for the train section.\n\n    Raises:\n        ValueError: If the store size is not divisible by the batch size.\n    \"\"\"\n    # Check the store size is divisible by the batch size\n    if store_size % (self.source_data_batch_size * self.source_dataset.context_size) != 0:\n        error_message = (\n            f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n            f\"got {store_size}\"\n        )\n        raise ValueError(error_message)\n\n    # Setup the store\n    source_model_device = get_model_device(self.source_model)\n    store = TensorActivationStore(\n        store_size, self.n_input_features, n_components=self.n_components\n    )\n\n    # Add the hook to the model (will automatically store the activations every time the model\n    # runs)\n    self.source_model.remove_all_hook_fns()\n    for component_idx, cache_name in enumerate(self.cache_names):\n        hook = partial(store_activations_hook, store=store, component_idx=component_idx)\n        self.source_model.add_hook(cache_name, hook)\n\n    # Loop through the dataloader until the store reaches the desired size\n    with torch.no_grad():\n        while len(store) &lt; store_size:\n            batch = next(self.source_data)\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n            self.source_model.forward(\n                input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n            )  # type: ignore (TLens is typed incorrectly)\n\n    self.source_model.remove_all_hook_fns()\n    store.shuffle()\n\n    return store\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.run_pipeline","title":"<code>run_pipeline(train_batch_size, max_store_size, max_activations, validation_n_activations=1024, validate_frequency=None, checkpoint_frequency=None)</code>","text":"<p>Run the full training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>PositiveInt</code> <p>Train batch size.</p> required <code>max_store_size</code> <code>PositiveInt</code> <p>Maximum size of the activation store.</p> required <code>max_activations</code> <code>PositiveInt</code> <p>Maximum total number of activations to train on (the original paper used 8bn, although others have had success with 100m+).</p> required <code>validation_n_activations</code> <code>PositiveInt</code> <p>Number of activations to use for validation.</p> <code>1024</code> <code>validate_frequency</code> <code>PositiveInt | None</code> <p>Frequency at which to get validation metrics.</p> <code>None</code> <code>checkpoint_frequency</code> <code>PositiveInt | None</code> <p>Frequency at which to save a checkpoint.</p> <code>None</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@validate_call\ndef run_pipeline(\n    self,\n    train_batch_size: PositiveInt,\n    max_store_size: PositiveInt,\n    max_activations: PositiveInt,\n    validation_n_activations: PositiveInt = 1024,\n    validate_frequency: PositiveInt | None = None,\n    checkpoint_frequency: PositiveInt | None = None,\n) -&gt; None:\n    \"\"\"Run the full training pipeline.\n\n    Args:\n        train_batch_size: Train batch size.\n        max_store_size: Maximum size of the activation store.\n        max_activations: Maximum total number of activations to train on (the original paper\n            used 8bn, although others have had success with 100m+).\n        validation_n_activations: Number of activations to use for validation.\n        validate_frequency: Frequency at which to get validation metrics.\n        checkpoint_frequency: Frequency at which to save a checkpoint.\n    \"\"\"\n    last_validated: int = 0\n    last_checkpoint: int = 0\n\n    self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n    # Get the store size\n    store_size: int = max_store_size - max_store_size % (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n\n    # Get the loss fn\n    loss_fn = self.autoencoder.loss_fn.clone()\n    loss_fn.keep_batch_dim = True\n\n    with tqdm(\n        desc=\"Activations trained on\",\n        total=max_activations,\n    ) as progress_bar:\n        for _ in range(0, max_activations, store_size):\n            # Generate\n            progress_bar.set_postfix({\"stage\": \"generate\"})\n            activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n            # Update the counters\n            n_activation_vectors_in_store = len(activation_store)\n            last_validated += n_activation_vectors_in_store\n            last_checkpoint += n_activation_vectors_in_store\n\n            # Train &amp; resample if needed\n            progress_bar.set_postfix({\"stage\": \"train\"})\n            self.train_autoencoder(activation_store, train_batch_size=train_batch_size)\n\n            # Get validation metrics (if needed)\n            progress_bar.set_postfix({\"stage\": \"validate\"})\n            if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                self.validate_sae(validation_n_activations)\n                last_validated = 0\n\n            # Checkpoint (if needed)\n            progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n            if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                last_checkpoint = 0\n                self.save_checkpoint()\n\n            # Update the progress bar\n            progress_bar.update(store_size)\n\n    # Save the final checkpoint\n    self.save_checkpoint(is_final=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.save_checkpoint","title":"<code>save_checkpoint(*, is_final=False)</code>","text":"<p>Save the model as a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>is_final</code> <code>bool</code> <p>Whether this is the final checkpoint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved checkpoint.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n    \"\"\"Save the model as a checkpoint.\n\n    Args:\n        is_final: Whether this is the final checkpoint.\n\n    Returns:\n        Path to the saved checkpoint.\n    \"\"\"\n    name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n\n    # Wandb\n    if wandb.run is not None:\n        self.autoencoder.sparse_autoencoder.save_to_wandb(name)\n\n    # Local\n    local_path = self.checkpoint_directory / f\"{name}.pt\"\n    self.autoencoder.sparse_autoencoder.save(local_path)\n    return local_path\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.train_autoencoder","title":"<code>train_autoencoder(activation_store, train_batch_size)</code>","text":"<p>Train the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store from the generate section.</p> required <code>train_batch_size</code> <code>PositiveInt</code> <p>Train batch size.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Number of times each neuron fired, for each component.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def train_autoencoder(\n    self,\n    activation_store: TensorActivationStore,\n    train_batch_size: PositiveInt,\n) -&gt; None:\n    \"\"\"Train the sparse autoencoder.\n\n    Args:\n        activation_store: Activation store from the generate section.\n        train_batch_size: Train batch size.\n\n    Returns:\n        Number of times each neuron fired, for each component.\n    \"\"\"\n    activations_dataloader = DataLoader(\n        activation_store, batch_size=train_batch_size, num_workers=4, persistent_workers=False\n    )\n\n    # Setup the trainer with no console logging\n    logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n    trainer = Trainer(\n        logger=WandbLogger() if wandb.run is not None else None,\n        max_epochs=1,\n        enable_progress_bar=False,\n        enable_model_summary=False,\n        enable_checkpointing=False,\n        precision=\"16-mixed\",\n    )\n    trainer.fit(self.autoencoder, activations_dataloader)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.validate_sae","title":"<code>validate_sae(validation_n_activations)</code>","text":"<p>Get validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>validation_n_activations</code> <code>PositiveInt</code> <p>Number of activations to use for validation.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@validate_call\ndef validate_sae(self, validation_n_activations: PositiveInt) -&gt; None:\n    \"\"\"Get validation metrics.\n\n    Args:\n        validation_n_activations: Number of activations to use for validation.\n    \"\"\"\n    n_batches = validation_n_activations // (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n    source_model_device = get_model_device(self.source_model)\n\n    # Create the metric data stores\n    losses: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n        self.n_components, device=source_model_device\n    )\n    losses_with_reconstruction: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n        self.n_components, device=source_model_device\n    )\n    losses_with_zero_ablation: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n        self.n_components, device=source_model_device\n    )\n\n    sae_model = self.autoencoder.sparse_autoencoder.clone()\n    sae_model.to(source_model_device)\n\n    for component_idx, cache_name in enumerate(self.cache_names):\n        for _batch_idx in range(n_batches):\n            batch = next(self.source_data)\n\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n\n            # Run a forward pass with and without the replaced activations\n            self.source_model.remove_all_hook_fns()\n            replacement_hook = partial(\n                replace_activations_hook,\n                sparse_autoencoder=sae_model,\n                component_idx=component_idx,\n                n_components=self.n_components,\n            )\n\n            with torch.no_grad():\n                loss: Float[\n                    Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)\n                ] = self.source_model.forward(input_ids, return_type=\"loss\")\n                loss_with_reconstruction = self.source_model.run_with_hooks(\n                    input_ids,\n                    return_type=\"loss\",\n                    fwd_hooks=[\n                        (\n                            cache_name,\n                            replacement_hook,\n                        )\n                    ],\n                )\n                loss_with_zero_ablation = self.source_model.run_with_hooks(\n                    input_ids, return_type=\"loss\", fwd_hooks=[(cache_name, zero_ablate_hook)]\n                )\n\n                self.reconstruction_score.update(\n                    source_model_loss=loss,\n                    source_model_loss_with_reconstruction=loss_with_reconstruction,\n                    source_model_loss_with_zero_ablation=loss_with_zero_ablation,\n                    component_idx=component_idx,\n                )\n\n                losses[component_idx] += loss.sum()\n                losses_with_reconstruction[component_idx] += loss_with_reconstruction.sum()\n                losses_with_zero_ablation[component_idx] += loss_with_zero_ablation.sum()\n\n    # Log\n    if wandb.run is not None:\n        log = {\n            f\"validation/source_model_losses/{c}\": val\n            for c, val in zip(self.cache_names, losses / n_batches)\n        }\n        log.update(\n            {\n                f\"validation/source_model_losses_with_reconstruction/{c}\": val\n                for c, val in zip(self.cache_names, losses_with_reconstruction / n_batches)\n            }\n        )\n        log.update(\n            {\n                f\"validation/source_model_losses_with_zero_ablation/{c}\": val\n                for c, val in zip(self.cache_names, losses_with_zero_ablation / n_batches)\n            }\n        )\n        log.update(self.reconstruction_score.compute())\n        wandb.log(log)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters","title":"<code>PipelineHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Pipeline hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass PipelineHyperparameters(NestedParameter):\n    \"\"\"Pipeline hyperparameters.\"\"\"\n\n    log_frequency: Parameter[int] = field(default=Parameter(100))\n    \"\"\"Training log frequency.\"\"\"\n\n    source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))\n    \"\"\"Source data batch size.\"\"\"\n\n    train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))\n    \"\"\"Train batch size.\"\"\"\n\n    max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))\n    \"\"\"Max store size.\"\"\"\n\n    max_activations: Parameter[int] = field(\n        default=Parameter(round_to_multiple(2e9, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Max activations.\"\"\"\n\n    num_workers_data_loading: Parameter[int] = field(default=Parameter(0))\n    \"\"\"Number of CPU workers for data loading.\"\"\"\n\n    checkpoint_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(5e7, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Checkpoint frequency.\"\"\"\n\n    validation_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(1e8, DEFAULT_BATCH_SIZE))\n    )\n    \"\"\"Validation frequency.\"\"\"\n\n    validation_n_activations: Parameter[int] = field(\n        default=Parameter(DEFAULT_SOURCE_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 2)\n    )\n    \"\"\"Number of activations to use for validation.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.checkpoint_frequency","title":"<code>checkpoint_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(50000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoint frequency.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.log_frequency","title":"<code>log_frequency: Parameter[int] = field(default=Parameter(100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training log frequency.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.max_activations","title":"<code>max_activations: Parameter[int] = field(default=Parameter(round_to_multiple(2000000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max activations.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.max_store_size","title":"<code>max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max store size.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.num_workers_data_loading","title":"<code>num_workers_data_loading: Parameter[int] = field(default=Parameter(0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of CPU workers for data loading.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.source_data_batch_size","title":"<code>source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source data batch size.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.train_batch_size","title":"<code>train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Train batch size.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.validation_frequency","title":"<code>validation_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(100000000.0, DEFAULT_BATCH_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation frequency.</p>"},{"location":"reference/#sparse_autoencoder.PipelineHyperparameters.validation_n_activations","title":"<code>validation_n_activations: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of activations to use for validation.</p>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset","title":"<code>PreTokenizedDataset</code>","text":"<p>             Bases: <code>SourceDataset[dict]</code></p> <p>General Pre-Tokenized Dataset from Hugging Face.</p> <p>Can be used for various datasets available on Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@final\nclass PreTokenizedDataset(SourceDataset[dict]):\n    \"\"\"General Pre-Tokenized Dataset from Hugging Face.\n\n    Can be used for various datasets available on Hugging Face.\n    \"\"\"\n\n    def preprocess(\n        self,\n        source_batch: dict,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        The method splits each pre-tokenized item based on the context size.\n\n        Args:\n            source_batch: A batch of source data.\n            context_size: The context size to use for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n\n        Raises:\n            ValueError: If the context size is larger than the tokenized prompt size.\n        \"\"\"\n        tokenized_prompts: list[list[int]] = source_batch[self._dataset_column_name]\n\n        # Check the context size is not too large\n        if context_size &gt; len(tokenized_prompts[0]):\n            error_message = (\n                f\"The context size ({context_size}) is larger than the \"\n                f\"tokenized prompt size ({len(tokenized_prompts[0])}).\"\n            )\n            raise ValueError(error_message)\n\n        # Chunk each tokenized prompt into blocks of context_size,\n        # discarding the last block if too small.\n        context_size_prompts = []\n        for encoding in tokenized_prompts:\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    @validate_call\n    def __init__(\n        self,\n        dataset_path: str,\n        context_size: PositiveInt = 256,\n        buffer_size: PositiveInt = 1000,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        dataset_column_name: str = \"input_ids\",\n        preprocess_batch_size: PositiveInt = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face (e.g.\n                `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n            context_size: The context size for tokenized prompts.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g. `train`).\n            dataset_column_name: The column name for the tokenized prompts.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            dataset_column_name=dataset_column_name,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset.__init__","title":"<code>__init__(dataset_path, context_size=256, buffer_size=1000, dataset_dir=None, dataset_files=None, dataset_split='train', dataset_column_name='input_ids', preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a pre-tokenized dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face (e.g. `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</p> required <code>context_size</code> <code>PositiveInt</code> <p>The context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>PositiveInt</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> <code>dataset_column_name</code> <code>str</code> <p>The column name for the tokenized prompts.</p> <code>'input_ids'</code> <code>preprocess_batch_size</code> <code>PositiveInt</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    dataset_path: str,\n    context_size: PositiveInt = 256,\n    buffer_size: PositiveInt = 1000,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    dataset_column_name: str = \"input_ids\",\n    preprocess_batch_size: PositiveInt = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face (e.g.\n            `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n        context_size: The context size for tokenized prompts.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g. `train`).\n        dataset_column_name: The column name for the tokenized prompts.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        dataset_column_name=dataset_column_name,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>The method splits each pre-tokenized item based on the context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>dict</code> <p>A batch of source data.</p> required <code>context_size</code> <code>int</code> <p>The context size to use for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the context size is larger than the tokenized prompt size.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: dict,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    The method splits each pre-tokenized item based on the context size.\n\n    Args:\n        source_batch: A batch of source data.\n        context_size: The context size to use for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n\n    Raises:\n        ValueError: If the context size is larger than the tokenized prompt size.\n    \"\"\"\n    tokenized_prompts: list[list[int]] = source_batch[self._dataset_column_name]\n\n    # Check the context size is not too large\n    if context_size &gt; len(tokenized_prompts[0]):\n        error_message = (\n            f\"The context size ({context_size}) is larger than the \"\n            f\"tokenized prompt size ({len(tokenized_prompts[0])}).\"\n        )\n        raise ValueError(error_message)\n\n    # Chunk each tokenized prompt into blocks of context_size,\n    # discarding the last block if too small.\n    context_size_prompts = []\n    for encoding in tokenized_prompts:\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ReconstructionScoreMetric","title":"<code>ReconstructionScoreMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Model reconstruction score.</p> <p>Creates a score that measures how well the model can reconstruct the data.</p> \\[ \\begin{align*}     v &amp;= \\text{number of validation items} \\\\     l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\     l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\     l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\     s &amp;= \\text{reconstruction score} \\\\     s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\     s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v \\end{align*} \\] Example <p>metric = ReconstructionScoreMetric(num_components=1) source_model_loss=torch.tensor([2.0, 2.0, 2.0]) source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0]) source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0]) metric.forward( ...     source_model_loss=source_model_loss, ...     source_model_loss_with_reconstruction=source_model_loss_with_reconstruction, ...     source_model_loss_with_zero_ablation=source_model_loss_with_zero_ablation ... ) tensor(0.6667)</p> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>class ReconstructionScoreMetric(Metric):\n    r\"\"\"Model reconstruction score.\n\n    Creates a score that measures how well the model can reconstruct the data.\n\n    $$\n    \\begin{align*}\n        v &amp;= \\text{number of validation items} \\\\\n        l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\\n        l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\\n        l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\\n        s &amp;= \\text{reconstruction score} \\\\\n        s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\\n        s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v\n    \\end{align*}\n    $$\n\n    Example:\n        &gt;&gt;&gt; metric = ReconstructionScoreMetric(num_components=1)\n        &gt;&gt;&gt; source_model_loss=torch.tensor([2.0, 2.0, 2.0])\n        &gt;&gt;&gt; source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0])\n        &gt;&gt;&gt; source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0])\n        &gt;&gt;&gt; metric.forward(\n        ...     source_model_loss=source_model_loss,\n        ...     source_model_loss_with_reconstruction=source_model_loss_with_reconstruction,\n        ...     source_model_loss_with_zero_ablation=source_model_loss_with_zero_ablation\n        ... )\n        tensor(0.6667)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = True\n\n    # State\n    source_model_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    source_model_loss_with_zero_ablation: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    source_model_loss_with_reconstruction: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(self, num_components: PositiveInt = 1) -&gt; None:\n        \"\"\"Initialise the metric.\"\"\"\n        super().__init__()\n\n        self.add_state(\n            \"source_model_loss\", default=torch.zeros(num_components), dist_reduce_fx=\"sum\"\n        )\n        self.add_state(\n            \"source_model_loss_with_zero_ablation\",\n            default=torch.zeros(num_components),\n            dist_reduce_fx=\"sum\",\n        )\n        self.add_state(\n            \"source_model_loss_with_reconstruction\",\n            default=torch.zeros(num_components),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        source_model_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n        source_model_loss_with_reconstruction: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n        source_model_loss_with_zero_ablation: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n        component_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            source_model_loss: Loss with no changes to the source model.\n            source_model_loss_with_reconstruction: Loss with SAE reconstruction.\n            source_model_loss_with_zero_ablation: Loss with zero ablation.\n            component_idx: Component idx.\n        \"\"\"\n        self.source_model_loss[component_idx] += source_model_loss.sum()\n        self.source_model_loss_with_zero_ablation[\n            component_idx\n        ] += source_model_loss_with_zero_ablation.sum()\n        self.source_model_loss_with_reconstruction[\n            component_idx\n        ] += source_model_loss_with_reconstruction.sum()\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n        \"\"\"Compute the metric.\"\"\"\n        zero_ablate_loss_minus_reconstruction_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n            self.source_model_loss_with_zero_ablation - self.source_model_loss_with_reconstruction\n        )\n\n        zero_ablate_loss_minus_default_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n            self.source_model_loss_with_zero_ablation - self.source_model_loss\n        )\n\n        return zero_ablate_loss_minus_reconstruction_loss / zero_ablate_loss_minus_default_loss\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ReconstructionScoreMetric.__init__","title":"<code>__init__(num_components=1)</code>","text":"<p>Initialise the metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>@validate_call\ndef __init__(self, num_components: PositiveInt = 1) -&gt; None:\n    \"\"\"Initialise the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\n        \"source_model_loss\", default=torch.zeros(num_components), dist_reduce_fx=\"sum\"\n    )\n    self.add_state(\n        \"source_model_loss_with_zero_ablation\",\n        default=torch.zeros(num_components),\n        dist_reduce_fx=\"sum\",\n    )\n    self.add_state(\n        \"source_model_loss_with_reconstruction\",\n        default=torch.zeros(num_components),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ReconstructionScoreMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n    \"\"\"Compute the metric.\"\"\"\n    zero_ablate_loss_minus_reconstruction_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n        self.source_model_loss_with_zero_ablation - self.source_model_loss_with_reconstruction\n    )\n\n    zero_ablate_loss_minus_default_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n        self.source_model_loss_with_zero_ablation - self.source_model_loss\n    )\n\n    return zero_ablate_loss_minus_reconstruction_loss / zero_ablate_loss_minus_default_loss\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ReconstructionScoreMetric.update","title":"<code>update(source_model_loss, source_model_loss_with_reconstruction, source_model_loss_with_zero_ablation, component_idx=0)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>source_model_loss</code> <code>Float[Tensor, COMPONENT_OPTIONAL]</code> <p>Loss with no changes to the source model.</p> required <code>source_model_loss_with_reconstruction</code> <code>Float[Tensor, COMPONENT_OPTIONAL]</code> <p>Loss with SAE reconstruction.</p> required <code>source_model_loss_with_zero_ablation</code> <code>Float[Tensor, COMPONENT_OPTIONAL]</code> <p>Loss with zero ablation.</p> required <code>component_idx</code> <code>int</code> <p>Component idx.</p> <code>0</code> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>def update(\n    self,\n    source_model_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n    source_model_loss_with_reconstruction: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n    source_model_loss_with_zero_ablation: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n    component_idx: int = 0,\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        source_model_loss: Loss with no changes to the source model.\n        source_model_loss_with_reconstruction: Loss with SAE reconstruction.\n        source_model_loss_with_zero_ablation: Loss with zero ablation.\n        component_idx: Component idx.\n    \"\"\"\n    self.source_model_loss[component_idx] += source_model_loss.sum()\n    self.source_model_loss_with_zero_ablation[\n        component_idx\n    ] += source_model_loss_with_zero_ablation.sum()\n    self.source_model_loss_with_reconstruction[\n        component_idx\n    ] += source_model_loss_with_reconstruction.sum()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters","title":"<code>SourceDataHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceDataHyperparameters(NestedParameter):\n    \"\"\"Source data hyperparameters.\"\"\"\n\n    dataset_path: Parameter[str]\n    \"\"\"Dataset path.\"\"\"\n\n    context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))\n    \"\"\"Context size.\"\"\"\n\n    dataset_column_name: Parameter[str] | None = field(default=Parameter(value=\"input_ids\"))\n    \"\"\"Dataset column name.\"\"\"\n\n    dataset_dir: Parameter[str] | None = field(default=None)\n    \"\"\"Dataset directory (within the HF dataset)\"\"\"\n\n    dataset_files: Parameter[list[str]] | None = field(default=None)\n    \"\"\"Dataset files (within the HF dataset).\"\"\"\n\n    pre_download: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Whether to pre-download the dataset.\"\"\"\n\n    pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))\n    \"\"\"If the dataset is pre-tokenized.\"\"\"\n\n    tokenizer_name: Parameter[str] | None = field(default=None)\n    \"\"\"Tokenizer name.\n\n    Only set this if the dataset is not pre-tokenized.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\n\n        Raises:\n            ValueError: If there is an error in the source data hyperparameters.\n        \"\"\"\n        if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n            raise ValueError(error_message)\n\n        if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n            raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.context_size","title":"<code>context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Context size.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_column_name","title":"<code>dataset_column_name: Parameter[str] | None = field(default=Parameter(value='input_ids'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset column name.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_dir","title":"<code>dataset_dir: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset directory (within the HF dataset)</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_files","title":"<code>dataset_files: Parameter[list[str]] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset files (within the HF dataset).</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.dataset_path","title":"<code>dataset_path: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Dataset path.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.pre_download","title":"<code>pre_download: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to pre-download the dataset.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.pre_tokenized","title":"<code>pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If the dataset is pre-tokenized.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.tokenizer_name","title":"<code>tokenizer_name: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tokenizer name.</p> <p>Only set this if the dataset is not pre-tokenized.</p>"},{"location":"reference/#sparse_autoencoder.SourceDataHyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error in the source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\n\n    Raises:\n        ValueError: If there is an error in the source data hyperparameters.\n    \"\"\"\n    if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n        raise ValueError(error_message)\n\n    if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters","title":"<code>SourceModelHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source model hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceModelHyperparameters(NestedParameter):\n    \"\"\"Source model hyperparameters.\"\"\"\n\n    name: Parameter[str]\n    \"\"\"Source model name.\"\"\"\n\n    cache_names: Parameter[list[str]]\n    \"\"\"Source model hook site.\"\"\"\n\n    hook_dimension: Parameter[int]\n    \"\"\"Source model hook point dimension.\"\"\"\n\n    dtype: Parameter[str] = field(default=Parameter(\"float32\"))\n    \"\"\"Source model dtype.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.cache_names","title":"<code>cache_names: Parameter[list[str]]</code>  <code>instance-attribute</code>","text":"<p>Source model hook site.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.dtype","title":"<code>dtype: Parameter[str] = field(default=Parameter('float32'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source model dtype.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.hook_dimension","title":"<code>hook_dimension: Parameter[int]</code>  <code>instance-attribute</code>","text":"<p>Source model hook point dimension.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelHyperparameters.name","title":"<code>name: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Source model name.</p>"},{"location":"reference/#sparse_autoencoder.SourceModelRuntimeHyperparameters","title":"<code>SourceModelRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Source model runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class SourceModelRuntimeHyperparameters(TypedDict):\n    \"\"\"Source model runtime hyperparameters.\"\"\"\n\n    name: str\n    cache_names: list[str]\n    hook_dimension: int\n    dtype: str\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>class SparseAutoencoder(Module):\n    \"\"\"Sparse Autoencoder Model.\"\"\"\n\n    config: SparseAutoencoderConfig\n    \"\"\"Model config.\"\"\"\n\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Estimated Geometric Median of the Dataset.\n\n    Used for initialising :attr:`tied_bias`.\n    \"\"\"\n\n    tied_bias: Float[\n        Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Tied Bias Parameter.\n\n    The same bias is used pre-encoder and post-decoder.\n    \"\"\"\n\n    pre_encoder_bias: TiedBias\n    \"\"\"Pre-Encoder Bias.\"\"\"\n\n    encoder: LinearEncoder\n    \"\"\"Encoder.\"\"\"\n\n    decoder: UnitNormDecoder\n    \"\"\"Decoder.\"\"\"\n\n    post_decoder_bias: TiedBias\n    \"\"\"Post-Decoder Bias.\"\"\"\n\n    def __init__(\n        self,\n        config: SparseAutoencoderConfig,\n        geometric_median_dataset: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ]\n        | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder Model.\n\n        Args:\n            config: Model config.\n            geometric_median_dataset: Estimated geometric median of the dataset.\n        \"\"\"\n        super().__init__()\n\n        self.config = config\n\n        # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n        # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n        tied_bias_shape = shape_with_optional_dimensions(\n            config.n_components, config.n_input_features\n        )\n        if geometric_median_dataset is not None:\n            self.geometric_median_dataset = geometric_median_dataset.clone()\n            self.geometric_median_dataset.requires_grad = False\n        else:\n            self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n            self.geometric_median_dataset.requires_grad = False\n\n        # Initialize the tied bias\n        self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n        self.initialize_tied_parameters()\n\n        # Initialize the components\n        self.pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n        self.encoder = LinearEncoder(\n            input_features=config.n_input_features,\n            learnt_features=config.n_learned_features,\n            n_components=config.n_components,\n        )\n\n        self.decoder = UnitNormDecoder(\n            learnt_features=config.n_learned_features,\n            decoded_features=config.n_input_features,\n            n_components=config.n_components,\n        )\n\n        self.post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; ForwardPassResult:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n        x = self.pre_encoder_bias(x)\n        learned_activations = self.encoder(x)\n        x = self.decoder(learned_activations)\n        decoded_activations = self.post_decoder_bias(x)\n\n        return ForwardPassResult(learned_activations, decoded_activations)\n\n    def initialize_tied_parameters(self) -&gt; None:\n        \"\"\"Initialize the tied parameters.\"\"\"\n        # The tied bias is initialised as the geometric median of the dataset\n        self.tied_bias.data = self.geometric_median_dataset\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n        self.initialize_tied_parameters()\n        for module in self.network:\n            if \"reset_parameters\" in dir(module):\n                module.reset_parameters()\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[ResetOptimizerParameterDetails]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return (\n            self.encoder.reset_optimizer_parameter_details\n            + self.decoder.reset_optimizer_parameter_details\n        )\n\n    def post_backwards_hook(self) -&gt; None:\n        \"\"\"Hook to be called after each learning step.\n\n        This can be used to e.g. constrain weights to unit norm.\n        \"\"\"\n        self.decoder.constrain_weights_unit_norm()\n\n    @staticmethod\n    @validate_call\n    def get_single_component_state_dict(\n        state: SparseAutoencoderState, component_idx: NonNegativeInt\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"Get the state dict for a single component.\n\n        Args:\n            state: Sparse Autoencoder state.\n            component_idx: Index of the component to get the state dict for.\n\n        Returns:\n            State dict for the component.\n\n        Raises:\n            ValueError: If the state dict doesn't contain a components dimension.\n        \"\"\"\n        # Check the state has a components dimension\n        if state.config.n_components is None:\n            error_message = (\n                \"Trying to load a single component from the state dict, but the state dict \"\n                \"doesn't contain a components dimension.\"\n            )\n            raise ValueError(error_message)\n\n        # Return the state dict for the component\n        return {key: value[component_idx] for key, value in state.state_dict.items()}\n\n    def save(self, file_path: Path) -&gt; None:\n        \"\"\"Save the model config and state dict to a file.\n\n        Args:\n            file_path: Path to save the model to.\n        \"\"\"\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        state = SparseAutoencoderState(config=self.config, state_dict=self.state_dict())\n        torch.save(state, file_path)\n\n    @staticmethod\n    def load(\n        file_path: FILE_LIKE,\n        component_idx: PositiveInt | None = None,\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"Load the model from a file.\n\n        Args:\n            file_path: Path to load the model from.\n            component_idx: If loading a state dict from a model that has been trained on multiple\n                components (e.g. all MLP layers) you may want to to load just one component. In this\n                case you can set `component_idx` to the index of the component to load. Note you\n                should not set this if you want to load a state dict from a model that has been\n                trained on a single component (or if you want to load all components).\n\n        Returns:\n            The loaded model.\n        \"\"\"\n        # Load the file\n        serialized_state = torch.load(file_path, map_location=torch.device(\"cpu\"))\n        state = SparseAutoencoderState.model_validate(serialized_state)\n\n        # Initialise the model\n        config = SparseAutoencoderConfig(\n            n_input_features=state.config.n_input_features,\n            n_learned_features=state.config.n_learned_features,\n            n_components=state.config.n_components if component_idx is None else None,\n        )\n        state_dict = (\n            SparseAutoencoder.get_single_component_state_dict(state, component_idx)\n            if component_idx is not None\n            else state.state_dict\n        )\n        model = SparseAutoencoder(config)\n        model.load_state_dict(state_dict)\n\n        return model\n\n    def save_to_wandb(\n        self,\n        artifact_name: str,\n        directory: DirectoryPath = DEFAULT_TMP_DIR,\n    ) -&gt; str:\n        \"\"\"Save the model to wandb.\n\n        Args:\n            artifact_name: A human-readable name for this artifact, which is how you can identify\n                this artifact in the UI or reference it in use_artifact calls. Names can contain\n                letters, numbers, underscores, hyphens, and dots. The name must be unique across a\n                project. Example: \"sweep_name 1e9 activations\".\n            directory: Directory to save the model to.\n\n        Returns:\n            Name of the wandb artifact.\n\n        Raises:\n            ValueError: If wandb is not initialised.\n        \"\"\"\n        # Save the file\n        directory.mkdir(parents=True, exist_ok=True)\n        file_name = artifact_name + \".pt\"\n        file_path = directory / file_name\n        self.save(file_path)\n\n        # Upload to wandb\n        if wandb.run is None:\n            error_message = \"Trying to save the model to wandb, but wandb is not initialised.\"\n            raise ValueError(error_message)\n        artifact = wandb.Artifact(\n            artifact_name,\n            type=\"model\",\n            description=\"Sparse Autoencoder model state, created with `sparse_autoencoder`.\",\n        )\n        artifact.add_file(str(file_path), name=\"sae-model-state.pt\")\n        artifact.save()\n        wandb.log_artifact(artifact)\n        artifact.wait()\n\n        return artifact.source_qualified_name\n\n    @staticmethod\n    def load_from_wandb(\n        wandb_artifact_name: str,\n        component_idx: PositiveInt | None = None,\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"Load the model from wandb.\n\n        Args:\n            wandb_artifact_name: Name of the wandb artifact to load the model from (e.g.\n                \"username/project/artifact_name:version\").\n            component_idx: If loading a state dict from a model that has been trained on multiple\n                components (e.g. all MLP layers) you may want to to load just one component. In this\n                case you can set `component_idx` to the index of the component to load. Note you\n                should not set this if you want to load a state dict from a model that has been\n                trained on a single component (or if you want to load all components).\n\n        Returns:\n            The loaded model.\n        \"\"\"\n        api = wandb.Api()\n        artifact = api.artifact(wandb_artifact_name, type=\"model\")\n        download_path = artifact.download()\n        return SparseAutoencoder.load(Path(download_path) / \"sae-model-state.pt\", component_idx)\n\n    def save_to_hugging_face(\n        self,\n        file_name: str,\n        repo_id: str,\n        directory: DirectoryPath = DEFAULT_TMP_DIR,\n        hf_access_token: str | None = None,\n    ) -&gt; None:\n        \"\"\"Save the model to Hugging Face.\n\n        Args:\n            file_name: Name of the file (e.g. \"model-something.pt\").\n            repo_id: ID of the repo to save the model to.\n            directory: Directory to save the model to.\n            hf_access_token: Hugging Face access token.\n        \"\"\"\n        # Save the file\n        directory.mkdir(parents=True, exist_ok=True)\n        file_path = directory / file_name\n        self.save(file_path)\n\n        # Upload to Hugging Face\n        api = HfApi(token=hf_access_token)\n        api.upload_file(\n            path_or_fileobj=file_path,\n            path_in_repo=file_name,\n            repo_id=repo_id,\n            repo_type=\"model\",\n        )\n\n    @staticmethod\n    def load_from_hugging_face(\n        file_name: str,\n        repo_id: str,\n        component_idx: PositiveInt | None = None,\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"Load the model from Hugging Face.\n\n        Args:\n            file_name: File name of the .pt state file.\n            repo_id: ID of the repo to load the model from.\n            component_idx: If loading a state dict from a model that has been trained on multiple\n                components (e.g. all MLP layers) you may want to to load just one component. In this\n                case you can set `component_idx` to the index of the component to load. Note you\n                should not set this if you want to load a state dict from a model that has been\n                trained on a single component (or if you want to load all components).\n\n        Returns:\n            The loaded model.\n        \"\"\"\n        local_file = hf_hub_download(\n            repo_id=repo_id,\n            repo_type=\"model\",\n            filename=file_name,\n            revision=\"main\",\n        )\n\n        return SparseAutoencoder.load(Path(local_file), component_idx)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.config","title":"<code>config: SparseAutoencoderConfig = config</code>  <code>instance-attribute</code>","text":"<p>Model config.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.decoder","title":"<code>decoder: UnitNormDecoder = UnitNormDecoder(learnt_features=config.n_learned_features, decoded_features=config.n_input_features, n_components=config.n_components)</code>  <code>instance-attribute</code>","text":"<p>Decoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.encoder","title":"<code>encoder: LinearEncoder = LinearEncoder(input_features=config.n_input_features, learnt_features=config.n_learned_features, n_components=config.n_components)</code>  <code>instance-attribute</code>","text":"<p>Encoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset","title":"<code>geometric_median_dataset: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Estimated Geometric Median of the Dataset.</p> <p>Used for initialising :attr:<code>tied_bias</code>.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: TiedBias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)</code>  <code>instance-attribute</code>","text":"<p>Post-Decoder Bias.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: TiedBias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)</code>  <code>instance-attribute</code>","text":"<p>Pre-Encoder Bias.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[ResetOptimizerParameterDetails]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[ResetOptimizerParameterDetails]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[ResetOptimizerParameterDetails]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.tied_bias","title":"<code>tied_bias: Float[Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)] = Parameter(torch.empty(tied_bias_shape))</code>  <code>instance-attribute</code>","text":"<p>Tied Bias Parameter.</p> <p>The same bias is used pre-encoder and post-decoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.__init__","title":"<code>__init__(config, geometric_median_dataset=None)</code>","text":"<p>Initialize the Sparse Autoencoder Model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SparseAutoencoderConfig</code> <p>Model config.</p> required <code>geometric_median_dataset</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)] | None</code> <p>Estimated geometric median of the dataset.</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def __init__(\n    self,\n    config: SparseAutoencoderConfig,\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder Model.\n\n    Args:\n        config: Model config.\n        geometric_median_dataset: Estimated geometric median of the dataset.\n    \"\"\"\n    super().__init__()\n\n    self.config = config\n\n    # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n    # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n    tied_bias_shape = shape_with_optional_dimensions(\n        config.n_components, config.n_input_features\n    )\n    if geometric_median_dataset is not None:\n        self.geometric_median_dataset = geometric_median_dataset.clone()\n        self.geometric_median_dataset.requires_grad = False\n    else:\n        self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n        self.geometric_median_dataset.requires_grad = False\n\n    # Initialize the tied bias\n    self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n    self.initialize_tied_parameters()\n\n    # Initialize the components\n    self.pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n    self.encoder = LinearEncoder(\n        input_features=config.n_input_features,\n        learnt_features=config.n_learned_features,\n        n_components=config.n_components,\n    )\n\n    self.decoder = UnitNormDecoder(\n        learnt_features=config.n_learned_features,\n        decoded_features=config.n_input_features,\n        n_components=config.n_components,\n    )\n\n    self.post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>ForwardPassResult</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; ForwardPassResult:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n    x = self.pre_encoder_bias(x)\n    learned_activations = self.encoder(x)\n    x = self.decoder(learned_activations)\n    decoded_activations = self.post_decoder_bias(x)\n\n    return ForwardPassResult(learned_activations, decoded_activations)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.get_single_component_state_dict","title":"<code>get_single_component_state_dict(state, component_idx)</code>  <code>staticmethod</code>","text":"<p>Get the state dict for a single component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SparseAutoencoderState</code> <p>Sparse Autoencoder state.</p> required <code>component_idx</code> <code>NonNegativeInt</code> <p>Index of the component to get the state dict for.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>State dict for the component.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the state dict doesn't contain a components dimension.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\n@validate_call\ndef get_single_component_state_dict(\n    state: SparseAutoencoderState, component_idx: NonNegativeInt\n) -&gt; dict[str, Tensor]:\n    \"\"\"Get the state dict for a single component.\n\n    Args:\n        state: Sparse Autoencoder state.\n        component_idx: Index of the component to get the state dict for.\n\n    Returns:\n        State dict for the component.\n\n    Raises:\n        ValueError: If the state dict doesn't contain a components dimension.\n    \"\"\"\n    # Check the state has a components dimension\n    if state.config.n_components is None:\n        error_message = (\n            \"Trying to load a single component from the state dict, but the state dict \"\n            \"doesn't contain a components dimension.\"\n        )\n        raise ValueError(error_message)\n\n    # Return the state dict for the component\n    return {key: value[component_idx] for key, value in state.state_dict.items()}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters","title":"<code>initialize_tied_parameters()</code>","text":"<p>Initialize the tied parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def initialize_tied_parameters(self) -&gt; None:\n    \"\"\"Initialize the tied parameters.\"\"\"\n    # The tied bias is initialised as the geometric median of the dataset\n    self.tied_bias.data = self.geometric_median_dataset\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.load","title":"<code>load(file_path, component_idx=None)</code>  <code>staticmethod</code>","text":"<p>Load the model from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>FILE_LIKE</code> <p>Path to load the model from.</p> required <code>component_idx</code> <code>PositiveInt | None</code> <p>If loading a state dict from a model that has been trained on multiple components (e.g. all MLP layers) you may want to to load just one component. In this case you can set <code>component_idx</code> to the index of the component to load. Note you should not set this if you want to load a state dict from a model that has been trained on a single component (or if you want to load all components).</p> <code>None</code> <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The loaded model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\ndef load(\n    file_path: FILE_LIKE,\n    component_idx: PositiveInt | None = None,\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"Load the model from a file.\n\n    Args:\n        file_path: Path to load the model from.\n        component_idx: If loading a state dict from a model that has been trained on multiple\n            components (e.g. all MLP layers) you may want to to load just one component. In this\n            case you can set `component_idx` to the index of the component to load. Note you\n            should not set this if you want to load a state dict from a model that has been\n            trained on a single component (or if you want to load all components).\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    # Load the file\n    serialized_state = torch.load(file_path, map_location=torch.device(\"cpu\"))\n    state = SparseAutoencoderState.model_validate(serialized_state)\n\n    # Initialise the model\n    config = SparseAutoencoderConfig(\n        n_input_features=state.config.n_input_features,\n        n_learned_features=state.config.n_learned_features,\n        n_components=state.config.n_components if component_idx is None else None,\n    )\n    state_dict = (\n        SparseAutoencoder.get_single_component_state_dict(state, component_idx)\n        if component_idx is not None\n        else state.state_dict\n    )\n    model = SparseAutoencoder(config)\n    model.load_state_dict(state_dict)\n\n    return model\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.load_from_hugging_face","title":"<code>load_from_hugging_face(file_name, repo_id, component_idx=None)</code>  <code>staticmethod</code>","text":"<p>Load the model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>File name of the .pt state file.</p> required <code>repo_id</code> <code>str</code> <p>ID of the repo to load the model from.</p> required <code>component_idx</code> <code>PositiveInt | None</code> <p>If loading a state dict from a model that has been trained on multiple components (e.g. all MLP layers) you may want to to load just one component. In this case you can set <code>component_idx</code> to the index of the component to load. Note you should not set this if you want to load a state dict from a model that has been trained on a single component (or if you want to load all components).</p> <code>None</code> <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The loaded model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\ndef load_from_hugging_face(\n    file_name: str,\n    repo_id: str,\n    component_idx: PositiveInt | None = None,\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"Load the model from Hugging Face.\n\n    Args:\n        file_name: File name of the .pt state file.\n        repo_id: ID of the repo to load the model from.\n        component_idx: If loading a state dict from a model that has been trained on multiple\n            components (e.g. all MLP layers) you may want to to load just one component. In this\n            case you can set `component_idx` to the index of the component to load. Note you\n            should not set this if you want to load a state dict from a model that has been\n            trained on a single component (or if you want to load all components).\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    local_file = hf_hub_download(\n        repo_id=repo_id,\n        repo_type=\"model\",\n        filename=file_name,\n        revision=\"main\",\n    )\n\n    return SparseAutoencoder.load(Path(local_file), component_idx)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.load_from_wandb","title":"<code>load_from_wandb(wandb_artifact_name, component_idx=None)</code>  <code>staticmethod</code>","text":"<p>Load the model from wandb.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_artifact_name</code> <code>str</code> <p>Name of the wandb artifact to load the model from (e.g. \"username/project/artifact_name:version\").</p> required <code>component_idx</code> <code>PositiveInt | None</code> <p>If loading a state dict from a model that has been trained on multiple components (e.g. all MLP layers) you may want to to load just one component. In this case you can set <code>component_idx</code> to the index of the component to load. Note you should not set this if you want to load a state dict from a model that has been trained on a single component (or if you want to load all components).</p> <code>None</code> <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The loaded model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\ndef load_from_wandb(\n    wandb_artifact_name: str,\n    component_idx: PositiveInt | None = None,\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"Load the model from wandb.\n\n    Args:\n        wandb_artifact_name: Name of the wandb artifact to load the model from (e.g.\n            \"username/project/artifact_name:version\").\n        component_idx: If loading a state dict from a model that has been trained on multiple\n            components (e.g. all MLP layers) you may want to to load just one component. In this\n            case you can set `component_idx` to the index of the component to load. Note you\n            should not set this if you want to load a state dict from a model that has been\n            trained on a single component (or if you want to load all components).\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    api = wandb.Api()\n    artifact = api.artifact(wandb_artifact_name, type=\"model\")\n    download_path = artifact.download()\n    return SparseAutoencoder.load(Path(download_path) / \"sae-model-state.pt\", component_idx)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.post_backwards_hook","title":"<code>post_backwards_hook()</code>","text":"<p>Hook to be called after each learning step.</p> <p>This can be used to e.g. constrain weights to unit norm.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def post_backwards_hook(self) -&gt; None:\n    \"\"\"Hook to be called after each learning step.\n\n    This can be used to e.g. constrain weights to unit norm.\n    \"\"\"\n    self.decoder.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n    self.initialize_tied_parameters()\n    for module in self.network:\n        if \"reset_parameters\" in dir(module):\n            module.reset_parameters()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.save","title":"<code>save(file_path)</code>","text":"<p>Save the model config and state dict to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to save the model to.</p> required Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def save(self, file_path: Path) -&gt; None:\n    \"\"\"Save the model config and state dict to a file.\n\n    Args:\n        file_path: Path to save the model to.\n    \"\"\"\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    state = SparseAutoencoderState(config=self.config, state_dict=self.state_dict())\n    torch.save(state, file_path)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.save_to_hugging_face","title":"<code>save_to_hugging_face(file_name, repo_id, directory=DEFAULT_TMP_DIR, hf_access_token=None)</code>","text":"<p>Save the model to Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the file (e.g. \"model-something.pt\").</p> required <code>repo_id</code> <code>str</code> <p>ID of the repo to save the model to.</p> required <code>directory</code> <code>DirectoryPath</code> <p>Directory to save the model to.</p> <code>DEFAULT_TMP_DIR</code> <code>hf_access_token</code> <code>str | None</code> <p>Hugging Face access token.</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def save_to_hugging_face(\n    self,\n    file_name: str,\n    repo_id: str,\n    directory: DirectoryPath = DEFAULT_TMP_DIR,\n    hf_access_token: str | None = None,\n) -&gt; None:\n    \"\"\"Save the model to Hugging Face.\n\n    Args:\n        file_name: Name of the file (e.g. \"model-something.pt\").\n        repo_id: ID of the repo to save the model to.\n        directory: Directory to save the model to.\n        hf_access_token: Hugging Face access token.\n    \"\"\"\n    # Save the file\n    directory.mkdir(parents=True, exist_ok=True)\n    file_path = directory / file_name\n    self.save(file_path)\n\n    # Upload to Hugging Face\n    api = HfApi(token=hf_access_token)\n    api.upload_file(\n        path_or_fileobj=file_path,\n        path_in_repo=file_name,\n        repo_id=repo_id,\n        repo_type=\"model\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.save_to_wandb","title":"<code>save_to_wandb(artifact_name, directory=DEFAULT_TMP_DIR)</code>","text":"<p>Save the model to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>A human-readable name for this artifact, which is how you can identify this artifact in the UI or reference it in use_artifact calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project. Example: \"sweep_name 1e9 activations\".</p> required <code>directory</code> <code>DirectoryPath</code> <p>Directory to save the model to.</p> <code>DEFAULT_TMP_DIR</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of the wandb artifact.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If wandb is not initialised.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def save_to_wandb(\n    self,\n    artifact_name: str,\n    directory: DirectoryPath = DEFAULT_TMP_DIR,\n) -&gt; str:\n    \"\"\"Save the model to wandb.\n\n    Args:\n        artifact_name: A human-readable name for this artifact, which is how you can identify\n            this artifact in the UI or reference it in use_artifact calls. Names can contain\n            letters, numbers, underscores, hyphens, and dots. The name must be unique across a\n            project. Example: \"sweep_name 1e9 activations\".\n        directory: Directory to save the model to.\n\n    Returns:\n        Name of the wandb artifact.\n\n    Raises:\n        ValueError: If wandb is not initialised.\n    \"\"\"\n    # Save the file\n    directory.mkdir(parents=True, exist_ok=True)\n    file_name = artifact_name + \".pt\"\n    file_path = directory / file_name\n    self.save(file_path)\n\n    # Upload to wandb\n    if wandb.run is None:\n        error_message = \"Trying to save the model to wandb, but wandb is not initialised.\"\n        raise ValueError(error_message)\n    artifact = wandb.Artifact(\n        artifact_name,\n        type=\"model\",\n        description=\"Sparse Autoencoder model state, created with `sparse_autoencoder`.\",\n    )\n    artifact.add_file(str(file_path), name=\"sae-model-state.pt\")\n    artifact.save()\n    wandb.log_artifact(artifact)\n    artifact.wait()\n\n    return artifact.source_qualified_name\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderConfig","title":"<code>SparseAutoencoderConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>SAE model config.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>class SparseAutoencoderConfig(BaseModel):\n    \"\"\"SAE model config.\"\"\"\n\n    n_input_features: PositiveInt\n    \"\"\"Number of input features.\n\n    E.g. `d_mlp` if training on MLP activations from TransformerLens).\n    \"\"\"\n\n    n_learned_features: PositiveInt\n    \"\"\"Number of learned features.\n\n    The initial paper experimented with 1 to 256 times the number of input features, and primarily\n    used a multiple of 8.\"\"\"\n\n    n_components: PositiveInt | None = None\n    \"\"\"Number of source model components the SAE is trained on.\"\"\n\n    This is useful if you want to train the SAE on several components of the source model at once.\n    If `None`, the SAE is assumed to be trained on just one component (in this case the model won't\n    contain a component axis in any of the parameters).\n    \"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderConfig.n_components","title":"<code>n_components: PositiveInt | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of source model components the SAE is trained on.\"\"</p> <p>This is useful if you want to train the SAE on several components of the source model at once. If <code>None</code>, the SAE is assumed to be trained on just one component (in this case the model won't contain a component axis in any of the parameters).</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderConfig.n_input_features","title":"<code>n_input_features: PositiveInt</code>  <code>instance-attribute</code>","text":"<p>Number of input features.</p> <p>E.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderConfig.n_learned_features","title":"<code>n_learned_features: PositiveInt</code>  <code>instance-attribute</code>","text":"<p>Number of learned features.</p> <p>The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderLoss","title":"<code>SparseAutoencoderLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Sparse Autoencoder loss.</p> <p>This is the same as composing <code>L1AbsoluteLoss() * l1_coefficient + L2ReconstructionLoss()</code>. It is separated out so that you can use all three metrics (l1, l2, total loss) in the same <code>MetricCollection</code> and they will then share state (to avoid calculating the same thing twice).</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>class SparseAutoencoderLoss(Metric):\n    \"\"\"Sparse Autoencoder loss.\n\n    This is the same as composing `L1AbsoluteLoss() * l1_coefficient + L2ReconstructionLoss()`. It\n    is separated out so that you can use all three metrics (l1, l2, total loss) in the same\n    `MetricCollection` and they will then share state (to avoid calculating the same thing twice).\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    higher_is_better = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # Settings\n    _num_components: int\n    _keep_batch_dim: bool\n    _l1_coefficient: float\n\n    @property\n    def keep_batch_dim(self) -&gt; bool:\n        \"\"\"Whether to keep the batch dimension in the loss output.\"\"\"\n        return self._keep_batch_dim\n\n    @keep_batch_dim.setter\n    def keep_batch_dim(self, keep_batch_dim: bool) -&gt; None:\n        \"\"\"Set whether to keep the batch dimension in the loss output.\n\n        When setting this we need to change the state to either a list if keeping the batch\n        dimension (so we can accumulate all the losses and concatenate them at the end along this\n        dimension). Alternatively it should be a tensor if not keeping the batch dimension (so we\n        can sum the losses over the batch dimension during update and then take the mean).\n\n        By doing this in a setter we allow changing of this setting after the metric is initialised.\n        \"\"\"\n        self._keep_batch_dim = keep_batch_dim\n        self.reset()  # Reset the metric to update the state\n        if keep_batch_dim and not isinstance(self.mse, list):\n            self.add_state(\n                \"mse\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n            self.add_state(\n                \"absolute_loss\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n        elif not isinstance(self.mse, Tensor):\n            self.add_state(\n                \"mse\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n            self.add_state(\n                \"absolute_loss\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n\n    # State\n    absolute_loss: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    mse: Float[Tensor, Axis.COMPONENT_OPTIONAL] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_components: PositiveInt = 1,\n        l1_coefficient: PositiveFloat = 0.001,\n        *,\n        keep_batch_dim: bool = False,\n    ):\n        \"\"\"Initialise the metric.\"\"\"\n        super().__init__()\n        self._num_components = num_components\n        self.keep_batch_dim = keep_batch_dim\n        self._l1_coefficient = l1_coefficient\n\n        # Add the state\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics))\n    ) -&gt; None:\n        \"\"\"Update the metric.\"\"\"\n        absolute_loss = L1AbsoluteLoss.calculate_abs_sum(learned_activations)\n        mse = L2ReconstructionLoss.calculate_mse(decoded_activations, source_activations)\n\n        if self.keep_batch_dim:\n            self.absolute_loss.append(absolute_loss)  # type: ignore\n            self.mse.append(mse)  # type: ignore\n        else:\n            self.absolute_loss += absolute_loss.sum(dim=0)\n            self.mse += mse.sum(dim=0)\n            self.num_activation_vectors += learned_activations.shape[0]\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute the metric.\"\"\"\n        l1 = (\n            torch.cat(self.absolute_loss)  # type: ignore\n            if self.keep_batch_dim\n            else self.absolute_loss / self.num_activation_vectors\n        )\n\n        l2 = (\n            torch.cat(self.mse)  # type: ignore\n            if self.keep_batch_dim\n            else self.mse / self.num_activation_vectors\n        )\n\n        return l1 * self._l1_coefficient + l2\n\n    def forward(  # type: ignore[override] (narrowing)\n        self,\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Tensor:\n        \"\"\"Forward pass.\"\"\"\n        return super().forward(\n            source_activations=source_activations,\n            learned_activations=learned_activations,\n            decoded_activations=decoded_activations,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderLoss.keep_batch_dim","title":"<code>keep_batch_dim: bool = keep_batch_dim</code>  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<p>Whether to keep the batch dimension in the loss output.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderLoss.__init__","title":"<code>__init__(num_components=1, l1_coefficient=0.001, *, keep_batch_dim=False)</code>","text":"<p>Initialise the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_components: PositiveInt = 1,\n    l1_coefficient: PositiveFloat = 0.001,\n    *,\n    keep_batch_dim: bool = False,\n):\n    \"\"\"Initialise the metric.\"\"\"\n    super().__init__()\n    self._num_components = num_components\n    self.keep_batch_dim = keep_batch_dim\n    self._l1_coefficient = l1_coefficient\n\n    # Add the state\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\"\"\"\n    l1 = (\n        torch.cat(self.absolute_loss)  # type: ignore\n        if self.keep_batch_dim\n        else self.absolute_loss / self.num_activation_vectors\n    )\n\n    l2 = (\n        torch.cat(self.mse)  # type: ignore\n        if self.keep_batch_dim\n        else self.mse / self.num_activation_vectors\n    )\n\n    return l1 * self._l1_coefficient + l2\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Forward pass.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>def forward(  # type: ignore[override] (narrowing)\n    self,\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Tensor:\n    \"\"\"Forward pass.\"\"\"\n    return super().forward(\n        source_activations=source_activations,\n        learned_activations=learned_activations,\n        decoded_activations=decoded_activations,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoderLoss.update","title":"<code>update(source_activations, learned_activations, decoded_activations, **kwargs)</code>","text":"<p>Update the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>def update(\n    self,\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics))\n) -&gt; None:\n    \"\"\"Update the metric.\"\"\"\n    absolute_loss = L1AbsoluteLoss.calculate_abs_sum(learned_activations)\n    mse = L2ReconstructionLoss.calculate_mse(decoded_activations, source_activations)\n\n    if self.keep_batch_dim:\n        self.absolute_loss.append(absolute_loss)  # type: ignore\n        self.mse.append(mse)  # type: ignore\n    else:\n        self.absolute_loss += absolute_loss.sum(dim=0)\n        self.mse += mse.sum(dim=0)\n        self.num_activation_vectors += learned_activations.shape[0]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SweepConfig","title":"<code>SweepConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>WandbSweepConfig</code></p> <p>Sweep Config.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass SweepConfig(WandbSweepConfig):\n    \"\"\"Sweep Config.\"\"\"\n\n    parameters: Hyperparameters\n\n    method: Method = Method.GRID\n\n    metric: Metric = field(default=Metric(name=\"train/loss/total_loss\"))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore","title":"<code>TensorActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Tensor Activation Store.</p> <p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=100, n_components=2)\n</code></pre> <p>Add a single activation vector to the dataset (for a component):</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n&gt;&gt;&gt; store.append(torch.randn(100), component_idx=1)\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a [batch, neurons] activation tensor to the dataset:</p> <pre><code>&gt;&gt;&gt; store.empty()\n&gt;&gt;&gt; batch = torch.randn(10, 100)\n&gt;&gt;&gt; store.extend(batch, component_idx=0)\n&gt;&gt;&gt; store.extend(batch, component_idx=1)\n&gt;&gt;&gt; len(store)\n10\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>class TensorActivationStore(ActivationStore):\n    \"\"\"Tensor Activation Store.\n\n    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation\n    vectors to be stored to be known in advance. Multiprocess safe.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=100, n_components=2)\n\n    Add a single activation vector to the dataset (for a component):\n\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=1)\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a [batch, neurons] activation tensor to the dataset:\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; store.extend(batch, component_idx=0)\n        &gt;&gt;&gt; store.extend(batch, component_idx=1)\n        &gt;&gt;&gt; len(store)\n        10\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 2, 100])\n    \"\"\"\n\n    _data: Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT, Axis.INPUT_OUTPUT_FEATURE)]\n    \"\"\"Underlying Tensor Data Store.\"\"\"\n\n    _items_stored: list[int]\n    \"\"\"Number of items stored.\"\"\"\n\n    max_items: int\n    \"\"\"Maximum Number of Items to Store.\"\"\"\n\n    _n_components: int\n    \"\"\"Number of components\"\"\"\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of components.\"\"\"\n        return self._n_components\n\n    @property\n    def current_activations_stored_per_component(self) -&gt; list[int]:\n        \"\"\"Number of activations stored per component.\"\"\"\n        return self._items_stored\n\n    @validate_call(config={\"arbitrary_types_allowed\": True})\n    def __init__(\n        self,\n        max_items: PositiveInt,\n        n_neurons: PositiveInt,\n        n_components: PositiveInt,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the Tensor Activation Store.\n\n        Args:\n            max_items: Maximum number of items to store per component (individual activation\n                vectors).\n            n_neurons: Number of neurons in each activation vector.\n            n_components: Number of components to store (i.e. number of source models).\n            device: Device to store the activation vectors on.\n        \"\"\"\n        self._n_components = n_components\n        self._items_stored = [0] * n_components\n        self._max_items = max_items\n        self._data = torch.empty((max_items, n_components, n_neurons), device=device)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors per component in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)\n            &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n            &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        # Min as this is the amount of activations that can be fetched by get_item\n        return min(self.current_activations_stored_per_component)\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)\n            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n            800\n\n        Returns:\n            The size of the underlying tensor in bytes.\n        \"\"\"\n        return self._data.element_size() * self._data.nelement()\n\n    def __getitem__(\n        self, index: tuple[int, ...] | slice | int\n    ) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n        \"\"\"Get Item Dunder Method.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)\n            &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n            &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n            &gt;&gt;&gt; store[1, 0]\n            tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)\n        &gt;&gt;&gt; store.append(torch.tensor([0.]), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.tensor([1.]), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.tensor([2.]), component_idx=0)\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; [store[i, 0].item() for i in range(3)]\n        [0.0, 2.0, 1.0]\n        \"\"\"\n        # Generate a permutation of the indices for the active data\n        perm = torch.randperm(len(self))\n\n        # Use this permutation to shuffle the active data in-place\n        self._data[: len(self)] = self._data[perm]\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE], component_idx: int) -&gt; None:\n        \"\"\"Add a single item to the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n        &gt;&gt;&gt; store[1, 0]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            item: The item to append to the dataset.\n            component_idx: The component index to append the item to.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        if self._items_stored[component_idx] + 1 &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[self._items_stored[component_idx], component_idx] = item.to(\n            self._data.device,\n        )\n        self._items_stored[component_idx] += 1\n\n    def extend(\n        self,\n        batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        component_idx: int,\n    ) -&gt; None:\n        \"\"\"Add a batch to the store.\n\n        Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n        &gt;&gt;&gt; len(store)\n        2\n\n        Args:\n            batch: The batch to append to the dataset.\n            component_idx: The component index to append the batch to.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        n_activation_tensors: int = batch.shape[0]\n        if self._items_stored[component_idx] + n_activation_tensors &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[\n            self._items_stored[component_idx] : self._items_stored[component_idx]\n            + n_activation_tensors,\n            component_idx,\n        ] = batch.to(self._data.device)\n        self._items_stored[component_idx] += n_activation_tensors\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n        &gt;&gt;&gt; len(store)\n        2\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; len(store)\n        0\n        \"\"\"\n        # We don't need to zero the data, just reset the number of items stored\n        self._items_stored = [0 for _ in self._items_stored]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.current_activations_stored_per_component","title":"<code>current_activations_stored_per_component: list[int]</code>  <code>property</code>","text":"<p>Number of activations stored per component.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.max_items","title":"<code>max_items: int</code>  <code>instance-attribute</code>","text":"<p>Maximum Number of Items to Store.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of components.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)\n&gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n&gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n&gt;&gt;&gt; store[1, 0]\ntensor([1., 1., 1., 1., 1.])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>tuple[int, ...] | slice | int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(ANY)]</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __getitem__(\n    self, index: tuple[int, ...] | slice | int\n) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n    \"\"\"Get Item Dunder Method.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n        &gt;&gt;&gt; store[1, 0]\n        tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    return self._data[index]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__init__","title":"<code>__init__(max_items, n_neurons, n_components, device=None)</code>","text":"<p>Initialise the Tensor Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>max_items</code> <code>PositiveInt</code> <p>Maximum number of items to store per component (individual activation vectors).</p> required <code>n_neurons</code> <code>PositiveInt</code> <p>Number of neurons in each activation vector.</p> required <code>n_components</code> <code>PositiveInt</code> <p>Number of components to store (i.e. number of source models).</p> required <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>@validate_call(config={\"arbitrary_types_allowed\": True})\ndef __init__(\n    self,\n    max_items: PositiveInt,\n    n_neurons: PositiveInt,\n    n_components: PositiveInt,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"Initialise the Tensor Activation Store.\n\n    Args:\n        max_items: Maximum number of items to store per component (individual activation\n            vectors).\n        n_neurons: Number of neurons in each activation vector.\n        n_components: Number of components to store (i.e. number of source models).\n        device: Device to store the activation vectors on.\n    \"\"\"\n    self._n_components = n_components\n    self._items_stored = [0] * n_components\n    self._max_items = max_items\n    self._data = torch.empty((max_items, n_components, n_neurons), device=device)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors per component in the dataset.</p> Example <p>import torch store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1) store.append(torch.randn(100), component_idx=0) store.append(torch.randn(100), component_idx=0) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors per component in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    # Min as this is the amount of activations that can be fetched by get_item\n    return min(self.current_activations_stored_per_component)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> Example <p>import torch store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1) store.sizeof() # Pre-allocated tensor of 2x100 800</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the underlying tensor in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)\n        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n        800\n\n    Returns:\n        The size of the underlying tensor in bytes.\n    \"\"\"\n    return self._data.element_size() * self._data.nelement()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.append","title":"<code>append(item, component_idx)</code>","text":"<p>Add a single item to the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1) store.append(torch.zeros(5), component_idx=0) store.append(torch.ones(5), component_idx=0) store[1, 0] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The item to append to the dataset.</p> required <code>component_idx</code> <code>int</code> <p>The component index to append the item to.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE], component_idx: int) -&gt; None:\n    \"\"\"Add a single item to the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n    &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n    &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n    &gt;&gt;&gt; store[1, 0]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        item: The item to append to the dataset.\n        component_idx: The component index to append the item to.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    if self._items_stored[component_idx] + 1 &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[self._items_stored[component_idx], component_idx] = item.to(\n        self._data.device,\n    )\n    self._items_stored[component_idx] += 1\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1) store.extend(torch.zeros(2, 5), component_idx=0) len(store) 2 store.empty() len(store) 0</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n    &gt;&gt;&gt; len(store)\n    2\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; len(store)\n    0\n    \"\"\"\n    # We don't need to zero the data, just reset the number of items stored\n    self._items_stored = [0 for _ in self._items_stored]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.extend","title":"<code>extend(batch, component_idx)</code>","text":"<p>Add a batch to the store.</p> <p>Examples:</p> <p>import torch store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1) store.extend(torch.zeros(2, 5), component_idx=0) len(store) 2</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>The batch to append to the dataset.</p> required <code>component_idx</code> <code>int</code> <p>The component index to append the batch to.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def extend(\n    self,\n    batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    component_idx: int,\n) -&gt; None:\n    \"\"\"Add a batch to the store.\n\n    Examples:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n    &gt;&gt;&gt; len(store)\n    2\n\n    Args:\n        batch: The batch to append to the dataset.\n        component_idx: The component index to append the batch to.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    n_activation_tensors: int = batch.shape[0]\n    if self._items_stored[component_idx] + n_activation_tensors &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[\n        self._items_stored[component_idx] : self._items_stored[component_idx]\n        + n_activation_tensors,\n        component_idx,\n    ] = batch.to(self._data.device)\n    self._items_stored[component_idx] += n_activation_tensors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1) store.append(torch.tensor([0.]), component_idx=0) store.append(torch.tensor([1.]), component_idx=0) store.append(torch.tensor([2.]), component_idx=0) store.shuffle() [store[i, 0].item() for i in range(3)] [0.0, 2.0, 1.0]</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)\n    &gt;&gt;&gt; store.append(torch.tensor([0.]), component_idx=0)\n    &gt;&gt;&gt; store.append(torch.tensor([1.]), component_idx=0)\n    &gt;&gt;&gt; store.append(torch.tensor([2.]), component_idx=0)\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; [store[i, 0].item() for i in range(3)]\n    [0.0, 2.0, 1.0]\n    \"\"\"\n    # Generate a permutation of the indices for the active data\n    perm = torch.randperm(len(self))\n\n    # Use this permutation to shuffle the active data in-place\n    self._data[: len(self)] = self._data[perm]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset","title":"<code>TextDataset</code>","text":"<p>             Bases: <code>SourceDataset[GenericTextDataBatch]</code></p> <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@final\nclass TextDataset(SourceDataset[GenericTextDataBatch]):\n    \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n\n    def preprocess(\n        self,\n        source_batch: GenericTextDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n        Args:\n            source_batch: A batch of source data, including 'text' with a list of strings.\n            context_size: Context size for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        prompts: list[str] = source_batch[\"text\"]\n\n        tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n        # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n        context_size_prompts = []\n        for encoding in list(tokenized_prompts[self._dataset_column_name]):  # type: ignore\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    @validate_call(config={\"arbitrary_types_allowed\": True})\n    def __init__(\n        self,\n        dataset_path: str,\n        tokenizer: PreTrainedTokenizerBase,\n        buffer_size: PositiveInt = 1000,\n        context_size: PositiveInt = 256,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        dataset_column_name: str = \"input_ids\",\n        n_processes_preprocessing: PositiveInt | None = None,\n        preprocess_batch_size: PositiveInt = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a generic text dataset from Hugging Face.\n\n        Args:\n            dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n            tokenizer: Tokenizer to process text data.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g., 'train').\n            dataset_column_name: The column name for the prompts.\n            n_processes_preprocessing: Number of processes to use for preprocessing.\n            preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        self.tokenizer = tokenizer\n\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            dataset_column_name=dataset_column_name,\n            n_processes_preprocessing=n_processes_preprocessing,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n\n    @validate_call\n    def push_to_hugging_face_hub(\n        self,\n        repo_id: str,\n        commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n        max_shard_size: str | None = None,\n        n_shards: PositiveInt = 64,\n        revision: str = \"main\",\n        *,\n        private: bool = False,\n    ) -&gt; None:\n        \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n        Motivation:\n            Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n            pre-processed dataset with others. This function allows you to do that by pushing the\n            pre-processed dataset to the Hugging Face hub.\n\n        Warning:\n            You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n            to use this.\n\n        Warning:\n            This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n            initializing the dataset).\n\n        Args:\n            repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n            commit_message: Commit message.\n            max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `n_shards`\n                is set.\n            n_shards: Number of shards to split the dataset into. A high number is recommended\n                here to allow for flexible distributed training of SAEs across nodes (where e.g.\n                each node fetches its own shard).\n            revision: Branch to push to.\n            private: Whether to save the dataset privately.\n\n        Raises:\n            TypeError: If the dataset is streamed.\n        \"\"\"\n        if isinstance(self.dataset, IterableDataset):\n            error_message = (\n                \"Cannot share a streamed dataset to Hugging Face. \"\n                \"Please use `pre_download=True` when initializing the dataset.\"\n            )\n            raise TypeError(error_message)\n\n        self.dataset.push_to_hub(\n            repo_id=repo_id,\n            commit_message=commit_message,\n            max_shard_size=max_shard_size,\n            num_shards=n_shards,\n            private=private,\n            revision=revision,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.__init__","title":"<code>__init__(dataset_path, tokenizer, buffer_size=1000, context_size=256, dataset_dir=None, dataset_files=None, dataset_split='train', dataset_column_name='input_ids', n_processes_preprocessing=None, preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a generic text dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the dataset on Hugging Face (e.g. <code>'monology/pile-uncopyright'</code>).</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer to process text data.</p> required <code>buffer_size</code> <code>PositiveInt</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>context_size</code> <code>PositiveInt</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>256</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., 'train').</p> <code>'train'</code> <code>dataset_column_name</code> <code>str</code> <p>The column name for the prompts.</p> <code>'input_ids'</code> <code>n_processes_preprocessing</code> <code>PositiveInt | None</code> <p>Number of processes to use for preprocessing.</p> <code>None</code> <code>preprocess_batch_size</code> <code>PositiveInt</code> <p>Batch size for preprocessing (tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@validate_call(config={\"arbitrary_types_allowed\": True})\ndef __init__(\n    self,\n    dataset_path: str,\n    tokenizer: PreTrainedTokenizerBase,\n    buffer_size: PositiveInt = 1000,\n    context_size: PositiveInt = 256,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    dataset_column_name: str = \"input_ids\",\n    n_processes_preprocessing: PositiveInt | None = None,\n    preprocess_batch_size: PositiveInt = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a generic text dataset from Hugging Face.\n\n    Args:\n        dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n        tokenizer: Tokenizer to process text data.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g., 'train').\n        dataset_column_name: The column name for the prompts.\n        n_processes_preprocessing: Number of processes to use for preprocessing.\n        preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    self.tokenizer = tokenizer\n\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        dataset_column_name=dataset_column_name,\n        n_processes_preprocessing=n_processes_preprocessing,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>GenericTextDataBatch</code> <p>A batch of source data, including 'text' with a list of strings.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: GenericTextDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n    Args:\n        source_batch: A batch of source data, including 'text' with a list of strings.\n        context_size: Context size for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    prompts: list[str] = source_batch[\"text\"]\n\n    tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n    # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n    context_size_prompts = []\n    for encoding in list(tokenized_prompts[self._dataset_column_name]):  # type: ignore\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.push_to_hugging_face_hub","title":"<code>push_to_hugging_face_hub(repo_id, commit_message='Upload preprocessed dataset using sparse_autoencoder.', max_shard_size=None, n_shards=64, revision='main', *, private=False)</code>","text":"<p>Share preprocessed dataset to Hugging Face hub.</p> Motivation <p>Pre-processing a dataset can be time-consuming, so it is useful to be able to share the pre-processed dataset with others. This function allows you to do that by pushing the pre-processed dataset to the Hugging Face hub.</p> Warning <p>You must be logged into HuggingFace (e.g with <code>huggingface-cli login</code> from the terminal) to use this.</p> Warning <p>This will only work if the dataset is not streamed (i.e. if <code>pre_download=True</code> when initializing the dataset).</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Hugging Face repo ID to save the dataset to (e.g. <code>username/dataset_name</code>).</p> required <code>commit_message</code> <code>str</code> <p>Commit message.</p> <code>'Upload preprocessed dataset using sparse_autoencoder.'</code> <code>max_shard_size</code> <code>str | None</code> <p>Maximum shard size (e.g. <code>'500MB'</code>). Should not be set if <code>n_shards</code> is set.</p> <code>None</code> <code>n_shards</code> <code>PositiveInt</code> <p>Number of shards to split the dataset into. A high number is recommended here to allow for flexible distributed training of SAEs across nodes (where e.g. each node fetches its own shard).</p> <code>64</code> <code>revision</code> <code>str</code> <p>Branch to push to.</p> <code>'main'</code> <code>private</code> <code>bool</code> <p>Whether to save the dataset privately.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset is streamed.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@validate_call\ndef push_to_hugging_face_hub(\n    self,\n    repo_id: str,\n    commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n    max_shard_size: str | None = None,\n    n_shards: PositiveInt = 64,\n    revision: str = \"main\",\n    *,\n    private: bool = False,\n) -&gt; None:\n    \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n    Motivation:\n        Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n        pre-processed dataset with others. This function allows you to do that by pushing the\n        pre-processed dataset to the Hugging Face hub.\n\n    Warning:\n        You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n        to use this.\n\n    Warning:\n        This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n        initializing the dataset).\n\n    Args:\n        repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n        commit_message: Commit message.\n        max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `n_shards`\n            is set.\n        n_shards: Number of shards to split the dataset into. A high number is recommended\n            here to allow for flexible distributed training of SAEs across nodes (where e.g.\n            each node fetches its own shard).\n        revision: Branch to push to.\n        private: Whether to save the dataset privately.\n\n    Raises:\n        TypeError: If the dataset is streamed.\n    \"\"\"\n    if isinstance(self.dataset, IterableDataset):\n        error_message = (\n            \"Cannot share a streamed dataset to Hugging Face. \"\n            \"Please use `pre_download=True` when initializing the dataset.\"\n        )\n        raise TypeError(error_message)\n\n    self.dataset.push_to_hub(\n        repo_id=repo_id,\n        commit_message=commit_message,\n        max_shard_size=max_shard_size,\n        num_shards=n_shards,\n        private=private,\n        revision=revision,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.sweep","title":"<code>sweep(sweep_config=None, sweep_id=None)</code>","text":"<p>Run the training pipeline with wandb hyperparameter sweep.</p> Warning <p>Either sweep_config or sweep_id must be specified, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>sweep_config</code> <code>SweepConfig | None</code> <p>The sweep configuration.</p> <code>None</code> <code>sweep_id</code> <code>str | None</code> <p>The sweep id for an existing sweep.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither sweep_config nor sweep_id is specified.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def sweep(\n    sweep_config: SweepConfig | None = None,\n    sweep_id: str | None = None,\n) -&gt; None:\n    \"\"\"Run the training pipeline with wandb hyperparameter sweep.\n\n    Warning:\n        Either sweep_config or sweep_id must be specified, but not both.\n\n    Args:\n        sweep_config: The sweep configuration.\n        sweep_id: The sweep id for an existing sweep.\n\n    Raises:\n        ValueError: If neither sweep_config nor sweep_id is specified.\n    \"\"\"\n    if sweep_id is not None:\n        wandb.agent(sweep_id, train, project=\"sparse-autoencoder\")\n\n    elif sweep_config is not None:\n        sweep_id = wandb.sweep(sweep_config.to_dict(), project=\"sparse-autoencoder\")\n        wandb.agent(sweep_id, train)\n\n    else:\n        error_message = \"Either sweep_config or sweep_id must be specified.\"\n        raise ValueError(error_message)\n\n    wandb.finish()\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>activation_resampler<ul> <li>activation_resampler</li> <li>utils<ul> <li>component_slice_tensor</li> </ul> </li> </ul> </li> <li>activation_store<ul> <li>base_store</li> <li>tensor_store</li> </ul> </li> <li>autoencoder<ul> <li>components<ul> <li>linear_encoder</li> <li>tied_bias</li> <li>unit_norm_decoder</li> </ul> </li> <li>lightning</li> <li>model</li> <li>types</li> </ul> </li> <li>metrics<ul> <li>loss<ul> <li>l1_absolute_loss</li> <li>l2_reconstruction_loss</li> <li>sae_loss</li> </ul> </li> <li>train<ul> <li>capacity</li> <li>feature_density</li> <li>l0_norm</li> <li>neuron_activity</li> <li>neuron_fired_count</li> </ul> </li> <li>validate<ul> <li>reconstruction_score</li> </ul> </li> <li>wrappers<ul> <li>classwise</li> </ul> </li> </ul> </li> <li>optimizer<ul> <li>adam_with_reset</li> </ul> </li> <li>source_data<ul> <li>abstract_dataset</li> <li>mock_dataset</li> <li>pretokenized_dataset</li> <li>text_dataset</li> </ul> </li> <li>source_model<ul> <li>replace_activations_hook</li> <li>reshape_activations</li> <li>store_activations_hook</li> <li>zero_ablate_hook</li> </ul> </li> <li>tensor_types</li> <li>train<ul> <li>join_sweep</li> <li>pipeline</li> <li>sweep</li> <li>sweep_config</li> <li>utils<ul> <li>get_model_device</li> <li>round_down</li> <li>wandb_sweep_types</li> </ul> </li> </ul> </li> <li>training_runs<ul> <li>gpt2</li> </ul> </li> <li>utils<ul> <li>data_parallel</li> <li>tensor_shape</li> </ul> </li> </ul>"},{"location":"reference/tensor_types/","title":"Tensor Axis Types","text":"<p>Tensor Axis Types.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis","title":"<code>Axis</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Tensor axis names.</p> <p>Used to annotate tensor types.</p> Example <p>When used directly it prints a string:</p> <p>print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature</p> <p>The primary use is to annotate tensor types:</p> <p>from jaxtyping import Float from torch import Tensor from typing import TypeAlias batch: TypeAlias = Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] print(batch)  <p>You can also join multiple axis together to represent the dimensions of a tensor:</p> <p>print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature</p> Source code in <code>sparse_autoencoder/tensor_types.py</code> <pre><code>class Axis(LowercaseStrEnum):\n    \"\"\"Tensor axis names.\n\n    Used to annotate tensor types.\n\n    Example:\n        When used directly it prints a string:\n\n        &gt;&gt;&gt; print(Axis.INPUT_OUTPUT_FEATURE)\n        input_output_feature\n\n        The primary use is to annotate tensor types:\n\n        &gt;&gt;&gt; from jaxtyping import Float\n        &gt;&gt;&gt; from torch import Tensor\n        &gt;&gt;&gt; from typing import TypeAlias\n        &gt;&gt;&gt; batch: TypeAlias = Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n        &gt;&gt;&gt; print(batch)\n        &lt;class 'jaxtyping.Float[Tensor, 'batch input_output_feature']'&gt;\n\n        You can also join multiple axis together to represent the dimensions of a tensor:\n\n        &gt;&gt;&gt; print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n        batch input_output_feature\n    \"\"\"\n\n    # Component idx\n    COMPONENT = auto()\n    \"\"\"Component index.\"\"\"\n\n    COMPONENT_OPTIONAL = \"*component\"\n    \"\"\"Optional component index.\"\"\"\n\n    # Batches\n    SOURCE_DATA_BATCH = auto()\n    \"\"\"Batch of prompts used to generate source model activations.\"\"\"\n\n    BATCH = auto()\n    \"\"\"Batch of items that the SAE is being trained on.\"\"\"\n\n    STORE_BATCH = auto()\n    \"\"\"Batch of items to be written to the store.\"\"\"\n\n    ITEMS = auto()\n    \"\"\"Arbitrary number of items.\"\"\"\n\n    # Features\n    INPUT_OUTPUT_FEATURE = auto()\n    \"\"\"Input or output feature (e.g. feature in activation vector from source model).\"\"\"\n\n    LEARNT_FEATURE = auto()\n    \"\"\"Learn feature (e.g. feature in learnt activation vector).\"\"\"\n\n    DEAD_FEATURE = auto()\n    \"\"\"Dead feature.\"\"\"\n\n    ALIVE_FEATURE = auto()\n    \"\"\"Alive feature.\"\"\"\n\n    # Feature indices\n    INPUT_OUTPUT_FEATURE_IDX = auto()\n    \"\"\"Input or output feature index.\"\"\"\n\n    LEARNT_FEATURE_IDX = auto()\n    \"\"\"Learn feature index.\"\"\"\n\n    # Other\n    POSITION = auto()\n    \"\"\"Token position.\"\"\"\n\n    SINGLE_ITEM = \"\"\n    \"\"\"Single item axis.\"\"\"\n\n    ANY = \"...\"\n    \"\"\"Any number of axis.\"\"\"\n\n    @staticmethod\n    def names(*axis: \"Axis\") -&gt; str:\n        \"\"\"Join multiple axis together, to represent the dimensions of a tensor.\n\n        Example:\n            &gt;&gt;&gt; print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n            batch input_output_feature\n\n        Args:\n            *axis: Axis to join.\n\n        Returns:\n            Joined axis string.\n        \"\"\"\n        return \" \".join(a.value for a in axis)\n</code></pre>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ALIVE_FEATURE","title":"<code>ALIVE_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Alive feature.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ANY","title":"<code>ANY = '...'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Any number of axis.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH","title":"<code>BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of items that the SAE is being trained on.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT","title":"<code>COMPONENT = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Component index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL","title":"<code>COMPONENT_OPTIONAL = '*component'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional component index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE","title":"<code>DEAD_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead feature.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE","title":"<code>INPUT_OUTPUT_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Input or output feature (e.g. feature in activation vector from source model).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE_IDX","title":"<code>INPUT_OUTPUT_FEATURE_IDX = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Input or output feature index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ITEMS","title":"<code>ITEMS = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrary number of items.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE","title":"<code>LEARNT_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learn feature (e.g. feature in learnt activation vector).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE_IDX","title":"<code>LEARNT_FEATURE_IDX = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learn feature index.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.POSITION","title":"<code>POSITION = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Token position.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SINGLE_ITEM","title":"<code>SINGLE_ITEM = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single item axis.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SOURCE_DATA_BATCH","title":"<code>SOURCE_DATA_BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of prompts used to generate source model activations.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.STORE_BATCH","title":"<code>STORE_BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of items to be written to the store.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.names","title":"<code>names(*axis)</code>  <code>staticmethod</code>","text":"<p>Join multiple axis together, to represent the dimensions of a tensor.</p> Example <p>print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature</p> <p>Parameters:</p> Name Type Description Default <code>*axis</code> <code>Axis</code> <p>Axis to join.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>Joined axis string.</p> Source code in <code>sparse_autoencoder/tensor_types.py</code> <pre><code>@staticmethod\ndef names(*axis: \"Axis\") -&gt; str:\n    \"\"\"Join multiple axis together, to represent the dimensions of a tensor.\n\n    Example:\n        &gt;&gt;&gt; print(Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n        batch input_output_feature\n\n    Args:\n        *axis: Axis to join.\n\n    Returns:\n        Joined axis string.\n    \"\"\"\n    return \" \".join(a.value for a in axis)\n</code></pre>"},{"location":"reference/activation_resampler/","title":"Activation Resampler","text":"<p>Activation Resampler.</p>"},{"location":"reference/activation_resampler/activation_resampler/","title":"Activation resampler","text":"<p>Activation resampler.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler","title":"<code>ActivationResampler</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Activation resampler.</p> <p>Collates the number of times each neuron fires over a set number of learned activation vectors, and then provides the parameters necessary to reset any dead neurons.</p> Motivation <p>Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.</p> <p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.</p> <p>This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network.</p> Warning <p>The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases.</p> Warning <p>This approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>class ActivationResampler(Metric):\n    \"\"\"Activation resampler.\n\n    Collates the number of times each neuron fires over a set number of learned activation vectors,\n    and then provides the parameters necessary to reset any dead neurons.\n\n    Motivation:\n        Over the course of training, a subset of autoencoder neurons will have zero activity across\n        a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language\n        Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training\n        improves the number of likely-interpretable features (i.e., those in the high density\n        cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket\n        Hypothesis and increase the number of chances the network has to find promising feature\n        directions.\n\n        An interesting nuance around dead neurons involves the ultralow density cluster. They found\n        that if we increase the number of training steps then networks will kill off more of these\n        ultralow density neurons. This reinforces the use of the high density cluster as a useful\n        metric because there can exist neurons that are de facto dead but will not appear to be when\n        looking at the number of dead neurons alone.\n\n        This approach is designed to seed new features to fit inputs where the current autoencoder\n        performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled\n        neuron will only fire weakly for inputs similar to the one used for its reinitialization.\n        This was done to minimize interference with the rest of the network.\n\n    Warning:\n        The optimizer should be reset after applying this function, as the Adam state will be\n        incorrect for the modified weights and biases.\n\n    Warning:\n        This approach is also known to create sudden loss spikes, and resampling too frequently\n        causes training to diverge.\n    \"\"\"\n\n    # Collated data from the train loop\n    _neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT, Axis.LEARNT_FEATURE)]\n    _loss: list[Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL)]] | Float[\n        Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL)\n    ]\n    _input_activations: list[\n        Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]\n    ] | Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]\n\n    # Tracking\n    _n_activations_seen_process: int\n    _n_times_resampled: int\n\n    # Settings\n    _n_components: int\n    _threshold_is_dead_portion_fires: float\n    _max_n_resamples: int\n    resample_interval: int\n    resample_interval_process: int\n    start_collecting_neuron_activity_process: int\n    start_collecting_loss_process: int\n\n    @validate_call\n    def __init__(\n        self,\n        n_learned_features: PositiveInt,\n        n_components: NonNegativeInt = 1,\n        resample_interval: PositiveInt = 200_000_000,\n        max_n_resamples: NonNegativeInt = 4,\n        n_activations_activity_collate: PositiveInt = 100_000_000,\n        resample_dataset_size: PositiveInt = 819_200,\n        threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n    ) -&gt; None:\n        r\"\"\"Initialize the activation resampler.\n\n        Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n        Args:\n            n_learned_features: Number of learned features\n            n_components: Number of components that the SAE is being trained on.\n            resample_interval: Interval in number of autoencoder input activation vectors trained\n                on, before resampling.\n            max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n                Set to inf if you want to have no limit.\n            n_activations_activity_collate: Number of autoencoder learned activation vectors to\n                collate before resampling (the activation resampler will start collecting on vector\n                $\\text{resample_interval} - \\text{n_steps_collate}$).\n            resample_dataset_size: Number of autoencoder input activations to use for calculating\n                the loss, as part of the resampling process to create the reset neuron weights.\n            threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n                \"fired\" in less than this portion of the collated sample).\n\n        Raises:\n            ValueError: If any of the arguments are invalid (e.g. negative integers).\n        \"\"\"\n        super().__init__(\n            sync_on_compute=False  # Manually sync instead in compute, where needed\n        )\n\n        # Error handling\n        if n_activations_activity_collate &gt; resample_interval:\n            error_message = \"Must collate less activation activity than the resample interval.\"\n            raise ValueError(error_message)\n\n        # Number of processes\n        world_size = (\n            get_world_size(group.WORLD)\n            if distributed.is_available() and distributed.is_initialized()\n            else 1\n        )\n        process_resample_dataset_size = resample_dataset_size // world_size\n\n        # State setup (note half precision is used as it's sufficient for resampling purposes)\n        self.add_state(\n            \"_neuron_fired_count\",\n            torch.zeros((n_components, n_learned_features)),\n            \"sum\",\n        )\n        self.add_state(\"_loss\", [], \"cat\")\n        self.add_state(\"_input_activations\", [], \"cat\")\n\n        # Tracking\n        self._n_activations_seen_process = 0\n        self._n_times_resampled = 0\n\n        # Settings\n        self._n_components = n_components\n        self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n        self._max_n_resamples = max_n_resamples\n        self.resample_interval = resample_interval\n        self.resample_interval_process = resample_interval // world_size\n        self.start_collecting_neuron_activity_process = (\n            self.resample_interval_process - n_activations_activity_collate // world_size\n        )\n        self.start_collecting_loss_process = (\n            self.resample_interval_process - process_resample_dataset_size\n        )\n\n    def update(\n        self,\n        input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n        encoder_weight_reference: Parameter,\n    ) -&gt; None:\n        \"\"\"Update the collated data from forward passes.\n\n        Args:\n            input_activations: Input activations to the SAE.\n            learned_activations: Learned activations from the SAE.\n            loss: Loss per input activation.\n            encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n        Raises:\n            TypeError: If the loss or input activations are not lists (e.g. from unsync having not\n                been called).\n        \"\"\"\n        if self._n_activations_seen_process &gt;= self.start_collecting_neuron_activity_process:\n            neuron_has_fired: Bool[\n                Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n            ] = torch.gt(learned_activations, 0)\n            self._neuron_fired_count += neuron_has_fired.sum(dim=0)\n\n        if self._n_activations_seen_process &gt;= self.start_collecting_loss_process:\n            # Typecast\n            if not isinstance(self._loss, list) or not isinstance(self._input_activations, list):\n                raise TypeError\n\n            # Append\n            self._loss.append(loss)\n            self._input_activations.append(input_activations)\n\n        self._n_activations_seen_process += len(learned_activations)\n        self._encoder_weight = encoder_weight_reference\n\n    def _get_dead_neuron_indices(\n        self,\n    ) -&gt; list[Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)]]:\n        \"\"\"Identify the indices of neurons that are dead.\n\n        Identifies any neurons that have fired less than the threshold portion of the collated\n        sample size.\n\n        Returns:\n            List of dead neuron indices for each component.\n\n        Raises:\n            ValueError: If no neuron activity has been collated yet.\n        \"\"\"\n        # Check we have already collated some neuron activity\n        if torch.all(self._neuron_fired_count == 0):\n            error_message = \"Cannot get dead neuron indices without neuron activity.\"\n            raise ValueError(error_message)\n\n        # Find any neurons that fire less than the threshold portion of times\n        threshold_is_dead_n_fires: int = int(\n            self.resample_interval * self._threshold_is_dead_portion_fires\n        )\n\n        return [\n            torch.where(self._neuron_fired_count[component_idx] &lt;= threshold_is_dead_n_fires)[0].to(\n                dtype=torch.int\n            )\n            for component_idx in range(self._n_components)\n        ]\n\n    @staticmethod\n    def assign_sampling_probabilities(\n        loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Assign the sampling probabilities for each input activations vector.\n\n        Assign each input vector a probability of being picked that is proportional to the square of\n        the autoencoder's loss on that input.\n\n        Examples:\n            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n            tensor([0.0700, 0.2900, 0.6400])\n\n            &gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n            tensor([[0.0700, 0.0700],\n                    [0.2900, 0.2900],\n                    [0.6400, 0.6400]])\n\n        Args:\n            loss: Loss per item.\n\n        Returns:\n            A tensor of probabilities for each item.\n        \"\"\"\n        square_loss = loss.pow(2)\n        return square_loss / square_loss.sum(0)\n\n    @staticmethod\n    def sample_input(\n        probabilities: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n        input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        n_samples: list[int],\n    ) -&gt; list[Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]]:\n        \"\"\"Sample an input vector based on the provided probabilities.\n\n        Example:\n            &gt;&gt;&gt; probabilities = torch.tensor([[0.1], [0.2], [0.7]])\n            &gt;&gt;&gt; input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n            ...     probabilities, input_activations, [2]\n            ... )\n            &gt;&gt;&gt; sampled_input[0].tolist()\n            [[5.0, 6.0], [3.0, 4.0]]\n\n        Args:\n            probabilities: Probabilities for each input.\n            input_activations: Input activation vectors.\n            n_samples: Number of samples to take (number of dead neurons).\n\n        Returns:\n            Sampled input activation vector.\n\n        Raises:\n            ValueError: If the number of samples is greater than the number of input activations.\n        \"\"\"\n        sampled_inputs: list[\n            Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n        ] = []\n\n        for component_idx, component_n_samples in enumerate(n_samples):\n            component_probabilities: Float[Tensor, Axis.BATCH] = get_component_slice_tensor(\n                input_tensor=probabilities,\n                n_dim_with_component=2,\n                component_dim=1,\n                component_idx=component_idx,\n            )\n\n            component_input_activations: Float[\n                Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n            ] = get_component_slice_tensor(\n                input_tensor=input_activations,\n                n_dim_with_component=3,\n                component_dim=1,\n                component_idx=component_idx,\n            )\n\n            if component_n_samples &gt; len(component_input_activations):\n                exception_message = (\n                    f\"Cannot sample {component_n_samples} inputs from \"\n                    f\"{len(component_input_activations)} input activations.\"\n                )\n                raise ValueError(exception_message)\n\n            # Handle the 0 dead neurons case\n            if component_n_samples == 0:\n                sampled_inputs.append(\n                    torch.empty(\n                        (0, component_input_activations.shape[-1]),\n                        dtype=component_input_activations.dtype,\n                        device=component_input_activations.device,\n                    )\n                )\n                continue\n\n            # Handle the 1+ dead neuron case\n            component_sample_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n                component_probabilities, num_samples=component_n_samples\n            )\n            sampled_inputs.append(component_input_activations[component_sample_indices, :])\n\n        return sampled_inputs\n\n    @staticmethod\n    def renormalize_and_scale(\n        sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n        neuron_activity: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE)],\n        encoder_weight: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n    ) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n        Renormalize the input vector to equal the average norm of the encoder weights for alive\n        neurons times 0.2.\n\n        Example:\n            &gt;&gt;&gt; from torch.nn import Parameter\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n            &gt;&gt;&gt; neuron_activity = torch.tensor([3.0, 0, 5, 0, 1, 3])\n            &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n            ...     sampled_input,\n            ...     neuron_activity,\n            ...     encoder_weight\n            ... )\n            &gt;&gt;&gt; rescaled_input.round(decimals=1)\n            tensor([[0.2000, 0.2000]])\n\n        Args:\n            sampled_input: Tensor of the sampled input activation.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            encoder_weight: Tensor of encoder weights.\n\n        Returns:\n            Rescaled sampled input.\n\n        Raises:\n            ValueError: If there are no alive neurons.\n        \"\"\"\n        alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n        # Check there is at least one alive neuron\n        if not torch.any(alive_neuron_mask):\n            error_message = \"No alive neurons found.\"\n            raise ValueError(error_message)\n\n        # Handle no dead neurons\n        n_dead_neurons = len(sampled_input)\n        if n_dead_neurons == 0:\n            return torch.empty(\n                (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n            )\n\n        # Calculate the average norm of the encoder weights for alive neurons.\n        detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n        alive_encoder_weights: Float[\n            Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = detached_encoder_weight[alive_neuron_mask, :]\n        average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n            dim=-1\n        ).mean()\n\n        # Renormalize the input vector to equal the average norm of the encoder weights for alive\n        # neurons times 0.2.\n        renormalized_input: Float[\n            Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n        ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n        return renormalized_input * (average_alive_norm * 0.2)\n\n    def compute(self) -&gt; list[ParameterUpdateResults] | None:\n        \"\"\"Compute the parameters that need to be updated.\n\n        Returns:\n            A list of parameter update results (for each component that the SAE is being trained\n            on), if an update is needed.\n        \"\"\"\n        # Resample if needed\n        if self._n_activations_seen_process &gt;= self.resample_interval_process:\n            with torch.no_grad():\n                # Initialise results\n                parameter_update_results: list[ParameterUpdateResults] = []\n\n                # Sync &amp; typecast\n                self.sync()\n                loss = dim_zero_cat(self._loss)\n                input_activations = dim_zero_cat(self._input_activations)\n\n                dead_neuron_indices: list[\n                    Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)]\n                ] = self._get_dead_neuron_indices()\n\n                # Assign each input vector a probability of being picked that is proportional to the\n                # square of the autoencoder's loss on that input.\n                sample_probabilities: Float[\n                    Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n                ] = self.assign_sampling_probabilities(loss)\n\n                # For each dead neuron sample an input according to these probabilities.\n                sampled_input: list[\n                    Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n                ] = self.sample_input(\n                    sample_probabilities,\n                    input_activations,\n                    [len(dead) for dead in dead_neuron_indices],\n                )\n\n                for component_idx in range(self._n_components):\n                    # Renormalize each input vector to have unit L2 norm and set this to be the\n                    # dictionary vector for the dead autoencoder neuron.\n                    renormalized_input: Float[\n                        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                    ] = torch.nn.functional.normalize(sampled_input[component_idx], dim=-1)\n\n                    dead_decoder_weight_updates = rearrange(\n                        renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n                    )\n\n                    # For the corresponding encoder vector, renormalize the input vector to equal\n                    # the average norm of the encoder weights for alive neurons times 0.2. Set the\n                    # corresponding encoder bias element to zero.\n                    encoder_weight: Float[\n                        Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                    ] = get_component_slice_tensor(self._encoder_weight, 3, 0, component_idx)\n\n                    rescaled_sampled_input = self.renormalize_and_scale(\n                        sampled_input=sampled_input[component_idx],\n                        neuron_activity=self._neuron_fired_count[component_idx],\n                        encoder_weight=encoder_weight,\n                    )\n\n                    dead_encoder_bias_updates = torch.zeros_like(\n                        dead_neuron_indices[component_idx],\n                        dtype=dead_decoder_weight_updates.dtype,\n                        device=dead_decoder_weight_updates.device,\n                    )\n\n                    parameter_update_results.append(\n                        ParameterUpdateResults(\n                            dead_neuron_indices=dead_neuron_indices[component_idx],\n                            dead_encoder_weight_updates=rescaled_sampled_input,\n                            dead_encoder_bias_updates=dead_encoder_bias_updates,\n                            dead_decoder_weight_updates=dead_decoder_weight_updates,\n                        )\n                    )\n\n                # Reset\n                self.unsync(should_unsync=self._is_synced)\n                self.reset()\n\n                return parameter_update_results\n\n        return None\n\n    def forward(  # type: ignore[override]\n        self,\n        input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n        encoder_weight_reference: Parameter,\n    ) -&gt; list[ParameterUpdateResults] | None:\n        \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n        Args:\n            input_activations: Input activations to the SAE.\n            learned_activations: Learned activations from the SAE.\n            loss: Loss per input activation.\n            encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n        Returns:\n            Parameter update results (for each component that the SAE is being trained on) if\n            resampling is due. Otherwise None.\n        \"\"\"\n        # Don't do anything if we have already completed all resamples\n        if self._n_times_resampled &gt;= self._max_n_resamples:\n            return None\n\n        self.update(\n            input_activations=input_activations,\n            learned_activations=learned_activations,\n            loss=loss,\n            encoder_weight_reference=encoder_weight_reference,\n        )\n\n        return self.compute()\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the activation resampler.\n\n        Warning:\n            This is only called when forward/compute has returned parameters to update (i.e.\n            resampling is due).\n        \"\"\"\n        self._n_activations_seen_process = 0\n        self._neuron_fired_count = torch.zeros_like(self._neuron_fired_count)\n        self._loss = []\n        self._input_activations = []\n        self._n_times_resampled += 1\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.__init__","title":"<code>__init__(n_learned_features, n_components=1, resample_interval=200000000, max_n_resamples=4, n_activations_activity_collate=100000000, resample_dataset_size=819200, threshold_is_dead_portion_fires=0.0)</code>","text":"<p>Initialize the activation resampler.</p> <p>Defaults to values used in the Anthropic Towards Monosemanticity paper.</p> <p>Parameters:</p> Name Type Description Default <code>n_learned_features</code> <code>PositiveInt</code> <p>Number of learned features</p> required <code>n_components</code> <code>NonNegativeInt</code> <p>Number of components that the SAE is being trained on.</p> <code>1</code> <code>resample_interval</code> <code>PositiveInt</code> <p>Interval in number of autoencoder input activation vectors trained on, before resampling.</p> <code>200000000</code> <code>max_n_resamples</code> <code>NonNegativeInt</code> <p>Maximum number of resamples to perform throughout the entire pipeline. Set to inf if you want to have no limit.</p> <code>4</code> <code>n_activations_activity_collate</code> <code>PositiveInt</code> <p>Number of autoencoder learned activation vectors to collate before resampling (the activation resampler will start collecting on vector \\(\\text{resample_interval} - \\text{n_steps_collate}\\)).</p> <code>100000000</code> <code>resample_dataset_size</code> <code>PositiveInt</code> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p> <code>819200</code> <code>threshold_is_dead_portion_fires</code> <code>Annotated[float, Field(strict=True, ge=0, le=1)]</code> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the arguments are invalid (e.g. negative integers).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    n_learned_features: PositiveInt,\n    n_components: NonNegativeInt = 1,\n    resample_interval: PositiveInt = 200_000_000,\n    max_n_resamples: NonNegativeInt = 4,\n    n_activations_activity_collate: PositiveInt = 100_000_000,\n    resample_dataset_size: PositiveInt = 819_200,\n    threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n) -&gt; None:\n    r\"\"\"Initialize the activation resampler.\n\n    Defaults to values used in the Anthropic Towards Monosemanticity paper.\n\n    Args:\n        n_learned_features: Number of learned features\n        n_components: Number of components that the SAE is being trained on.\n        resample_interval: Interval in number of autoencoder input activation vectors trained\n            on, before resampling.\n        max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.\n            Set to inf if you want to have no limit.\n        n_activations_activity_collate: Number of autoencoder learned activation vectors to\n            collate before resampling (the activation resampler will start collecting on vector\n            $\\text{resample_interval} - \\text{n_steps_collate}$).\n        resample_dataset_size: Number of autoencoder input activations to use for calculating\n            the loss, as part of the resampling process to create the reset neuron weights.\n        threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has\n            \"fired\" in less than this portion of the collated sample).\n\n    Raises:\n        ValueError: If any of the arguments are invalid (e.g. negative integers).\n    \"\"\"\n    super().__init__(\n        sync_on_compute=False  # Manually sync instead in compute, where needed\n    )\n\n    # Error handling\n    if n_activations_activity_collate &gt; resample_interval:\n        error_message = \"Must collate less activation activity than the resample interval.\"\n        raise ValueError(error_message)\n\n    # Number of processes\n    world_size = (\n        get_world_size(group.WORLD)\n        if distributed.is_available() and distributed.is_initialized()\n        else 1\n    )\n    process_resample_dataset_size = resample_dataset_size // world_size\n\n    # State setup (note half precision is used as it's sufficient for resampling purposes)\n    self.add_state(\n        \"_neuron_fired_count\",\n        torch.zeros((n_components, n_learned_features)),\n        \"sum\",\n    )\n    self.add_state(\"_loss\", [], \"cat\")\n    self.add_state(\"_input_activations\", [], \"cat\")\n\n    # Tracking\n    self._n_activations_seen_process = 0\n    self._n_times_resampled = 0\n\n    # Settings\n    self._n_components = n_components\n    self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n    self._max_n_resamples = max_n_resamples\n    self.resample_interval = resample_interval\n    self.resample_interval_process = resample_interval // world_size\n    self.start_collecting_neuron_activity_process = (\n        self.resample_interval_process - n_activations_activity_collate // world_size\n    )\n    self.start_collecting_loss_process = (\n        self.resample_interval_process - process_resample_dataset_size\n    )\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.assign_sampling_probabilities","title":"<code>assign_sampling_probabilities(loss)</code>  <code>staticmethod</code>","text":"<p>Assign the sampling probabilities for each input activations vector.</p> <p>Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n&gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\ntensor([0.0700, 0.2900, 0.6400])\n</code></pre> <pre><code>&gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])\n&gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\ntensor([[0.0700, 0.0700],\n        [0.2900, 0.2900],\n        [0.6400, 0.6400]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Loss per item.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>A tensor of probabilities for each item.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef assign_sampling_probabilities(\n    loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Assign the sampling probabilities for each input activations vector.\n\n    Assign each input vector a probability of being picked that is proportional to the square of\n    the autoencoder's loss on that input.\n\n    Examples:\n        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n        tensor([0.0700, 0.2900, 0.6400])\n\n        &gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)\n        tensor([[0.0700, 0.0700],\n                [0.2900, 0.2900],\n                [0.6400, 0.6400]])\n\n    Args:\n        loss: Loss per item.\n\n    Returns:\n        A tensor of probabilities for each item.\n    \"\"\"\n    square_loss = loss.pow(2)\n    return square_loss / square_loss.sum(0)\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.compute","title":"<code>compute()</code>","text":"<p>Compute the parameters that need to be updated.</p> <p>Returns:</p> Type Description <code>list[ParameterUpdateResults] | None</code> <p>A list of parameter update results (for each component that the SAE is being trained</p> <code>list[ParameterUpdateResults] | None</code> <p>on), if an update is needed.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def compute(self) -&gt; list[ParameterUpdateResults] | None:\n    \"\"\"Compute the parameters that need to be updated.\n\n    Returns:\n        A list of parameter update results (for each component that the SAE is being trained\n        on), if an update is needed.\n    \"\"\"\n    # Resample if needed\n    if self._n_activations_seen_process &gt;= self.resample_interval_process:\n        with torch.no_grad():\n            # Initialise results\n            parameter_update_results: list[ParameterUpdateResults] = []\n\n            # Sync &amp; typecast\n            self.sync()\n            loss = dim_zero_cat(self._loss)\n            input_activations = dim_zero_cat(self._input_activations)\n\n            dead_neuron_indices: list[\n                Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)]\n            ] = self._get_dead_neuron_indices()\n\n            # Assign each input vector a probability of being picked that is proportional to the\n            # square of the autoencoder's loss on that input.\n            sample_probabilities: Float[\n                Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n            ] = self.assign_sampling_probabilities(loss)\n\n            # For each dead neuron sample an input according to these probabilities.\n            sampled_input: list[\n                Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n            ] = self.sample_input(\n                sample_probabilities,\n                input_activations,\n                [len(dead) for dead in dead_neuron_indices],\n            )\n\n            for component_idx in range(self._n_components):\n                # Renormalize each input vector to have unit L2 norm and set this to be the\n                # dictionary vector for the dead autoencoder neuron.\n                renormalized_input: Float[\n                    Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                ] = torch.nn.functional.normalize(sampled_input[component_idx], dim=-1)\n\n                dead_decoder_weight_updates = rearrange(\n                    renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n                )\n\n                # For the corresponding encoder vector, renormalize the input vector to equal\n                # the average norm of the encoder weights for alive neurons times 0.2. Set the\n                # corresponding encoder bias element to zero.\n                encoder_weight: Float[\n                    Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n                ] = get_component_slice_tensor(self._encoder_weight, 3, 0, component_idx)\n\n                rescaled_sampled_input = self.renormalize_and_scale(\n                    sampled_input=sampled_input[component_idx],\n                    neuron_activity=self._neuron_fired_count[component_idx],\n                    encoder_weight=encoder_weight,\n                )\n\n                dead_encoder_bias_updates = torch.zeros_like(\n                    dead_neuron_indices[component_idx],\n                    dtype=dead_decoder_weight_updates.dtype,\n                    device=dead_decoder_weight_updates.device,\n                )\n\n                parameter_update_results.append(\n                    ParameterUpdateResults(\n                        dead_neuron_indices=dead_neuron_indices[component_idx],\n                        dead_encoder_weight_updates=rescaled_sampled_input,\n                        dead_encoder_bias_updates=dead_encoder_bias_updates,\n                        dead_decoder_weight_updates=dead_decoder_weight_updates,\n                    )\n                )\n\n            # Reset\n            self.unsync(should_unsync=self._is_synced)\n            self.reset()\n\n            return parameter_update_results\n\n    return None\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.forward","title":"<code>forward(input_activations, learned_activations, loss, encoder_weight_reference)</code>","text":"<p>Step the resampler, collating neuron activity and resampling if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>input_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations to the SAE.</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations from the SAE.</p> required <code>loss</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Loss per input activation.</p> required <code>encoder_weight_reference</code> <code>Parameter</code> <p>Reference to the SAE encoder weight tensor.</p> required <p>Returns:</p> Type Description <code>list[ParameterUpdateResults] | None</code> <p>Parameter update results (for each component that the SAE is being trained on) if</p> <code>list[ParameterUpdateResults] | None</code> <p>resampling is due. Otherwise None.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def forward(  # type: ignore[override]\n    self,\n    input_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    encoder_weight_reference: Parameter,\n) -&gt; list[ParameterUpdateResults] | None:\n    \"\"\"Step the resampler, collating neuron activity and resampling if necessary.\n\n    Args:\n        input_activations: Input activations to the SAE.\n        learned_activations: Learned activations from the SAE.\n        loss: Loss per input activation.\n        encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n    Returns:\n        Parameter update results (for each component that the SAE is being trained on) if\n        resampling is due. Otherwise None.\n    \"\"\"\n    # Don't do anything if we have already completed all resamples\n    if self._n_times_resampled &gt;= self._max_n_resamples:\n        return None\n\n    self.update(\n        input_activations=input_activations,\n        learned_activations=learned_activations,\n        loss=loss,\n        encoder_weight_reference=encoder_weight_reference,\n    )\n\n    return self.compute()\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.renormalize_and_scale","title":"<code>renormalize_and_scale(sampled_input, neuron_activity, encoder_weight)</code>  <code>staticmethod</code>","text":"<p>Renormalize and scale the resampled dictionary vectors.</p> <p>Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2.</p> Example <p>from torch.nn import Parameter _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3.0, 0, 5, 0, 1, 3]) encoder_weight = Parameter(torch.ones((6, 2))) rescaled_input = ActivationResampler.renormalize_and_scale( ...     sampled_input, ...     neuron_activity, ...     encoder_weight ... ) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]])</p> <p>Parameters:</p> Name Type Description Default <code>sampled_input</code> <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of the sampled input activation.</p> required <code>neuron_activity</code> <code>Float[Tensor, names(LEARNT_FEATURE)]</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>encoder_weight</code> <code>Float[Tensor, names(LEARNT_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Tensor of encoder weights.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]</code> <p>Rescaled sampled input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no alive neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef renormalize_and_scale(\n    sampled_input: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n    neuron_activity: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE)],\n    encoder_weight: Float[Tensor, Axis.names(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)],\n) -&gt; Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n    Renormalize the input vector to equal the average norm of the encoder weights for alive\n    neurons times 0.2.\n\n    Example:\n        &gt;&gt;&gt; from torch.nn import Parameter\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n        &gt;&gt;&gt; neuron_activity = torch.tensor([3.0, 0, 5, 0, 1, 3])\n        &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))\n        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n        ...     sampled_input,\n        ...     neuron_activity,\n        ...     encoder_weight\n        ... )\n        &gt;&gt;&gt; rescaled_input.round(decimals=1)\n        tensor([[0.2000, 0.2000]])\n\n    Args:\n        sampled_input: Tensor of the sampled input activation.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        encoder_weight: Tensor of encoder weights.\n\n    Returns:\n        Rescaled sampled input.\n\n    Raises:\n        ValueError: If there are no alive neurons.\n    \"\"\"\n    alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n    # Check there is at least one alive neuron\n    if not torch.any(alive_neuron_mask):\n        error_message = \"No alive neurons found.\"\n        raise ValueError(error_message)\n\n    # Handle no dead neurons\n    n_dead_neurons = len(sampled_input)\n    if n_dead_neurons == 0:\n        return torch.empty(\n            (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n        )\n\n    # Calculate the average norm of the encoder weights for alive neurons.\n    detached_encoder_weight = encoder_weight.detach()  # Don't track gradients\n    alive_encoder_weights: Float[\n        Tensor, Axis.names(Axis.ALIVE_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = detached_encoder_weight[alive_neuron_mask, :]\n    average_alive_norm: Float[Tensor, Axis.SINGLE_ITEM] = alive_encoder_weights.norm(\n        dim=-1\n    ).mean()\n\n    # Renormalize the input vector to equal the average norm of the encoder weights for alive\n    # neurons times 0.2.\n    renormalized_input: Float[\n        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ] = torch.nn.functional.normalize(sampled_input, dim=-1)\n    return renormalized_input * (average_alive_norm * 0.2)\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.reset","title":"<code>reset()</code>","text":"<p>Reset the activation resampler.</p> Warning <p>This is only called when forward/compute has returned parameters to update (i.e. resampling is due).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the activation resampler.\n\n    Warning:\n        This is only called when forward/compute has returned parameters to update (i.e.\n        resampling is due).\n    \"\"\"\n    self._n_activations_seen_process = 0\n    self._neuron_fired_count = torch.zeros_like(self._neuron_fired_count)\n    self._loss = []\n    self._input_activations = []\n    self._n_times_resampled += 1\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.sample_input","title":"<code>sample_input(probabilities, input_activations, n_samples)</code>  <code>staticmethod</code>","text":"<p>Sample an input vector based on the provided probabilities.</p> Example <p>probabilities = torch.tensor([[0.1], [0.2], [0.7]]) input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]]) _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = ActivationResampler.sample_input( ...     probabilities, input_activations, [2] ... ) sampled_input[0].tolist() [[5.0, 6.0], [3.0, 4.0]]</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Probabilities for each input.</p> required <code>input_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activation vectors.</p> required <code>n_samples</code> <code>list[int]</code> <p>Number of samples to take (number of dead neurons).</p> required <p>Returns:</p> Type Description <code>list[Float[Tensor, names(DEAD_FEATURE, INPUT_OUTPUT_FEATURE)]]</code> <p>Sampled input activation vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of samples is greater than the number of input activations.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef sample_input(\n    probabilities: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    input_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    n_samples: list[int],\n) -&gt; list[Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]]:\n    \"\"\"Sample an input vector based on the provided probabilities.\n\n    Example:\n        &gt;&gt;&gt; probabilities = torch.tensor([[0.1], [0.2], [0.7]])\n        &gt;&gt;&gt; input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n        ...     probabilities, input_activations, [2]\n        ... )\n        &gt;&gt;&gt; sampled_input[0].tolist()\n        [[5.0, 6.0], [3.0, 4.0]]\n\n    Args:\n        probabilities: Probabilities for each input.\n        input_activations: Input activation vectors.\n        n_samples: Number of samples to take (number of dead neurons).\n\n    Returns:\n        Sampled input activation vector.\n\n    Raises:\n        ValueError: If the number of samples is greater than the number of input activations.\n    \"\"\"\n    sampled_inputs: list[\n        Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]\n    ] = []\n\n    for component_idx, component_n_samples in enumerate(n_samples):\n        component_probabilities: Float[Tensor, Axis.BATCH] = get_component_slice_tensor(\n            input_tensor=probabilities,\n            n_dim_with_component=2,\n            component_dim=1,\n            component_idx=component_idx,\n        )\n\n        component_input_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)\n        ] = get_component_slice_tensor(\n            input_tensor=input_activations,\n            n_dim_with_component=3,\n            component_dim=1,\n            component_idx=component_idx,\n        )\n\n        if component_n_samples &gt; len(component_input_activations):\n            exception_message = (\n                f\"Cannot sample {component_n_samples} inputs from \"\n                f\"{len(component_input_activations)} input activations.\"\n            )\n            raise ValueError(exception_message)\n\n        # Handle the 0 dead neurons case\n        if component_n_samples == 0:\n            sampled_inputs.append(\n                torch.empty(\n                    (0, component_input_activations.shape[-1]),\n                    dtype=component_input_activations.dtype,\n                    device=component_input_activations.device,\n                )\n            )\n            continue\n\n        # Handle the 1+ dead neuron case\n        component_sample_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX] = torch.multinomial(\n            component_probabilities, num_samples=component_n_samples\n        )\n        sampled_inputs.append(component_input_activations[component_sample_indices, :])\n\n    return sampled_inputs\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.update","title":"<code>update(input_activations, learned_activations, loss, encoder_weight_reference)</code>","text":"<p>Update the collated data from forward passes.</p> <p>Parameters:</p> Name Type Description Default <code>input_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations to the SAE.</p> required <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations from the SAE.</p> required <code>loss</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Loss per input activation.</p> required <code>encoder_weight_reference</code> <code>Parameter</code> <p>Reference to the SAE encoder weight tensor.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the loss or input activations are not lists (e.g. from unsync having not been called).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def update(\n    self,\n    input_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    loss: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)],\n    encoder_weight_reference: Parameter,\n) -&gt; None:\n    \"\"\"Update the collated data from forward passes.\n\n    Args:\n        input_activations: Input activations to the SAE.\n        learned_activations: Learned activations from the SAE.\n        loss: Loss per input activation.\n        encoder_weight_reference: Reference to the SAE encoder weight tensor.\n\n    Raises:\n        TypeError: If the loss or input activations are not lists (e.g. from unsync having not\n            been called).\n    \"\"\"\n    if self._n_activations_seen_process &gt;= self.start_collecting_neuron_activity_process:\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n        self._neuron_fired_count += neuron_has_fired.sum(dim=0)\n\n    if self._n_activations_seen_process &gt;= self.start_collecting_loss_process:\n        # Typecast\n        if not isinstance(self._loss, list) or not isinstance(self._input_activations, list):\n            raise TypeError\n\n        # Append\n        self._loss.append(loss)\n        self._input_activations.append(input_activations)\n\n    self._n_activations_seen_process += len(learned_activations)\n    self._encoder_weight = encoder_weight_reference\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults","title":"<code>ParameterUpdateResults</code>  <code>dataclass</code>","text":"<p>Parameter update results from resampling dead neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@dataclass\nclass ParameterUpdateResults:\n    \"\"\"Parameter update results from resampling dead neurons.\"\"\"\n\n    dead_neuron_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX]\n    \"\"\"Dead neuron indices.\"\"\"\n\n    dead_encoder_weight_updates: Float[\n        Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Dead encoder weight updates.\"\"\"\n\n    dead_encoder_bias_updates: Float[Tensor, Axis.DEAD_FEATURE]\n    \"\"\"Dead encoder bias updates.\"\"\"\n\n    dead_decoder_weight_updates: Float[\n        Tensor, Axis.names(Axis.INPUT_OUTPUT_FEATURE, Axis.DEAD_FEATURE)\n    ]\n    \"\"\"Dead decoder weight updates.\"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults.dead_decoder_weight_updates","title":"<code>dead_decoder_weight_updates: Float[Tensor, Axis.names(Axis.INPUT_OUTPUT_FEATURE, Axis.DEAD_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Dead decoder weight updates.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults.dead_encoder_bias_updates","title":"<code>dead_encoder_bias_updates: Float[Tensor, Axis.DEAD_FEATURE]</code>  <code>instance-attribute</code>","text":"<p>Dead encoder bias updates.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults.dead_encoder_weight_updates","title":"<code>dead_encoder_weight_updates: Float[Tensor, Axis.names(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Dead encoder weight updates.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults.dead_neuron_indices","title":"<code>dead_neuron_indices: Int[Tensor, Axis.LEARNT_FEATURE_IDX]</code>  <code>instance-attribute</code>","text":"<p>Dead neuron indices.</p>"},{"location":"reference/activation_resampler/utils/","title":"Activation resampler utils","text":"<p>Activation resampler utils.</p>"},{"location":"reference/activation_resampler/utils/component_slice_tensor/","title":"Component slice tensor utils","text":"<p>Component slice tensor utils.</p>"},{"location":"reference/activation_resampler/utils/component_slice_tensor/#sparse_autoencoder.activation_resampler.utils.component_slice_tensor.get_component_slice_tensor","title":"<code>get_component_slice_tensor(input_tensor, n_dim_with_component, component_dim, component_idx)</code>","text":"<p>Get a slice of a tensor for a specific component.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; input_tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n&gt;&gt;&gt; get_component_slice_tensor(input_tensor, 2, 1, 0)\ntensor([1, 3, 5, 7])\n</code></pre> <pre><code>&gt;&gt;&gt; input_tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n&gt;&gt;&gt; get_component_slice_tensor(input_tensor, 3, 1, 0)\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor.</p> required <code>n_dim_with_component</code> <code>int</code> <p>Number of dimensions in the input tensor with the component axis.</p> required <code>component_dim</code> <code>int</code> <p>Dimension of the component axis.</p> required <code>component_idx</code> <code>int</code> <p>Index of the component to get the slice for.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor slice.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input tensor does not have the expected number of dimensions.</p> Source code in <code>sparse_autoencoder/activation_resampler/utils/component_slice_tensor.py</code> <pre><code>def get_component_slice_tensor(\n    input_tensor: Tensor,\n    n_dim_with_component: int,\n    component_dim: int,\n    component_idx: int,\n) -&gt; Tensor:\n    \"\"\"Get a slice of a tensor for a specific component.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; input_tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n        &gt;&gt;&gt; get_component_slice_tensor(input_tensor, 2, 1, 0)\n        tensor([1, 3, 5, 7])\n\n        &gt;&gt;&gt; input_tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n        &gt;&gt;&gt; get_component_slice_tensor(input_tensor, 3, 1, 0)\n        tensor([[1, 2],\n                [3, 4],\n                [5, 6],\n                [7, 8]])\n\n    Args:\n        input_tensor: Input tensor.\n        n_dim_with_component: Number of dimensions in the input tensor with the component axis.\n        component_dim: Dimension of the component axis.\n        component_idx: Index of the component to get the slice for.\n\n    Returns:\n        Tensor slice.\n\n    Raises:\n        ValueError: If the input tensor does not have the expected number of dimensions.\n    \"\"\"\n    if n_dim_with_component - 1 == input_tensor.ndim:\n        return input_tensor\n\n    if n_dim_with_component != input_tensor.ndim:\n        error_message = (\n            f\"Cannot get component slice for tensor with {input_tensor.ndim} dimensions \"\n            f\"and {n_dim_with_component} dimensions with component.\"\n        )\n        raise ValueError(error_message)\n\n    # Create a tuple of slices for each dimension\n    slice_tuple = tuple(\n        component_idx if i == component_dim else slice(None) for i in range(input_tensor.ndim)\n    )\n\n    return input_tensor[slice_tuple]\n</code></pre>"},{"location":"reference/activation_store/","title":"Activation Stores","text":"<p>Activation Stores.</p>"},{"location":"reference/activation_store/base_store/","title":"Activation Store Base Class","text":"<p>Activation Store Base Class.</p>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore","title":"<code>ActivationStore</code>","text":"<p>             Bases: <code>Dataset[Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]]</code>, <code>ABC</code></p> <p>Activation Store Abstract Class.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide an activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a <code>torch.utils.data.DataLoader</code> to iterate over the dataset.</p> <p>Extend this class if you want to create a new activation store (noting you also need to create <code>__getitem__</code> and <code>__len__</code> methods from the underlying <code>torch.utils.data.Dataset</code> class).</p> <p>Example:</p> <p>import torch class MyActivationStore(ActivationStore): ... ...     @property ...     def current_activations_stored_per_component(self): ...        raise NotImplementedError ... ...     @property ...     def n_components(self): ...         raise NotImplementedError ... ...     def init(self): ...         super().init() ...         self._data = [] # In this example, we just store in a list ... ...     def append(self, item) -&gt; None: ...         self._data.append(item) ... ...     def extend(self, batch): ...         self._data.extend(batch) ... ...     def empty(self): ...         self._data = [] ... ...     def getitem(self, index: int): ...         return self._data[index] ... ...     def len(self) -&gt; int: ...         return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>class ActivationStore(\n    Dataset[Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]], ABC\n):\n    \"\"\"Activation Store Abstract Class.\n\n    Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional\n    :meth:`append` and :meth:`extend` methods (the latter of which should typically be\n    non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader`\n    to iterate over the dataset.\n\n    Extend this class if you want to create a new activation store (noting you also need to create\n    `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class).\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; class MyActivationStore(ActivationStore):\n    ...\n    ...     @property\n    ...     def current_activations_stored_per_component(self):\n    ...        raise NotImplementedError\n    ...\n    ...     @property\n    ...     def n_components(self):\n    ...         raise NotImplementedError\n    ...\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self._data = [] # In this example, we just store in a list\n    ...\n    ...     def append(self, item) -&gt; None:\n    ...         self._data.append(item)\n    ...\n    ...     def extend(self, batch):\n    ...         self._data.extend(batch)\n    ...\n    ...     def empty(self):\n    ...         self._data = []\n    ...\n    ...     def __getitem__(self, index: int):\n    ...         return self._data[index]\n    ...\n    ...     def __len__(self) -&gt; int:\n    ...         return len(self._data)\n    ...\n    &gt;&gt;&gt; store = MyActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n\n    @abstractmethod\n    def append(\n        self,\n        item: Float[Tensor, Axis.names(Axis.INPUT_OUTPUT_FEATURE)],\n        component_idx: int,\n    ) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\"\"\"\n\n    @abstractmethod\n    def extend(\n        self,\n        batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        component_idx: int,\n    ) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\"\"\"\n\n    @abstractmethod\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\"\"\"\n\n    @property\n    @abstractmethod\n    def n_components(self) -&gt; int:\n        \"\"\"Number of components.\"\"\"\n\n    @property\n    @abstractmethod\n    def current_activations_stored_per_component(self) -&gt; list[int]:\n        \"\"\"Current activations stored per component.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Get the Length of the Store.\"\"\"\n\n    @abstractmethod\n    def __getitem__(\n        self, index: tuple[int, ...] | slice | int\n    ) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n        \"\"\"Get an Item from the Store.\"\"\"\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Optional shuffle method.\"\"\"\n\n    @final\n    @validate_call\n    def fill_with_test_data(\n        self,\n        n_batches: PositiveInt = 1,\n        batch_size: PositiveInt = 16,\n        n_components: PositiveInt = 1,\n        input_features: PositiveInt = 256,\n    ) -&gt; None:\n        \"\"\"Fill the store with test data.\n\n        For use when testing your code, to ensure it works with a real activation store.\n\n        Warning:\n            You may want to use `torch.seed(0)` to make the random data deterministic, if your test\n            requires inspecting the data itself.\n\n        Example:\n            &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=100, n_neurons=256, n_components=1)\n            &gt;&gt;&gt; store.fill_with_test_data(batch_size=100)\n            &gt;&gt;&gt; len(store)\n            100\n\n        Args:\n            n_batches: Number of batches to fill the store with.\n            batch_size: Number of items per batch.\n            n_components: Number of source model components the SAE is trained on.\n            input_features: Number of input features per item.\n        \"\"\"\n        for _ in range(n_batches):\n            for component_idx in range(n_components):\n                sample = torch.rand(batch_size, input_features)\n                self.extend(sample, component_idx)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.current_activations_stored_per_component","title":"<code>current_activations_stored_per_component: list[int]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Current activations stored per component.</p>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.n_components","title":"<code>n_components: int</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of components.</p>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>Get an Item from the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef __getitem__(\n    self, index: tuple[int, ...] | slice | int\n) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n    \"\"\"Get an Item from the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Get the Length of the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Get the Length of the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.append","title":"<code>append(item, component_idx)</code>  <code>abstractmethod</code>","text":"<p>Add a Single Item to the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef append(\n    self,\n    item: Float[Tensor, Axis.names(Axis.INPUT_OUTPUT_FEATURE)],\n    component_idx: int,\n) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.empty","title":"<code>empty()</code>  <code>abstractmethod</code>","text":"<p>Empty the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef empty(self) -&gt; None:\n    \"\"\"Empty the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.extend","title":"<code>extend(batch, component_idx)</code>  <code>abstractmethod</code>","text":"<p>Add a Batch to the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef extend(\n    self,\n    batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    component_idx: int,\n) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.fill_with_test_data","title":"<code>fill_with_test_data(n_batches=1, batch_size=16, n_components=1, input_features=256)</code>","text":"<p>Fill the store with test data.</p> <p>For use when testing your code, to ensure it works with a real activation store.</p> Warning <p>You may want to use <code>torch.seed(0)</code> to make the random data deterministic, if your test requires inspecting the data itself.</p> Example <p>from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=100, n_neurons=256, n_components=1) store.fill_with_test_data(batch_size=100) len(store) 100</p> <p>Parameters:</p> Name Type Description Default <code>n_batches</code> <code>PositiveInt</code> <p>Number of batches to fill the store with.</p> <code>1</code> <code>batch_size</code> <code>PositiveInt</code> <p>Number of items per batch.</p> <code>16</code> <code>n_components</code> <code>PositiveInt</code> <p>Number of source model components the SAE is trained on.</p> <code>1</code> <code>input_features</code> <code>PositiveInt</code> <p>Number of input features per item.</p> <code>256</code> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@final\n@validate_call\ndef fill_with_test_data(\n    self,\n    n_batches: PositiveInt = 1,\n    batch_size: PositiveInt = 16,\n    n_components: PositiveInt = 1,\n    input_features: PositiveInt = 256,\n) -&gt; None:\n    \"\"\"Fill the store with test data.\n\n    For use when testing your code, to ensure it works with a real activation store.\n\n    Warning:\n        You may want to use `torch.seed(0)` to make the random data deterministic, if your test\n        requires inspecting the data itself.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=100, n_neurons=256, n_components=1)\n        &gt;&gt;&gt; store.fill_with_test_data(batch_size=100)\n        &gt;&gt;&gt; len(store)\n        100\n\n    Args:\n        n_batches: Number of batches to fill the store with.\n        batch_size: Number of items per batch.\n        n_components: Number of source model components the SAE is trained on.\n        input_features: Number of input features per item.\n    \"\"\"\n    for _ in range(n_batches):\n        for component_idx in range(n_components):\n            sample = torch.rand(batch_size, input_features)\n            self.extend(sample, component_idx)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Optional shuffle method.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Optional shuffle method.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError","title":"<code>StoreFullError</code>","text":"<p>             Bases: <code>IndexError</code></p> <p>Exception raised when the activation store is full.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>class StoreFullError(IndexError):\n    \"\"\"Exception raised when the activation store is full.\"\"\"\n\n    def __init__(self, message: str = \"Activation store is full\"):\n        \"\"\"Initialise the exception.\n\n        Args:\n            message: Override the default message.\n        \"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError.__init__","title":"<code>__init__(message='Activation store is full')</code>","text":"<p>Initialise the exception.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Override the default message.</p> <code>'Activation store is full'</code> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>def __init__(self, message: str = \"Activation store is full\"):\n    \"\"\"Initialise the exception.\n\n    Args:\n        message: Override the default message.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/activation_store/tensor_store/","title":"Tensor Activation Store","text":"<p>Tensor Activation Store.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore","title":"<code>TensorActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Tensor Activation Store.</p> <p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=100, n_components=2)\n</code></pre> <p>Add a single activation vector to the dataset (for a component):</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n&gt;&gt;&gt; store.append(torch.randn(100), component_idx=1)\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a [batch, neurons] activation tensor to the dataset:</p> <pre><code>&gt;&gt;&gt; store.empty()\n&gt;&gt;&gt; batch = torch.randn(10, 100)\n&gt;&gt;&gt; store.extend(batch, component_idx=0)\n&gt;&gt;&gt; store.extend(batch, component_idx=1)\n&gt;&gt;&gt; len(store)\n10\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>class TensorActivationStore(ActivationStore):\n    \"\"\"Tensor Activation Store.\n\n    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation\n    vectors to be stored to be known in advance. Multiprocess safe.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=100, n_components=2)\n\n    Add a single activation vector to the dataset (for a component):\n\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=1)\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a [batch, neurons] activation tensor to the dataset:\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; store.extend(batch, component_idx=0)\n        &gt;&gt;&gt; store.extend(batch, component_idx=1)\n        &gt;&gt;&gt; len(store)\n        10\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 2, 100])\n    \"\"\"\n\n    _data: Float[Tensor, Axis.names(Axis.ITEMS, Axis.COMPONENT, Axis.INPUT_OUTPUT_FEATURE)]\n    \"\"\"Underlying Tensor Data Store.\"\"\"\n\n    _items_stored: list[int]\n    \"\"\"Number of items stored.\"\"\"\n\n    max_items: int\n    \"\"\"Maximum Number of Items to Store.\"\"\"\n\n    _n_components: int\n    \"\"\"Number of components\"\"\"\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of components.\"\"\"\n        return self._n_components\n\n    @property\n    def current_activations_stored_per_component(self) -&gt; list[int]:\n        \"\"\"Number of activations stored per component.\"\"\"\n        return self._items_stored\n\n    @validate_call(config={\"arbitrary_types_allowed\": True})\n    def __init__(\n        self,\n        max_items: PositiveInt,\n        n_neurons: PositiveInt,\n        n_components: PositiveInt,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the Tensor Activation Store.\n\n        Args:\n            max_items: Maximum number of items to store per component (individual activation\n                vectors).\n            n_neurons: Number of neurons in each activation vector.\n            n_components: Number of components to store (i.e. number of source models).\n            device: Device to store the activation vectors on.\n        \"\"\"\n        self._n_components = n_components\n        self._items_stored = [0] * n_components\n        self._max_items = max_items\n        self._data = torch.empty((max_items, n_components, n_neurons), device=device)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors per component in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)\n            &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n            &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        # Min as this is the amount of activations that can be fetched by get_item\n        return min(self.current_activations_stored_per_component)\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)\n            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n            800\n\n        Returns:\n            The size of the underlying tensor in bytes.\n        \"\"\"\n        return self._data.element_size() * self._data.nelement()\n\n    def __getitem__(\n        self, index: tuple[int, ...] | slice | int\n    ) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n        \"\"\"Get Item Dunder Method.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)\n            &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n            &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n            &gt;&gt;&gt; store[1, 0]\n            tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)\n        &gt;&gt;&gt; store.append(torch.tensor([0.]), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.tensor([1.]), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.tensor([2.]), component_idx=0)\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; [store[i, 0].item() for i in range(3)]\n        [0.0, 2.0, 1.0]\n        \"\"\"\n        # Generate a permutation of the indices for the active data\n        perm = torch.randperm(len(self))\n\n        # Use this permutation to shuffle the active data in-place\n        self._data[: len(self)] = self._data[perm]\n\n    def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE], component_idx: int) -&gt; None:\n        \"\"\"Add a single item to the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n        &gt;&gt;&gt; store[1, 0]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            item: The item to append to the dataset.\n            component_idx: The component index to append the item to.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        if self._items_stored[component_idx] + 1 &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[self._items_stored[component_idx], component_idx] = item.to(\n            self._data.device,\n        )\n        self._items_stored[component_idx] += 1\n\n    def extend(\n        self,\n        batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n        component_idx: int,\n    ) -&gt; None:\n        \"\"\"Add a batch to the store.\n\n        Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n        &gt;&gt;&gt; len(store)\n        2\n\n        Args:\n            batch: The batch to append to the dataset.\n            component_idx: The component index to append the batch to.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        n_activation_tensors: int = batch.shape[0]\n        if self._items_stored[component_idx] + n_activation_tensors &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[\n            self._items_stored[component_idx] : self._items_stored[component_idx]\n            + n_activation_tensors,\n            component_idx,\n        ] = batch.to(self._data.device)\n        self._items_stored[component_idx] += n_activation_tensors\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n        &gt;&gt;&gt; len(store)\n        2\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; len(store)\n        0\n        \"\"\"\n        # We don't need to zero the data, just reset the number of items stored\n        self._items_stored = [0 for _ in self._items_stored]\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.current_activations_stored_per_component","title":"<code>current_activations_stored_per_component: list[int]</code>  <code>property</code>","text":"<p>Number of activations stored per component.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.max_items","title":"<code>max_items: int</code>  <code>instance-attribute</code>","text":"<p>Maximum Number of Items to Store.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of components.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)\n&gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n&gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n&gt;&gt;&gt; store[1, 0]\ntensor([1., 1., 1., 1., 1.])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>tuple[int, ...] | slice | int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(ANY)]</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __getitem__(\n    self, index: tuple[int, ...] | slice | int\n) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n    \"\"\"Get Item Dunder Method.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)\n        &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n        &gt;&gt;&gt; store[1, 0]\n        tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    return self._data[index]\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__init__","title":"<code>__init__(max_items, n_neurons, n_components, device=None)</code>","text":"<p>Initialise the Tensor Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>max_items</code> <code>PositiveInt</code> <p>Maximum number of items to store per component (individual activation vectors).</p> required <code>n_neurons</code> <code>PositiveInt</code> <p>Number of neurons in each activation vector.</p> required <code>n_components</code> <code>PositiveInt</code> <p>Number of components to store (i.e. number of source models).</p> required <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>@validate_call(config={\"arbitrary_types_allowed\": True})\ndef __init__(\n    self,\n    max_items: PositiveInt,\n    n_neurons: PositiveInt,\n    n_components: PositiveInt,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"Initialise the Tensor Activation Store.\n\n    Args:\n        max_items: Maximum number of items to store per component (individual activation\n            vectors).\n        n_neurons: Number of neurons in each activation vector.\n        n_components: Number of components to store (i.e. number of source models).\n        device: Device to store the activation vectors on.\n    \"\"\"\n    self._n_components = n_components\n    self._items_stored = [0] * n_components\n    self._max_items = max_items\n    self._data = torch.empty((max_items, n_components, n_neurons), device=device)\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors per component in the dataset.</p> Example <p>import torch store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1) store.append(torch.randn(100), component_idx=0) store.append(torch.randn(100), component_idx=0) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors per component in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    # Min as this is the amount of activations that can be fetched by get_item\n    return min(self.current_activations_stored_per_component)\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> Example <p>import torch store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1) store.sizeof() # Pre-allocated tensor of 2x100 800</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the underlying tensor in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)\n        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n        800\n\n    Returns:\n        The size of the underlying tensor in bytes.\n    \"\"\"\n    return self._data.element_size() * self._data.nelement()\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.append","title":"<code>append(item, component_idx)</code>","text":"<p>Add a single item to the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1) store.append(torch.zeros(5), component_idx=0) store.append(torch.ones(5), component_idx=0) store[1, 0] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Float[Tensor, INPUT_OUTPUT_FEATURE]</code> <p>The item to append to the dataset.</p> required <code>component_idx</code> <code>int</code> <p>The component index to append the item to.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def append(self, item: Float[Tensor, Axis.INPUT_OUTPUT_FEATURE], component_idx: int) -&gt; None:\n    \"\"\"Add a single item to the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n    &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)\n    &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)\n    &gt;&gt;&gt; store[1, 0]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        item: The item to append to the dataset.\n        component_idx: The component index to append the item to.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    if self._items_stored[component_idx] + 1 &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[self._items_stored[component_idx], component_idx] = item.to(\n        self._data.device,\n    )\n    self._items_stored[component_idx] += 1\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1) store.extend(torch.zeros(2, 5), component_idx=0) len(store) 2 store.empty() len(store) 0</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n    &gt;&gt;&gt; len(store)\n    2\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; len(store)\n    0\n    \"\"\"\n    # We don't need to zero the data, just reset the number of items stored\n    self._items_stored = [0 for _ in self._items_stored]\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.extend","title":"<code>extend(batch, component_idx)</code>","text":"<p>Add a batch to the store.</p> <p>Examples:</p> <p>import torch store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1) store.extend(torch.zeros(2, 5), component_idx=0) len(store) 2</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Float[Tensor, names(BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>The batch to append to the dataset.</p> required <code>component_idx</code> <code>int</code> <p>The component index to append the batch to.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def extend(\n    self,\n    batch: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)],\n    component_idx: int,\n) -&gt; None:\n    \"\"\"Add a batch to the store.\n\n    Examples:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)\n    &gt;&gt;&gt; len(store)\n    2\n\n    Args:\n        batch: The batch to append to the dataset.\n        component_idx: The component index to append the batch to.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    n_activation_tensors: int = batch.shape[0]\n    if self._items_stored[component_idx] + n_activation_tensors &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[\n        self._items_stored[component_idx] : self._items_stored[component_idx]\n        + n_activation_tensors,\n        component_idx,\n    ] = batch.to(self._data.device)\n    self._items_stored[component_idx] += n_activation_tensors\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1) store.append(torch.tensor([0.]), component_idx=0) store.append(torch.tensor([1.]), component_idx=0) store.append(torch.tensor([2.]), component_idx=0) store.shuffle() [store[i, 0].item() for i in range(3)] [0.0, 2.0, 1.0]</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)\n    &gt;&gt;&gt; store.append(torch.tensor([0.]), component_idx=0)\n    &gt;&gt;&gt; store.append(torch.tensor([1.]), component_idx=0)\n    &gt;&gt;&gt; store.append(torch.tensor([2.]), component_idx=0)\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; [store[i, 0].item() for i in range(3)]\n    [0.0, 2.0, 1.0]\n    \"\"\"\n    # Generate a permutation of the indices for the active data\n    perm = torch.randperm(len(self))\n\n    # Use this permutation to shuffle the active data in-place\n    self._data[: len(self)] = self._data[perm]\n</code></pre>"},{"location":"reference/autoencoder/","title":"Sparse autoencoder model &amp; components","text":"<p>Sparse autoencoder model &amp; components.</p>"},{"location":"reference/autoencoder/lightning/","title":"PyTorch Lightning module for training a sparse autoencoder","text":"<p>PyTorch Lightning module for training a sparse autoencoder.</p>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder","title":"<code>LitSparseAutoencoder</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>Lightning Sparse Autoencoder.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>class LitSparseAutoencoder(LightningModule):\n    \"\"\"Lightning Sparse Autoencoder.\"\"\"\n\n    sparse_autoencoder: SparseAutoencoder\n\n    config: LitSparseAutoencoderConfig\n\n    loss_fn: SparseAutoencoderLoss\n\n    train_metrics: MetricCollection\n\n    def __init__(\n        self,\n        config: LitSparseAutoencoderConfig,\n    ):\n        \"\"\"Initialise the module.\"\"\"\n        super().__init__()\n        self.sparse_autoencoder = SparseAutoencoder(config)\n        self.config = config\n\n        num_components = config.n_components or 1\n        add_component_names = partial(\n            ClasswiseWrapperWithMean, component_names=config.component_names\n        )\n\n        # Create the loss &amp; metrics\n        self.loss_fn = SparseAutoencoderLoss(\n            num_components, config.l1_coefficient, keep_batch_dim=True\n        )\n\n        self.train_metrics = MetricCollection(\n            {\n                \"l0\": add_component_names(L0NormMetric(num_components), prefix=\"train/l0_norm\"),\n                \"activity\": add_component_names(\n                    NeuronActivityMetric(config.n_learned_features, num_components),\n                    prefix=\"train/neuron_activity\",\n                ),\n                \"l1\": add_component_names(\n                    L1AbsoluteLoss(num_components), prefix=\"loss/l1_learned_activations\"\n                ),\n                \"l2\": add_component_names(\n                    L2ReconstructionLoss(num_components), prefix=\"loss/l2_reconstruction\"\n                ),\n                \"loss\": add_component_names(\n                    SparseAutoencoderLoss(num_components, config.l1_coefficient),\n                    prefix=\"loss/total\",\n                ),\n            },\n            # Share state &amp; updates across groups (to avoid e.g. computing l1 twice for both the\n            # loss and l1 metrics). Note the metric that goes first must calculate all the states\n            # needed by the rest of the group.\n            compute_groups=[\n                [\"loss\", \"l1\", \"l2\"],\n                [\"activity\"],\n                [\"l0\"],\n            ],\n        )\n\n        self.activation_resampler = ActivationResampler(\n            n_learned_features=config.n_learned_features,\n            n_components=num_components,\n            resample_interval=config.resample_interval,\n            max_n_resamples=config.max_n_resamples,\n            n_activations_activity_collate=config.resample_dead_neurons_dataset_size,\n            resample_dataset_size=config.resample_loss_dataset_size,\n            threshold_is_dead_portion_fires=config.resample_threshold_is_dead_portion_fires,\n        )\n\n    def forward(  # type: ignore[override]\n        self,\n        inputs: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; ForwardPassResult:\n        \"\"\"Forward pass.\"\"\"\n        return self.sparse_autoencoder.forward(inputs)\n\n    def update_parameters(self, parameter_updates: list[ParameterUpdateResults]) -&gt; None:\n        \"\"\"Update the parameters of the model from the results of the resampler.\n\n        Args:\n            parameter_updates: Parameter updates from the resampler.\n\n        Raises:\n            TypeError: If the optimizer is not an AdamWithReset.\n        \"\"\"\n        for component_idx, component_parameter_update in enumerate(parameter_updates):\n            # Update the weights and biases\n            self.sparse_autoencoder.encoder.update_dictionary_vectors(\n                component_parameter_update.dead_neuron_indices,\n                component_parameter_update.dead_encoder_weight_updates,\n                component_idx=component_idx,\n            )\n            self.sparse_autoencoder.encoder.update_bias(\n                component_parameter_update.dead_neuron_indices,\n                component_parameter_update.dead_encoder_bias_updates,\n                component_idx=component_idx,\n            )\n            self.sparse_autoencoder.decoder.update_dictionary_vectors(\n                component_parameter_update.dead_neuron_indices,\n                component_parameter_update.dead_decoder_weight_updates,\n                component_idx=component_idx,\n            )\n\n            # Reset the optimizer\n            for (\n                parameter,\n                axis,\n            ) in self.reset_optimizer_parameter_details:\n                optimizer = self.optimizers(use_pl_optimizer=False)\n                if not isinstance(optimizer, AdamWithReset):\n                    error_message = \"Cannot reset the optimizer. \"\n                    raise TypeError(error_message)\n\n                optimizer.reset_neurons_state(\n                    parameter=parameter,\n                    neuron_indices=component_parameter_update.dead_neuron_indices,\n                    axis=axis,\n                    component_idx=component_idx,\n                )\n\n    def training_step(  # type: ignore[override]\n        self,\n        batch: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        batch_idx: int | None = None,  # noqa: ARG002\n    ) -&gt; Float[Tensor, Axis.SINGLE_ITEM]:\n        \"\"\"Training step.\"\"\"\n        # Forward pass\n        output: ForwardPassResult = self.forward(batch)\n\n        # Metrics &amp; loss\n        train_metrics = self.train_metrics.forward(\n            source_activations=batch,\n            learned_activations=output.learned_activations,\n            decoded_activations=output.decoded_activations,\n        )\n\n        loss = self.loss_fn.forward(\n            source_activations=batch,\n            learned_activations=output.learned_activations,\n            decoded_activations=output.decoded_activations,\n        )\n\n        if wandb.run is not None:\n            self.log_dict(train_metrics)\n\n        # Resample dead neurons\n        parameter_updates = self.activation_resampler.forward(\n            input_activations=batch,\n            learned_activations=output.learned_activations,\n            loss=loss,\n            encoder_weight_reference=self.sparse_autoencoder.encoder.weight,\n        )\n        if parameter_updates is not None:\n            self.update_parameters(parameter_updates)\n\n        # Return the mean loss\n        return loss.mean()\n\n    def on_after_backward(self) -&gt; None:\n        \"\"\"After-backward pass hook.\"\"\"\n        self.sparse_autoencoder.post_backwards_hook()\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"Configure the optimizer.\"\"\"\n        return AdamWithReset(\n            self.sparse_autoencoder.parameters(),\n            named_parameters=self.sparse_autoencoder.named_parameters(),\n            has_components_dim=True,\n        )\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[ResetOptimizerParameterDetails]:\n        \"\"\"Reset optimizer parameter details.\"\"\"\n        return self.sparse_autoencoder.reset_optimizer_parameter_details\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[ResetOptimizerParameterDetails]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialise the module.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def __init__(\n    self,\n    config: LitSparseAutoencoderConfig,\n):\n    \"\"\"Initialise the module.\"\"\"\n    super().__init__()\n    self.sparse_autoencoder = SparseAutoencoder(config)\n    self.config = config\n\n    num_components = config.n_components or 1\n    add_component_names = partial(\n        ClasswiseWrapperWithMean, component_names=config.component_names\n    )\n\n    # Create the loss &amp; metrics\n    self.loss_fn = SparseAutoencoderLoss(\n        num_components, config.l1_coefficient, keep_batch_dim=True\n    )\n\n    self.train_metrics = MetricCollection(\n        {\n            \"l0\": add_component_names(L0NormMetric(num_components), prefix=\"train/l0_norm\"),\n            \"activity\": add_component_names(\n                NeuronActivityMetric(config.n_learned_features, num_components),\n                prefix=\"train/neuron_activity\",\n            ),\n            \"l1\": add_component_names(\n                L1AbsoluteLoss(num_components), prefix=\"loss/l1_learned_activations\"\n            ),\n            \"l2\": add_component_names(\n                L2ReconstructionLoss(num_components), prefix=\"loss/l2_reconstruction\"\n            ),\n            \"loss\": add_component_names(\n                SparseAutoencoderLoss(num_components, config.l1_coefficient),\n                prefix=\"loss/total\",\n            ),\n        },\n        # Share state &amp; updates across groups (to avoid e.g. computing l1 twice for both the\n        # loss and l1 metrics). Note the metric that goes first must calculate all the states\n        # needed by the rest of the group.\n        compute_groups=[\n            [\"loss\", \"l1\", \"l2\"],\n            [\"activity\"],\n            [\"l0\"],\n        ],\n    )\n\n    self.activation_resampler = ActivationResampler(\n        n_learned_features=config.n_learned_features,\n        n_components=num_components,\n        resample_interval=config.resample_interval,\n        max_n_resamples=config.max_n_resamples,\n        n_activations_activity_collate=config.resample_dead_neurons_dataset_size,\n        resample_dataset_size=config.resample_loss_dataset_size,\n        threshold_is_dead_portion_fires=config.resample_threshold_is_dead_portion_fires,\n    )\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"Configure the optimizer.\"\"\"\n    return AdamWithReset(\n        self.sparse_autoencoder.parameters(),\n        named_parameters=self.sparse_autoencoder.named_parameters(),\n        has_components_dim=True,\n    )\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.forward","title":"<code>forward(inputs)</code>","text":"<p>Forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def forward(  # type: ignore[override]\n    self,\n    inputs: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; ForwardPassResult:\n    \"\"\"Forward pass.\"\"\"\n    return self.sparse_autoencoder.forward(inputs)\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.on_after_backward","title":"<code>on_after_backward()</code>","text":"<p>After-backward pass hook.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def on_after_backward(self) -&gt; None:\n    \"\"\"After-backward pass hook.\"\"\"\n    self.sparse_autoencoder.post_backwards_hook()\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>Training step.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def training_step(  # type: ignore[override]\n    self,\n    batch: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    batch_idx: int | None = None,  # noqa: ARG002\n) -&gt; Float[Tensor, Axis.SINGLE_ITEM]:\n    \"\"\"Training step.\"\"\"\n    # Forward pass\n    output: ForwardPassResult = self.forward(batch)\n\n    # Metrics &amp; loss\n    train_metrics = self.train_metrics.forward(\n        source_activations=batch,\n        learned_activations=output.learned_activations,\n        decoded_activations=output.decoded_activations,\n    )\n\n    loss = self.loss_fn.forward(\n        source_activations=batch,\n        learned_activations=output.learned_activations,\n        decoded_activations=output.decoded_activations,\n    )\n\n    if wandb.run is not None:\n        self.log_dict(train_metrics)\n\n    # Resample dead neurons\n    parameter_updates = self.activation_resampler.forward(\n        input_activations=batch,\n        learned_activations=output.learned_activations,\n        loss=loss,\n        encoder_weight_reference=self.sparse_autoencoder.encoder.weight,\n    )\n    if parameter_updates is not None:\n        self.update_parameters(parameter_updates)\n\n    # Return the mean loss\n    return loss.mean()\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoder.update_parameters","title":"<code>update_parameters(parameter_updates)</code>","text":"<p>Update the parameters of the model from the results of the resampler.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_updates</code> <code>list[ParameterUpdateResults]</code> <p>Parameter updates from the resampler.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the optimizer is not an AdamWithReset.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def update_parameters(self, parameter_updates: list[ParameterUpdateResults]) -&gt; None:\n    \"\"\"Update the parameters of the model from the results of the resampler.\n\n    Args:\n        parameter_updates: Parameter updates from the resampler.\n\n    Raises:\n        TypeError: If the optimizer is not an AdamWithReset.\n    \"\"\"\n    for component_idx, component_parameter_update in enumerate(parameter_updates):\n        # Update the weights and biases\n        self.sparse_autoencoder.encoder.update_dictionary_vectors(\n            component_parameter_update.dead_neuron_indices,\n            component_parameter_update.dead_encoder_weight_updates,\n            component_idx=component_idx,\n        )\n        self.sparse_autoencoder.encoder.update_bias(\n            component_parameter_update.dead_neuron_indices,\n            component_parameter_update.dead_encoder_bias_updates,\n            component_idx=component_idx,\n        )\n        self.sparse_autoencoder.decoder.update_dictionary_vectors(\n            component_parameter_update.dead_neuron_indices,\n            component_parameter_update.dead_decoder_weight_updates,\n            component_idx=component_idx,\n        )\n\n        # Reset the optimizer\n        for (\n            parameter,\n            axis,\n        ) in self.reset_optimizer_parameter_details:\n            optimizer = self.optimizers(use_pl_optimizer=False)\n            if not isinstance(optimizer, AdamWithReset):\n                error_message = \"Cannot reset the optimizer. \"\n                raise TypeError(error_message)\n\n            optimizer.reset_neurons_state(\n                parameter=parameter,\n                neuron_indices=component_parameter_update.dead_neuron_indices,\n                axis=axis,\n                component_idx=component_idx,\n            )\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoderConfig","title":"<code>LitSparseAutoencoderConfig</code>","text":"<p>             Bases: <code>SparseAutoencoderConfig</code></p> <p>PyTorch Lightning Sparse Autoencoder config.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>class LitSparseAutoencoderConfig(SparseAutoencoderConfig):\n    \"\"\"PyTorch Lightning Sparse Autoencoder config.\"\"\"\n\n    component_names: list[str]\n\n    l1_coefficient: float = 0.001\n\n    resample_interval: PositiveInt = 200000000\n\n    max_n_resamples: NonNegativeInt = 4\n\n    resample_dead_neurons_dataset_size: PositiveInt = 100000000\n\n    resample_loss_dataset_size: PositiveInt = 819200\n\n    resample_threshold_is_dead_portion_fires: NonNegativeFloat = 0.0\n\n    def model_post_init(self, __context: Any) -&gt; None:  # noqa: ANN401\n        \"\"\"Model post init validation.\n\n        Args:\n            __context: Pydantic context.\n\n        Raises:\n            ValueError: If the number of component names does not match the number of components.\n        \"\"\"\n        if self.n_components and len(self.component_names) != self.n_components:\n            error_message = (\n                f\"Number of component names ({len(self.component_names)}) must match the number of \"\n                f\"components ({self.n_components})\"\n            )\n            raise ValueError(error_message)\n</code></pre>"},{"location":"reference/autoencoder/lightning/#sparse_autoencoder.autoencoder.lightning.LitSparseAutoencoderConfig.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Model post init validation.</p> <p>Parameters:</p> Name Type Description Default <code>__context</code> <code>Any</code> <p>Pydantic context.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of component names does not match the number of components.</p> Source code in <code>sparse_autoencoder/autoencoder/lightning.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:  # noqa: ANN401\n    \"\"\"Model post init validation.\n\n    Args:\n        __context: Pydantic context.\n\n    Raises:\n        ValueError: If the number of component names does not match the number of components.\n    \"\"\"\n    if self.n_components and len(self.component_names) != self.n_components:\n        error_message = (\n            f\"Number of component names ({len(self.component_names)}) must match the number of \"\n            f\"components ({self.n_components})\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/autoencoder/model/","title":"The Sparse Autoencoder Model","text":"<p>The Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.ForwardPassResult","title":"<code>ForwardPassResult</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>SAE model forward pass result.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>class ForwardPassResult(NamedTuple):\n    \"\"\"SAE model forward pass result.\"\"\"\n\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ]\n\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>class SparseAutoencoder(Module):\n    \"\"\"Sparse Autoencoder Model.\"\"\"\n\n    config: SparseAutoencoderConfig\n    \"\"\"Model config.\"\"\"\n\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Estimated Geometric Median of the Dataset.\n\n    Used for initialising :attr:`tied_bias`.\n    \"\"\"\n\n    tied_bias: Float[\n        Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    \"\"\"Tied Bias Parameter.\n\n    The same bias is used pre-encoder and post-decoder.\n    \"\"\"\n\n    pre_encoder_bias: TiedBias\n    \"\"\"Pre-Encoder Bias.\"\"\"\n\n    encoder: LinearEncoder\n    \"\"\"Encoder.\"\"\"\n\n    decoder: UnitNormDecoder\n    \"\"\"Decoder.\"\"\"\n\n    post_decoder_bias: TiedBias\n    \"\"\"Post-Decoder Bias.\"\"\"\n\n    def __init__(\n        self,\n        config: SparseAutoencoderConfig,\n        geometric_median_dataset: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ]\n        | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder Model.\n\n        Args:\n            config: Model config.\n            geometric_median_dataset: Estimated geometric median of the dataset.\n        \"\"\"\n        super().__init__()\n\n        self.config = config\n\n        # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n        # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n        tied_bias_shape = shape_with_optional_dimensions(\n            config.n_components, config.n_input_features\n        )\n        if geometric_median_dataset is not None:\n            self.geometric_median_dataset = geometric_median_dataset.clone()\n            self.geometric_median_dataset.requires_grad = False\n        else:\n            self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n            self.geometric_median_dataset.requires_grad = False\n\n        # Initialize the tied bias\n        self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n        self.initialize_tied_parameters()\n\n        # Initialize the components\n        self.pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n        self.encoder = LinearEncoder(\n            input_features=config.n_input_features,\n            learnt_features=config.n_learned_features,\n            n_components=config.n_components,\n        )\n\n        self.decoder = UnitNormDecoder(\n            learnt_features=config.n_learned_features,\n            decoded_features=config.n_input_features,\n            n_components=config.n_components,\n        )\n\n        self.post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; ForwardPassResult:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n        x = self.pre_encoder_bias(x)\n        learned_activations = self.encoder(x)\n        x = self.decoder(learned_activations)\n        decoded_activations = self.post_decoder_bias(x)\n\n        return ForwardPassResult(learned_activations, decoded_activations)\n\n    def initialize_tied_parameters(self) -&gt; None:\n        \"\"\"Initialize the tied parameters.\"\"\"\n        # The tied bias is initialised as the geometric median of the dataset\n        self.tied_bias.data = self.geometric_median_dataset\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n        self.initialize_tied_parameters()\n        for module in self.network:\n            if \"reset_parameters\" in dir(module):\n                module.reset_parameters()\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[ResetOptimizerParameterDetails]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return (\n            self.encoder.reset_optimizer_parameter_details\n            + self.decoder.reset_optimizer_parameter_details\n        )\n\n    def post_backwards_hook(self) -&gt; None:\n        \"\"\"Hook to be called after each learning step.\n\n        This can be used to e.g. constrain weights to unit norm.\n        \"\"\"\n        self.decoder.constrain_weights_unit_norm()\n\n    @staticmethod\n    @validate_call\n    def get_single_component_state_dict(\n        state: SparseAutoencoderState, component_idx: NonNegativeInt\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"Get the state dict for a single component.\n\n        Args:\n            state: Sparse Autoencoder state.\n            component_idx: Index of the component to get the state dict for.\n\n        Returns:\n            State dict for the component.\n\n        Raises:\n            ValueError: If the state dict doesn't contain a components dimension.\n        \"\"\"\n        # Check the state has a components dimension\n        if state.config.n_components is None:\n            error_message = (\n                \"Trying to load a single component from the state dict, but the state dict \"\n                \"doesn't contain a components dimension.\"\n            )\n            raise ValueError(error_message)\n\n        # Return the state dict for the component\n        return {key: value[component_idx] for key, value in state.state_dict.items()}\n\n    def save(self, file_path: Path) -&gt; None:\n        \"\"\"Save the model config and state dict to a file.\n\n        Args:\n            file_path: Path to save the model to.\n        \"\"\"\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        state = SparseAutoencoderState(config=self.config, state_dict=self.state_dict())\n        torch.save(state, file_path)\n\n    @staticmethod\n    def load(\n        file_path: FILE_LIKE,\n        component_idx: PositiveInt | None = None,\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"Load the model from a file.\n\n        Args:\n            file_path: Path to load the model from.\n            component_idx: If loading a state dict from a model that has been trained on multiple\n                components (e.g. all MLP layers) you may want to to load just one component. In this\n                case you can set `component_idx` to the index of the component to load. Note you\n                should not set this if you want to load a state dict from a model that has been\n                trained on a single component (or if you want to load all components).\n\n        Returns:\n            The loaded model.\n        \"\"\"\n        # Load the file\n        serialized_state = torch.load(file_path, map_location=torch.device(\"cpu\"))\n        state = SparseAutoencoderState.model_validate(serialized_state)\n\n        # Initialise the model\n        config = SparseAutoencoderConfig(\n            n_input_features=state.config.n_input_features,\n            n_learned_features=state.config.n_learned_features,\n            n_components=state.config.n_components if component_idx is None else None,\n        )\n        state_dict = (\n            SparseAutoencoder.get_single_component_state_dict(state, component_idx)\n            if component_idx is not None\n            else state.state_dict\n        )\n        model = SparseAutoencoder(config)\n        model.load_state_dict(state_dict)\n\n        return model\n\n    def save_to_wandb(\n        self,\n        artifact_name: str,\n        directory: DirectoryPath = DEFAULT_TMP_DIR,\n    ) -&gt; str:\n        \"\"\"Save the model to wandb.\n\n        Args:\n            artifact_name: A human-readable name for this artifact, which is how you can identify\n                this artifact in the UI or reference it in use_artifact calls. Names can contain\n                letters, numbers, underscores, hyphens, and dots. The name must be unique across a\n                project. Example: \"sweep_name 1e9 activations\".\n            directory: Directory to save the model to.\n\n        Returns:\n            Name of the wandb artifact.\n\n        Raises:\n            ValueError: If wandb is not initialised.\n        \"\"\"\n        # Save the file\n        directory.mkdir(parents=True, exist_ok=True)\n        file_name = artifact_name + \".pt\"\n        file_path = directory / file_name\n        self.save(file_path)\n\n        # Upload to wandb\n        if wandb.run is None:\n            error_message = \"Trying to save the model to wandb, but wandb is not initialised.\"\n            raise ValueError(error_message)\n        artifact = wandb.Artifact(\n            artifact_name,\n            type=\"model\",\n            description=\"Sparse Autoencoder model state, created with `sparse_autoencoder`.\",\n        )\n        artifact.add_file(str(file_path), name=\"sae-model-state.pt\")\n        artifact.save()\n        wandb.log_artifact(artifact)\n        artifact.wait()\n\n        return artifact.source_qualified_name\n\n    @staticmethod\n    def load_from_wandb(\n        wandb_artifact_name: str,\n        component_idx: PositiveInt | None = None,\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"Load the model from wandb.\n\n        Args:\n            wandb_artifact_name: Name of the wandb artifact to load the model from (e.g.\n                \"username/project/artifact_name:version\").\n            component_idx: If loading a state dict from a model that has been trained on multiple\n                components (e.g. all MLP layers) you may want to to load just one component. In this\n                case you can set `component_idx` to the index of the component to load. Note you\n                should not set this if you want to load a state dict from a model that has been\n                trained on a single component (or if you want to load all components).\n\n        Returns:\n            The loaded model.\n        \"\"\"\n        api = wandb.Api()\n        artifact = api.artifact(wandb_artifact_name, type=\"model\")\n        download_path = artifact.download()\n        return SparseAutoencoder.load(Path(download_path) / \"sae-model-state.pt\", component_idx)\n\n    def save_to_hugging_face(\n        self,\n        file_name: str,\n        repo_id: str,\n        directory: DirectoryPath = DEFAULT_TMP_DIR,\n        hf_access_token: str | None = None,\n    ) -&gt; None:\n        \"\"\"Save the model to Hugging Face.\n\n        Args:\n            file_name: Name of the file (e.g. \"model-something.pt\").\n            repo_id: ID of the repo to save the model to.\n            directory: Directory to save the model to.\n            hf_access_token: Hugging Face access token.\n        \"\"\"\n        # Save the file\n        directory.mkdir(parents=True, exist_ok=True)\n        file_path = directory / file_name\n        self.save(file_path)\n\n        # Upload to Hugging Face\n        api = HfApi(token=hf_access_token)\n        api.upload_file(\n            path_or_fileobj=file_path,\n            path_in_repo=file_name,\n            repo_id=repo_id,\n            repo_type=\"model\",\n        )\n\n    @staticmethod\n    def load_from_hugging_face(\n        file_name: str,\n        repo_id: str,\n        component_idx: PositiveInt | None = None,\n    ) -&gt; \"SparseAutoencoder\":\n        \"\"\"Load the model from Hugging Face.\n\n        Args:\n            file_name: File name of the .pt state file.\n            repo_id: ID of the repo to load the model from.\n            component_idx: If loading a state dict from a model that has been trained on multiple\n                components (e.g. all MLP layers) you may want to to load just one component. In this\n                case you can set `component_idx` to the index of the component to load. Note you\n                should not set this if you want to load a state dict from a model that has been\n                trained on a single component (or if you want to load all components).\n\n        Returns:\n            The loaded model.\n        \"\"\"\n        local_file = hf_hub_download(\n            repo_id=repo_id,\n            repo_type=\"model\",\n            filename=file_name,\n            revision=\"main\",\n        )\n\n        return SparseAutoencoder.load(Path(local_file), component_idx)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.config","title":"<code>config: SparseAutoencoderConfig = config</code>  <code>instance-attribute</code>","text":"<p>Model config.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.decoder","title":"<code>decoder: UnitNormDecoder = UnitNormDecoder(learnt_features=config.n_learned_features, decoded_features=config.n_input_features, n_components=config.n_components)</code>  <code>instance-attribute</code>","text":"<p>Decoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.encoder","title":"<code>encoder: LinearEncoder = LinearEncoder(input_features=config.n_input_features, learnt_features=config.n_learned_features, n_components=config.n_components)</code>  <code>instance-attribute</code>","text":"<p>Encoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.geometric_median_dataset","title":"<code>geometric_median_dataset: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>instance-attribute</code>","text":"<p>Estimated Geometric Median of the Dataset.</p> <p>Used for initialising :attr:<code>tied_bias</code>.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: TiedBias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)</code>  <code>instance-attribute</code>","text":"<p>Post-Decoder Bias.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: TiedBias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)</code>  <code>instance-attribute</code>","text":"<p>Pre-Encoder Bias.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[ResetOptimizerParameterDetails]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[ResetOptimizerParameterDetails]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[ResetOptimizerParameterDetails]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.tied_bias","title":"<code>tied_bias: Float[Parameter, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)] = Parameter(torch.empty(tied_bias_shape))</code>  <code>instance-attribute</code>","text":"<p>Tied Bias Parameter.</p> <p>The same bias is used pre-encoder and post-decoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.__init__","title":"<code>__init__(config, geometric_median_dataset=None)</code>","text":"<p>Initialize the Sparse Autoencoder Model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SparseAutoencoderConfig</code> <p>Model config.</p> required <code>geometric_median_dataset</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)] | None</code> <p>Estimated geometric median of the dataset.</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def __init__(\n    self,\n    config: SparseAutoencoderConfig,\n    geometric_median_dataset: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n    | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder Model.\n\n    Args:\n        config: Model config.\n        geometric_median_dataset: Estimated geometric median of the dataset.\n    \"\"\"\n    super().__init__()\n\n    self.config = config\n\n    # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n    # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n    tied_bias_shape = shape_with_optional_dimensions(\n        config.n_components, config.n_input_features\n    )\n    if geometric_median_dataset is not None:\n        self.geometric_median_dataset = geometric_median_dataset.clone()\n        self.geometric_median_dataset.requires_grad = False\n    else:\n        self.geometric_median_dataset = torch.zeros(tied_bias_shape)\n        self.geometric_median_dataset.requires_grad = False\n\n    # Initialize the tied bias\n    self.tied_bias = Parameter(torch.empty(tied_bias_shape))\n    self.initialize_tied_parameters()\n\n    # Initialize the components\n    self.pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n    self.encoder = LinearEncoder(\n        input_features=config.n_input_features,\n        learnt_features=config.n_learned_features,\n        n_components=config.n_components,\n    )\n\n    self.decoder = UnitNormDecoder(\n        learnt_features=config.n_learned_features,\n        decoded_features=config.n_input_features,\n        n_components=config.n_components,\n    )\n\n    self.post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>ForwardPassResult</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; ForwardPassResult:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n    x = self.pre_encoder_bias(x)\n    learned_activations = self.encoder(x)\n    x = self.decoder(learned_activations)\n    decoded_activations = self.post_decoder_bias(x)\n\n    return ForwardPassResult(learned_activations, decoded_activations)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.get_single_component_state_dict","title":"<code>get_single_component_state_dict(state, component_idx)</code>  <code>staticmethod</code>","text":"<p>Get the state dict for a single component.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SparseAutoencoderState</code> <p>Sparse Autoencoder state.</p> required <code>component_idx</code> <code>NonNegativeInt</code> <p>Index of the component to get the state dict for.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>State dict for the component.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the state dict doesn't contain a components dimension.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\n@validate_call\ndef get_single_component_state_dict(\n    state: SparseAutoencoderState, component_idx: NonNegativeInt\n) -&gt; dict[str, Tensor]:\n    \"\"\"Get the state dict for a single component.\n\n    Args:\n        state: Sparse Autoencoder state.\n        component_idx: Index of the component to get the state dict for.\n\n    Returns:\n        State dict for the component.\n\n    Raises:\n        ValueError: If the state dict doesn't contain a components dimension.\n    \"\"\"\n    # Check the state has a components dimension\n    if state.config.n_components is None:\n        error_message = (\n            \"Trying to load a single component from the state dict, but the state dict \"\n            \"doesn't contain a components dimension.\"\n        )\n        raise ValueError(error_message)\n\n    # Return the state dict for the component\n    return {key: value[component_idx] for key, value in state.state_dict.items()}\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.initialize_tied_parameters","title":"<code>initialize_tied_parameters()</code>","text":"<p>Initialize the tied parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def initialize_tied_parameters(self) -&gt; None:\n    \"\"\"Initialize the tied parameters.\"\"\"\n    # The tied bias is initialised as the geometric median of the dataset\n    self.tied_bias.data = self.geometric_median_dataset\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.load","title":"<code>load(file_path, component_idx=None)</code>  <code>staticmethod</code>","text":"<p>Load the model from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>FILE_LIKE</code> <p>Path to load the model from.</p> required <code>component_idx</code> <code>PositiveInt | None</code> <p>If loading a state dict from a model that has been trained on multiple components (e.g. all MLP layers) you may want to to load just one component. In this case you can set <code>component_idx</code> to the index of the component to load. Note you should not set this if you want to load a state dict from a model that has been trained on a single component (or if you want to load all components).</p> <code>None</code> <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The loaded model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\ndef load(\n    file_path: FILE_LIKE,\n    component_idx: PositiveInt | None = None,\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"Load the model from a file.\n\n    Args:\n        file_path: Path to load the model from.\n        component_idx: If loading a state dict from a model that has been trained on multiple\n            components (e.g. all MLP layers) you may want to to load just one component. In this\n            case you can set `component_idx` to the index of the component to load. Note you\n            should not set this if you want to load a state dict from a model that has been\n            trained on a single component (or if you want to load all components).\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    # Load the file\n    serialized_state = torch.load(file_path, map_location=torch.device(\"cpu\"))\n    state = SparseAutoencoderState.model_validate(serialized_state)\n\n    # Initialise the model\n    config = SparseAutoencoderConfig(\n        n_input_features=state.config.n_input_features,\n        n_learned_features=state.config.n_learned_features,\n        n_components=state.config.n_components if component_idx is None else None,\n    )\n    state_dict = (\n        SparseAutoencoder.get_single_component_state_dict(state, component_idx)\n        if component_idx is not None\n        else state.state_dict\n    )\n    model = SparseAutoencoder(config)\n    model.load_state_dict(state_dict)\n\n    return model\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.load_from_hugging_face","title":"<code>load_from_hugging_face(file_name, repo_id, component_idx=None)</code>  <code>staticmethod</code>","text":"<p>Load the model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>File name of the .pt state file.</p> required <code>repo_id</code> <code>str</code> <p>ID of the repo to load the model from.</p> required <code>component_idx</code> <code>PositiveInt | None</code> <p>If loading a state dict from a model that has been trained on multiple components (e.g. all MLP layers) you may want to to load just one component. In this case you can set <code>component_idx</code> to the index of the component to load. Note you should not set this if you want to load a state dict from a model that has been trained on a single component (or if you want to load all components).</p> <code>None</code> <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The loaded model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\ndef load_from_hugging_face(\n    file_name: str,\n    repo_id: str,\n    component_idx: PositiveInt | None = None,\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"Load the model from Hugging Face.\n\n    Args:\n        file_name: File name of the .pt state file.\n        repo_id: ID of the repo to load the model from.\n        component_idx: If loading a state dict from a model that has been trained on multiple\n            components (e.g. all MLP layers) you may want to to load just one component. In this\n            case you can set `component_idx` to the index of the component to load. Note you\n            should not set this if you want to load a state dict from a model that has been\n            trained on a single component (or if you want to load all components).\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    local_file = hf_hub_download(\n        repo_id=repo_id,\n        repo_type=\"model\",\n        filename=file_name,\n        revision=\"main\",\n    )\n\n    return SparseAutoencoder.load(Path(local_file), component_idx)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.load_from_wandb","title":"<code>load_from_wandb(wandb_artifact_name, component_idx=None)</code>  <code>staticmethod</code>","text":"<p>Load the model from wandb.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_artifact_name</code> <code>str</code> <p>Name of the wandb artifact to load the model from (e.g. \"username/project/artifact_name:version\").</p> required <code>component_idx</code> <code>PositiveInt | None</code> <p>If loading a state dict from a model that has been trained on multiple components (e.g. all MLP layers) you may want to to load just one component. In this case you can set <code>component_idx</code> to the index of the component to load. Note you should not set this if you want to load a state dict from a model that has been trained on a single component (or if you want to load all components).</p> <code>None</code> <p>Returns:</p> Type Description <code>SparseAutoencoder</code> <p>The loaded model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@staticmethod\ndef load_from_wandb(\n    wandb_artifact_name: str,\n    component_idx: PositiveInt | None = None,\n) -&gt; \"SparseAutoencoder\":\n    \"\"\"Load the model from wandb.\n\n    Args:\n        wandb_artifact_name: Name of the wandb artifact to load the model from (e.g.\n            \"username/project/artifact_name:version\").\n        component_idx: If loading a state dict from a model that has been trained on multiple\n            components (e.g. all MLP layers) you may want to to load just one component. In this\n            case you can set `component_idx` to the index of the component to load. Note you\n            should not set this if you want to load a state dict from a model that has been\n            trained on a single component (or if you want to load all components).\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    api = wandb.Api()\n    artifact = api.artifact(wandb_artifact_name, type=\"model\")\n    download_path = artifact.download()\n    return SparseAutoencoder.load(Path(download_path) / \"sae-model-state.pt\", component_idx)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.post_backwards_hook","title":"<code>post_backwards_hook()</code>","text":"<p>Hook to be called after each learning step.</p> <p>This can be used to e.g. constrain weights to unit norm.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def post_backwards_hook(self) -&gt; None:\n    \"\"\"Hook to be called after each learning step.\n\n    This can be used to e.g. constrain weights to unit norm.\n    \"\"\"\n    self.decoder.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n    self.initialize_tied_parameters()\n    for module in self.network:\n        if \"reset_parameters\" in dir(module):\n            module.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.save","title":"<code>save(file_path)</code>","text":"<p>Save the model config and state dict to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to save the model to.</p> required Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def save(self, file_path: Path) -&gt; None:\n    \"\"\"Save the model config and state dict to a file.\n\n    Args:\n        file_path: Path to save the model to.\n    \"\"\"\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    state = SparseAutoencoderState(config=self.config, state_dict=self.state_dict())\n    torch.save(state, file_path)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.save_to_hugging_face","title":"<code>save_to_hugging_face(file_name, repo_id, directory=DEFAULT_TMP_DIR, hf_access_token=None)</code>","text":"<p>Save the model to Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the file (e.g. \"model-something.pt\").</p> required <code>repo_id</code> <code>str</code> <p>ID of the repo to save the model to.</p> required <code>directory</code> <code>DirectoryPath</code> <p>Directory to save the model to.</p> <code>DEFAULT_TMP_DIR</code> <code>hf_access_token</code> <code>str | None</code> <p>Hugging Face access token.</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def save_to_hugging_face(\n    self,\n    file_name: str,\n    repo_id: str,\n    directory: DirectoryPath = DEFAULT_TMP_DIR,\n    hf_access_token: str | None = None,\n) -&gt; None:\n    \"\"\"Save the model to Hugging Face.\n\n    Args:\n        file_name: Name of the file (e.g. \"model-something.pt\").\n        repo_id: ID of the repo to save the model to.\n        directory: Directory to save the model to.\n        hf_access_token: Hugging Face access token.\n    \"\"\"\n    # Save the file\n    directory.mkdir(parents=True, exist_ok=True)\n    file_path = directory / file_name\n    self.save(file_path)\n\n    # Upload to Hugging Face\n    api = HfApi(token=hf_access_token)\n    api.upload_file(\n        path_or_fileobj=file_path,\n        path_in_repo=file_name,\n        repo_id=repo_id,\n        repo_type=\"model\",\n    )\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.save_to_wandb","title":"<code>save_to_wandb(artifact_name, directory=DEFAULT_TMP_DIR)</code>","text":"<p>Save the model to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_name</code> <code>str</code> <p>A human-readable name for this artifact, which is how you can identify this artifact in the UI or reference it in use_artifact calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project. Example: \"sweep_name 1e9 activations\".</p> required <code>directory</code> <code>DirectoryPath</code> <p>Directory to save the model to.</p> <code>DEFAULT_TMP_DIR</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of the wandb artifact.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If wandb is not initialised.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def save_to_wandb(\n    self,\n    artifact_name: str,\n    directory: DirectoryPath = DEFAULT_TMP_DIR,\n) -&gt; str:\n    \"\"\"Save the model to wandb.\n\n    Args:\n        artifact_name: A human-readable name for this artifact, which is how you can identify\n            this artifact in the UI or reference it in use_artifact calls. Names can contain\n            letters, numbers, underscores, hyphens, and dots. The name must be unique across a\n            project. Example: \"sweep_name 1e9 activations\".\n        directory: Directory to save the model to.\n\n    Returns:\n        Name of the wandb artifact.\n\n    Raises:\n        ValueError: If wandb is not initialised.\n    \"\"\"\n    # Save the file\n    directory.mkdir(parents=True, exist_ok=True)\n    file_name = artifact_name + \".pt\"\n    file_path = directory / file_name\n    self.save(file_path)\n\n    # Upload to wandb\n    if wandb.run is None:\n        error_message = \"Trying to save the model to wandb, but wandb is not initialised.\"\n        raise ValueError(error_message)\n    artifact = wandb.Artifact(\n        artifact_name,\n        type=\"model\",\n        description=\"Sparse Autoencoder model state, created with `sparse_autoencoder`.\",\n    )\n    artifact.add_file(str(file_path), name=\"sae-model-state.pt\")\n    artifact.save()\n    wandb.log_artifact(artifact)\n    artifact.wait()\n\n    return artifact.source_qualified_name\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderConfig","title":"<code>SparseAutoencoderConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>SAE model config.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>class SparseAutoencoderConfig(BaseModel):\n    \"\"\"SAE model config.\"\"\"\n\n    n_input_features: PositiveInt\n    \"\"\"Number of input features.\n\n    E.g. `d_mlp` if training on MLP activations from TransformerLens).\n    \"\"\"\n\n    n_learned_features: PositiveInt\n    \"\"\"Number of learned features.\n\n    The initial paper experimented with 1 to 256 times the number of input features, and primarily\n    used a multiple of 8.\"\"\"\n\n    n_components: PositiveInt | None = None\n    \"\"\"Number of source model components the SAE is trained on.\"\"\n\n    This is useful if you want to train the SAE on several components of the source model at once.\n    If `None`, the SAE is assumed to be trained on just one component (in this case the model won't\n    contain a component axis in any of the parameters).\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderConfig.n_components","title":"<code>n_components: PositiveInt | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of source model components the SAE is trained on.\"\"</p> <p>This is useful if you want to train the SAE on several components of the source model at once. If <code>None</code>, the SAE is assumed to be trained on just one component (in this case the model won't contain a component axis in any of the parameters).</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderConfig.n_input_features","title":"<code>n_input_features: PositiveInt</code>  <code>instance-attribute</code>","text":"<p>Number of input features.</p> <p>E.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderConfig.n_learned_features","title":"<code>n_learned_features: PositiveInt</code>  <code>instance-attribute</code>","text":"<p>Number of learned features.</p> <p>The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderState","title":"<code>SparseAutoencoderState</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>SAE model state.</p> <p>Used for saving and loading the model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>class SparseAutoencoderState(BaseModel, arbitrary_types_allowed=True):\n    \"\"\"SAE model state.\n\n    Used for saving and loading the model.\n    \"\"\"\n\n    config: SparseAutoencoderConfig\n    \"\"\"Model config.\"\"\"\n\n    state_dict: dict[str, Tensor]\n    \"\"\"Model state dict.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderState.config","title":"<code>config: SparseAutoencoderConfig</code>  <code>instance-attribute</code>","text":"<p>Model config.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderState.state_dict","title":"<code>state_dict: dict[str, Tensor]</code>  <code>instance-attribute</code>","text":"<p>Model state dict.</p>"},{"location":"reference/autoencoder/types/","title":"Autoencoder types","text":"<p>Autoencoder types.</p>"},{"location":"reference/autoencoder/types/#sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails","title":"<code>ResetOptimizerParameterDetails</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Reset Optimizer Parameter Details.</p> <p>Details of a parameter that should be reset in the optimizer, when resetting its corresponding dictionary vectors.</p> Source code in <code>sparse_autoencoder/autoencoder/types.py</code> <pre><code>class ResetOptimizerParameterDetails(NamedTuple):\n    \"\"\"Reset Optimizer Parameter Details.\n\n    Details of a parameter that should be reset in the optimizer, when resetting\n    its corresponding dictionary vectors.\n    \"\"\"\n\n    parameter: Parameter\n    \"\"\"Parameter to reset.\"\"\"\n\n    axis: int\n    \"\"\"Axis of the parameter to reset.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/types/#sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails.axis","title":"<code>axis: int</code>  <code>instance-attribute</code>","text":"<p>Axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/types/#sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails.parameter","title":"<code>parameter: Parameter</code>  <code>instance-attribute</code>","text":"<p>Parameter to reset.</p>"},{"location":"reference/autoencoder/components/","title":"Sparse autoencoder components","text":"<p>Sparse autoencoder components.</p>"},{"location":"reference/autoencoder/components/linear_encoder/","title":"Linear encoder layer","text":"<p>Linear encoder layer.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder","title":"<code>LinearEncoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Linear encoder layer.</p> <p>Linear encoder layer (essentially <code>nn.Linear</code>, with a ReLU activation function). Designed to be used as the encoder in a sparse autoencoder (excluding any outer tied bias).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\     W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\     b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\     f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output} \\end{align*} \\] Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\nclass LinearEncoder(Module):\n    r\"\"\"Linear encoder layer.\n\n    Linear encoder layer (essentially `nn.Linear`, with a ReLU activation function). Designed to be\n    used as the encoder in a sparse autoencoder (excluding any outer tied bias).\n\n    $$\n    \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\\n        W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\\n        b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\\n        f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output}\n    \\end{align*}\n    $$\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _input_features: int\n    \"\"\"Number of input features from the source model.\"\"\"\n\n    _n_components: int | None\n\n    weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ]\n    \"\"\"Weight parameter.\n\n    Each row in the weights matrix acts as a dictionary vector, representing a single basis\n    element in the learned activation space.\n    \"\"\"\n\n    bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    \"\"\"Bias parameter.\"\"\"\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[ResetOptimizerParameterDetails]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return [\n            ResetOptimizerParameterDetails(parameter=self.weight, axis=-2),\n            ResetOptimizerParameterDetails(parameter=self.bias, axis=-1),\n        ]\n\n    activation_function: ReLU\n    \"\"\"Activation function.\"\"\"\n\n    @validate_call\n    def __init__(\n        self,\n        input_features: PositiveInt,\n        learnt_features: PositiveInt,\n        n_components: PositiveInt | None,\n    ):\n        \"\"\"Initialize the linear encoder layer.\n\n        Args:\n            input_features: Number of input features to the autoencoder.\n            learnt_features: Number of learnt features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n        \"\"\"\n        super().__init__()\n\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n        self._n_components = n_components\n\n        self.weight = Parameter(\n            torch.empty(\n                shape_with_optional_dimensions(n_components, learnt_features, input_features),\n            )\n        )\n        self.bias = Parameter(\n            torch.zeros(shape_with_optional_dimensions(n_components, learnt_features))\n        )\n        self.activation_function = ReLU()\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\"\"\"\n        # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n        # `nonlinerity` must be changed.\n        init.kaiming_uniform_(self.weight, nonlinearity=\"relu\")\n\n        # Bias (approach from nn.Linear)\n        fan_in = self.weight.size(1)\n        bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n        init.uniform_(self.bias, -bound, bound)\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        z = (\n            einops.einsum(\n                x,\n                self.weight,\n                f\"{Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}, \\\n                    ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                    -&gt; {Axis.BATCH} ... {Axis.LEARNT_FEATURE}\",\n            )\n            + self.bias\n        )\n\n        return self.activation_function(z)\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: Int64[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)],\n        updated_dictionary_weights: Float[\n            Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        component_idx: int | None = None,\n    ) -&gt; None:\n        \"\"\"Update encoder dictionary vectors.\n\n        Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_dictionary_weights: Updated weights for just these dictionary vectors.\n            component_idx: Component index to update.\n\n        Raises:\n            ValueError: If there are multiple components and `component_idx` is not specified.\n        \"\"\"\n        if dictionary_vector_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if component_idx is None:\n                if self._n_components is not None:\n                    error_message = \"component_idx must be specified when n_components is not None\"\n                    raise ValueError(error_message)\n\n                self.weight[dictionary_vector_indices] = updated_dictionary_weights\n            else:\n                self.weight[component_idx, dictionary_vector_indices] = updated_dictionary_weights\n\n    @final\n    def update_bias(\n        self,\n        update_parameter_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_bias_features: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        component_idx: int | None = None,\n    ) -&gt; None:\n        \"\"\"Update encoder bias.\n\n        Args:\n            update_parameter_indices: Indices of the bias features to update.\n            updated_bias_features: Updated bias features for just these indices.\n            component_idx: Component index to update.\n\n        Raises:\n            ValueError: If there are multiple components and `component_idx` is not specified.\n        \"\"\"\n        if update_parameter_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if component_idx is None:\n                if self._n_components is not None:\n                    error_message = \"component_idx must be specified when n_components is not None\"\n                    raise ValueError(error_message)\n\n                self.bias[update_parameter_indices] = updated_bias_features\n            else:\n                self.bias[component_idx, update_parameter_indices] = updated_bias_features\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return (\n            f\"input_features={self._input_features}, \"\n            f\"learnt_features={self._learnt_features}, \"\n            f\"n_components={self._n_components}\"\n        )\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.activation_function","title":"<code>activation_function: ReLU = ReLU()</code>  <code>instance-attribute</code>","text":"<p>Activation function.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)] = Parameter(torch.zeros(shape_with_optional_dimensions(n_components, learnt_features)))</code>  <code>instance-attribute</code>","text":"<p>Bias parameter.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[ResetOptimizerParameterDetails]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[ResetOptimizerParameterDetails]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[ResetOptimizerParameterDetails]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)] = Parameter(torch.empty(shape_with_optional_dimensions(n_components, learnt_features, input_features)))</code>  <code>instance-attribute</code>","text":"<p>Weight parameter.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.__init__","title":"<code>__init__(input_features, learnt_features, n_components)</code>","text":"<p>Initialize the linear encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>PositiveInt</code> <p>Number of input features to the autoencoder.</p> required <code>learnt_features</code> <code>PositiveInt</code> <p>Number of learnt features in the autoencoder.</p> required <code>n_components</code> <code>PositiveInt | None</code> <p>Number of source model components the SAE is trained on.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    input_features: PositiveInt,\n    learnt_features: PositiveInt,\n    n_components: PositiveInt | None,\n):\n    \"\"\"Initialize the linear encoder layer.\n\n    Args:\n        input_features: Number of input features to the autoencoder.\n        learnt_features: Number of learnt features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n    \"\"\"\n    super().__init__()\n\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n    self._n_components = n_components\n\n    self.weight = Parameter(\n        torch.empty(\n            shape_with_optional_dimensions(n_components, learnt_features, input_features),\n        )\n    )\n    self.bias = Parameter(\n        torch.zeros(shape_with_optional_dimensions(n_components, learnt_features))\n    )\n    self.activation_function = ReLU()\n\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return (\n        f\"input_features={self._input_features}, \"\n        f\"learnt_features={self._learnt_features}, \"\n        f\"n_components={self._n_components}\"\n    )\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    z = (\n        einops.einsum(\n            x,\n            self.weight,\n            f\"{Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.BATCH} ... {Axis.LEARNT_FEATURE}\",\n        )\n        + self.bias\n    )\n\n    return self.activation_function(z)\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\"\"\"\n    # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n    # `nonlinerity` must be changed.\n    init.kaiming_uniform_(self.weight, nonlinearity=\"relu\")\n\n    # Bias (approach from nn.Linear)\n    fan_in = self.weight.size(1)\n    bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n    init.uniform_(self.bias, -bound, bound)\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.update_bias","title":"<code>update_bias(update_parameter_indices, updated_bias_features, component_idx=None)</code>","text":"<p>Update encoder bias.</p> <p>Parameters:</p> Name Type Description Default <code>update_parameter_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the bias features to update.</p> required <code>updated_bias_features</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Updated bias features for just these indices.</p> required <code>component_idx</code> <code>int | None</code> <p>Component index to update.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are multiple components and <code>component_idx</code> is not specified.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\ndef update_bias(\n    self,\n    update_parameter_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_bias_features: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    component_idx: int | None = None,\n) -&gt; None:\n    \"\"\"Update encoder bias.\n\n    Args:\n        update_parameter_indices: Indices of the bias features to update.\n        updated_bias_features: Updated bias features for just these indices.\n        component_idx: Component index to update.\n\n    Raises:\n        ValueError: If there are multiple components and `component_idx` is not specified.\n    \"\"\"\n    if update_parameter_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if component_idx is None:\n            if self._n_components is not None:\n                error_message = \"component_idx must be specified when n_components is not None\"\n                raise ValueError(error_message)\n\n            self.bias[update_parameter_indices] = updated_bias_features\n        else:\n            self.bias[component_idx, update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_dictionary_weights, component_idx=None)</code>","text":"<p>Update encoder dictionary vectors.</p> <p>Updates the dictionary vectors (columns in the weight matrix) with the given values.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>Int64[Tensor, names(LEARNT_FEATURE_IDX)]</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_dictionary_weights</code> <code>Float[Tensor, names(LEARNT_FEATURE_IDX, INPUT_OUTPUT_FEATURE)]</code> <p>Updated weights for just these dictionary vectors.</p> required <code>component_idx</code> <code>int | None</code> <p>Component index to update.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are multiple components and <code>component_idx</code> is not specified.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: Int64[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)],\n    updated_dictionary_weights: Float[\n        Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    component_idx: int | None = None,\n) -&gt; None:\n    \"\"\"Update encoder dictionary vectors.\n\n    Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_dictionary_weights: Updated weights for just these dictionary vectors.\n        component_idx: Component index to update.\n\n    Raises:\n        ValueError: If there are multiple components and `component_idx` is not specified.\n    \"\"\"\n    if dictionary_vector_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if component_idx is None:\n            if self._n_components is not None:\n                error_message = \"component_idx must be specified when n_components is not None\"\n                raise ValueError(error_message)\n\n            self.weight[dictionary_vector_indices] = updated_dictionary_weights\n        else:\n            self.weight[component_idx, dictionary_vector_indices] = updated_dictionary_weights\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/","title":"Tied Biases (Pre-Encoder and Post-Decoder)","text":"<p>Tied Biases (Pre-Encoder and Post-Decoder).</p>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias","title":"<code>TiedBias</code>","text":"<p>             Bases: <code>Module</code></p> <p>Tied Bias Layer.</p> <p>The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding.</p> <p>The bias parameter must be initialised in the parent module, and then passed to this layer.</p> <p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>@final\nclass TiedBias(Module):\n    \"\"\"Tied Bias Layer.\n\n    The tied pre-encoder bias is a learned bias term that is subtracted from the input before\n    encoding, and added back after decoding.\n\n    The bias parameter must be initialised in the parent module, and then passed to this layer.\n\n    https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias\n    \"\"\"\n\n    _bias_position: TiedBiasPosition\n\n    _bias_reference: Float[\n        Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ]\n\n    @property\n    def bias(\n        self,\n    ) -&gt; Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Bias.\"\"\"\n        return self._bias_reference\n\n    def __init__(\n        self,\n        bias_reference: Float[\n            Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        position: TiedBiasPosition,\n    ) -&gt; None:\n        \"\"\"Initialize the bias layer.\n\n        Args:\n            bias_reference: Tied bias parameter (initialised in the parent module), used for both\n                the pre-encoder and post-encoder bias. The original paper initialised this using the\n                geometric median of the dataset.\n            position: Whether this is the pre-encoder or post-encoder bias.\n        \"\"\"\n        super().__init__()\n\n        self._bias_reference = bias_reference\n\n        # Support string literals as well as enums\n        self._bias_position = position\n\n    def forward(\n        self,\n        x: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        # If this is the pre-encoder bias, we subtract the bias from the input.\n        if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n            return x - self.bias\n\n        # If it's the post-encoder bias, we add the bias to the input.\n        return x + self.bias\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.bias","title":"<code>bias: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.__init__","title":"<code>__init__(bias_reference, position)</code>","text":"<p>Initialize the bias layer.</p> <p>Parameters:</p> Name Type Description Default <code>bias_reference</code> <code>Float[Parameter, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset.</p> required <code>position</code> <code>TiedBiasPosition</code> <p>Whether this is the pre-encoder or post-encoder bias.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def __init__(\n    self,\n    bias_reference: Float[\n        Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    position: TiedBiasPosition,\n) -&gt; None:\n    \"\"\"Initialize the bias layer.\n\n    Args:\n        bias_reference: Tied bias parameter (initialised in the parent module), used for both\n            the pre-encoder and post-encoder bias. The original paper initialised this using the\n            geometric median of the dataset.\n        position: Whether this is the pre-encoder or post-encoder bias.\n    \"\"\"\n    super().__init__()\n\n    self._bias_reference = bias_reference\n\n    # Support string literals as well as enums\n    self._bias_position = position\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def forward(\n    self,\n    x: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    # If this is the pre-encoder bias, we subtract the bias from the input.\n    if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n        return x - self.bias\n\n    # If it's the post-encoder bias, we add the bias to the input.\n    return x + self.bias\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBiasPosition","title":"<code>TiedBiasPosition</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Tied Bias Position.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>class TiedBiasPosition(str, Enum):\n    \"\"\"Tied Bias Position.\"\"\"\n\n    PRE_ENCODER = \"pre_encoder\"\n    POST_DECODER = \"post_decoder\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/","title":"Linear layer with unit norm weights","text":"<p>Linear layer with unit norm weights.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder","title":"<code>UnitNormDecoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Constrained unit norm linear decoder layer.</p> <p>Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. It also requires <code>constrain_weights_unit_norm</code> to be called after each gradient step, to prevent drift of the dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\     W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\     z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)} \\end{align*} \\] Motivation <p>Normalisation of the columns (dictionary features) prevents the model from reducing the sparsity loss term by increasing the size of the feature vectors in \\(W_d\\).</p> <p>Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>@final\nclass UnitNormDecoder(Module):\n    r\"\"\"Constrained unit norm linear decoder layer.\n\n    Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are\n    constrained to have unit norm. This is done by removing the gradient information parallel to the\n    dictionary vectors before applying the gradient step, using a backward hook. It also requires\n    `constrain_weights_unit_norm` to be called after each gradient step, to prevent drift of the\n    dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the\n    gradient, but instead follow a modified gradient that includes momentum).\n\n    $$ \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\\n        W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\\n        z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)}\n    \\end{align*} $$\n\n    Motivation:\n        Normalisation of the columns (dictionary features) prevents the model from reducing the\n        sparsity loss term by increasing the size of the feature vectors in $W_d$.\n\n        Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary\n        Learning* paper found that removing the gradient information parallel to the dictionary\n        vectors before applying the gradient step, rather than resetting the dictionary vectors to\n        unit norm after each gradient step, results in a small but real reduction in total\n        loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _decoded_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _n_components: int | None\n\n    weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE),\n    ]\n    \"\"\"Weight parameter.\n\n    Each column in the weights matrix acts as a dictionary vector, representing a single basis\n    element in the learned activation space.\n    \"\"\"\n\n    @property\n    def reset_optimizer_parameter_details(self) -&gt; list[ResetOptimizerParameterDetails]:\n        \"\"\"Reset optimizer parameter details.\n\n        Details of the parameters that should be reset in the optimizer, when resetting\n        dictionary vectors.\n\n        Returns:\n            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to\n            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.\n        \"\"\"\n        return [ResetOptimizerParameterDetails(parameter=self.weight, axis=-1)]\n\n    @validate_call\n    def __init__(\n        self,\n        learnt_features: PositiveInt,\n        decoded_features: PositiveInt,\n        n_components: PositiveInt | None,\n        *,\n        enable_gradient_hook: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the constrained unit norm linear layer.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            n_components: Number of source model components the SAE is trained on.\n            enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n                applying the gradient step, to maintain unit norm of the dictionary vectors).\n        \"\"\"\n        super().__init__()\n\n        self._learnt_features = learnt_features\n        self._decoded_features = decoded_features\n        self._n_components = n_components\n\n        # Create the linear layer as per the standard PyTorch linear layer\n        self.weight = Parameter(\n            torch.empty(\n                shape_with_optional_dimensions(n_components, decoded_features, learnt_features),\n            )\n        )\n        self.reset_parameters()\n\n        # Register backward hook to remove any gradient information parallel to the dictionary\n        # vectors (columns of the weight matrix) before applying the gradient step.\n        if enable_gradient_hook:\n            self.weight.register_hook(self._weight_backward_hook)\n\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: Int64[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n        ],\n        updated_weights: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE_IDX),\n        ],\n        component_idx: int | None = None,\n    ) -&gt; None:\n        \"\"\"Update decoder dictionary vectors.\n\n        Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n        this is used when resampling neurons (dictionary vectors) that have died.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_weights: Updated weights for just these dictionary vectors.\n            component_idx: Component index to update.\n\n        Raises:\n            ValueError: If `component_idx` is not specified when `n_components` is not None.\n        \"\"\"\n        if dictionary_vector_indices.numel() == 0:\n            return\n\n        with torch.no_grad():\n            if component_idx is None:\n                if self._n_components is not None:\n                    error_message = \"component_idx must be specified when n_components is not None\"\n                    raise ValueError(error_message)\n\n                self.weight[:, dictionary_vector_indices] = updated_weights\n            else:\n                self.weight[component_idx, :, dictionary_vector_indices] = updated_weights\n\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\n\n        Warning:\n            Note this must be called after each gradient step. This is because optimisers such as\n            Adam don't strictly follow the gradient, but instead follow a modified gradient that\n            includes momentum. This means that the gradient step can change the norm of the\n            dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n            Note this can't be applied directly in the backward hook, as it would interfere with a\n            variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n            with asynchronous operations, etc).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; layer = UnitNormDecoder(3, 3, None)\n            &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n            &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n            &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0]\n\n        \"\"\"\n        with torch.no_grad():\n            torch.nn.functional.normalize(self.weight, dim=-2, out=self.weight)\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n            &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None)\n            &gt;&gt;&gt; layer.reset_parameters()\n            &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n            &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0, 1.0]\n\n        \"\"\"\n        # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n        # normalisation here, since we immediately scale the weights to have unit norm (so the\n        # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n        self.weight: Float[\n            Parameter,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ] = init.normal_(self.weight, mean=0, std=1)  # type: ignore\n\n        # Scale so that each row has unit norm\n        self.constrain_weights_unit_norm()\n\n    def _weight_backward_hook(\n        self,\n        grad: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ],\n    ) -&gt; Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n    ]:\n        r\"\"\"Unit norm backward hook.\n\n        By subtracting the projection of the gradient onto the dictionary vectors, we remove the\n        component of the gradient that is parallel to the dictionary vectors and just keep the\n        component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere).\n        The result is that the backward pass does not change the norm of the dictionary vectors.\n\n        $$\n        \\begin{align*}\n            W_d &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Decoder weight matrix} \\\\\n            g &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Gradient w.r.t. } W_d\n                \\text{ from the backward pass} \\\\\n            W_{d, \\text{norm}} &amp;= \\frac{W_d}{\\|W_d\\|} = \\text{Normalized decoder weight matrix\n                (over columns)} \\\\\n            g_{\\parallel} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g\n                \\text{ parallel to } W_{d, \\text{norm}} \\\\\n            g_{\\perp} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g \\text{ orthogonal to }\n                W_{d, \\text{norm}} \\\\\n            g_{\\parallel} &amp;= W_{d, \\text{norm}} \\cdot (W_{d, \\text{norm}}^\\top \\cdot g) \\\\\n            g_{\\perp} &amp;= g - g_{\\parallel} =\n                \\text{Adjusted gradient with parallel component removed} \\\\\n        \\end{align*}\n        $$\n\n        Args:\n            grad: Gradient with respect to the weights.\n\n        Returns:\n            Gradient with respect to the weights, with the component parallel to the dictionary\n            vectors removed.\n        \"\"\"\n        # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can\n        # be thought of as vectors that end on the circumference of a hypersphere. The projection of\n        # the gradient onto the dictionary vectors is the component of the gradient that is parallel\n        # to the dictionary vectors, i.e. the component that moves to or from the center of the\n        # hypersphere.\n        normalized_weight: Float[\n            Tensor,\n            Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n        ] = self.weight / torch.norm(self.weight, dim=-2, keepdim=True)\n\n        scalar_projections = einops.einsum(\n            grad,\n            normalized_weight,\n            f\"... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; ... {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        projection = einops.einsum(\n            scalar_projections,\n            normalized_weight,\n            f\"... {Axis.INPUT_OUTPUT_FEATURE}, \\\n                ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; ... {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        # Subtracting the parallel component from the gradient leaves only the component that is\n        # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of\n        # the hypersphere.\n        return grad - projection\n\n    def forward(\n        self, x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        return einops.einsum(\n            x,\n            self.weight,\n            f\"{Axis.BATCH} ... {Axis.LEARNT_FEATURE}, \\\n            ... {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \\\n                -&gt; {Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return (\n            f\"learnt_features={self._learnt_features}, \"\n            f\"decoded_features={self._decoded_features}, \"\n            f\"n_components={self._n_components}\"\n        )\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_optimizer_parameter_details","title":"<code>reset_optimizer_parameter_details: list[ResetOptimizerParameterDetails]</code>  <code>property</code>","text":"<p>Reset optimizer parameter details.</p> <p>Details of the parameters that should be reset in the optimizer, when resetting dictionary vectors.</p> <p>Returns:</p> Type Description <code>list[ResetOptimizerParameterDetails]</code> <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p> <code>list[ResetOptimizerParameterDetails]</code> <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.weight","title":"<code>weight: Float[Parameter, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE)] = Parameter(torch.empty(shape_with_optional_dimensions(n_components, decoded_features, learnt_features)))</code>  <code>instance-attribute</code>","text":"<p>Weight parameter.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, n_components, *, enable_gradient_hook=True)</code>","text":"<p>Initialize the constrained unit norm linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>PositiveInt</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>PositiveInt</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>n_components</code> <code>PositiveInt | None</code> <p>Number of source model components the SAE is trained on.</p> required <code>enable_gradient_hook</code> <code>bool</code> <p>Enable the gradient backwards hook (modify the gradient before applying the gradient step, to maintain unit norm of the dictionary vectors).</p> <code>True</code> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    learnt_features: PositiveInt,\n    decoded_features: PositiveInt,\n    n_components: PositiveInt | None,\n    *,\n    enable_gradient_hook: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the constrained unit norm linear layer.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        n_components: Number of source model components the SAE is trained on.\n        enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n            applying the gradient step, to maintain unit norm of the dictionary vectors).\n    \"\"\"\n    super().__init__()\n\n    self._learnt_features = learnt_features\n    self._decoded_features = decoded_features\n    self._n_components = n_components\n\n    # Create the linear layer as per the standard PyTorch linear layer\n    self.weight = Parameter(\n        torch.empty(\n            shape_with_optional_dimensions(n_components, decoded_features, learnt_features),\n        )\n    )\n    self.reset_parameters()\n\n    # Register backward hook to remove any gradient information parallel to the dictionary\n    # vectors (columns of the weight matrix) before applying the gradient step.\n    if enable_gradient_hook:\n        self.weight.register_hook(self._weight_backward_hook)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>","text":"<p>Constrain the weights to have unit norm.</p> Warning <p>Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook <code>_weight_backward_hook</code> is applied.</p> <p>Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc).</p> Example <p>import torch layer = UnitNormDecoder(3, 3, None) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0)) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0]</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\n\n    Warning:\n        Note this must be called after each gradient step. This is because optimisers such as\n        Adam don't strictly follow the gradient, but instead follow a modified gradient that\n        includes momentum. This means that the gradient step can change the norm of the\n        dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n        Note this can't be applied directly in the backward hook, as it would interfere with a\n        variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n        with asynchronous operations, etc).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; layer = UnitNormDecoder(3, 3, None)\n        &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n        &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n        &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0]\n\n    \"\"\"\n    with torch.no_grad():\n        torch.nn.functional.normalize(self.weight, dim=-2, out=self.weight)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return (\n        f\"learnt_features={self._learnt_features}, \"\n        f\"decoded_features={self._decoded_features}, \"\n        f\"n_components={self._n_components}\"\n    )\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    return einops.einsum(\n        x,\n        self.weight,\n        f\"{Axis.BATCH} ... {Axis.LEARNT_FEATURE}, \\\n        ... {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \\\n            -&gt; {Axis.BATCH} ... {Axis.INPUT_OUTPUT_FEATURE}\",\n    )\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Example <p>import torch</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n        &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None)\n        &gt;&gt;&gt; layer.reset_parameters()\n        &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n        &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0, 1.0]\n\n    \"\"\"\n    # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n    # normalisation here, since we immediately scale the weights to have unit norm (so the\n    # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n    self.weight: Float[\n        Parameter,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE),\n    ] = init.normal_(self.weight, mean=0, std=1)  # type: ignore\n\n    # Scale so that each row has unit norm\n    self.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features","title":"Create a layer with 4 columns (learnt features) and 3 rows (decoded features)","text":"<p>layer = UnitNormDecoder(learnt_features=4, decoded_features=3, n_components=None) layer.reset_parameters()</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns","title":"Get the norm across the rows (by summing across the columns)","text":"<p>column_norms = torch.sum(layer.weight ** 2, dim=0) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0, 1.0]</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_weights, component_idx=None)</code>","text":"<p>Update decoder dictionary vectors.</p> <p>Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically this is used when resampling neurons (dictionary vectors) that have died.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>Int64[Tensor, names(COMPONENT_OPTIONAL, LEARNT_FEATURE_IDX)]</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_weights</code> <code>Float[Tensor, names(COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE, LEARNT_FEATURE_IDX)]</code> <p>Updated weights for just these dictionary vectors.</p> required <code>component_idx</code> <code>int | None</code> <p>Component index to update.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>component_idx</code> is not specified when <code>n_components</code> is not None.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: Int64[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE_IDX)\n    ],\n    updated_weights: Float[\n        Tensor,\n        Axis.names(Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE_IDX),\n    ],\n    component_idx: int | None = None,\n) -&gt; None:\n    \"\"\"Update decoder dictionary vectors.\n\n    Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n    this is used when resampling neurons (dictionary vectors) that have died.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_weights: Updated weights for just these dictionary vectors.\n        component_idx: Component index to update.\n\n    Raises:\n        ValueError: If `component_idx` is not specified when `n_components` is not None.\n    \"\"\"\n    if dictionary_vector_indices.numel() == 0:\n        return\n\n    with torch.no_grad():\n        if component_idx is None:\n            if self._n_components is not None:\n                error_message = \"component_idx must be specified when n_components is not None\"\n                raise ValueError(error_message)\n\n            self.weight[:, dictionary_vector_indices] = updated_weights\n        else:\n            self.weight[component_idx, :, dictionary_vector_indices] = updated_weights\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics","text":"<p>Metrics.</p> <p>All metrics are based on torchmetrics, which means they support distributed training and can be combined with other metrics easily.</p>"},{"location":"reference/metrics/loss/","title":"Loss metrics","text":"<p>Loss metrics.</p>"},{"location":"reference/metrics/loss/l1_absolute_loss/","title":"L1 (absolute error) loss","text":"<p>L1 (absolute error) loss.</p>"},{"location":"reference/metrics/loss/l1_absolute_loss/#sparse_autoencoder.metrics.loss.l1_absolute_loss.L1AbsoluteLoss","title":"<code>L1AbsoluteLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>L1 (absolute error) loss.</p> <p>L1 loss penalty is the absolute sum of the learned activations, averaged over the number of activation vectors.</p> Example <p>l1_loss = L1AbsoluteLoss() learned_activations = torch.tensor([ ...     [ # Batch 1 ...         [1., 0., 1.] # Component 1: learned features (L1 of 2) ...     ], ...     [ # Batch 2 ...         [0., 1., 0.] # Component 1: learned features (L1 of 1) ...     ] ... ]) l1_loss.forward(learned_activations=learned_activations) tensor(1.5000)</p> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>class L1AbsoluteLoss(Metric):\n    \"\"\"L1 (absolute error) loss.\n\n    L1 loss penalty is the absolute sum of the learned activations, averaged over the number of\n    activation vectors.\n\n    Example:\n        &gt;&gt;&gt; l1_loss = L1AbsoluteLoss()\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.] # Component 1: learned features (L1 of 2)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 1., 0.] # Component 1: learned features (L1 of 1)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; l1_loss.forward(learned_activations=learned_activations)\n        tensor(1.5000)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # Settings\n    _num_components: int\n    _keep_batch_dim: bool\n\n    @property\n    def keep_batch_dim(self) -&gt; bool:\n        \"\"\"Whether to keep the batch dimension in the loss output.\"\"\"\n        return self._keep_batch_dim\n\n    @keep_batch_dim.setter\n    def keep_batch_dim(self, keep_batch_dim: bool) -&gt; None:\n        \"\"\"Set whether to keep the batch dimension in the loss output.\n\n        When setting this we need to change the state to either a list if keeping the batch\n        dimension (so we can accumulate all the losses and concatenate them at the end along this\n        dimension). Alternatively it should be a tensor if not keeping the batch dimension (so we\n        can sum the losses over the batch dimension during update and then take the mean).\n\n        By doing this in a setter we allow changing of this setting after the metric is initialised.\n        \"\"\"\n        self._keep_batch_dim = keep_batch_dim\n        self.reset()  # Reset the metric to update the state\n        if keep_batch_dim and not isinstance(self.absolute_loss, list):\n            self.add_state(\n                \"absolute_loss\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n        elif not isinstance(self.absolute_loss, Tensor):\n            self.add_state(\n                \"absolute_loss\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n\n    # State\n    absolute_loss: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_components: PositiveInt = 1,\n        *,\n        keep_batch_dim: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\n\n        Args:\n            num_components: Number of components.\n            keep_batch_dim: Whether to keep the batch dimension in the loss output.\n        \"\"\"\n        super().__init__()\n        self._num_components = num_components\n        self.keep_batch_dim = keep_batch_dim\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    @staticmethod\n    def calculate_abs_sum(\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Calculate the absolute sum of the learned activations.\n\n        Args:\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n\n        Returns:\n            Absolute sum of the learned activations (keeping the batch and component axis).\n        \"\"\"\n        return torch.abs(learned_activations).sum(dim=-1)\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        If we're keeping the batch dimension, we simply take the absolute sum of the activations\n        (over the features dimension) and then append this tensor to a list. Then during compute we\n        just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n        when resampling neurons (see the neuron resampler for details).\n\n        By contrast if we're averaging over the batch dimension, we sum the activations over the\n        batch dimension during update (on each process), and then divide by the number of activation\n        vectors on compute to get the mean.\n\n        Args:\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        absolute_loss = self.calculate_abs_sum(learned_activations)\n\n        if self.keep_batch_dim:\n            self.absolute_loss.append(absolute_loss)  # type: ignore\n        else:\n            self.absolute_loss += absolute_loss.sum(dim=0)\n            self.num_activation_vectors += learned_activations.shape[0]\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute the metric.\"\"\"\n        return (\n            torch.cat(self.absolute_loss)  # type: ignore\n            if self.keep_batch_dim\n            else self.absolute_loss / self.num_activation_vectors\n        )\n</code></pre>"},{"location":"reference/metrics/loss/l1_absolute_loss/#sparse_autoencoder.metrics.loss.l1_absolute_loss.L1AbsoluteLoss.keep_batch_dim","title":"<code>keep_batch_dim: bool = keep_batch_dim</code>  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<p>Whether to keep the batch dimension in the loss output.</p>"},{"location":"reference/metrics/loss/l1_absolute_loss/#sparse_autoencoder.metrics.loss.l1_absolute_loss.L1AbsoluteLoss.__init__","title":"<code>__init__(num_components=1, *, keep_batch_dim=False)</code>","text":"<p>Initialize the metric.</p> <p>Parameters:</p> Name Type Description Default <code>num_components</code> <code>PositiveInt</code> <p>Number of components.</p> <code>1</code> <code>keep_batch_dim</code> <code>bool</code> <p>Whether to keep the batch dimension in the loss output.</p> <code>False</code> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_components: PositiveInt = 1,\n    *,\n    keep_batch_dim: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the metric.\n\n    Args:\n        num_components: Number of components.\n        keep_batch_dim: Whether to keep the batch dimension in the loss output.\n    \"\"\"\n    super().__init__()\n    self._num_components = num_components\n    self.keep_batch_dim = keep_batch_dim\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/loss/l1_absolute_loss/#sparse_autoencoder.metrics.loss.l1_absolute_loss.L1AbsoluteLoss.calculate_abs_sum","title":"<code>calculate_abs_sum(learned_activations)</code>  <code>staticmethod</code>","text":"<p>Calculate the absolute sum of the learned activations.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL)]</code> <p>Absolute sum of the learned activations (keeping the batch and component axis).</p> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>@staticmethod\ndef calculate_abs_sum(\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Calculate the absolute sum of the learned activations.\n\n    Args:\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n\n    Returns:\n        Absolute sum of the learned activations (keeping the batch and component axis).\n    \"\"\"\n    return torch.abs(learned_activations).sum(dim=-1)\n</code></pre>"},{"location":"reference/metrics/loss/l1_absolute_loss/#sparse_autoencoder.metrics.loss.l1_absolute_loss.L1AbsoluteLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\"\"\"\n    return (\n        torch.cat(self.absolute_loss)  # type: ignore\n        if self.keep_batch_dim\n        else self.absolute_loss / self.num_activation_vectors\n    )\n</code></pre>"},{"location":"reference/metrics/loss/l1_absolute_loss/#sparse_autoencoder.metrics.loss.l1_absolute_loss.L1AbsoluteLoss.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>If we're keeping the batch dimension, we simply take the absolute sum of the activations (over the features dimension) and then append this tensor to a list. Then during compute we just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item when resampling neurons (see the neuron resampler for details).</p> <p>By contrast if we're averaging over the batch dimension, we sum the activations over the batch dimension during update (on each process), and then divide by the number of activation vectors on compute to get the mean.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/loss/l1_absolute_loss.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If we're keeping the batch dimension, we simply take the absolute sum of the activations\n    (over the features dimension) and then append this tensor to a list. Then during compute we\n    just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n    when resampling neurons (see the neuron resampler for details).\n\n    By contrast if we're averaging over the batch dimension, we sum the activations over the\n    batch dimension during update (on each process), and then divide by the number of activation\n    vectors on compute to get the mean.\n\n    Args:\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    absolute_loss = self.calculate_abs_sum(learned_activations)\n\n    if self.keep_batch_dim:\n        self.absolute_loss.append(absolute_loss)  # type: ignore\n    else:\n        self.absolute_loss += absolute_loss.sum(dim=0)\n        self.num_activation_vectors += learned_activations.shape[0]\n</code></pre>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/","title":"L2 Reconstruction loss","text":"<p>L2 Reconstruction loss.</p>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/#sparse_autoencoder.metrics.loss.l2_reconstruction_loss.L2ReconstructionLoss","title":"<code>L2ReconstructionLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>L2 Reconstruction loss (MSE).</p> <p>L2 reconstruction loss is calculated as the sum squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with L2 may achieve the same loss for both polysemantic and monosemantic representations of true features.</p> Example <p>import torch loss = L2ReconstructionLoss(num_components=1) source_activations = torch.tensor([ ...     [ # Batch 1 ...         [4., 2.] # Component 1 ...     ], ...     [ # Batch 2 ...         [2., 0.] # Component 1 ...     ] ... ]) decoded_activations = torch.tensor([ ...     [ # Batch 1 ...         [2., 0.] # Component 1 (MSE of 4) ...     ], ...     [ # Batch 2 ...         [0., 0.] # Component 1 (MSE of 2) ...     ] ... ]) loss.forward( ...     decoded_activations=decoded_activations, source_activations=source_activations ... ) tensor(3.)</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>class L2ReconstructionLoss(Metric):\n    \"\"\"L2 Reconstruction loss (MSE).\n\n    L2 reconstruction loss is calculated as the sum squared error between each each input vector\n    and it's corresponding decoded vector. The original paper found that models trained with some\n    loss functions such as cross-entropy loss generally prefer to represent features\n    polysemantically, whereas models trained with L2 may achieve the same loss for both\n    polysemantic and monosemantic representations of true features.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; loss = L2ReconstructionLoss(num_components=1)\n        &gt;&gt;&gt; source_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [4., 2.] # Component 1\n        ...     ],\n        ...     [ # Batch 2\n        ...         [2., 0.] # Component 1\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; decoded_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [2., 0.] # Component 1 (MSE of 4)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 0.] # Component 1 (MSE of 2)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; loss.forward(\n        ...     decoded_activations=decoded_activations, source_activations=source_activations\n        ... )\n        tensor(3.)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    higher_is_better = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # Settings\n    _num_components: int\n    _keep_batch_dim: bool\n\n    @property\n    def keep_batch_dim(self) -&gt; bool:\n        \"\"\"Whether to keep the batch dimension in the loss output.\"\"\"\n        return self._keep_batch_dim\n\n    @keep_batch_dim.setter\n    def keep_batch_dim(self, keep_batch_dim: bool) -&gt; None:\n        \"\"\"Set whether to keep the batch dimension in the loss output.\n\n        When setting this we need to change the state to either a list if keeping the batch\n        dimension (so we can accumulate all the losses and concatenate them at the end along this\n        dimension). Alternatively it should be a tensor if not keeping the batch dimension (so we\n        can sum the losses over the batch dimension during update and then take the mean).\n\n        By doing this in a setter we allow changing of this setting after the metric is initialised.\n        \"\"\"\n        self._keep_batch_dim = keep_batch_dim\n        self.reset()  # Reset the metric to update the state\n        if keep_batch_dim and not isinstance(self.mse, list):\n            self.add_state(\n                \"mse\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n        elif not isinstance(self.mse, Tensor):\n            self.add_state(\n                \"mse\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n\n    # State\n    mse: Float[Tensor, Axis.COMPONENT_OPTIONAL] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_components: PositiveInt = 1,\n        *,\n        keep_batch_dim: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the L2 reconstruction loss.\"\"\"\n        super().__init__()\n        self._num_components = num_components\n        self.keep_batch_dim = keep_batch_dim\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    @staticmethod\n    def calculate_mse(\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Calculate the MSE.\"\"\"\n        return (decoded_activations - source_activations).pow(2).mean(dim=-1)\n\n    def update(\n        self,\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        If we're keeping the batch dimension, we simply take the mse of the activations\n        (over the features dimension) and then append this tensor to a list. Then during compute we\n        just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n        when resampling neurons (see the neuron resampler for details).\n\n        By contrast if we're averaging over the batch dimension, we sum the activations over the\n        batch dimension during update (on each process), and then divide by the number of activation\n        vectors on compute to get the mean.\n\n        Args:\n            decoded_activations: The decoded activations from the autoencoder.\n            source_activations: The source activations from the autoencoder.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        mse = self.calculate_mse(decoded_activations, source_activations)\n\n        if self.keep_batch_dim:\n            self.mse.append(mse)  # type: ignore\n        else:\n            self.mse += mse.sum(dim=0)\n            self.num_activation_vectors += source_activations.shape[0]\n\n    def compute(self) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n        \"\"\"Compute the metric.\"\"\"\n        return (\n            torch.cat(self.mse)  # type: ignore\n            if self.keep_batch_dim\n            else self.mse / self.num_activation_vectors\n        )\n</code></pre>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/#sparse_autoencoder.metrics.loss.l2_reconstruction_loss.L2ReconstructionLoss.keep_batch_dim","title":"<code>keep_batch_dim: bool = keep_batch_dim</code>  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<p>Whether to keep the batch dimension in the loss output.</p>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/#sparse_autoencoder.metrics.loss.l2_reconstruction_loss.L2ReconstructionLoss.__init__","title":"<code>__init__(num_components=1, *, keep_batch_dim=False)</code>","text":"<p>Initialise the L2 reconstruction loss.</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_components: PositiveInt = 1,\n    *,\n    keep_batch_dim: bool = False,\n) -&gt; None:\n    \"\"\"Initialise the L2 reconstruction loss.\"\"\"\n    super().__init__()\n    self._num_components = num_components\n    self.keep_batch_dim = keep_batch_dim\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/#sparse_autoencoder.metrics.loss.l2_reconstruction_loss.L2ReconstructionLoss.calculate_mse","title":"<code>calculate_mse(decoded_activations, source_activations)</code>  <code>staticmethod</code>","text":"<p>Calculate the MSE.</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>@staticmethod\ndef calculate_mse(\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Calculate the MSE.\"\"\"\n    return (decoded_activations - source_activations).pow(2).mean(dim=-1)\n</code></pre>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/#sparse_autoencoder.metrics.loss.l2_reconstruction_loss.L2ReconstructionLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>def compute(self) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n    \"\"\"Compute the metric.\"\"\"\n    return (\n        torch.cat(self.mse)  # type: ignore\n        if self.keep_batch_dim\n        else self.mse / self.num_activation_vectors\n    )\n</code></pre>"},{"location":"reference/metrics/loss/l2_reconstruction_loss/#sparse_autoencoder.metrics.loss.l2_reconstruction_loss.L2ReconstructionLoss.update","title":"<code>update(decoded_activations, source_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>If we're keeping the batch dimension, we simply take the mse of the activations (over the features dimension) and then append this tensor to a list. Then during compute we just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item when resampling neurons (see the neuron resampler for details).</p> <p>By contrast if we're averaging over the batch dimension, we sum the activations over the batch dimension during update (on each process), and then divide by the number of activation vectors on compute to get the mean.</p> <p>Parameters:</p> Name Type Description Default <code>decoded_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>The decoded activations from the autoencoder.</p> required <code>source_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, INPUT_OUTPUT_FEATURE)]</code> <p>The source activations from the autoencoder.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/loss/l2_reconstruction_loss.py</code> <pre><code>def update(\n    self,\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If we're keeping the batch dimension, we simply take the mse of the activations\n    (over the features dimension) and then append this tensor to a list. Then during compute we\n    just concatenate and return this list. This is useful for e.g. getting L1 loss by batch item\n    when resampling neurons (see the neuron resampler for details).\n\n    By contrast if we're averaging over the batch dimension, we sum the activations over the\n    batch dimension during update (on each process), and then divide by the number of activation\n    vectors on compute to get the mean.\n\n    Args:\n        decoded_activations: The decoded activations from the autoencoder.\n        source_activations: The source activations from the autoencoder.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    mse = self.calculate_mse(decoded_activations, source_activations)\n\n    if self.keep_batch_dim:\n        self.mse.append(mse)  # type: ignore\n    else:\n        self.mse += mse.sum(dim=0)\n        self.num_activation_vectors += source_activations.shape[0]\n</code></pre>"},{"location":"reference/metrics/loss/sae_loss/","title":"Sparse Autoencoder loss","text":"<p>Sparse Autoencoder loss.</p>"},{"location":"reference/metrics/loss/sae_loss/#sparse_autoencoder.metrics.loss.sae_loss.SparseAutoencoderLoss","title":"<code>SparseAutoencoderLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Sparse Autoencoder loss.</p> <p>This is the same as composing <code>L1AbsoluteLoss() * l1_coefficient + L2ReconstructionLoss()</code>. It is separated out so that you can use all three metrics (l1, l2, total loss) in the same <code>MetricCollection</code> and they will then share state (to avoid calculating the same thing twice).</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>class SparseAutoencoderLoss(Metric):\n    \"\"\"Sparse Autoencoder loss.\n\n    This is the same as composing `L1AbsoluteLoss() * l1_coefficient + L2ReconstructionLoss()`. It\n    is separated out so that you can use all three metrics (l1, l2, total loss) in the same\n    `MetricCollection` and they will then share state (to avoid calculating the same thing twice).\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    higher_is_better = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # Settings\n    _num_components: int\n    _keep_batch_dim: bool\n    _l1_coefficient: float\n\n    @property\n    def keep_batch_dim(self) -&gt; bool:\n        \"\"\"Whether to keep the batch dimension in the loss output.\"\"\"\n        return self._keep_batch_dim\n\n    @keep_batch_dim.setter\n    def keep_batch_dim(self, keep_batch_dim: bool) -&gt; None:\n        \"\"\"Set whether to keep the batch dimension in the loss output.\n\n        When setting this we need to change the state to either a list if keeping the batch\n        dimension (so we can accumulate all the losses and concatenate them at the end along this\n        dimension). Alternatively it should be a tensor if not keeping the batch dimension (so we\n        can sum the losses over the batch dimension during update and then take the mean).\n\n        By doing this in a setter we allow changing of this setting after the metric is initialised.\n        \"\"\"\n        self._keep_batch_dim = keep_batch_dim\n        self.reset()  # Reset the metric to update the state\n        if keep_batch_dim and not isinstance(self.mse, list):\n            self.add_state(\n                \"mse\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n            self.add_state(\n                \"absolute_loss\",\n                default=[],\n                dist_reduce_fx=\"sum\",\n            )\n        elif not isinstance(self.mse, Tensor):\n            self.add_state(\n                \"mse\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n            self.add_state(\n                \"absolute_loss\",\n                default=torch.zeros(self._num_components),\n                dist_reduce_fx=\"sum\",\n            )\n\n    # State\n    absolute_loss: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    mse: Float[Tensor, Axis.COMPONENT_OPTIONAL] | list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL)]\n    ] | None = None\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_components: PositiveInt = 1,\n        l1_coefficient: PositiveFloat = 0.001,\n        *,\n        keep_batch_dim: bool = False,\n    ):\n        \"\"\"Initialise the metric.\"\"\"\n        super().__init__()\n        self._num_components = num_components\n        self.keep_batch_dim = keep_batch_dim\n        self._l1_coefficient = l1_coefficient\n\n        # Add the state\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics))\n    ) -&gt; None:\n        \"\"\"Update the metric.\"\"\"\n        absolute_loss = L1AbsoluteLoss.calculate_abs_sum(learned_activations)\n        mse = L2ReconstructionLoss.calculate_mse(decoded_activations, source_activations)\n\n        if self.keep_batch_dim:\n            self.absolute_loss.append(absolute_loss)  # type: ignore\n            self.mse.append(mse)  # type: ignore\n        else:\n            self.absolute_loss += absolute_loss.sum(dim=0)\n            self.mse += mse.sum(dim=0)\n            self.num_activation_vectors += learned_activations.shape[0]\n\n    def compute(self) -&gt; Tensor:\n        \"\"\"Compute the metric.\"\"\"\n        l1 = (\n            torch.cat(self.absolute_loss)  # type: ignore\n            if self.keep_batch_dim\n            else self.absolute_loss / self.num_activation_vectors\n        )\n\n        l2 = (\n            torch.cat(self.mse)  # type: ignore\n            if self.keep_batch_dim\n            else self.mse / self.num_activation_vectors\n        )\n\n        return l1 * self._l1_coefficient + l2\n\n    def forward(  # type: ignore[override] (narrowing)\n        self,\n        source_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        decoded_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n        ],\n    ) -&gt; Tensor:\n        \"\"\"Forward pass.\"\"\"\n        return super().forward(\n            source_activations=source_activations,\n            learned_activations=learned_activations,\n            decoded_activations=decoded_activations,\n        )\n</code></pre>"},{"location":"reference/metrics/loss/sae_loss/#sparse_autoencoder.metrics.loss.sae_loss.SparseAutoencoderLoss.keep_batch_dim","title":"<code>keep_batch_dim: bool = keep_batch_dim</code>  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<p>Whether to keep the batch dimension in the loss output.</p>"},{"location":"reference/metrics/loss/sae_loss/#sparse_autoencoder.metrics.loss.sae_loss.SparseAutoencoderLoss.__init__","title":"<code>__init__(num_components=1, l1_coefficient=0.001, *, keep_batch_dim=False)</code>","text":"<p>Initialise the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_components: PositiveInt = 1,\n    l1_coefficient: PositiveFloat = 0.001,\n    *,\n    keep_batch_dim: bool = False,\n):\n    \"\"\"Initialise the metric.\"\"\"\n    super().__init__()\n    self._num_components = num_components\n    self.keep_batch_dim = keep_batch_dim\n    self._l1_coefficient = l1_coefficient\n\n    # Add the state\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/loss/sae_loss/#sparse_autoencoder.metrics.loss.sae_loss.SparseAutoencoderLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\"\"\"\n    l1 = (\n        torch.cat(self.absolute_loss)  # type: ignore\n        if self.keep_batch_dim\n        else self.absolute_loss / self.num_activation_vectors\n    )\n\n    l2 = (\n        torch.cat(self.mse)  # type: ignore\n        if self.keep_batch_dim\n        else self.mse / self.num_activation_vectors\n    )\n\n    return l1 * self._l1_coefficient + l2\n</code></pre>"},{"location":"reference/metrics/loss/sae_loss/#sparse_autoencoder.metrics.loss.sae_loss.SparseAutoencoderLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Forward pass.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>def forward(  # type: ignore[override] (narrowing)\n    self,\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n) -&gt; Tensor:\n    \"\"\"Forward pass.\"\"\"\n    return super().forward(\n        source_activations=source_activations,\n        learned_activations=learned_activations,\n        decoded_activations=decoded_activations,\n    )\n</code></pre>"},{"location":"reference/metrics/loss/sae_loss/#sparse_autoencoder.metrics.loss.sae_loss.SparseAutoencoderLoss.update","title":"<code>update(source_activations, learned_activations, decoded_activations, **kwargs)</code>","text":"<p>Update the metric.</p> Source code in <code>sparse_autoencoder/metrics/loss/sae_loss.py</code> <pre><code>def update(\n    self,\n    source_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    decoded_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.INPUT_OUTPUT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics))\n) -&gt; None:\n    \"\"\"Update the metric.\"\"\"\n    absolute_loss = L1AbsoluteLoss.calculate_abs_sum(learned_activations)\n    mse = L2ReconstructionLoss.calculate_mse(decoded_activations, source_activations)\n\n    if self.keep_batch_dim:\n        self.absolute_loss.append(absolute_loss)  # type: ignore\n        self.mse.append(mse)  # type: ignore\n    else:\n        self.absolute_loss += absolute_loss.sum(dim=0)\n        self.mse += mse.sum(dim=0)\n        self.num_activation_vectors += learned_activations.shape[0]\n</code></pre>"},{"location":"reference/metrics/train/","title":"Train step metrics","text":"<p>Train step metrics.</p>"},{"location":"reference/metrics/train/capacity/","title":"Capacity Metrics","text":"<p>Capacity Metrics.</p>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric","title":"<code>CapacityMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Capacities metric.</p> <p>Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks.</p> <p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features.</p> Warning <p>This is memory intensive as it requires caching all learned activations for a batch.</p> <p>Examples:</p> <p>If the features are orthogonal, the capacity is 1.</p> <pre><code>&gt;&gt;&gt; metric = CapacityMetric()\n&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [ # Batch 1\n...         [1., 0., 1.] # Component 1: learned features\n...     ],\n...     [ # Batch 2\n...         [0., 1., 0.] # Component 1: learned features (orthogonal)\n...     ]\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor([[1., 1.]])\n</code></pre> <p>If they are all the same, the capacity is 1/n.</p> <pre><code>&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [ # Batch 1\n...         [1., 1., 1.] # Component 1: learned features\n...     ],\n...     [ # Batch 2\n...         [1., 1., 1.] # Component 1: learned features (same)\n...     ]\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor([[0.5000, 0.5000]])\n</code></pre> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>class CapacityMetric(Metric):\n    \"\"\"Capacities metric.\n\n    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural\n    Networks](https://arxiv.org/pdf/2210.01892.pdf).\n\n    Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.\n    Formally it's the ratio of the squared dot product of a feature with itself to the sum of its\n    squared dot products of all features.\n\n    Warning:\n        This is memory intensive as it requires caching all learned activations for a batch.\n\n    Examples:\n        If the features are orthogonal, the capacity is 1.\n\n        &gt;&gt;&gt; metric = CapacityMetric()\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.] # Component 1: learned features\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 1., 0.] # Component 1: learned features (orthogonal)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([[1., 1.]])\n\n        If they are all the same, the capacity is 1/n.\n\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 1., 1.] # Component 1: learned features\n        ...     ],\n        ...     [ # Batch 2\n        ...         [1., 1., 1.] # Component 1: learned features (same)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([[0.5000, 0.5000]])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n    plot_upper_bound: float | None = 1.0\n\n    # State\n    learned_activations: list[\n        Float[Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    ]\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__()\n        self.add_state(\"learned_activations\", default=[])\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        self.learned_activations.append(learned_activations)\n\n    @staticmethod\n    def capacities(\n        features: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n        r\"\"\"Calculate capacities.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])\n            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n            &gt;&gt;&gt; orthogonal_caps\n            tensor([[1., 1., 1.]])\n\n        Args:\n            features: A collection of features.\n\n        Returns:\n            A 1D tensor of capacities, where each element is the capacity of the corresponding\n            feature.\n        \"\"\"\n        squared_dot_products: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n        ] = (\n            einops.einsum(\n                features,\n                features,\n                f\"batch_1 ... {Axis.LEARNT_FEATURE}, \\\n                    batch_2 ... {Axis.LEARNT_FEATURE} \\\n                    -&gt; ... batch_1 batch_2\",\n            )\n            ** 2\n        )\n\n        sum_of_sq_dot: Float[\n            Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)\n        ] = squared_dot_products.sum(dim=-1)\n\n        diagonal: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)] = torch.diagonal(\n            squared_dot_products, dim1=1, dim2=2\n        )\n\n        return diagonal / sum_of_sq_dot\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n        \"\"\"Compute the metric.\"\"\"\n        batch_learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.cat(self.learned_activations)\n\n        return self.capacities(batch_learned_activations)\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__()\n    self.add_state(\"learned_activations\", default=[])\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.capacities","title":"<code>capacities(features)</code>  <code>staticmethod</code>","text":"<p>Calculate capacities.</p> Example <p>import torch orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]]) orthogonal_caps = CapacityMetric.capacities(orthogonal_features) orthogonal_caps tensor([[1., 1., 1.]])</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>A collection of features.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(COMPONENT_OPTIONAL, BATCH)]</code> <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p> <code>Float[Tensor, names(COMPONENT_OPTIONAL, BATCH)]</code> <p>feature.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef capacities(\n    features: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n    r\"\"\"Calculate capacities.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])\n        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n        &gt;&gt;&gt; orthogonal_caps\n        tensor([[1., 1., 1.]])\n\n    Args:\n        features: A collection of features.\n\n    Returns:\n        A 1D tensor of capacities, where each element is the capacity of the corresponding\n        feature.\n    \"\"\"\n    squared_dot_products: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.BATCH, Axis.COMPONENT_OPTIONAL)\n    ] = (\n        einops.einsum(\n            features,\n            features,\n            f\"batch_1 ... {Axis.LEARNT_FEATURE}, \\\n                batch_2 ... {Axis.LEARNT_FEATURE} \\\n                -&gt; ... batch_1 batch_2\",\n        )\n        ** 2\n    )\n\n    sum_of_sq_dot: Float[\n        Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)\n    ] = squared_dot_products.sum(dim=-1)\n\n    diagonal: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)] = torch.diagonal(\n        squared_dot_products, dim1=1, dim2=2\n    )\n\n    return diagonal / sum_of_sq_dot\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.BATCH)]:\n    \"\"\"Compute the metric.\"\"\"\n    batch_learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.cat(self.learned_activations)\n\n    return self.capacities(batch_learned_activations)\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    self.learned_activations.append(learned_activations)\n</code></pre>"},{"location":"reference/metrics/train/feature_density/","title":"Train batch feature density","text":"<p>Train batch feature density.</p>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.FeatureDensityMetric","title":"<code>FeatureDensityMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Feature density metric.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a training batch.</p> <p>Generally we want a small number of features to be active in each batch, so average feature density should be low. By contrast if the average feature density is high, it means that the features are not sparse enough.</p> Example <p>metric = FeatureDensityMetric(num_learned_features=3, num_components=1) learned_activations = torch.tensor([ ...     [ # Batch 1 ...         [1., 0., 1.] # Component 1: learned features (2 active neurons) ...     ], ...     [ # Batch 2 ...         [0., 0., 0.] # Component 1: learned features (0 active neuron) ...     ] ... ]) metric.forward(learned_activations) tensor([[0.5000, 0.0000, 0.5000]])</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>class FeatureDensityMetric(Metric):\n    \"\"\"Feature density metric.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a\n    training batch.\n\n    Generally we want a small number of features to be active in each batch, so average feature\n    density should be low. By contrast if the average feature density is high, it means that the\n    features are not sparse enough.\n\n    Example:\n        &gt;&gt;&gt; metric = FeatureDensityMetric(num_learned_features=3, num_components=1)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.] # Component 1: learned features (2 active neurons)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 0., 0.] # Component 1: learned features (0 active neuron)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([[0.5000, 0.0000, 0.5000]])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = True\n    plot_lower_bound: float | None = 0.0\n    plot_upper_bound: float | None = 1.0\n\n    # State\n    neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self, num_learned_features: PositiveInt, num_components: PositiveInt | None = None\n    ) -&gt; None:\n        \"\"\"Initialise the metric.\"\"\"\n        super().__init__()\n\n        self.add_state(\n            \"neuron_fired_count\",\n            default=torch.zeros(\n                size=shape_with_optional_dimensions(num_components, num_learned_features),\n                dtype=torch.float,  # Float is needed for dist reduce to work\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        # Increment the counter of activations seen since the last compute step\n        self.num_activation_vectors += learned_activations.shape[0]\n\n        # Count the number of active neurons in the batch\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n\n        self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.int64)\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Compute the metric.\"\"\"\n        return self.neuron_fired_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.FeatureDensityMetric.__init__","title":"<code>__init__(num_learned_features, num_components=None)</code>","text":"<p>Initialise the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>@validate_call\ndef __init__(\n    self, num_learned_features: PositiveInt, num_components: PositiveInt | None = None\n) -&gt; None:\n    \"\"\"Initialise the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\n        \"neuron_fired_count\",\n        default=torch.zeros(\n            size=shape_with_optional_dimensions(num_components, num_learned_features),\n            dtype=torch.float,  # Float is needed for dist reduce to work\n        ),\n        dist_reduce_fx=\"sum\",\n    )\n\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.FeatureDensityMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Compute the metric.\"\"\"\n    return self.neuron_fired_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.FeatureDensityMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    # Increment the counter of activations seen since the last compute step\n    self.num_activation_vectors += learned_activations.shape[0]\n\n    # Count the number of active neurons in the batch\n    neuron_has_fired: Bool[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.gt(learned_activations, 0)\n\n    self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.int64)\n</code></pre>"},{"location":"reference/metrics/train/l0_norm/","title":"L0 norm sparsity metric","text":"<p>L0 norm sparsity metric.</p>"},{"location":"reference/metrics/train/l0_norm/#sparse_autoencoder.metrics.train.l0_norm.L0NormMetric","title":"<code>L0NormMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Learned activations L0 norm metric.</p> <p>The L0 norm is the number of non-zero elements in a learned activation vector, averaged over the number of activation vectors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metric = L0NormMetric()\n&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n...     [0., 1., 0.]  # Batch 2 (single component): learned features (1 active neuron)\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor(1.5000)\n</code></pre> <p>With 2 components, the metric will return the average number of active (non-zero) neurons as a 1d tensor.</p> <pre><code>&gt;&gt;&gt; metric = L0NormMetric(num_components=2)\n&gt;&gt;&gt; learned_activations = torch.tensor([\n...     [ # Batch 1\n...         [1., 0., 1.], # Component 1: learned features (2 active neurons)\n...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n...     ],\n...     [ # Batch 2\n...         [0., 1., 0.], # Component 1: learned features (1 active neuron)\n...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n...     ]\n... ])\n&gt;&gt;&gt; metric.forward(learned_activations)\ntensor([1.5000, 2.0000])\n</code></pre> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>class L0NormMetric(Metric):\n    \"\"\"Learned activations L0 norm metric.\n\n    The L0 norm is the number of non-zero elements in a learned activation vector, averaged over the\n    number of activation vectors.\n\n    Examples:\n        &gt;&gt;&gt; metric = L0NormMetric()\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n        ...     [0., 1., 0.]  # Batch 2 (single component): learned features (1 active neuron)\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor(1.5000)\n\n        With 2 components, the metric will return the average number of active (non-zero)\n        neurons as a 1d tensor.\n\n        &gt;&gt;&gt; metric = L0NormMetric(num_components=2)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [ # Batch 1\n        ...         [1., 0., 1.], # Component 1: learned features (2 active neurons)\n        ...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n        ...     ],\n        ...     [ # Batch 2\n        ...         [0., 1., 0.], # Component 1: learned features (1 active neuron)\n        ...         [1., 0., 1.]  # Component 2: learned features (2 active neurons)\n        ...     ]\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([1.5000, 2.0000])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = False\n    plot_lower_bound: float | None = 0.0\n\n    # State\n    active_neurons_count: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(self, num_components: PositiveInt | None = None) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__()\n\n        self.add_state(\n            \"active_neurons_count\",\n            default=torch.zeros(shape_with_optional_dimensions(num_components), dtype=torch.float),\n            dist_reduce_fx=\"sum\",  # Float is needed for dist reduce to work\n        )\n\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        self.num_activation_vectors += learned_activations.shape[0]\n\n        self.active_neurons_count += torch.count_nonzero(learned_activations, dim=-1).sum(\n            dim=0, dtype=torch.int64\n        )\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)]:\n        \"\"\"Compute the metric.\n\n        Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n        \"\"\"\n        return self.active_neurons_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/metrics/train/l0_norm/#sparse_autoencoder.metrics.train.l0_norm.L0NormMetric.__init__","title":"<code>__init__(num_components=None)</code>","text":"<p>Initialize the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>@validate_call\ndef __init__(self, num_components: PositiveInt | None = None) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\n        \"active_neurons_count\",\n        default=torch.zeros(shape_with_optional_dimensions(num_components), dtype=torch.float),\n        dist_reduce_fx=\"sum\",  # Float is needed for dist reduce to work\n    )\n\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/train/l0_norm/#sparse_autoencoder.metrics.train.l0_norm.L0NormMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> <p>Note that torchmetrics converts shape <code>[0]</code> tensors into scalars (shape <code>0</code>).</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL)]:\n    \"\"\"Compute the metric.\n\n    Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n    \"\"\"\n    return self.active_neurons_count / self.num_activation_vectors\n</code></pre>"},{"location":"reference/metrics/train/l0_norm/#sparse_autoencoder.metrics.train.l0_norm.L0NormMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/l0_norm.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    self.num_activation_vectors += learned_activations.shape[0]\n\n    self.active_neurons_count += torch.count_nonzero(learned_activations, dim=-1).sum(\n        dim=0, dtype=torch.int64\n    )\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity/","title":"Neuron activity metric","text":"<p>Neuron activity metric.</p>"},{"location":"reference/metrics/train/neuron_activity/#sparse_autoencoder.metrics.train.neuron_activity.NeuronActivityMetric","title":"<code>NeuronActivityMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Neuron activity metric.</p> Example <p>With a single component and a horizon of 2 activations, the metric will return nothing after the first activation is added and then computed, and then return the number of dead neurons after the second activation is added (with update). The breakdown by component isn't shown here as there is just one component.</p> <p>metric = NeuronActivityMetric(num_learned_features=3) learned_activations = torch.tensor([ ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons) ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron) ... ]) metric.forward(learned_activations) tensor(1)</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>class NeuronActivityMetric(Metric):\n    \"\"\"Neuron activity metric.\n\n    Example:\n        With a single component and a horizon of 2 activations, the metric will return nothing\n        after the first activation is added and then computed, and then return the number of dead\n        neurons after the second activation is added (with update). The breakdown by component isn't\n        shown here as there is just one component.\n\n        &gt;&gt;&gt; metric = NeuronActivityMetric(num_learned_features=3)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n        ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron)\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor(1)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = True\n    plot_lower_bound: float | None = 0.0\n\n    # Metric settings\n    _threshold_is_dead_portion_fires: NonNegativeFloat\n\n    # State\n    neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(\n        self,\n        num_learned_features: PositiveInt,\n        num_components: PositiveInt | None = None,\n        threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n    ) -&gt; None:\n        \"\"\"Initialise the metric.\n\n        Args:\n            num_learned_features: Number of learned features.\n            num_components: Number of components.\n            threshold_is_dead_portion_fires: Thresholds for counting a neuron as dead (portion of\n                activation vectors that it fires for must be less than or equal to this number).\n                Commonly used values are 0.0, 1e-5 and 1e-6.\n        \"\"\"\n        super().__init__()\n        self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n\n        self.add_state(\n            \"neuron_fired_count\",\n            default=torch.zeros(\n                shape_with_optional_dimensions(num_components, num_learned_features),\n                dtype=torch.float,  # Float is needed for dist reduce to work\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n        self.add_state(\n            \"num_activation_vectors\",\n            default=torch.tensor(0, dtype=torch.int64),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        # Increment the counter of activations seen since the last compute step\n        self.num_activation_vectors += learned_activations.shape[0]\n\n        # Count the number of active neurons in the batch\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n\n        self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n\n    def compute(self) -&gt; Int64[Tensor, Axis.COMPONENT_OPTIONAL]:\n        \"\"\"Compute the metric.\n\n        Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n        \"\"\"\n        threshold_activations: Float[Tensor, Axis.SINGLE_ITEM] = (\n            self._threshold_is_dead_portion_fires * self.num_activation_vectors\n        )\n\n        return torch.sum(\n            self.neuron_fired_count &lt;= threshold_activations, dim=-1, dtype=torch.int64\n        )\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity/#sparse_autoencoder.metrics.train.neuron_activity.NeuronActivityMetric.__init__","title":"<code>__init__(num_learned_features, num_components=None, threshold_is_dead_portion_fires=0.0)</code>","text":"<p>Initialise the metric.</p> <p>Parameters:</p> Name Type Description Default <code>num_learned_features</code> <code>PositiveInt</code> <p>Number of learned features.</p> required <code>num_components</code> <code>PositiveInt | None</code> <p>Number of components.</p> <code>None</code> <code>threshold_is_dead_portion_fires</code> <code>Annotated[float, Field(strict=True, ge=0, le=1)]</code> <p>Thresholds for counting a neuron as dead (portion of activation vectors that it fires for must be less than or equal to this number). Commonly used values are 0.0, 1e-5 and 1e-6.</p> <code>0.0</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_learned_features: PositiveInt,\n    num_components: PositiveInt | None = None,\n    threshold_is_dead_portion_fires: Annotated[float, Field(strict=True, ge=0, le=1)] = 0.0,\n) -&gt; None:\n    \"\"\"Initialise the metric.\n\n    Args:\n        num_learned_features: Number of learned features.\n        num_components: Number of components.\n        threshold_is_dead_portion_fires: Thresholds for counting a neuron as dead (portion of\n            activation vectors that it fires for must be less than or equal to this number).\n            Commonly used values are 0.0, 1e-5 and 1e-6.\n    \"\"\"\n    super().__init__()\n    self._threshold_is_dead_portion_fires = threshold_is_dead_portion_fires\n\n    self.add_state(\n        \"neuron_fired_count\",\n        default=torch.zeros(\n            shape_with_optional_dimensions(num_components, num_learned_features),\n            dtype=torch.float,  # Float is needed for dist reduce to work\n        ),\n        dist_reduce_fx=\"sum\",\n    )\n\n    self.add_state(\n        \"num_activation_vectors\",\n        default=torch.tensor(0, dtype=torch.int64),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity/#sparse_autoencoder.metrics.train.neuron_activity.NeuronActivityMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> <p>Note that torchmetrics converts shape <code>[0]</code> tensors into scalars (shape <code>0</code>).</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>def compute(self) -&gt; Int64[Tensor, Axis.COMPONENT_OPTIONAL]:\n    \"\"\"Compute the metric.\n\n    Note that torchmetrics converts shape `[0]` tensors into scalars (shape `0`).\n    \"\"\"\n    threshold_activations: Float[Tensor, Axis.SINGLE_ITEM] = (\n        self._threshold_is_dead_portion_fires * self.num_activation_vectors\n    )\n\n    return torch.sum(\n        self.neuron_fired_count &lt;= threshold_activations, dim=-1, dtype=torch.int64\n    )\n</code></pre>"},{"location":"reference/metrics/train/neuron_activity/#sparse_autoencoder.metrics.train.neuron_activity.NeuronActivityMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_activity.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    # Increment the counter of activations seen since the last compute step\n    self.num_activation_vectors += learned_activations.shape[0]\n\n    # Count the number of active neurons in the batch\n    neuron_has_fired: Bool[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.gt(learned_activations, 0)\n\n    self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n</code></pre>"},{"location":"reference/metrics/train/neuron_fired_count/","title":"Neuron fired count metric","text":"<p>Neuron fired count metric.</p>"},{"location":"reference/metrics/train/neuron_fired_count/#sparse_autoencoder.metrics.train.neuron_fired_count.NeuronFiredCountMetric","title":"<code>NeuronFiredCountMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Neuron activity metric.</p> Example <p>metric = NeuronFiredCountMetric(num_learned_features=3) learned_activations = torch.tensor([ ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons) ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron) ... ]) metric.forward(learned_activations) tensor([1, 0, 1])</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>class NeuronFiredCountMetric(Metric):\n    \"\"\"Neuron activity metric.\n\n    Example:\n        &gt;&gt;&gt; metric = NeuronFiredCountMetric(num_learned_features=3)\n        &gt;&gt;&gt; learned_activations = torch.tensor([\n        ...     [1., 0., 1.], # Batch 1 (single component): learned features (2 active neurons)\n        ...     [0., 0., 0.]  # Batch 2 (single component): learned features (0 active neuron)\n        ... ])\n        &gt;&gt;&gt; metric.forward(learned_activations)\n        tensor([1, 0, 1])\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = True\n    full_state_update: bool | None = True\n    plot_lower_bound: float | None = 0.0\n\n    # State\n    neuron_fired_count: Float[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]\n\n    @validate_call\n    def __init__(\n        self,\n        num_learned_features: PositiveInt,\n        num_components: PositiveInt | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the metric.\n\n        Args:\n            num_learned_features: Number of learned features.\n            num_components: Number of components.\n        \"\"\"\n        super().__init__()\n        self.add_state(\n            \"neuron_fired_count\",\n            default=torch.zeros(\n                shape_with_optional_dimensions(num_components, num_learned_features),\n                dtype=torch.float,  # Float is needed for dist reduce to work\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        learned_activations: Float[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ],\n        **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            learned_activations: The learned activations.\n            **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n        \"\"\"\n        neuron_has_fired: Bool[\n            Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n        ] = torch.gt(learned_activations, 0)\n\n        self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n\n    def compute(self) -&gt; Int[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n        \"\"\"Compute the metric.\"\"\"\n        return self.neuron_fired_count.to(dtype=torch.int64)\n</code></pre>"},{"location":"reference/metrics/train/neuron_fired_count/#sparse_autoencoder.metrics.train.neuron_fired_count.NeuronFiredCountMetric.__init__","title":"<code>__init__(num_learned_features, num_components=None)</code>","text":"<p>Initialise the metric.</p> <p>Parameters:</p> Name Type Description Default <code>num_learned_features</code> <code>PositiveInt</code> <p>Number of learned features.</p> required <code>num_components</code> <code>PositiveInt | None</code> <p>Number of components.</p> <code>None</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    num_learned_features: PositiveInt,\n    num_components: PositiveInt | None = None,\n) -&gt; None:\n    \"\"\"Initialise the metric.\n\n    Args:\n        num_learned_features: Number of learned features.\n        num_components: Number of components.\n    \"\"\"\n    super().__init__()\n    self.add_state(\n        \"neuron_fired_count\",\n        default=torch.zeros(\n            shape_with_optional_dimensions(num_components, num_learned_features),\n            dtype=torch.float,  # Float is needed for dist reduce to work\n        ),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/train/neuron_fired_count/#sparse_autoencoder.metrics.train.neuron_fired_count.NeuronFiredCountMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>def compute(self) -&gt; Int[Tensor, Axis.names(Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)]:\n    \"\"\"Compute the metric.\"\"\"\n    return self.neuron_fired_count.to(dtype=torch.int64)\n</code></pre>"},{"location":"reference/metrics/train/neuron_fired_count/#sparse_autoencoder.metrics.train.neuron_fired_count.NeuronFiredCountMetric.update","title":"<code>update(learned_activations, **kwargs)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>learned_activations</code> <code>Float[Tensor, names(BATCH, COMPONENT_OPTIONAL, LEARNT_FEATURE)]</code> <p>The learned activations.</p> required <code>**kwargs</code> <code>Any</code> <p>Ignored keyword arguments (to allow use with other metrics in a collection).</p> <code>{}</code> Source code in <code>sparse_autoencoder/metrics/train/neuron_fired_count.py</code> <pre><code>def update(\n    self,\n    learned_activations: Float[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ],\n    **kwargs: Any,  # type: ignore # noqa: ARG002, ANN401 (allows combining with other metrics)\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        learned_activations: The learned activations.\n        **kwargs: Ignored keyword arguments (to allow use with other metrics in a collection).\n    \"\"\"\n    neuron_has_fired: Bool[\n        Tensor, Axis.names(Axis.BATCH, Axis.COMPONENT_OPTIONAL, Axis.LEARNT_FEATURE)\n    ] = torch.gt(learned_activations, 0)\n\n    self.neuron_fired_count += neuron_has_fired.sum(dim=0, dtype=torch.float)\n</code></pre>"},{"location":"reference/metrics/validate/","title":"Validate step metrics","text":"<p>Validate step metrics.</p>"},{"location":"reference/metrics/validate/reconstruction_score/","title":"Reconstruction score metric","text":"<p>Reconstruction score metric.</p>"},{"location":"reference/metrics/validate/reconstruction_score/#sparse_autoencoder.metrics.validate.reconstruction_score.ReconstructionScoreMetric","title":"<code>ReconstructionScoreMetric</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Model reconstruction score.</p> <p>Creates a score that measures how well the model can reconstruct the data.</p> \\[ \\begin{align*}     v &amp;= \\text{number of validation items} \\\\     l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\     l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\     l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\     s &amp;= \\text{reconstruction score} \\\\     s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\     s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v \\end{align*} \\] Example <p>metric = ReconstructionScoreMetric(num_components=1) source_model_loss=torch.tensor([2.0, 2.0, 2.0]) source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0]) source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0]) metric.forward( ...     source_model_loss=source_model_loss, ...     source_model_loss_with_reconstruction=source_model_loss_with_reconstruction, ...     source_model_loss_with_zero_ablation=source_model_loss_with_zero_ablation ... ) tensor(0.6667)</p> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>class ReconstructionScoreMetric(Metric):\n    r\"\"\"Model reconstruction score.\n\n    Creates a score that measures how well the model can reconstruct the data.\n\n    $$\n    \\begin{align*}\n        v &amp;= \\text{number of validation items} \\\\\n        l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\\n        l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\\n        l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\\n        s &amp;= \\text{reconstruction score} \\\\\n        s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\\n        s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v\n    \\end{align*}\n    $$\n\n    Example:\n        &gt;&gt;&gt; metric = ReconstructionScoreMetric(num_components=1)\n        &gt;&gt;&gt; source_model_loss=torch.tensor([2.0, 2.0, 2.0])\n        &gt;&gt;&gt; source_model_loss_with_reconstruction=torch.tensor([3.0, 3.0, 3.0])\n        &gt;&gt;&gt; source_model_loss_with_zero_ablation=torch.tensor([5.0, 5.0, 5.0])\n        &gt;&gt;&gt; metric.forward(\n        ...     source_model_loss=source_model_loss,\n        ...     source_model_loss_with_reconstruction=source_model_loss_with_reconstruction,\n        ...     source_model_loss_with_zero_ablation=source_model_loss_with_zero_ablation\n        ... )\n        tensor(0.6667)\n    \"\"\"\n\n    # Torchmetrics settings\n    is_differentiable: bool | None = False\n    full_state_update: bool | None = True\n\n    # State\n    source_model_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    source_model_loss_with_zero_ablation: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    source_model_loss_with_reconstruction: Float[Tensor, Axis.COMPONENT_OPTIONAL]\n    num_activation_vectors: Int64[Tensor, Axis.SINGLE_ITEM]\n\n    @validate_call\n    def __init__(self, num_components: PositiveInt = 1) -&gt; None:\n        \"\"\"Initialise the metric.\"\"\"\n        super().__init__()\n\n        self.add_state(\n            \"source_model_loss\", default=torch.zeros(num_components), dist_reduce_fx=\"sum\"\n        )\n        self.add_state(\n            \"source_model_loss_with_zero_ablation\",\n            default=torch.zeros(num_components),\n            dist_reduce_fx=\"sum\",\n        )\n        self.add_state(\n            \"source_model_loss_with_reconstruction\",\n            default=torch.zeros(num_components),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        source_model_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n        source_model_loss_with_reconstruction: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n        source_model_loss_with_zero_ablation: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n        component_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Update the metric state.\n\n        Args:\n            source_model_loss: Loss with no changes to the source model.\n            source_model_loss_with_reconstruction: Loss with SAE reconstruction.\n            source_model_loss_with_zero_ablation: Loss with zero ablation.\n            component_idx: Component idx.\n        \"\"\"\n        self.source_model_loss[component_idx] += source_model_loss.sum()\n        self.source_model_loss_with_zero_ablation[\n            component_idx\n        ] += source_model_loss_with_zero_ablation.sum()\n        self.source_model_loss_with_reconstruction[\n            component_idx\n        ] += source_model_loss_with_reconstruction.sum()\n\n    def compute(\n        self,\n    ) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n        \"\"\"Compute the metric.\"\"\"\n        zero_ablate_loss_minus_reconstruction_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n            self.source_model_loss_with_zero_ablation - self.source_model_loss_with_reconstruction\n        )\n\n        zero_ablate_loss_minus_default_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n            self.source_model_loss_with_zero_ablation - self.source_model_loss\n        )\n\n        return zero_ablate_loss_minus_reconstruction_loss / zero_ablate_loss_minus_default_loss\n</code></pre>"},{"location":"reference/metrics/validate/reconstruction_score/#sparse_autoencoder.metrics.validate.reconstruction_score.ReconstructionScoreMetric.__init__","title":"<code>__init__(num_components=1)</code>","text":"<p>Initialise the metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>@validate_call\ndef __init__(self, num_components: PositiveInt = 1) -&gt; None:\n    \"\"\"Initialise the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\n        \"source_model_loss\", default=torch.zeros(num_components), dist_reduce_fx=\"sum\"\n    )\n    self.add_state(\n        \"source_model_loss_with_zero_ablation\",\n        default=torch.zeros(num_components),\n        dist_reduce_fx=\"sum\",\n    )\n    self.add_state(\n        \"source_model_loss_with_reconstruction\",\n        default=torch.zeros(num_components),\n        dist_reduce_fx=\"sum\",\n    )\n</code></pre>"},{"location":"reference/metrics/validate/reconstruction_score/#sparse_autoencoder.metrics.validate.reconstruction_score.ReconstructionScoreMetric.compute","title":"<code>compute()</code>","text":"<p>Compute the metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>def compute(\n    self,\n) -&gt; Float[Tensor, Axis.COMPONENT_OPTIONAL]:\n    \"\"\"Compute the metric.\"\"\"\n    zero_ablate_loss_minus_reconstruction_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n        self.source_model_loss_with_zero_ablation - self.source_model_loss_with_reconstruction\n    )\n\n    zero_ablate_loss_minus_default_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL] = (\n        self.source_model_loss_with_zero_ablation - self.source_model_loss\n    )\n\n    return zero_ablate_loss_minus_reconstruction_loss / zero_ablate_loss_minus_default_loss\n</code></pre>"},{"location":"reference/metrics/validate/reconstruction_score/#sparse_autoencoder.metrics.validate.reconstruction_score.ReconstructionScoreMetric.update","title":"<code>update(source_model_loss, source_model_loss_with_reconstruction, source_model_loss_with_zero_ablation, component_idx=0)</code>","text":"<p>Update the metric state.</p> <p>Parameters:</p> Name Type Description Default <code>source_model_loss</code> <code>Float[Tensor, COMPONENT_OPTIONAL]</code> <p>Loss with no changes to the source model.</p> required <code>source_model_loss_with_reconstruction</code> <code>Float[Tensor, COMPONENT_OPTIONAL]</code> <p>Loss with SAE reconstruction.</p> required <code>source_model_loss_with_zero_ablation</code> <code>Float[Tensor, COMPONENT_OPTIONAL]</code> <p>Loss with zero ablation.</p> required <code>component_idx</code> <code>int</code> <p>Component idx.</p> <code>0</code> Source code in <code>sparse_autoencoder/metrics/validate/reconstruction_score.py</code> <pre><code>def update(\n    self,\n    source_model_loss: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n    source_model_loss_with_reconstruction: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n    source_model_loss_with_zero_ablation: Float[Tensor, Axis.COMPONENT_OPTIONAL],\n    component_idx: int = 0,\n) -&gt; None:\n    \"\"\"Update the metric state.\n\n    Args:\n        source_model_loss: Loss with no changes to the source model.\n        source_model_loss_with_reconstruction: Loss with SAE reconstruction.\n        source_model_loss_with_zero_ablation: Loss with zero ablation.\n        component_idx: Component idx.\n    \"\"\"\n    self.source_model_loss[component_idx] += source_model_loss.sum()\n    self.source_model_loss_with_zero_ablation[\n        component_idx\n    ] += source_model_loss_with_zero_ablation.sum()\n    self.source_model_loss_with_reconstruction[\n        component_idx\n    ] += source_model_loss_with_reconstruction.sum()\n</code></pre>"},{"location":"reference/metrics/wrappers/","title":"Metric wrappers","text":"<p>Metric wrappers.</p>"},{"location":"reference/metrics/wrappers/classwise/","title":"Classwise metrics wrapper","text":"<p>Classwise metrics wrapper.</p>"},{"location":"reference/metrics/wrappers/classwise/#sparse_autoencoder.metrics.wrappers.classwise.ClasswiseWrapperWithMean","title":"<code>ClasswiseWrapperWithMean</code>","text":"<p>             Bases: <code>ClasswiseWrapper</code></p> <p>Classwise wrapper with mean.</p> <p>This metric works together with classification metrics that returns multiple values (one value per class) such that label information can be automatically included in the output. It extends the standard torchmetrics wrapper that does this, adding in an additional mean value (across all classes).</p> Source code in <code>sparse_autoencoder/metrics/wrappers/classwise.py</code> <pre><code>class ClasswiseWrapperWithMean(ClasswiseWrapper):\n    \"\"\"Classwise wrapper with mean.\n\n    This metric works together with classification metrics that returns multiple values (one value\n    per class) such that label information can be automatically included in the output. It extends\n    the standard torchmetrics wrapper that does this, adding in an additional mean value (across all\n    classes).\n    \"\"\"\n\n    _prefix: str\n\n    labels: list[str]\n\n    def __init__(\n        self,\n        metric: Metric,\n        component_names: list[str] | None = None,\n        prefix: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the classwise wrapper.\n\n        Args:\n            metric: Metric to wrap.\n            component_names: Component names.\n            prefix: Prefix for the name (will replace the default of the class name).\n        \"\"\"\n        super().__init__(metric, component_names, prefix)\n\n        # Default prefix\n        if not self._prefix:\n            self._prefix = f\"{self.metric.__class__.__name__.lower()}\"\n\n    def _convert(self, x: Tensor) -&gt; dict[str, Tensor]:\n        \"\"\"Convert the input tensor to a dictionary of metrics.\n\n        Args:\n            x: The input tensor.\n\n        Returns:\n            A dictionary of metric results.\n        \"\"\"\n        # Add a component axis if not present (as Metric squeezes it out)\n        if x.ndim == 0:\n            x = x.unsqueeze(dim=0)\n\n        # Same splitting as the original classwise wrapper\n        res = {f\"{self._prefix}/{lab}\": val for lab, val in zip(self.labels, x)}\n\n        # Add in the mean\n        res[f\"{self._prefix}/mean\"] = x.mean(0, dtype=torch.float)\n\n        return res\n</code></pre>"},{"location":"reference/metrics/wrappers/classwise/#sparse_autoencoder.metrics.wrappers.classwise.ClasswiseWrapperWithMean.__init__","title":"<code>__init__(metric, component_names=None, prefix=None)</code>","text":"<p>Initialise the classwise wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Metric</code> <p>Metric to wrap.</p> required <code>component_names</code> <code>list[str] | None</code> <p>Component names.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>Prefix for the name (will replace the default of the class name).</p> <code>None</code> Source code in <code>sparse_autoencoder/metrics/wrappers/classwise.py</code> <pre><code>def __init__(\n    self,\n    metric: Metric,\n    component_names: list[str] | None = None,\n    prefix: str | None = None,\n) -&gt; None:\n    \"\"\"Initialise the classwise wrapper.\n\n    Args:\n        metric: Metric to wrap.\n        component_names: Component names.\n        prefix: Prefix for the name (will replace the default of the class name).\n    \"\"\"\n    super().__init__(metric, component_names, prefix)\n\n    # Default prefix\n    if not self._prefix:\n        self._prefix = f\"{self.metric.__class__.__name__.lower()}\"\n</code></pre>"},{"location":"reference/optimizer/","title":"Optimizers for Sparse Autoencoders","text":"<p>Optimizers for Sparse Autoencoders.</p>"},{"location":"reference/optimizer/adam_with_reset/","title":"Adam Optimizer with a reset method","text":"<p>Adam Optimizer with a reset method.</p> <p>This reset method is useful when resampling dead neurons during training.</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset","title":"<code>AdamWithReset</code>","text":"<p>             Bases: <code>Adam</code></p> <p>Adam Optimizer with a reset method.</p> <p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed.</p> <p>Otherwise this is the same as the standard Adam optimizer.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>class AdamWithReset(Adam):\n    \"\"\"Adam Optimizer with a reset method.\n\n    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when\n    manually editing the model parameters during training (e.g. when resampling dead neurons). This\n    is because Adam maintains running averages of the gradients and the squares of gradients, which\n    will be incorrect if the parameters are changed.\n\n    Otherwise this is the same as the standard Adam optimizer.\n    \"\"\"\n\n    parameter_names: list[str]\n    \"\"\"Parameter Names.\n\n    The names of the parameters, so that we can find them later when resetting the state.\n    \"\"\"\n\n    _has_components_dim: bool\n    \"\"\"Whether the parameters have a components dimension.\"\"\"\n\n    def __init__(  # (extending existing implementation)\n        self,\n        params: params_t,\n        lr: float | Float[Tensor, Axis.names(Axis.SINGLE_ITEM)] = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0.0,\n        *,\n        amsgrad: bool = False,\n        foreach: bool | None = None,\n        maximize: bool = False,\n        capturable: bool = False,\n        differentiable: bool = False,\n        fused: bool | None = None,\n        named_parameters: Iterator[tuple[str, Parameter]],\n        has_components_dim: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer.\n\n        Warning:\n            Named parameters must be with default settings (remove duplicates and not recursive).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n            ...     SparseAutoencoder, SparseAutoencoderConfig\n            ... )\n            &gt;&gt;&gt; model = SparseAutoencoder(\n            ...        SparseAutoencoderConfig(\n            ...             n_input_features=5,\n            ...             n_learned_features=10,\n            ...             n_components=2\n            ...         )\n            ...    )\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ...     has_components_dim=True,\n            ... )\n            &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n        Args:\n            params: Iterable of parameters to optimize or dicts defining parameter groups.\n            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n                float LR unless specifying fused=True or capturable=True.\n            betas: Coefficients used for computing running averages of gradient and its square.\n            eps: Term added to the denominator to improve numerical stability.\n            weight_decay: Weight decay (L2 penalty).\n            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n                Convergence of Adam and Beyond\".\n            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n                over the for-loop implementation on CUDA if more performant. Note that foreach uses\n                more peak memory.\n            maximize: If True, maximizes the parameters based on the objective, instead of\n                minimizing.\n            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n                ungraphed performance.\n            differentiable: Whether autograd should occur through the optimizer step in training.\n                Setting to True can impair performance.\n            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n                torch.float32, torch.float16, and torch.bfloat16.\n            named_parameters: An iterator over the named parameters of the model. This is used to\n                find the parameters when resetting their state. You should set this as\n                `model.named_parameters()`.\n            has_components_dim: If the parameters have a components dimension (i.e. if you are\n                training an SAE on more than one component).\n\n        Raises:\n            ValueError: If the number of parameter names does not match the number of parameters.\n        \"\"\"\n        # Initialise the parent class (note we repeat the parameter names so that type hints work).\n        super().__init__(\n            params=params,\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n            differentiable=differentiable,\n            fused=fused,\n        )\n\n        self._has_components_dim = has_components_dim\n\n        # Store the names of the parameters, so that we can find them later when resetting the\n        # state.\n        self.parameter_names = [name for name, _value in named_parameters]\n\n        if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n            error_message = (\n                \"The number of parameter names does not match the number of parameters. \"\n                \"If using model.named_parameters() make sure remove_duplicates is True \"\n                \"and recursive is False (the default settings).\"\n            )\n            raise ValueError(error_message)\n\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Iterates over all parameters and resets both the running averages of the gradients and the\n        squares of gradients.\n        \"\"\"\n        # Iterate over every parameter\n        for group in self.param_groups:\n            for parameter in group[\"params\"]:\n                # Get the state\n                state = self.state[parameter]\n\n                # Check if state is initialized\n                if len(state) == 0:\n                    continue\n\n                # Reset running averages\n                exp_avg: Tensor = state[\"exp_avg\"]\n                exp_avg.zero_()\n                exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n                exp_avg_sq.zero_()\n\n                # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n                if \"max_exp_avg_sq\" in state:\n                    max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                    max_exp_avg_sq.zero_()\n\n    def reset_neurons_state(\n        self,\n        parameter: Parameter,\n        neuron_indices: Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)],\n        axis: int,\n        component_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n            ...     SparseAutoencoder, SparseAutoencoderConfig\n            ... )\n            &gt;&gt;&gt; model = SparseAutoencoder(\n            ...        SparseAutoencoderConfig(\n            ...             n_input_features=5,\n            ...             n_learned_features=10,\n            ...             n_components=2\n            ...         )\n            ...    )\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ...     has_components_dim=True,\n            ... )\n            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\n            ...     model.decoder.weight,\n            ...     dead_neurons_indices,\n            ...     axis=1\n            ... )\n\n        Args:\n            parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the state values to reset (i.e. the input/output features axis, as\n                we're resetting all input/output features for a specific dead neuron).\n            component_idx: The component index of the state values to reset.\n\n        Raises:\n            ValueError: If the parameter has a components dimension, but has_components_dim is\n                False.\n        \"\"\"\n        # Get the state of the parameter\n        state = self.state[parameter]\n\n        # If the number of dimensions is 3, we definitely have a components dimension. If 2, we may\n        # do (as the bias has 2 dimensions with components, but the weight has 2 dimensions without\n        # components).\n        definitely_has_components_dimension = 3\n        if (\n            not self._has_components_dim\n            and state[\"exp_avg\"].ndim == definitely_has_components_dimension\n        ):\n            error_message = (\n                \"The parameter has a components dimension, but has_components_dim is False. \"\n                \"This should not happen.\"\n            )\n            raise ValueError(error_message)\n\n        # Check if state is initialized\n        if len(state) == 0:\n            return\n\n        # Check there are any neurons to reset\n        if neuron_indices.numel() == 0:\n            return\n\n        # Move the neuron indices to the correct device\n        neuron_indices = neuron_indices.to(device=state[\"exp_avg\"].device)\n\n        # Reset running averages for the specified neurons\n        if \"exp_avg\" in state:\n            if self._has_components_dim:\n                state[\"exp_avg\"][component_idx].index_fill_(axis, neuron_indices, 0)\n            else:\n                state[\"exp_avg\"].index_fill_(axis, neuron_indices, 0)\n\n        if \"exp_avg_sq\" in state:\n            if self._has_components_dim:\n                state[\"exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n            else:\n                state[\"exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n\n        # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n        if \"max_exp_avg_sq\" in state:\n            if self._has_components_dim:\n                state[\"max_exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n            else:\n                state[\"max_exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.parameter_names","title":"<code>parameter_names: list[str] = [name for (name, _value) in named_parameters]</code>  <code>instance-attribute</code>","text":"<p>Parameter Names.</p> <p>The names of the parameters, so that we can find them later when resetting the state.</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.__init__","title":"<code>__init__(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0, *, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, named_parameters, has_components_dim)</code>","text":"<p>Initialize the optimizer.</p> Warning <p>Named parameters must be with default settings (remove duplicates and not recursive).</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import ( ...     SparseAutoencoder, SparseAutoencoderConfig ... ) model = SparseAutoencoder( ...        SparseAutoencoderConfig( ...             n_input_features=5, ...             n_learned_features=10, ...             n_components=2 ...         ) ...    ) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ...     has_components_dim=True, ... ) optimizer.reset_state_all_parameters()</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>params_t</code> <p>Iterable of parameters to optimize or dicts defining parameter groups.</p> required <code>lr</code> <code>float | Float[Tensor, names(SINGLE_ITEM)]</code> <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a float LR unless specifying fused=True or capturable=True.</p> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients used for computing running averages of gradient and its square.</p> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <p>Term added to the denominator to improve numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (L2 penalty).</p> <code>0.0</code> <code>amsgrad</code> <code>bool</code> <p>Whether to use the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Whether foreach implementation of optimizer is used. If None, foreach is used over the for-loop implementation on CUDA if more performant. Note that foreach uses more peak memory.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, maximizes the parameters based on the objective, instead of minimizing.</p> <code>False</code> <code>capturable</code> <code>bool</code> <p>Whether this instance is safe to capture in a CUDA graph. True can impair ungraphed performance.</p> <code>False</code> <code>differentiable</code> <code>bool</code> <p>Whether autograd should occur through the optimizer step in training. Setting to True can impair performance.</p> <code>False</code> <code>fused</code> <code>bool | None</code> <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64, torch.float32, torch.float16, and torch.bfloat16.</p> <code>None</code> <code>named_parameters</code> <code>Iterator[tuple[str, Parameter]]</code> <p>An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as <code>model.named_parameters()</code>.</p> required <code>has_components_dim</code> <code>bool</code> <p>If the parameters have a components dimension (i.e. if you are training an SAE on more than one component).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of parameter names does not match the number of parameters.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def __init__(  # (extending existing implementation)\n    self,\n    params: params_t,\n    lr: float | Float[Tensor, Axis.names(Axis.SINGLE_ITEM)] = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    *,\n    amsgrad: bool = False,\n    foreach: bool | None = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: bool | None = None,\n    named_parameters: Iterator[tuple[str, Parameter]],\n    has_components_dim: bool,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Warning:\n        Named parameters must be with default settings (remove duplicates and not recursive).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n        ...     SparseAutoencoder, SparseAutoencoderConfig\n        ... )\n        &gt;&gt;&gt; model = SparseAutoencoder(\n        ...        SparseAutoencoderConfig(\n        ...             n_input_features=5,\n        ...             n_learned_features=10,\n        ...             n_components=2\n        ...         )\n        ...    )\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ...     has_components_dim=True,\n        ... )\n        &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n    Args:\n        params: Iterable of parameters to optimize or dicts defining parameter groups.\n        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n            float LR unless specifying fused=True or capturable=True.\n        betas: Coefficients used for computing running averages of gradient and its square.\n        eps: Term added to the denominator to improve numerical stability.\n        weight_decay: Weight decay (L2 penalty).\n        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n            Convergence of Adam and Beyond\".\n        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n            over the for-loop implementation on CUDA if more performant. Note that foreach uses\n            more peak memory.\n        maximize: If True, maximizes the parameters based on the objective, instead of\n            minimizing.\n        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n            ungraphed performance.\n        differentiable: Whether autograd should occur through the optimizer step in training.\n            Setting to True can impair performance.\n        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n            torch.float32, torch.float16, and torch.bfloat16.\n        named_parameters: An iterator over the named parameters of the model. This is used to\n            find the parameters when resetting their state. You should set this as\n            `model.named_parameters()`.\n        has_components_dim: If the parameters have a components dimension (i.e. if you are\n            training an SAE on more than one component).\n\n    Raises:\n        ValueError: If the number of parameter names does not match the number of parameters.\n    \"\"\"\n    # Initialise the parent class (note we repeat the parameter names so that type hints work).\n    super().__init__(\n        params=params,\n        lr=lr,\n        betas=betas,\n        eps=eps,\n        weight_decay=weight_decay,\n        amsgrad=amsgrad,\n        foreach=foreach,\n        maximize=maximize,\n        capturable=capturable,\n        differentiable=differentiable,\n        fused=fused,\n    )\n\n    self._has_components_dim = has_components_dim\n\n    # Store the names of the parameters, so that we can find them later when resetting the\n    # state.\n    self.parameter_names = [name for name, _value in named_parameters]\n\n    if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n        error_message = (\n            \"The number of parameter names does not match the number of parameters. \"\n            \"If using model.named_parameters() make sure remove_duplicates is True \"\n            \"and recursive is False (the default settings).\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter, neuron_indices, axis, component_idx=0)</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import ( ...     SparseAutoencoder, SparseAutoencoderConfig ... ) model = SparseAutoencoder( ...        SparseAutoencoderConfig( ...             n_input_features=5, ...             n_learned_features=10, ...             n_components=2 ...         ) ...    ) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ...     has_components_dim=True, ... )</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>Parameter</code> <p>The parameter to be reset. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>,</p> required <code>neuron_indices</code> <code>Int[Tensor, names(LEARNT_FEATURE_IDX)]</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the state values to reset (i.e. the input/output features axis, as we're resetting all input/output features for a specific dead neuron).</p> required <code>component_idx</code> <code>int</code> <p>The component index of the state values to reset.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the parameter has a components dimension, but has_components_dim is False.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_neurons_state(\n    self,\n    parameter: Parameter,\n    neuron_indices: Int[Tensor, Axis.names(Axis.LEARNT_FEATURE_IDX)],\n    axis: int,\n    component_idx: int = 0,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (\n        ...     SparseAutoencoder, SparseAutoencoderConfig\n        ... )\n        &gt;&gt;&gt; model = SparseAutoencoder(\n        ...        SparseAutoencoderConfig(\n        ...             n_input_features=5,\n        ...             n_learned_features=10,\n        ...             n_components=2\n        ...         )\n        ...    )\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ...     has_components_dim=True,\n        ... )\n        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\n        ...     model.decoder.weight,\n        ...     dead_neurons_indices,\n        ...     axis=1\n        ... )\n\n    Args:\n        parameter: The parameter to be reset. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the state values to reset (i.e. the input/output features axis, as\n            we're resetting all input/output features for a specific dead neuron).\n        component_idx: The component index of the state values to reset.\n\n    Raises:\n        ValueError: If the parameter has a components dimension, but has_components_dim is\n            False.\n    \"\"\"\n    # Get the state of the parameter\n    state = self.state[parameter]\n\n    # If the number of dimensions is 3, we definitely have a components dimension. If 2, we may\n    # do (as the bias has 2 dimensions with components, but the weight has 2 dimensions without\n    # components).\n    definitely_has_components_dimension = 3\n    if (\n        not self._has_components_dim\n        and state[\"exp_avg\"].ndim == definitely_has_components_dimension\n    ):\n        error_message = (\n            \"The parameter has a components dimension, but has_components_dim is False. \"\n            \"This should not happen.\"\n        )\n        raise ValueError(error_message)\n\n    # Check if state is initialized\n    if len(state) == 0:\n        return\n\n    # Check there are any neurons to reset\n    if neuron_indices.numel() == 0:\n        return\n\n    # Move the neuron indices to the correct device\n    neuron_indices = neuron_indices.to(device=state[\"exp_avg\"].device)\n\n    # Reset running averages for the specified neurons\n    if \"exp_avg\" in state:\n        if self._has_components_dim:\n            state[\"exp_avg\"][component_idx].index_fill_(axis, neuron_indices, 0)\n        else:\n            state[\"exp_avg\"].index_fill_(axis, neuron_indices, 0)\n\n    if \"exp_avg_sq\" in state:\n        if self._has_components_dim:\n            state[\"exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n        else:\n            state[\"exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n\n    # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n    if \"max_exp_avg_sq\" in state:\n        if self._has_components_dim:\n            state[\"max_exp_avg_sq\"][component_idx].index_fill_(axis, neuron_indices, 0)\n        else:\n            state[\"max_exp_avg_sq\"].index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","title":"... train the model and then resample some dead neurons, then do this ...","text":"<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","title":"Reset the optimizer state for parameters that have been updated","text":"<p>optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0) optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ...     model.decoder.weight, ...     dead_neurons_indices, ...     axis=1 ... )</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>","text":"<p>Reset the state for all parameters.</p> <p>Iterates over all parameters and resets both the running averages of the gradients and the squares of gradients.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Iterates over all parameters and resets both the running averages of the gradients and the\n    squares of gradients.\n    \"\"\"\n    # Iterate over every parameter\n    for group in self.param_groups:\n        for parameter in group[\"params\"]:\n            # Get the state\n            state = self.state[parameter]\n\n            # Check if state is initialized\n            if len(state) == 0:\n                continue\n\n            # Reset running averages\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.zero_()\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.zero_()\n\n            # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n            if \"max_exp_avg_sq\" in state:\n                max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                max_exp_avg_sq.zero_()\n</code></pre>"},{"location":"reference/source_data/","title":"Source Data","text":"<p>Source Data.</p>"},{"location":"reference/source_data/abstract_dataset/","title":"Abstract tokenized prompts dataset class","text":"<p>Abstract tokenized prompts dataset class.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem","title":"<code>HuggingFaceDatasetItem = TypeVar('HuggingFaceDatasetItem', bound=Any)</code>  <code>module-attribute</code>","text":"<p>Hugging face dataset item typed dict.</p> <p>When extending :class:<code>SourceDataset</code> you should create a <code>TypedDict</code> that matches the structure of each dataset item in the underlying Hugging Face dataset.</p> Example <p>With the Uncopyrighted Pile this should be a typed dict with text and meta properties.</p> <p>class PileUncopyrightedSourceDataBatch(TypedDict): ...    text: list[str] ...    meta: list[dict[str, dict[str, str]]]</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompt","title":"<code>TokenizedPrompt = list[int]</code>  <code>module-attribute</code>","text":"<p>A tokenized prompt.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset","title":"<code>SourceDataset</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[HuggingFaceDatasetItem]</code></p> <p>Abstract source dataset.</p> <p>Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset.</p> <p>Wraps an HuggingFace IterableDataset.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class SourceDataset(ABC, Generic[HuggingFaceDatasetItem]):\n    \"\"\"Abstract source dataset.\n\n    Source dataset that is used to generate the activations dataset (by running forward passes of\n    the source model with this data). It should contain prompts that have been tokenized with no\n    padding tokens (apart from an optional single first padding token). This enables efficient\n    generation of the activations dataset.\n\n    Wraps an HuggingFace IterableDataset.\n    \"\"\"\n\n    context_size: int\n    \"\"\"Number of tokens in the context window.\n\n    The paper *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n    a context size of 250.\n    \"\"\"\n\n    dataset: Dataset | IterableDataset\n    \"\"\"Underlying HuggingFace Dataset.\n\n    Warning:\n        Hugging Face `Dataset` objects are confusingly not the same as PyTorch `Dataset` objects.\n    \"\"\"\n\n    _dataset_column_name: str\n    \"\"\"Dataset column name for the prompts.\"\"\"\n\n    @abstractmethod\n    def preprocess(\n        self,\n        source_batch: HuggingFaceDatasetItem,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess function.\n\n        Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string\n        prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary\n        length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.\n\n        Applied to the dataset with the [Hugging Face\n        Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)\n        `map` function.\n\n        Warning:\n            The returned tokenized prompts should not have any padding tokens (apart from an\n            optional single first padding token).\n\n        Args:\n            source_batch: A batch of source data. For example, with The Pile dataset this would be a\n                dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n\n    @abstractmethod\n    @validate_call\n    def __init__(\n        self,\n        dataset_path: str,\n        dataset_split: str,\n        context_size: PositiveInt,\n        buffer_size: PositiveInt = 1000,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_column_name: str = \"input_ids\",\n        n_processes_preprocessing: PositiveInt | None = None,\n        preprocess_batch_size: PositiveInt = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialise the dataset.\n\n        Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the\n        underlying Hugging Face `IterableDataset`.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g. `train`).\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_column_name: The column name for the prompts.\n            n_processes_preprocessing: The number of processes to use for preprocessing.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n\n        Raises:\n            TypeError: If the loaded dataset is not a Hugging Face `Dataset` or `IterableDataset`.\n        \"\"\"\n        self.context_size = context_size\n        self._dataset_column_name = dataset_column_name\n\n        # Load the dataset\n        should_stream = not pre_download\n        dataset = load_dataset(\n            dataset_path,\n            streaming=should_stream,\n            split=dataset_split,\n            data_dir=dataset_dir,\n            data_files=dataset_files,\n            verification_mode=VerificationMode.NO_CHECKS,  # As it fails when data_files is set\n        )\n\n        # Setup preprocessing (we remove all columns except for input ids)\n        remove_columns: list[str] = list(next(iter(dataset)).keys())\n        if \"input_ids\" in remove_columns:\n            remove_columns.remove(\"input_ids\")\n\n        if pre_download:\n            if not isinstance(dataset, Dataset):\n                error_message = (\n                    f\"Expected Hugging Face dataset to be a Dataset when pre-downloading, but got \"\n                    f\"{type(dataset)}.\"\n                )\n                raise TypeError(error_message)\n\n            # Download the whole dataset\n            mapped_dataset = dataset.map(\n                self.preprocess,\n                batched=True,\n                batch_size=preprocess_batch_size,\n                fn_kwargs={\"context_size\": context_size},\n                remove_columns=remove_columns,\n                num_proc=n_processes_preprocessing,\n            )\n            self.dataset = mapped_dataset.shuffle()\n        else:\n            # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at\n            # least `buffer_size` items and then shuffles just that buffer.\n            # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n            if not isinstance(dataset, IterableDataset):\n                error_message = (\n                    f\"Expected Hugging Face dataset to be an IterableDataset when streaming, but \"\n                    f\"got {type(dataset)}.\"\n                )\n                raise TypeError(error_message)\n\n            mapped_dataset = dataset.map(\n                self.preprocess,\n                batched=True,\n                batch_size=preprocess_batch_size,\n                fn_kwargs={\"context_size\": context_size},\n                remove_columns=remove_columns,\n            )\n            self.dataset = mapped_dataset.shuffle(buffer_size=buffer_size)  # type: ignore\n\n    @final\n    def __iter__(self) -&gt; Any:  # noqa: ANN401\n        \"\"\"Iterate Dunder Method.\n\n        Enables direct access to :attr:`dataset` with e.g. `for` loops.\n        \"\"\"\n        return self.dataset.__iter__()\n\n    @final\n    def get_dataloader(\n        self, batch_size: int, num_workers: NonNegativeInt = 0\n    ) -&gt; DataLoader[TorchTokenizedPrompts]:\n        \"\"\"Get a PyTorch DataLoader.\n\n        Args:\n            batch_size: The batch size to use.\n            num_workers: Number of CPU workers.\n\n        Returns:\n            PyTorch DataLoader.\n        \"\"\"\n        torch_dataset: TorchDataset[TorchTokenizedPrompts] = self.dataset.with_format(\"torch\")  # type: ignore\n\n        return DataLoader[TorchTokenizedPrompts](\n            torch_dataset,\n            batch_size=batch_size,\n            # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not\n            # here.\n            shuffle=False,\n            num_workers=num_workers,\n        )\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.context_size","title":"<code>context_size: int = context_size</code>  <code>instance-attribute</code>","text":"<p>Number of tokens in the context window.</p> <p>The paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.dataset","title":"<code>dataset: Dataset | IterableDataset</code>  <code>instance-attribute</code>","text":"<p>Underlying HuggingFace Dataset.</p> Warning <p>Hugging Face <code>Dataset</code> objects are confusingly not the same as PyTorch <code>Dataset</code> objects.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__init__","title":"<code>__init__(dataset_path, dataset_split, context_size, buffer_size=1000, dataset_dir=None, dataset_files=None, dataset_column_name='input_ids', n_processes_preprocessing=None, preprocess_batch_size=1000, *, pre_download=False)</code>  <code>abstractmethod</code>","text":"<p>Initialise the dataset.</p> <p>Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face <code>IterableDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> required <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> required <code>context_size</code> <code>PositiveInt</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> required <code>buffer_size</code> <code>PositiveInt</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_column_name</code> <code>str</code> <p>The column name for the prompts.</p> <code>'input_ids'</code> <code>n_processes_preprocessing</code> <code>PositiveInt | None</code> <p>The number of processes to use for preprocessing.</p> <code>None</code> <code>preprocess_batch_size</code> <code>PositiveInt</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the loaded dataset is not a Hugging Face <code>Dataset</code> or <code>IterableDataset</code>.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@abstractmethod\n@validate_call\ndef __init__(\n    self,\n    dataset_path: str,\n    dataset_split: str,\n    context_size: PositiveInt,\n    buffer_size: PositiveInt = 1000,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_column_name: str = \"input_ids\",\n    n_processes_preprocessing: PositiveInt | None = None,\n    preprocess_batch_size: PositiveInt = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialise the dataset.\n\n    Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the\n    underlying Hugging Face `IterableDataset`.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g. `train`).\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_column_name: The column name for the prompts.\n        n_processes_preprocessing: The number of processes to use for preprocessing.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n\n    Raises:\n        TypeError: If the loaded dataset is not a Hugging Face `Dataset` or `IterableDataset`.\n    \"\"\"\n    self.context_size = context_size\n    self._dataset_column_name = dataset_column_name\n\n    # Load the dataset\n    should_stream = not pre_download\n    dataset = load_dataset(\n        dataset_path,\n        streaming=should_stream,\n        split=dataset_split,\n        data_dir=dataset_dir,\n        data_files=dataset_files,\n        verification_mode=VerificationMode.NO_CHECKS,  # As it fails when data_files is set\n    )\n\n    # Setup preprocessing (we remove all columns except for input ids)\n    remove_columns: list[str] = list(next(iter(dataset)).keys())\n    if \"input_ids\" in remove_columns:\n        remove_columns.remove(\"input_ids\")\n\n    if pre_download:\n        if not isinstance(dataset, Dataset):\n            error_message = (\n                f\"Expected Hugging Face dataset to be a Dataset when pre-downloading, but got \"\n                f\"{type(dataset)}.\"\n            )\n            raise TypeError(error_message)\n\n        # Download the whole dataset\n        mapped_dataset = dataset.map(\n            self.preprocess,\n            batched=True,\n            batch_size=preprocess_batch_size,\n            fn_kwargs={\"context_size\": context_size},\n            remove_columns=remove_columns,\n            num_proc=n_processes_preprocessing,\n        )\n        self.dataset = mapped_dataset.shuffle()\n    else:\n        # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at\n        # least `buffer_size` items and then shuffles just that buffer.\n        # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n        if not isinstance(dataset, IterableDataset):\n            error_message = (\n                f\"Expected Hugging Face dataset to be an IterableDataset when streaming, but \"\n                f\"got {type(dataset)}.\"\n            )\n            raise TypeError(error_message)\n\n        mapped_dataset = dataset.map(\n            self.preprocess,\n            batched=True,\n            batch_size=preprocess_batch_size,\n            fn_kwargs={\"context_size\": context_size},\n            remove_columns=remove_columns,\n        )\n        self.dataset = mapped_dataset.shuffle(buffer_size=buffer_size)  # type: ignore\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate Dunder Method.</p> <p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>for</code> loops.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef __iter__(self) -&gt; Any:  # noqa: ANN401\n    \"\"\"Iterate Dunder Method.\n\n    Enables direct access to :attr:`dataset` with e.g. `for` loops.\n    \"\"\"\n    return self.dataset.__iter__()\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.get_dataloader","title":"<code>get_dataloader(batch_size, num_workers=0)</code>","text":"<p>Get a PyTorch DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size to use.</p> required <code>num_workers</code> <code>NonNegativeInt</code> <p>Number of CPU workers.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef get_dataloader(\n    self, batch_size: int, num_workers: NonNegativeInt = 0\n) -&gt; DataLoader[TorchTokenizedPrompts]:\n    \"\"\"Get a PyTorch DataLoader.\n\n    Args:\n        batch_size: The batch size to use.\n        num_workers: Number of CPU workers.\n\n    Returns:\n        PyTorch DataLoader.\n    \"\"\"\n    torch_dataset: TorchDataset[TorchTokenizedPrompts] = self.dataset.with_format(\"torch\")  # type: ignore\n\n    return DataLoader[TorchTokenizedPrompts](\n        torch_dataset,\n        batch_size=batch_size,\n        # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not\n        # here.\n        shuffle=False,\n        num_workers=num_workers,\n    )\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>  <code>abstractmethod</code>","text":"<p>Preprocess function.</p> <p>Takes a <code>preprocess_batch_size</code> (\\(m\\)) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of <code>input_ids</code> and a value of an arbitrary length list (\\(n\\)) of tokenized prompts. Note that \\(m\\) does not have to be equal to \\(n\\).</p> <p>Applied to the dataset with the Hugging Face Dataset <code>map</code> function.</p> Warning <p>The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token).</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>HuggingFaceDatasetItem</code> <p>A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized).</p> required <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@abstractmethod\ndef preprocess(\n    self,\n    source_batch: HuggingFaceDatasetItem,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess function.\n\n    Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string\n    prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary\n    length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.\n\n    Applied to the dataset with the [Hugging Face\n    Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)\n    `map` function.\n\n    Warning:\n        The returned tokenized prompts should not have any padding tokens (apart from an\n        optional single first padding token).\n\n    Args:\n        source_batch: A batch of source data. For example, with The Pile dataset this would be a\n            dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts","title":"<code>TokenizedPrompts</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class TokenizedPrompts(TypedDict):\n    \"\"\"Tokenized prompts.\"\"\"\n\n    input_ids: list[TokenizedPrompt]\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts","title":"<code>TorchTokenizedPrompts</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Tokenized prompts prepared for PyTorch.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class TorchTokenizedPrompts(TypedDict):\n    \"\"\"Tokenized prompts prepared for PyTorch.\"\"\"\n\n    input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)]\n</code></pre>"},{"location":"reference/source_data/mock_dataset/","title":"Mock dataset","text":"<p>Mock dataset.</p> <p>For use with tests and simple examples.</p>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset","title":"<code>ConsecutiveIntHuggingFaceDataset</code>","text":"<p>             Bases: <code>IterableDataset</code></p> <p>Consecutive integers Hugging Face dataset for testing.</p> <p>Creates a dataset where the first item is [0,1,2...], and the second item is [1,2,3...] and so on.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>class ConsecutiveIntHuggingFaceDataset(IterableDataset):\n    \"\"\"Consecutive integers Hugging Face dataset for testing.\n\n    Creates a dataset where the first item is [0,1,2...], and the second item is [1,2,3...] and so\n    on.\n    \"\"\"\n\n    _data: Int[Tensor, \"items context_size\"]\n    \"\"\"Generated data.\"\"\"\n\n    _length: int\n    \"\"\"Size of the dataset.\"\"\"\n\n    _format: Literal[\"torch\", \"list\"] = \"list\"\n    \"\"\"Format of the data.\"\"\"\n\n    def create_data(self, n_items: int, context_size: int) -&gt; Int[Tensor, \"items context_size\"]:\n        \"\"\"Create the data.\n\n        Args:\n            n_items: The number of items in the dataset.\n            context_size: The number of tokens in the context window.\n\n        Returns:\n            The generated data.\n        \"\"\"\n        rows = torch.arange(n_items).unsqueeze(1)\n        columns = torch.arange(context_size).unsqueeze(0)\n        return rows + columns\n\n    def __init__(self, context_size: int, vocab_size: int = 50_000, n_items: int = 10_000) -&gt; None:\n        \"\"\"Initialize the mock HF dataset.\n\n        Args:\n            context_size: The number of tokens in the context window\n            vocab_size: The size of the vocabulary to use.\n            n_items: The number of items in the dataset.\n\n        Raises:\n            ValueError: If more items are requested than we can create with the vocab size (given\n                that each item is a consecutive list of integers and unique).\n        \"\"\"\n        self._length = n_items\n\n        # Check we can create the data\n        if n_items + context_size &gt; vocab_size:\n            error_message = (\n                f\"n_items ({n_items}) + context_size ({context_size}) must be less than \"\n                f\"vocab_size ({vocab_size})\"\n            )\n            raise ValueError(error_message)\n\n        # Initialise the data\n        self._data = self.create_data(n_items, context_size)\n\n    def __iter__(self) -&gt; Iterator:  # type: ignore (HF typing is incorrect)\n        \"\"\"Initialize the iterator.\n\n        Returns:\n            Iterator.\n        \"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n        \"\"\"Return the next item in the dataset.\n\n        Returns:\n            TokenizedPrompts: The next item in the dataset.\n\n        Raises:\n            StopIteration: If the end of the dataset is reached.\n        \"\"\"\n        if self._index &lt; self._length:\n            item = self[self._index]\n            self._index += 1\n            return item\n\n        raise StopIteration\n\n    def __len__(self) -&gt; int:\n        \"\"\"Len Dunder Method.\"\"\"\n        return self._length\n\n    def __getitem__(self, index: int) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n        \"\"\"Get Item.\"\"\"\n        item = self._data[index]\n\n        if self._format == \"torch\":\n            return {\"input_ids\": item}\n\n        return {\"input_ids\": item.tolist()}\n\n    def with_format(  # type: ignore (only support 2 types)\n        self,\n        type: Literal[\"torch\", \"list\"],  # noqa: A002\n    ) -&gt; \"ConsecutiveIntHuggingFaceDataset\":\n        \"\"\"With Format.\"\"\"\n        self._format = type\n        return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n    \"\"\"Get Item.\"\"\"\n    item = self._data[index]\n\n    if self._format == \"torch\":\n        return {\"input_ids\": item}\n\n    return {\"input_ids\": item.tolist()}\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__init__","title":"<code>__init__(context_size, vocab_size=50000, n_items=10000)</code>","text":"<p>Initialize the mock HF dataset.</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>int</code> <p>The number of tokens in the context window</p> required <code>vocab_size</code> <code>int</code> <p>The size of the vocabulary to use.</p> <code>50000</code> <code>n_items</code> <code>int</code> <p>The number of items in the dataset.</p> <code>10000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If more items are requested than we can create with the vocab size (given that each item is a consecutive list of integers and unique).</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __init__(self, context_size: int, vocab_size: int = 50_000, n_items: int = 10_000) -&gt; None:\n    \"\"\"Initialize the mock HF dataset.\n\n    Args:\n        context_size: The number of tokens in the context window\n        vocab_size: The size of the vocabulary to use.\n        n_items: The number of items in the dataset.\n\n    Raises:\n        ValueError: If more items are requested than we can create with the vocab size (given\n            that each item is a consecutive list of integers and unique).\n    \"\"\"\n    self._length = n_items\n\n    # Check we can create the data\n    if n_items + context_size &gt; vocab_size:\n        error_message = (\n            f\"n_items ({n_items}) + context_size ({context_size}) must be less than \"\n            f\"vocab_size ({vocab_size})\"\n        )\n        raise ValueError(error_message)\n\n    # Initialise the data\n    self._data = self.create_data(n_items, context_size)\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Initialize the iterator.</p> <p>Returns:</p> Type Description <code>Iterator</code> <p>Iterator.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __iter__(self) -&gt; Iterator:  # type: ignore (HF typing is incorrect)\n    \"\"\"Initialize the iterator.\n\n    Returns:\n        Iterator.\n    \"\"\"\n    self._index = 0\n    return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__len__","title":"<code>__len__()</code>","text":"<p>Len Dunder Method.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Len Dunder Method.\"\"\"\n    return self._length\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item in the dataset.</p> <p>Returns:</p> Name Type Description <code>TokenizedPrompts</code> <code>TokenizedPrompts | TorchTokenizedPrompts</code> <p>The next item in the dataset.</p> <p>Raises:</p> Type Description <code>StopIteration</code> <p>If the end of the dataset is reached.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __next__(self) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n    \"\"\"Return the next item in the dataset.\n\n    Returns:\n        TokenizedPrompts: The next item in the dataset.\n\n    Raises:\n        StopIteration: If the end of the dataset is reached.\n    \"\"\"\n    if self._index &lt; self._length:\n        item = self[self._index]\n        self._index += 1\n        return item\n\n    raise StopIteration\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.create_data","title":"<code>create_data(n_items, context_size)</code>","text":"<p>Create the data.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>The number of items in the dataset.</p> required <code>context_size</code> <code>int</code> <p>The number of tokens in the context window.</p> required <p>Returns:</p> Type Description <code>Int[Tensor, 'items context_size']</code> <p>The generated data.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def create_data(self, n_items: int, context_size: int) -&gt; Int[Tensor, \"items context_size\"]:\n    \"\"\"Create the data.\n\n    Args:\n        n_items: The number of items in the dataset.\n        context_size: The number of tokens in the context window.\n\n    Returns:\n        The generated data.\n    \"\"\"\n    rows = torch.arange(n_items).unsqueeze(1)\n    columns = torch.arange(context_size).unsqueeze(0)\n    return rows + columns\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.with_format","title":"<code>with_format(type)</code>","text":"<p>With Format.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def with_format(  # type: ignore (only support 2 types)\n    self,\n    type: Literal[\"torch\", \"list\"],  # noqa: A002\n) -&gt; \"ConsecutiveIntHuggingFaceDataset\":\n    \"\"\"With Format.\"\"\"\n    self._format = type\n    return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset","title":"<code>MockDataset</code>","text":"<p>             Bases: <code>SourceDataset[TokenizedPrompts]</code></p> <p>Mock dataset for testing.</p> <p>For use with tests and simple examples.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>@final\nclass MockDataset(SourceDataset[TokenizedPrompts]):\n    \"\"\"Mock dataset for testing.\n\n    For use with tests and simple examples.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerFast\n\n    def preprocess(\n        self,\n        source_batch: TokenizedPrompts,\n        *,\n        context_size: int,  # noqa: ARG002\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\"\"\"\n        # Nothing to do here\n        return source_batch\n\n    @validate_call\n    def __init__(\n        self,\n        context_size: PositiveInt = 250,\n        buffer_size: PositiveInt = 1000,  # noqa: ARG002\n        preprocess_batch_size: PositiveInt = 1000,  # noqa: ARG002\n        dataset_path: str = \"dummy\",  # noqa: ARG002\n        dataset_split: str = \"train\",  # noqa: ARG002\n    ):\n        \"\"\"Initialize the Random Int Dummy dataset.\n\n        Example:\n            &gt;&gt;&gt; data = MockDataset()\n            &gt;&gt;&gt; first_item = next(iter(data))\n            &gt;&gt;&gt; len(first_item[\"input_ids\"])\n            250\n\n        Args:\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n                streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n                just that buffer. Note that the generated activations should also be shuffled before\n                training the sparse autoencoder, so a large buffer may not be strictly necessary\n                here. Note also that this is the number of items in the dataset (e.g. number of\n                prompts) and is typically significantly less than the number of tokenized prompts\n                once the preprocessing function has been applied.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            dataset_path: The path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g. `train`).\n        \"\"\"\n        self.dataset = ConsecutiveIntHuggingFaceDataset(context_size=context_size)  # type: ignore\n        self.context_size = context_size\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset.__init__","title":"<code>__init__(context_size=250, buffer_size=1000, preprocess_batch_size=1000, dataset_path='dummy', dataset_split='train')</code>","text":"<p>Initialize the Random Int Dummy dataset.</p> Example <p>data = MockDataset() first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>PositiveInt</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>250</code> <code>buffer_size</code> <code>PositiveInt</code> <p>The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>PositiveInt</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> <code>'dummy'</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    context_size: PositiveInt = 250,\n    buffer_size: PositiveInt = 1000,  # noqa: ARG002\n    preprocess_batch_size: PositiveInt = 1000,  # noqa: ARG002\n    dataset_path: str = \"dummy\",  # noqa: ARG002\n    dataset_split: str = \"train\",  # noqa: ARG002\n):\n    \"\"\"Initialize the Random Int Dummy dataset.\n\n    Example:\n        &gt;&gt;&gt; data = MockDataset()\n        &gt;&gt;&gt; first_item = next(iter(data))\n        &gt;&gt;&gt; len(first_item[\"input_ids\"])\n        250\n\n    Args:\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n            streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n            just that buffer. Note that the generated activations should also be shuffled before\n            training the sparse autoencoder, so a large buffer may not be strictly necessary\n            here. Note also that this is the number of items in the dataset (e.g. number of\n            prompts) and is typically significantly less than the number of tokenized prompts\n            once the preprocessing function has been applied.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        dataset_path: The path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g. `train`).\n    \"\"\"\n    self.dataset = ConsecutiveIntHuggingFaceDataset(context_size=context_size)  # type: ignore\n    self.context_size = context_size\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: TokenizedPrompts,\n    *,\n    context_size: int,  # noqa: ARG002\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\"\"\"\n    # Nothing to do here\n    return source_batch\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/","title":"Pre-Tokenized Dataset from Hugging Face","text":"<p>Pre-Tokenized Dataset from Hugging Face.</p> <p>PreTokenizedDataset should work with any of the following tokenized datasets: - NeelNanda/pile-small-tokenized-2b - NeelNanda/pile-tokenized-10b - NeelNanda/openwebtext-tokenized-9b - NeelNanda/c4-tokenized-2b - NeelNanda/code-tokenized - NeelNanda/c4-code-tokenized-2b - NeelNanda/pile-old-tokenized-2b - alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2</p>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset","title":"<code>PreTokenizedDataset</code>","text":"<p>             Bases: <code>SourceDataset[dict]</code></p> <p>General Pre-Tokenized Dataset from Hugging Face.</p> <p>Can be used for various datasets available on Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@final\nclass PreTokenizedDataset(SourceDataset[dict]):\n    \"\"\"General Pre-Tokenized Dataset from Hugging Face.\n\n    Can be used for various datasets available on Hugging Face.\n    \"\"\"\n\n    def preprocess(\n        self,\n        source_batch: dict,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        The method splits each pre-tokenized item based on the context size.\n\n        Args:\n            source_batch: A batch of source data.\n            context_size: The context size to use for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n\n        Raises:\n            ValueError: If the context size is larger than the tokenized prompt size.\n        \"\"\"\n        tokenized_prompts: list[list[int]] = source_batch[self._dataset_column_name]\n\n        # Check the context size is not too large\n        if context_size &gt; len(tokenized_prompts[0]):\n            error_message = (\n                f\"The context size ({context_size}) is larger than the \"\n                f\"tokenized prompt size ({len(tokenized_prompts[0])}).\"\n            )\n            raise ValueError(error_message)\n\n        # Chunk each tokenized prompt into blocks of context_size,\n        # discarding the last block if too small.\n        context_size_prompts = []\n        for encoding in tokenized_prompts:\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    @validate_call\n    def __init__(\n        self,\n        dataset_path: str,\n        context_size: PositiveInt = 256,\n        buffer_size: PositiveInt = 1000,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        dataset_column_name: str = \"input_ids\",\n        preprocess_batch_size: PositiveInt = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face (e.g.\n                `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n            context_size: The context size for tokenized prompts.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g. `train`).\n            dataset_column_name: The column name for the tokenized prompts.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            dataset_column_name=dataset_column_name,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.__init__","title":"<code>__init__(dataset_path, context_size=256, buffer_size=1000, dataset_dir=None, dataset_files=None, dataset_split='train', dataset_column_name='input_ids', preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a pre-tokenized dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face (e.g. `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</p> required <code>context_size</code> <code>PositiveInt</code> <p>The context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>PositiveInt</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> <code>dataset_column_name</code> <code>str</code> <p>The column name for the tokenized prompts.</p> <code>'input_ids'</code> <code>preprocess_batch_size</code> <code>PositiveInt</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@validate_call\ndef __init__(\n    self,\n    dataset_path: str,\n    context_size: PositiveInt = 256,\n    buffer_size: PositiveInt = 1000,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    dataset_column_name: str = \"input_ids\",\n    preprocess_batch_size: PositiveInt = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face (e.g.\n            `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).\n        context_size: The context size for tokenized prompts.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g. `train`).\n        dataset_column_name: The column name for the tokenized prompts.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        dataset_column_name=dataset_column_name,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>The method splits each pre-tokenized item based on the context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>dict</code> <p>A batch of source data.</p> required <code>context_size</code> <code>int</code> <p>The context size to use for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the context size is larger than the tokenized prompt size.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: dict,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    The method splits each pre-tokenized item based on the context size.\n\n    Args:\n        source_batch: A batch of source data.\n        context_size: The context size to use for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n\n    Raises:\n        ValueError: If the context size is larger than the tokenized prompt size.\n    \"\"\"\n    tokenized_prompts: list[list[int]] = source_batch[self._dataset_column_name]\n\n    # Check the context size is not too large\n    if context_size &gt; len(tokenized_prompts[0]):\n        error_message = (\n            f\"The context size ({context_size}) is larger than the \"\n            f\"tokenized prompt size ({len(tokenized_prompts[0])}).\"\n        )\n        raise ValueError(error_message)\n\n    # Chunk each tokenized prompt into blocks of context_size,\n    # discarding the last block if too small.\n    context_size_prompts = []\n    for encoding in tokenized_prompts:\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/source_data/text_dataset/","title":"Generic Text Dataset Module for Hugging Face Datasets","text":"<p>Generic Text Dataset Module for Hugging Face Datasets.</p> <p>GenericTextDataset should work with the following datasets: - monology/pile-uncopyrighted - the_pile_openwebtext2 - roneneldan/TinyStories</p>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch","title":"<code>GenericTextDataBatch</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Generic Text Dataset Batch.</p> <p>Assumes the dataset provides a 'text' field with a list of strings.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>class GenericTextDataBatch(TypedDict):\n    \"\"\"Generic Text Dataset Batch.\n\n    Assumes the dataset provides a 'text' field with a list of strings.\n    \"\"\"\n\n    text: list[str]\n    meta: list[dict[str, dict[str, str]]]  # Optional, depending on the dataset structure.\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset","title":"<code>TextDataset</code>","text":"<p>             Bases: <code>SourceDataset[GenericTextDataBatch]</code></p> <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@final\nclass TextDataset(SourceDataset[GenericTextDataBatch]):\n    \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n\n    def preprocess(\n        self,\n        source_batch: GenericTextDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n        Args:\n            source_batch: A batch of source data, including 'text' with a list of strings.\n            context_size: Context size for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        prompts: list[str] = source_batch[\"text\"]\n\n        tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n        # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n        context_size_prompts = []\n        for encoding in list(tokenized_prompts[self._dataset_column_name]):  # type: ignore\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    @validate_call(config={\"arbitrary_types_allowed\": True})\n    def __init__(\n        self,\n        dataset_path: str,\n        tokenizer: PreTrainedTokenizerBase,\n        buffer_size: PositiveInt = 1000,\n        context_size: PositiveInt = 256,\n        dataset_dir: str | None = None,\n        dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n        dataset_split: str = \"train\",\n        dataset_column_name: str = \"input_ids\",\n        n_processes_preprocessing: PositiveInt | None = None,\n        preprocess_batch_size: PositiveInt = 1000,\n        *,\n        pre_download: bool = False,\n    ):\n        \"\"\"Initialize a generic text dataset from Hugging Face.\n\n        Args:\n            dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n            tokenizer: Tokenizer to process text data.\n            buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n                streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n                shuffles just that buffer. Note that the generated activations should also be\n                shuffled before training the sparse autoencoder, so a large buffer may not be\n                strictly necessary here. Note also that this is the number of items in the dataset\n                (e.g. number of prompts) and is typically significantly less than the number of\n                tokenized prompts once the preprocessing function has been applied.\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            dataset_dir: Defining the `data_dir` of the dataset configuration.\n            dataset_files: Path(s) to source data file(s).\n            dataset_split: Dataset split (e.g., 'train').\n            dataset_column_name: The column name for the prompts.\n            n_processes_preprocessing: Number of processes to use for preprocessing.\n            preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n            pre_download: Whether to pre-download the whole dataset.\n        \"\"\"\n        self.tokenizer = tokenizer\n\n        super().__init__(\n            buffer_size=buffer_size,\n            context_size=context_size,\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            dataset_column_name=dataset_column_name,\n            n_processes_preprocessing=n_processes_preprocessing,\n            pre_download=pre_download,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n\n    @validate_call\n    def push_to_hugging_face_hub(\n        self,\n        repo_id: str,\n        commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n        max_shard_size: str | None = None,\n        n_shards: PositiveInt = 64,\n        revision: str = \"main\",\n        *,\n        private: bool = False,\n    ) -&gt; None:\n        \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n        Motivation:\n            Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n            pre-processed dataset with others. This function allows you to do that by pushing the\n            pre-processed dataset to the Hugging Face hub.\n\n        Warning:\n            You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n            to use this.\n\n        Warning:\n            This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n            initializing the dataset).\n\n        Args:\n            repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n            commit_message: Commit message.\n            max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `n_shards`\n                is set.\n            n_shards: Number of shards to split the dataset into. A high number is recommended\n                here to allow for flexible distributed training of SAEs across nodes (where e.g.\n                each node fetches its own shard).\n            revision: Branch to push to.\n            private: Whether to save the dataset privately.\n\n        Raises:\n            TypeError: If the dataset is streamed.\n        \"\"\"\n        if isinstance(self.dataset, IterableDataset):\n            error_message = (\n                \"Cannot share a streamed dataset to Hugging Face. \"\n                \"Please use `pre_download=True` when initializing the dataset.\"\n            )\n            raise TypeError(error_message)\n\n        self.dataset.push_to_hub(\n            repo_id=repo_id,\n            commit_message=commit_message,\n            max_shard_size=max_shard_size,\n            num_shards=n_shards,\n            private=private,\n            revision=revision,\n        )\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.__init__","title":"<code>__init__(dataset_path, tokenizer, buffer_size=1000, context_size=256, dataset_dir=None, dataset_files=None, dataset_split='train', dataset_column_name='input_ids', n_processes_preprocessing=None, preprocess_batch_size=1000, *, pre_download=False)</code>","text":"<p>Initialize a generic text dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the dataset on Hugging Face (e.g. <code>'monology/pile-uncopyright'</code>).</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer to process text data.</p> required <code>buffer_size</code> <code>PositiveInt</code> <p>The buffer size to use when shuffling the dataset when streaming. When streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>context_size</code> <code>PositiveInt</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>256</code> <code>dataset_dir</code> <code>str | None</code> <p>Defining the <code>data_dir</code> of the dataset configuration.</p> <code>None</code> <code>dataset_files</code> <code>str | Sequence[str] | Mapping[str, str | Sequence[str]] | None</code> <p>Path(s) to source data file(s).</p> <code>None</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., 'train').</p> <code>'train'</code> <code>dataset_column_name</code> <code>str</code> <p>The column name for the prompts.</p> <code>'input_ids'</code> <code>n_processes_preprocessing</code> <code>PositiveInt | None</code> <p>Number of processes to use for preprocessing.</p> <code>None</code> <code>preprocess_batch_size</code> <code>PositiveInt</code> <p>Batch size for preprocessing (tokenizing prompts).</p> <code>1000</code> <code>pre_download</code> <code>bool</code> <p>Whether to pre-download the whole dataset.</p> <code>False</code> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@validate_call(config={\"arbitrary_types_allowed\": True})\ndef __init__(\n    self,\n    dataset_path: str,\n    tokenizer: PreTrainedTokenizerBase,\n    buffer_size: PositiveInt = 1000,\n    context_size: PositiveInt = 256,\n    dataset_dir: str | None = None,\n    dataset_files: str | Sequence[str] | Mapping[str, str | Sequence[str]] | None = None,\n    dataset_split: str = \"train\",\n    dataset_column_name: str = \"input_ids\",\n    n_processes_preprocessing: PositiveInt | None = None,\n    preprocess_batch_size: PositiveInt = 1000,\n    *,\n    pre_download: bool = False,\n):\n    \"\"\"Initialize a generic text dataset from Hugging Face.\n\n    Args:\n        dataset_path: Path to the dataset on Hugging Face (e.g. `'monology/pile-uncopyright'`).\n        tokenizer: Tokenizer to process text data.\n        buffer_size: The buffer size to use when shuffling the dataset when streaming. When\n            streaming a dataset, this just pre-downloads at least `buffer_size` items and then\n            shuffles just that buffer. Note that the generated activations should also be\n            shuffled before training the sparse autoencoder, so a large buffer may not be\n            strictly necessary here. Note also that this is the number of items in the dataset\n            (e.g. number of prompts) and is typically significantly less than the number of\n            tokenized prompts once the preprocessing function has been applied.\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        dataset_dir: Defining the `data_dir` of the dataset configuration.\n        dataset_files: Path(s) to source data file(s).\n        dataset_split: Dataset split (e.g., 'train').\n        dataset_column_name: The column name for the prompts.\n        n_processes_preprocessing: Number of processes to use for preprocessing.\n        preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).\n        pre_download: Whether to pre-download the whole dataset.\n    \"\"\"\n    self.tokenizer = tokenizer\n\n    super().__init__(\n        buffer_size=buffer_size,\n        context_size=context_size,\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        dataset_column_name=dataset_column_name,\n        n_processes_preprocessing=n_processes_preprocessing,\n        pre_download=pre_download,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>GenericTextDataBatch</code> <p>A batch of source data, including 'text' with a list of strings.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: GenericTextDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n    Args:\n        source_batch: A batch of source data, including 'text' with a list of strings.\n        context_size: Context size for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    prompts: list[str] = source_batch[\"text\"]\n\n    tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n    # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n    context_size_prompts = []\n    for encoding in list(tokenized_prompts[self._dataset_column_name]):  # type: ignore\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.push_to_hugging_face_hub","title":"<code>push_to_hugging_face_hub(repo_id, commit_message='Upload preprocessed dataset using sparse_autoencoder.', max_shard_size=None, n_shards=64, revision='main', *, private=False)</code>","text":"<p>Share preprocessed dataset to Hugging Face hub.</p> Motivation <p>Pre-processing a dataset can be time-consuming, so it is useful to be able to share the pre-processed dataset with others. This function allows you to do that by pushing the pre-processed dataset to the Hugging Face hub.</p> Warning <p>You must be logged into HuggingFace (e.g with <code>huggingface-cli login</code> from the terminal) to use this.</p> Warning <p>This will only work if the dataset is not streamed (i.e. if <code>pre_download=True</code> when initializing the dataset).</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Hugging Face repo ID to save the dataset to (e.g. <code>username/dataset_name</code>).</p> required <code>commit_message</code> <code>str</code> <p>Commit message.</p> <code>'Upload preprocessed dataset using sparse_autoencoder.'</code> <code>max_shard_size</code> <code>str | None</code> <p>Maximum shard size (e.g. <code>'500MB'</code>). Should not be set if <code>n_shards</code> is set.</p> <code>None</code> <code>n_shards</code> <code>PositiveInt</code> <p>Number of shards to split the dataset into. A high number is recommended here to allow for flexible distributed training of SAEs across nodes (where e.g. each node fetches its own shard).</p> <code>64</code> <code>revision</code> <code>str</code> <p>Branch to push to.</p> <code>'main'</code> <code>private</code> <code>bool</code> <p>Whether to save the dataset privately.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset is streamed.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@validate_call\ndef push_to_hugging_face_hub(\n    self,\n    repo_id: str,\n    commit_message: str = \"Upload preprocessed dataset using sparse_autoencoder.\",\n    max_shard_size: str | None = None,\n    n_shards: PositiveInt = 64,\n    revision: str = \"main\",\n    *,\n    private: bool = False,\n) -&gt; None:\n    \"\"\"Share preprocessed dataset to Hugging Face hub.\n\n    Motivation:\n        Pre-processing a dataset can be time-consuming, so it is useful to be able to share the\n        pre-processed dataset with others. This function allows you to do that by pushing the\n        pre-processed dataset to the Hugging Face hub.\n\n    Warning:\n        You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)\n        to use this.\n\n    Warning:\n        This will only work if the dataset is not streamed (i.e. if `pre_download=True` when\n        initializing the dataset).\n\n    Args:\n        repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).\n        commit_message: Commit message.\n        max_shard_size: Maximum shard size (e.g. `'500MB'`). Should not be set if `n_shards`\n            is set.\n        n_shards: Number of shards to split the dataset into. A high number is recommended\n            here to allow for flexible distributed training of SAEs across nodes (where e.g.\n            each node fetches its own shard).\n        revision: Branch to push to.\n        private: Whether to save the dataset privately.\n\n    Raises:\n        TypeError: If the dataset is streamed.\n    \"\"\"\n    if isinstance(self.dataset, IterableDataset):\n        error_message = (\n            \"Cannot share a streamed dataset to Hugging Face. \"\n            \"Please use `pre_download=True` when initializing the dataset.\"\n        )\n        raise TypeError(error_message)\n\n    self.dataset.push_to_hub(\n        repo_id=repo_id,\n        commit_message=commit_message,\n        max_shard_size=max_shard_size,\n        num_shards=n_shards,\n        private=private,\n        revision=revision,\n    )\n</code></pre>"},{"location":"reference/source_model/","title":"Source Model","text":"<p>Source Model.</p>"},{"location":"reference/source_model/replace_activations_hook/","title":"Replace activations hook","text":"<p>Replace activations hook.</p>"},{"location":"reference/source_model/replace_activations_hook/#sparse_autoencoder.source_model.replace_activations_hook.replace_activations_hook","title":"<code>replace_activations_hook(value, hook, sparse_autoencoder, component_idx=None, n_components=None)</code>","text":"<p>Replace activations hook.</p> <p>This should be pre-initialised with <code>functools.partial</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The activations to replace.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required <code>sparse_autoencoder</code> <code>SparseAutoencoder | DataParallel[SparseAutoencoder] | LitSparseAutoencoder | Module</code> <p>The sparse autoencoder.</p> required <code>component_idx</code> <code>int | None</code> <p>The component index to replace the activations with, if just replacing activations for a single component. Requires the model to have a component axis.</p> <code>None</code> <code>n_components</code> <code>int | None</code> <p>The number of components that the SAE is trained on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Replaced activations.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>component_idx</code> is specified, but the model does not have a component</p> Source code in <code>sparse_autoencoder/source_model/replace_activations_hook.py</code> <pre><code>def replace_activations_hook(\n    value: Tensor,\n    hook: HookPoint,  # noqa: ARG001\n    sparse_autoencoder: SparseAutoencoder\n    | DataParallel[SparseAutoencoder]\n    | LitSparseAutoencoder\n    | Module,\n    component_idx: int | None = None,\n    n_components: int | None = None,\n) -&gt; Tensor:\n    \"\"\"Replace activations hook.\n\n    This should be pre-initialised with `functools.partial`.\n\n    Args:\n        value: The activations to replace.\n        hook: The hook point.\n        sparse_autoencoder: The sparse autoencoder.\n        component_idx: The component index to replace the activations with, if just replacing\n            activations for a single component. Requires the model to have a component axis.\n        n_components: The number of components that the SAE is trained on.\n\n    Returns:\n        Replaced activations.\n\n    Raises:\n        RuntimeError: If `component_idx` is specified, but the model does not have a component\n    \"\"\"\n    # Squash to just have a \"*items\" and a \"batch\" dimension\n    original_shape = value.shape\n\n    squashed_value: Float[Tensor, Axis.names(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] = value.view(\n        -1, value.size(-1)\n    )\n\n    if component_idx is not None:\n        if n_components is None:\n            error_message = \"The number of model components must be set if component_idx is set.\"\n            raise RuntimeError(error_message)\n\n        # The approach here is to run a forward pass with dummy values for all components other than\n        # the one we want to replace. This is done by expanding the inputs to the SAE for a specific\n        # component across all components. We then simply discard the activations for all other\n        # components.\n        expanded_shape = [\n            squashed_value.shape[0],\n            n_components,\n            squashed_value.shape[-1],\n        ]\n        expanded = squashed_value.unsqueeze(1).expand(*expanded_shape)\n\n        _learned_activations, output_activations = sparse_autoencoder.forward(expanded)\n        component_output_activations = output_activations[:, component_idx]\n\n        return component_output_activations.view(*original_shape)\n\n    # Get the output activations from a forward pass of the SAE\n    _learned_activations, output_activations = sparse_autoencoder.forward(squashed_value)\n\n    # Reshape to the original shape\n    return output_activations.view(*original_shape)\n</code></pre>"},{"location":"reference/source_model/reshape_activations/","title":"Methods to reshape activation tensors","text":"<p>Methods to reshape activation tensors.</p>"},{"location":"reference/source_model/reshape_activations/#sparse_autoencoder.source_model.reshape_activations.ReshapeActivationsFunction","title":"<code>ReshapeActivationsFunction: TypeAlias = Callable[[Float[Tensor, Axis.names(Axis.ANY)]], Float[Tensor, Axis.names(Axis.STORE_BATCH, Axis.INPUT_OUTPUT_FEATURE)]]</code>  <code>module-attribute</code>","text":"<p>Reshape Activations Function.</p> <p>Used within hooks to e.g. reshape activations before storing them in the activation store.</p>"},{"location":"reference/source_model/reshape_activations/#sparse_autoencoder.source_model.reshape_activations.reshape_concat_last_dimensions","title":"<code>reshape_concat_last_dimensions(batch_activations, concat_dims)</code>","text":"<p>Reshape to Last Dimension, Concatenating the Specified Dimensions.</p> <p>Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last <code>concat_dims</code> of which are the neuron dimensions), and returns a single tensor of size [item, neurons].</p> <p>Examples:</p> <p>With 3 axis (e.g. batch, pos, neuron), concatenating the last 2 dimensions:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; input = torch.randn(3, 4, 5)\n&gt;&gt;&gt; res = reshape_concat_last_dimensions(input, 2)\n&gt;&gt;&gt; res.shape\ntorch.Size([3, 20])\n</code></pre> <p>With 4 axis (e.g. batch, pos, head_idx, neuron), concatenating the last 3 dimensions:</p> <pre><code>&gt;&gt;&gt; input = torch.rand(2, 3, 4, 5)\n&gt;&gt;&gt; res = reshape_concat_last_dimensions(input, 3)\n&gt;&gt;&gt; res.shape\ntorch.Size([2, 60])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_activations</code> <code>Float[Tensor, names(ANY)]</code> <p>Input Activation Store Batch</p> required <code>concat_dims</code> <code>int</code> <p>Number of dimensions to concatenate</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(STORE_BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Single Tensor of Activation Store Items</p> Source code in <code>sparse_autoencoder/source_model/reshape_activations.py</code> <pre><code>def reshape_concat_last_dimensions(\n    batch_activations: Float[Tensor, Axis.names(Axis.ANY)],\n    concat_dims: int,\n) -&gt; Float[Tensor, Axis.names(Axis.STORE_BATCH, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Reshape to Last Dimension, Concatenating the Specified Dimensions.\n\n    Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last\n    `concat_dims` of which are the neuron dimensions), and returns a single tensor of size\n    [item, neurons].\n\n    Examples:\n        With 3 axis (e.g. batch, pos, neuron), concatenating the last 2 dimensions:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; input = torch.randn(3, 4, 5)\n        &gt;&gt;&gt; res = reshape_concat_last_dimensions(input, 2)\n        &gt;&gt;&gt; res.shape\n        torch.Size([3, 20])\n\n        With 4 axis (e.g. batch, pos, head_idx, neuron), concatenating the last 3 dimensions:\n\n        &gt;&gt;&gt; input = torch.rand(2, 3, 4, 5)\n        &gt;&gt;&gt; res = reshape_concat_last_dimensions(input, 3)\n        &gt;&gt;&gt; res.shape\n        torch.Size([2, 60])\n\n    Args:\n        batch_activations: Input Activation Store Batch\n        concat_dims: Number of dimensions to concatenate\n\n    Returns:\n        Single Tensor of Activation Store Items\n    \"\"\"\n    neurons = reduce(lambda x, y: x * y, batch_activations.shape[-concat_dims:])\n    items = reduce(lambda x, y: x * y, batch_activations.shape[:-concat_dims])\n\n    return batch_activations.reshape(items, neurons)\n</code></pre>"},{"location":"reference/source_model/reshape_activations/#sparse_autoencoder.source_model.reshape_activations.reshape_to_last_dimension","title":"<code>reshape_to_last_dimension(batch_activations)</code>","text":"<p>Reshape to Last Dimension.</p> <p>Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons].</p> <p>Examples:</p> <p>With 2 axis (e.g. pos neuron):</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; input = torch.rand(3, 100)\n&gt;&gt;&gt; res = reshape_to_last_dimension(input)\n&gt;&gt;&gt; res.shape\ntorch.Size([3, 100])\n</code></pre> <p>With 3 axis (e.g. batch, pos, neuron):</p> <pre><code>&gt;&gt;&gt; input = torch.randn(3, 3, 100)\n&gt;&gt;&gt; res = reshape_to_last_dimension(input)\n&gt;&gt;&gt; res.shape\ntorch.Size([9, 100])\n</code></pre> <p>With 4 axis (e.g. batch, pos, head_idx, neuron)</p> <pre><code>&gt;&gt;&gt; input = torch.rand(3, 3, 3, 100)\n&gt;&gt;&gt; res = reshape_to_last_dimension(input)\n&gt;&gt;&gt; res.shape\ntorch.Size([27, 100])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_activations</code> <code>Float[Tensor, names(ANY)]</code> <p>Input Activation Store Batch</p> required <p>Returns:</p> Type Description <code>Float[Tensor, names(STORE_BATCH, INPUT_OUTPUT_FEATURE)]</code> <p>Single Tensor of Activation Store Items</p> Source code in <code>sparse_autoencoder/source_model/reshape_activations.py</code> <pre><code>def reshape_to_last_dimension(\n    batch_activations: Float[Tensor, Axis.names(Axis.ANY)],\n) -&gt; Float[Tensor, Axis.names(Axis.STORE_BATCH, Axis.INPUT_OUTPUT_FEATURE)]:\n    \"\"\"Reshape to Last Dimension.\n\n    Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is\n    the neurons dimension), and returns a single tensor of size [item, neurons].\n\n    Examples:\n        With 2 axis (e.g. pos neuron):\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; input = torch.rand(3, 100)\n        &gt;&gt;&gt; res = reshape_to_last_dimension(input)\n        &gt;&gt;&gt; res.shape\n        torch.Size([3, 100])\n\n        With 3 axis (e.g. batch, pos, neuron):\n\n        &gt;&gt;&gt; input = torch.randn(3, 3, 100)\n        &gt;&gt;&gt; res = reshape_to_last_dimension(input)\n        &gt;&gt;&gt; res.shape\n        torch.Size([9, 100])\n\n        With 4 axis (e.g. batch, pos, head_idx, neuron)\n\n        &gt;&gt;&gt; input = torch.rand(3, 3, 3, 100)\n        &gt;&gt;&gt; res = reshape_to_last_dimension(input)\n        &gt;&gt;&gt; res.shape\n        torch.Size([27, 100])\n\n    Args:\n        batch_activations: Input Activation Store Batch\n\n    Returns:\n        Single Tensor of Activation Store Items\n    \"\"\"\n    return rearrange(batch_activations, \"... input_output_feature -&gt; (...) input_output_feature\")\n</code></pre>"},{"location":"reference/source_model/store_activations_hook/","title":"TransformerLens Hook for storing activations","text":"<p>TransformerLens Hook for storing activations.</p>"},{"location":"reference/source_model/store_activations_hook/#sparse_autoencoder.source_model.store_activations_hook.store_activations_hook","title":"<code>store_activations_hook(value, hook, store, reshape_method=reshape_to_last_dimension, component_idx=0)</code>","text":"<p>Store Activations Hook.</p> <p>Useful for getting just the specific activations wanted, rather than the full cache.</p> Example <p>First we'll need a source model from TransformerLens and an activation store.</p> <p>from functools import partial from transformer_lens import HookedTransformer from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=1000, n_neurons=64, n_components=1) model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer</p> <p>Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass.</p> <p>model.add_hook( ...     \"blocks.0.hook_mlp_out\", partial(store_activations_hook, store=store) ... ) tokens = model.to_tokens(\"Hello world\") tokens.shape torch.Size([1, 3])</p> <p>Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set <code>stop_at_layer=1</code> as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer).</p> <p>_output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required len(store) 3</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Float[Tensor, names(ANY)]</code> <p>The activations to store.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required <code>store</code> <code>ActivationStore</code> <p>The activation store. This should be pre-initialised with <code>functools.partial</code>.</p> required <code>reshape_method</code> <code>ReshapeActivationsFunction</code> <p>The method to reshape the activations before storing them.</p> <code>reshape_to_last_dimension</code> <code>component_idx</code> <code>int</code> <p>The component index of the activations to store.</p> <code>0</code> <p>Returns:</p> Type Description <code>Float[Tensor, names(ANY)]</code> <p>Unmodified activations.</p> Source code in <code>sparse_autoencoder/source_model/store_activations_hook.py</code> <pre><code>def store_activations_hook(\n    value: Float[Tensor, Axis.names(Axis.ANY)],\n    hook: HookPoint,  # noqa: ARG001\n    store: ActivationStore,\n    reshape_method: ReshapeActivationsFunction = reshape_to_last_dimension,\n    component_idx: int = 0,\n) -&gt; Float[Tensor, Axis.names(Axis.ANY)]:\n    \"\"\"Store Activations Hook.\n\n    Useful for getting just the specific activations wanted, rather than the full cache.\n\n    Example:\n        First we'll need a source model from TransformerLens and an activation store.\n\n        &gt;&gt;&gt; from functools import partial\n        &gt;&gt;&gt; from transformer_lens import HookedTransformer\n        &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=64, n_components=1)\n        &gt;&gt;&gt; model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n        Loaded pretrained model tiny-stories-1M into HookedTransformer\n\n        Next we can add the hook to specific neurons (in this case the first MLP neurons), and\n        create the tokens for a forward pass.\n\n        &gt;&gt;&gt; model.add_hook(\n        ...     \"blocks.0.hook_mlp_out\", partial(store_activations_hook, store=store)\n        ... )\n        &gt;&gt;&gt; tokens = model.to_tokens(\"Hello world\")\n        &gt;&gt;&gt; tokens.shape\n        torch.Size([1, 3])\n\n        Then when we run the model, we should get one activation vector for each token (as we just\n        have one batch item). Note we also set `stop_at_layer=1` as we don't need the logits or any\n        other activations after the hook point that we've specified (in this case the first MLP\n        layer).\n\n        &gt;&gt;&gt; _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required\n        &gt;&gt;&gt; len(store)\n        3\n\n    Args:\n        value: The activations to store.\n        hook: The hook point.\n        store: The activation store. This should be pre-initialised with `functools.partial`.\n        reshape_method: The method to reshape the activations before storing them.\n        component_idx: The component index of the activations to store.\n\n    Returns:\n        Unmodified activations.\n    \"\"\"\n    reshaped: Float[\n        Tensor, Axis.names(Axis.STORE_BATCH, Axis.INPUT_OUTPUT_FEATURE)\n    ] = reshape_method(value)\n\n    store.extend(reshaped, component_idx=component_idx)\n\n    # Return the unmodified value\n    return value\n</code></pre>"},{"location":"reference/source_model/zero_ablate_hook/","title":"Zero ablate hook","text":"<p>Zero ablate hook.</p>"},{"location":"reference/source_model/zero_ablate_hook/#sparse_autoencoder.source_model.zero_ablate_hook.zero_ablate_hook","title":"<code>zero_ablate_hook(value, hook)</code>","text":"<p>Zero ablate hook.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The activations to store.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required Example <p>dummy_hook_point = HookPoint() value = torch.ones(2, 3) zero_ablate_hook(value, dummy_hook_point) tensor([[0., 0., 0.],         [0., 0., 0.]])</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Replaced activations.</p> Source code in <code>sparse_autoencoder/source_model/zero_ablate_hook.py</code> <pre><code>def zero_ablate_hook(\n    value: Tensor,\n    hook: HookPoint,  # noqa: ARG001\n) -&gt; Tensor:\n    \"\"\"Zero ablate hook.\n\n    Args:\n        value: The activations to store.\n        hook: The hook point.\n\n    Example:\n        &gt;&gt;&gt; dummy_hook_point = HookPoint()\n        &gt;&gt;&gt; value = torch.ones(2, 3)\n        &gt;&gt;&gt; zero_ablate_hook(value, dummy_hook_point)\n        tensor([[0., 0., 0.],\n                [0., 0., 0.]])\n\n    Returns:\n        Replaced activations.\n    \"\"\"\n    return torch.zeros_like(value)\n</code></pre>"},{"location":"reference/train/","title":"Train","text":"<p>Train.</p>"},{"location":"reference/train/join_sweep/","title":"Join an existing Weights and Biases sweep, as a new agent","text":"<p>Join an existing Weights and Biases sweep, as a new agent.</p>"},{"location":"reference/train/join_sweep/#sparse_autoencoder.train.join_sweep.parse_arguments","title":"<code>parse_arguments()</code>","text":"<p>Parse command line arguments.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: Parsed command line arguments.</p> Source code in <code>sparse_autoencoder/train/join_sweep.py</code> <pre><code>def parse_arguments() -&gt; argparse.Namespace:\n    \"\"\"Parse command line arguments.\n\n    Returns:\n        argparse.Namespace: Parsed command line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Join an existing W&amp;B sweep.\")\n    parser.add_argument(\n        \"--id\", type=str, default=None, help=\"Sweep ID for the existing sweep.\", required=True\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/train/join_sweep/#sparse_autoencoder.train.join_sweep.run","title":"<code>run()</code>","text":"<p>Run the join_sweep script.</p> Source code in <code>sparse_autoencoder/train/join_sweep.py</code> <pre><code>def run() -&gt; None:\n    \"\"\"Run the join_sweep script.\"\"\"\n    args = parse_arguments()\n\n    sweep(sweep_id=args.id)\n</code></pre>"},{"location":"reference/train/pipeline/","title":"Default pipeline","text":"<p>Default pipeline.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p> <p>Includes all the key functionality to train a sparse autoencoder, with a specific set of     hyperparameters.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations.\n\n    Includes all the key functionality to train a sparse autoencoder, with a specific set of\n        hyperparameters.\n    \"\"\"\n\n    autoencoder: LitSparseAutoencoder\n    \"\"\"Sparse autoencoder to train.\"\"\"\n\n    n_input_features: int\n    \"\"\"Number of input features in the sparse autoencoder.\"\"\"\n\n    n_learned_features: int\n    \"\"\"Number of learned features in the sparse autoencoder.\"\"\"\n\n    cache_names: list[str]\n    \"\"\"Names of the cache hook points to use in the source model.\"\"\"\n\n    layer: int\n    \"\"\"Layer to stope the source model at (if we don't need activations after this layer).\"\"\"\n\n    log_frequency: int\n    \"\"\"Frequency at which to log metrics (in steps).\"\"\"\n\n    progress_bar: tqdm | None\n    \"\"\"Progress bar for the pipeline.\"\"\"\n\n    source_data: Iterator[TorchTokenizedPrompts]\n    \"\"\"Iterable over the source data.\"\"\"\n\n    source_dataset: SourceDataset\n    \"\"\"Source dataset to generate activation data from (tokenized prompts).\"\"\"\n\n    source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]\n    \"\"\"Source model to get activations from.\"\"\"\n\n    total_activations_trained_on: int = 0\n    \"\"\"Total number of activations trained on state.\"\"\"\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of source model components the SAE is trained on.\"\"\"\n        return len(self.cache_names)\n\n    @final\n    @validate_call(config={\"arbitrary_types_allowed\": True})\n    def __init__(\n        self,\n        autoencoder: LitSparseAutoencoder,\n        cache_names: list[str],\n        layer: NonNegativeInt,\n        source_dataset: SourceDataset,\n        source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer],\n        n_input_features: int,\n        n_learned_features: int,\n        run_name: str = \"sparse_autoencoder\",\n        checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n        log_frequency: PositiveInt = 100,\n        num_workers_data_loading: NonNegativeInt = 0,\n        source_data_batch_size: PositiveInt = 12,\n    ) -&gt; None:\n        \"\"\"Initialize the pipeline.\n\n        Args:\n            autoencoder: Sparse autoencoder to train.\n            cache_names: Names of the cache hook points to use in the source model.\n            layer: Layer to stope the source model at (if we don't need activations after this\n                layer).\n            source_dataset: Source dataset to get data from.\n            source_model: Source model to get activations from.\n            n_input_features: Number of input features in the sparse autoencoder.\n            n_learned_features: Number of learned features in the sparse autoencoder.\n            run_name: Name of the run for saving checkpoints.\n            checkpoint_directory: Directory to save checkpoints to.\n            log_frequency: Frequency at which to log metrics (in steps)\n            num_workers_data_loading: Number of CPU workers for the dataloader.\n            source_data_batch_size: Batch size for the source data.\n        \"\"\"\n        self.autoencoder = autoencoder\n        self.cache_names = cache_names\n        self.checkpoint_directory = checkpoint_directory\n        self.layer = layer\n        self.log_frequency = log_frequency\n        self.run_name = run_name\n        self.source_data_batch_size = source_data_batch_size\n        self.source_dataset = source_dataset\n        self.source_model = source_model\n        self.n_input_features = n_input_features\n        self.n_learned_features = n_learned_features\n\n        # Add validate metric\n        self.reconstruction_score = ClasswiseWrapperWithMean(\n            ReconstructionScoreMetric(len(cache_names)),\n            component_names=cache_names,\n            prefix=\"validation/reconstruction_score\",\n        )\n        self.reconstruction_score.to(get_model_device(self.autoencoder))\n\n        # Create a stateful iterator\n        source_dataloader = source_dataset.get_dataloader(\n            source_data_batch_size, num_workers=num_workers_data_loading\n        )\n        self.source_data = iter(source_dataloader)\n\n    @validate_call\n    def generate_activations(self, store_size: PositiveInt) -&gt; TensorActivationStore:\n        \"\"\"Generate activations.\n\n        Args:\n            store_size: Number of activations to generate.\n\n        Returns:\n            Activation store for the train section.\n\n        Raises:\n            ValueError: If the store size is not divisible by the batch size.\n        \"\"\"\n        # Check the store size is divisible by the batch size\n        if store_size % (self.source_data_batch_size * self.source_dataset.context_size) != 0:\n            error_message = (\n                f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n                f\"got {store_size}\"\n            )\n            raise ValueError(error_message)\n\n        # Setup the store\n        source_model_device = get_model_device(self.source_model)\n        store = TensorActivationStore(\n            store_size, self.n_input_features, n_components=self.n_components\n        )\n\n        # Add the hook to the model (will automatically store the activations every time the model\n        # runs)\n        self.source_model.remove_all_hook_fns()\n        for component_idx, cache_name in enumerate(self.cache_names):\n            hook = partial(store_activations_hook, store=store, component_idx=component_idx)\n            self.source_model.add_hook(cache_name, hook)\n\n        # Loop through the dataloader until the store reaches the desired size\n        with torch.no_grad():\n            while len(store) &lt; store_size:\n                batch = next(self.source_data)\n                input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                    \"input_ids\"\n                ].to(source_model_device)\n                self.source_model.forward(\n                    input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n                )  # type: ignore (TLens is typed incorrectly)\n\n        self.source_model.remove_all_hook_fns()\n        store.shuffle()\n\n        return store\n\n    def train_autoencoder(\n        self,\n        activation_store: TensorActivationStore,\n        train_batch_size: PositiveInt,\n    ) -&gt; None:\n        \"\"\"Train the sparse autoencoder.\n\n        Args:\n            activation_store: Activation store from the generate section.\n            train_batch_size: Train batch size.\n\n        Returns:\n            Number of times each neuron fired, for each component.\n        \"\"\"\n        activations_dataloader = DataLoader(\n            activation_store, batch_size=train_batch_size, num_workers=4, persistent_workers=False\n        )\n\n        # Setup the trainer with no console logging\n        logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n        trainer = Trainer(\n            logger=WandbLogger() if wandb.run is not None else None,\n            max_epochs=1,\n            enable_progress_bar=False,\n            enable_model_summary=False,\n            enable_checkpointing=False,\n            precision=\"16-mixed\",\n        )\n        trainer.fit(self.autoencoder, activations_dataloader)\n\n    @validate_call\n    def validate_sae(self, validation_n_activations: PositiveInt) -&gt; None:\n        \"\"\"Get validation metrics.\n\n        Args:\n            validation_n_activations: Number of activations to use for validation.\n        \"\"\"\n        n_batches = validation_n_activations // (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n        source_model_device = get_model_device(self.source_model)\n\n        # Create the metric data stores\n        losses: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n            self.n_components, device=source_model_device\n        )\n        losses_with_reconstruction: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n            self.n_components, device=source_model_device\n        )\n        losses_with_zero_ablation: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n            self.n_components, device=source_model_device\n        )\n\n        sae_model = self.autoencoder.sparse_autoencoder.clone()\n        sae_model.to(source_model_device)\n\n        for component_idx, cache_name in enumerate(self.cache_names):\n            for _batch_idx in range(n_batches):\n                batch = next(self.source_data)\n\n                input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                    \"input_ids\"\n                ].to(source_model_device)\n\n                # Run a forward pass with and without the replaced activations\n                self.source_model.remove_all_hook_fns()\n                replacement_hook = partial(\n                    replace_activations_hook,\n                    sparse_autoencoder=sae_model,\n                    component_idx=component_idx,\n                    n_components=self.n_components,\n                )\n\n                with torch.no_grad():\n                    loss: Float[\n                        Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)\n                    ] = self.source_model.forward(input_ids, return_type=\"loss\")\n                    loss_with_reconstruction = self.source_model.run_with_hooks(\n                        input_ids,\n                        return_type=\"loss\",\n                        fwd_hooks=[\n                            (\n                                cache_name,\n                                replacement_hook,\n                            )\n                        ],\n                    )\n                    loss_with_zero_ablation = self.source_model.run_with_hooks(\n                        input_ids, return_type=\"loss\", fwd_hooks=[(cache_name, zero_ablate_hook)]\n                    )\n\n                    self.reconstruction_score.update(\n                        source_model_loss=loss,\n                        source_model_loss_with_reconstruction=loss_with_reconstruction,\n                        source_model_loss_with_zero_ablation=loss_with_zero_ablation,\n                        component_idx=component_idx,\n                    )\n\n                    losses[component_idx] += loss.sum()\n                    losses_with_reconstruction[component_idx] += loss_with_reconstruction.sum()\n                    losses_with_zero_ablation[component_idx] += loss_with_zero_ablation.sum()\n\n        # Log\n        if wandb.run is not None:\n            log = {\n                f\"validation/source_model_losses/{c}\": val\n                for c, val in zip(self.cache_names, losses / n_batches)\n            }\n            log.update(\n                {\n                    f\"validation/source_model_losses_with_reconstruction/{c}\": val\n                    for c, val in zip(self.cache_names, losses_with_reconstruction / n_batches)\n                }\n            )\n            log.update(\n                {\n                    f\"validation/source_model_losses_with_zero_ablation/{c}\": val\n                    for c, val in zip(self.cache_names, losses_with_zero_ablation / n_batches)\n                }\n            )\n            log.update(self.reconstruction_score.compute())\n            wandb.log(log)\n\n    @final\n    def save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n        \"\"\"Save the model as a checkpoint.\n\n        Args:\n            is_final: Whether this is the final checkpoint.\n\n        Returns:\n            Path to the saved checkpoint.\n        \"\"\"\n        name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n\n        # Wandb\n        if wandb.run is not None:\n            self.autoencoder.sparse_autoencoder.save_to_wandb(name)\n\n        # Local\n        local_path = self.checkpoint_directory / f\"{name}.pt\"\n        self.autoencoder.sparse_autoencoder.save(local_path)\n        return local_path\n\n    @validate_call\n    def run_pipeline(\n        self,\n        train_batch_size: PositiveInt,\n        max_store_size: PositiveInt,\n        max_activations: PositiveInt,\n        validation_n_activations: PositiveInt = 1024,\n        validate_frequency: PositiveInt | None = None,\n        checkpoint_frequency: PositiveInt | None = None,\n    ) -&gt; None:\n        \"\"\"Run the full training pipeline.\n\n        Args:\n            train_batch_size: Train batch size.\n            max_store_size: Maximum size of the activation store.\n            max_activations: Maximum total number of activations to train on (the original paper\n                used 8bn, although others have had success with 100m+).\n            validation_n_activations: Number of activations to use for validation.\n            validate_frequency: Frequency at which to get validation metrics.\n            checkpoint_frequency: Frequency at which to save a checkpoint.\n        \"\"\"\n        last_validated: int = 0\n        last_checkpoint: int = 0\n\n        self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n        # Get the store size\n        store_size: int = max_store_size - max_store_size % (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n\n        # Get the loss fn\n        loss_fn = self.autoencoder.loss_fn.clone()\n        loss_fn.keep_batch_dim = True\n\n        with tqdm(\n            desc=\"Activations trained on\",\n            total=max_activations,\n        ) as progress_bar:\n            for _ in range(0, max_activations, store_size):\n                # Generate\n                progress_bar.set_postfix({\"stage\": \"generate\"})\n                activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n                # Update the counters\n                n_activation_vectors_in_store = len(activation_store)\n                last_validated += n_activation_vectors_in_store\n                last_checkpoint += n_activation_vectors_in_store\n\n                # Train &amp; resample if needed\n                progress_bar.set_postfix({\"stage\": \"train\"})\n                self.train_autoencoder(activation_store, train_batch_size=train_batch_size)\n\n                # Get validation metrics (if needed)\n                progress_bar.set_postfix({\"stage\": \"validate\"})\n                if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                    self.validate_sae(validation_n_activations)\n                    last_validated = 0\n\n                # Checkpoint (if needed)\n                progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n                if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                    last_checkpoint = 0\n                    self.save_checkpoint()\n\n                # Update the progress bar\n                progress_bar.update(store_size)\n\n        # Save the final checkpoint\n        self.save_checkpoint(is_final=True)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.autoencoder","title":"<code>autoencoder: LitSparseAutoencoder = autoencoder</code>  <code>instance-attribute</code>","text":"<p>Sparse autoencoder to train.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.cache_names","title":"<code>cache_names: list[str] = cache_names</code>  <code>instance-attribute</code>","text":"<p>Names of the cache hook points to use in the source model.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.layer","title":"<code>layer: int = layer</code>  <code>instance-attribute</code>","text":"<p>Layer to stope the source model at (if we don't need activations after this layer).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.log_frequency","title":"<code>log_frequency: int = log_frequency</code>  <code>instance-attribute</code>","text":"<p>Frequency at which to log metrics (in steps).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of source model components the SAE is trained on.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.n_input_features","title":"<code>n_input_features: int = n_input_features</code>  <code>instance-attribute</code>","text":"<p>Number of input features in the sparse autoencoder.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.n_learned_features","title":"<code>n_learned_features: int = n_learned_features</code>  <code>instance-attribute</code>","text":"<p>Number of learned features in the sparse autoencoder.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.progress_bar","title":"<code>progress_bar: tqdm | None</code>  <code>instance-attribute</code>","text":"<p>Progress bar for the pipeline.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_data","title":"<code>source_data: Iterator[TorchTokenizedPrompts] = iter(source_dataloader)</code>  <code>instance-attribute</code>","text":"<p>Iterable over the source data.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_dataset","title":"<code>source_dataset: SourceDataset = source_dataset</code>  <code>instance-attribute</code>","text":"<p>Source dataset to generate activation data from (tokenized prompts).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_model","title":"<code>source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer] = source_model</code>  <code>instance-attribute</code>","text":"<p>Source model to get activations from.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.total_activations_trained_on","title":"<code>total_activations_trained_on: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of activations trained on state.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.__init__","title":"<code>__init__(autoencoder, cache_names, layer, source_dataset, source_model, n_input_features, n_learned_features, run_name='sparse_autoencoder', checkpoint_directory=DEFAULT_CHECKPOINT_DIRECTORY, log_frequency=100, num_workers_data_loading=0, source_data_batch_size=12)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>autoencoder</code> <code>LitSparseAutoencoder</code> <p>Sparse autoencoder to train.</p> required <code>cache_names</code> <code>list[str]</code> <p>Names of the cache hook points to use in the source model.</p> required <code>layer</code> <code>NonNegativeInt</code> <p>Layer to stope the source model at (if we don't need activations after this layer).</p> required <code>source_dataset</code> <code>SourceDataset</code> <p>Source dataset to get data from.</p> required <code>source_model</code> <code>HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]</code> <p>Source model to get activations from.</p> required <code>n_input_features</code> <code>int</code> <p>Number of input features in the sparse autoencoder.</p> required <code>n_learned_features</code> <code>int</code> <p>Number of learned features in the sparse autoencoder.</p> required <code>run_name</code> <code>str</code> <p>Name of the run for saving checkpoints.</p> <code>'sparse_autoencoder'</code> <code>checkpoint_directory</code> <code>Path</code> <p>Directory to save checkpoints to.</p> <code>DEFAULT_CHECKPOINT_DIRECTORY</code> <code>log_frequency</code> <code>PositiveInt</code> <p>Frequency at which to log metrics (in steps)</p> <code>100</code> <code>num_workers_data_loading</code> <code>NonNegativeInt</code> <p>Number of CPU workers for the dataloader.</p> <code>0</code> <code>source_data_batch_size</code> <code>PositiveInt</code> <p>Batch size for the source data.</p> <code>12</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\n@validate_call(config={\"arbitrary_types_allowed\": True})\ndef __init__(\n    self,\n    autoencoder: LitSparseAutoencoder,\n    cache_names: list[str],\n    layer: NonNegativeInt,\n    source_dataset: SourceDataset,\n    source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer],\n    n_input_features: int,\n    n_learned_features: int,\n    run_name: str = \"sparse_autoencoder\",\n    checkpoint_directory: Path = DEFAULT_CHECKPOINT_DIRECTORY,\n    log_frequency: PositiveInt = 100,\n    num_workers_data_loading: NonNegativeInt = 0,\n    source_data_batch_size: PositiveInt = 12,\n) -&gt; None:\n    \"\"\"Initialize the pipeline.\n\n    Args:\n        autoencoder: Sparse autoencoder to train.\n        cache_names: Names of the cache hook points to use in the source model.\n        layer: Layer to stope the source model at (if we don't need activations after this\n            layer).\n        source_dataset: Source dataset to get data from.\n        source_model: Source model to get activations from.\n        n_input_features: Number of input features in the sparse autoencoder.\n        n_learned_features: Number of learned features in the sparse autoencoder.\n        run_name: Name of the run for saving checkpoints.\n        checkpoint_directory: Directory to save checkpoints to.\n        log_frequency: Frequency at which to log metrics (in steps)\n        num_workers_data_loading: Number of CPU workers for the dataloader.\n        source_data_batch_size: Batch size for the source data.\n    \"\"\"\n    self.autoencoder = autoencoder\n    self.cache_names = cache_names\n    self.checkpoint_directory = checkpoint_directory\n    self.layer = layer\n    self.log_frequency = log_frequency\n    self.run_name = run_name\n    self.source_data_batch_size = source_data_batch_size\n    self.source_dataset = source_dataset\n    self.source_model = source_model\n    self.n_input_features = n_input_features\n    self.n_learned_features = n_learned_features\n\n    # Add validate metric\n    self.reconstruction_score = ClasswiseWrapperWithMean(\n        ReconstructionScoreMetric(len(cache_names)),\n        component_names=cache_names,\n        prefix=\"validation/reconstruction_score\",\n    )\n    self.reconstruction_score.to(get_model_device(self.autoencoder))\n\n    # Create a stateful iterator\n    source_dataloader = source_dataset.get_dataloader(\n        source_data_batch_size, num_workers=num_workers_data_loading\n    )\n    self.source_data = iter(source_dataloader)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.generate_activations","title":"<code>generate_activations(store_size)</code>","text":"<p>Generate activations.</p> <p>Parameters:</p> Name Type Description Default <code>store_size</code> <code>PositiveInt</code> <p>Number of activations to generate.</p> required <p>Returns:</p> Type Description <code>TensorActivationStore</code> <p>Activation store for the train section.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store size is not divisible by the batch size.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@validate_call\ndef generate_activations(self, store_size: PositiveInt) -&gt; TensorActivationStore:\n    \"\"\"Generate activations.\n\n    Args:\n        store_size: Number of activations to generate.\n\n    Returns:\n        Activation store for the train section.\n\n    Raises:\n        ValueError: If the store size is not divisible by the batch size.\n    \"\"\"\n    # Check the store size is divisible by the batch size\n    if store_size % (self.source_data_batch_size * self.source_dataset.context_size) != 0:\n        error_message = (\n            f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n            f\"got {store_size}\"\n        )\n        raise ValueError(error_message)\n\n    # Setup the store\n    source_model_device = get_model_device(self.source_model)\n    store = TensorActivationStore(\n        store_size, self.n_input_features, n_components=self.n_components\n    )\n\n    # Add the hook to the model (will automatically store the activations every time the model\n    # runs)\n    self.source_model.remove_all_hook_fns()\n    for component_idx, cache_name in enumerate(self.cache_names):\n        hook = partial(store_activations_hook, store=store, component_idx=component_idx)\n        self.source_model.add_hook(cache_name, hook)\n\n    # Loop through the dataloader until the store reaches the desired size\n    with torch.no_grad():\n        while len(store) &lt; store_size:\n            batch = next(self.source_data)\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n            self.source_model.forward(\n                input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n            )  # type: ignore (TLens is typed incorrectly)\n\n    self.source_model.remove_all_hook_fns()\n    store.shuffle()\n\n    return store\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.run_pipeline","title":"<code>run_pipeline(train_batch_size, max_store_size, max_activations, validation_n_activations=1024, validate_frequency=None, checkpoint_frequency=None)</code>","text":"<p>Run the full training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>PositiveInt</code> <p>Train batch size.</p> required <code>max_store_size</code> <code>PositiveInt</code> <p>Maximum size of the activation store.</p> required <code>max_activations</code> <code>PositiveInt</code> <p>Maximum total number of activations to train on (the original paper used 8bn, although others have had success with 100m+).</p> required <code>validation_n_activations</code> <code>PositiveInt</code> <p>Number of activations to use for validation.</p> <code>1024</code> <code>validate_frequency</code> <code>PositiveInt | None</code> <p>Frequency at which to get validation metrics.</p> <code>None</code> <code>checkpoint_frequency</code> <code>PositiveInt | None</code> <p>Frequency at which to save a checkpoint.</p> <code>None</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@validate_call\ndef run_pipeline(\n    self,\n    train_batch_size: PositiveInt,\n    max_store_size: PositiveInt,\n    max_activations: PositiveInt,\n    validation_n_activations: PositiveInt = 1024,\n    validate_frequency: PositiveInt | None = None,\n    checkpoint_frequency: PositiveInt | None = None,\n) -&gt; None:\n    \"\"\"Run the full training pipeline.\n\n    Args:\n        train_batch_size: Train batch size.\n        max_store_size: Maximum size of the activation store.\n        max_activations: Maximum total number of activations to train on (the original paper\n            used 8bn, although others have had success with 100m+).\n        validation_n_activations: Number of activations to use for validation.\n        validate_frequency: Frequency at which to get validation metrics.\n        checkpoint_frequency: Frequency at which to save a checkpoint.\n    \"\"\"\n    last_validated: int = 0\n    last_checkpoint: int = 0\n\n    self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n    # Get the store size\n    store_size: int = max_store_size - max_store_size % (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n\n    # Get the loss fn\n    loss_fn = self.autoencoder.loss_fn.clone()\n    loss_fn.keep_batch_dim = True\n\n    with tqdm(\n        desc=\"Activations trained on\",\n        total=max_activations,\n    ) as progress_bar:\n        for _ in range(0, max_activations, store_size):\n            # Generate\n            progress_bar.set_postfix({\"stage\": \"generate\"})\n            activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n            # Update the counters\n            n_activation_vectors_in_store = len(activation_store)\n            last_validated += n_activation_vectors_in_store\n            last_checkpoint += n_activation_vectors_in_store\n\n            # Train &amp; resample if needed\n            progress_bar.set_postfix({\"stage\": \"train\"})\n            self.train_autoencoder(activation_store, train_batch_size=train_batch_size)\n\n            # Get validation metrics (if needed)\n            progress_bar.set_postfix({\"stage\": \"validate\"})\n            if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                self.validate_sae(validation_n_activations)\n                last_validated = 0\n\n            # Checkpoint (if needed)\n            progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n            if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                last_checkpoint = 0\n                self.save_checkpoint()\n\n            # Update the progress bar\n            progress_bar.update(store_size)\n\n    # Save the final checkpoint\n    self.save_checkpoint(is_final=True)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.save_checkpoint","title":"<code>save_checkpoint(*, is_final=False)</code>","text":"<p>Save the model as a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>is_final</code> <code>bool</code> <p>Whether this is the final checkpoint.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved checkpoint.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef save_checkpoint(self, *, is_final: bool = False) -&gt; Path:\n    \"\"\"Save the model as a checkpoint.\n\n    Args:\n        is_final: Whether this is the final checkpoint.\n\n    Returns:\n        Path to the saved checkpoint.\n    \"\"\"\n    name: str = f\"{self.run_name}_{'final' if is_final else self.total_activations_trained_on}\"\n\n    # Wandb\n    if wandb.run is not None:\n        self.autoencoder.sparse_autoencoder.save_to_wandb(name)\n\n    # Local\n    local_path = self.checkpoint_directory / f\"{name}.pt\"\n    self.autoencoder.sparse_autoencoder.save(local_path)\n    return local_path\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.train_autoencoder","title":"<code>train_autoencoder(activation_store, train_batch_size)</code>","text":"<p>Train the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store from the generate section.</p> required <code>train_batch_size</code> <code>PositiveInt</code> <p>Train batch size.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Number of times each neuron fired, for each component.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def train_autoencoder(\n    self,\n    activation_store: TensorActivationStore,\n    train_batch_size: PositiveInt,\n) -&gt; None:\n    \"\"\"Train the sparse autoencoder.\n\n    Args:\n        activation_store: Activation store from the generate section.\n        train_batch_size: Train batch size.\n\n    Returns:\n        Number of times each neuron fired, for each component.\n    \"\"\"\n    activations_dataloader = DataLoader(\n        activation_store, batch_size=train_batch_size, num_workers=4, persistent_workers=False\n    )\n\n    # Setup the trainer with no console logging\n    logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n    trainer = Trainer(\n        logger=WandbLogger() if wandb.run is not None else None,\n        max_epochs=1,\n        enable_progress_bar=False,\n        enable_model_summary=False,\n        enable_checkpointing=False,\n        precision=\"16-mixed\",\n    )\n    trainer.fit(self.autoencoder, activations_dataloader)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.validate_sae","title":"<code>validate_sae(validation_n_activations)</code>","text":"<p>Get validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>validation_n_activations</code> <code>PositiveInt</code> <p>Number of activations to use for validation.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@validate_call\ndef validate_sae(self, validation_n_activations: PositiveInt) -&gt; None:\n    \"\"\"Get validation metrics.\n\n    Args:\n        validation_n_activations: Number of activations to use for validation.\n    \"\"\"\n    n_batches = validation_n_activations // (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n    source_model_device = get_model_device(self.source_model)\n\n    # Create the metric data stores\n    losses: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n        self.n_components, device=source_model_device\n    )\n    losses_with_reconstruction: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n        self.n_components, device=source_model_device\n    )\n    losses_with_zero_ablation: Float[Tensor, Axis.COMPONENT] = torch.zeros(\n        self.n_components, device=source_model_device\n    )\n\n    sae_model = self.autoencoder.sparse_autoencoder.clone()\n    sae_model.to(source_model_device)\n\n    for component_idx, cache_name in enumerate(self.cache_names):\n        for _batch_idx in range(n_batches):\n            batch = next(self.source_data)\n\n            input_ids: Int[Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)] = batch[\n                \"input_ids\"\n            ].to(source_model_device)\n\n            # Run a forward pass with and without the replaced activations\n            self.source_model.remove_all_hook_fns()\n            replacement_hook = partial(\n                replace_activations_hook,\n                sparse_autoencoder=sae_model,\n                component_idx=component_idx,\n                n_components=self.n_components,\n            )\n\n            with torch.no_grad():\n                loss: Float[\n                    Tensor, Axis.names(Axis.SOURCE_DATA_BATCH, Axis.POSITION)\n                ] = self.source_model.forward(input_ids, return_type=\"loss\")\n                loss_with_reconstruction = self.source_model.run_with_hooks(\n                    input_ids,\n                    return_type=\"loss\",\n                    fwd_hooks=[\n                        (\n                            cache_name,\n                            replacement_hook,\n                        )\n                    ],\n                )\n                loss_with_zero_ablation = self.source_model.run_with_hooks(\n                    input_ids, return_type=\"loss\", fwd_hooks=[(cache_name, zero_ablate_hook)]\n                )\n\n                self.reconstruction_score.update(\n                    source_model_loss=loss,\n                    source_model_loss_with_reconstruction=loss_with_reconstruction,\n                    source_model_loss_with_zero_ablation=loss_with_zero_ablation,\n                    component_idx=component_idx,\n                )\n\n                losses[component_idx] += loss.sum()\n                losses_with_reconstruction[component_idx] += loss_with_reconstruction.sum()\n                losses_with_zero_ablation[component_idx] += loss_with_zero_ablation.sum()\n\n    # Log\n    if wandb.run is not None:\n        log = {\n            f\"validation/source_model_losses/{c}\": val\n            for c, val in zip(self.cache_names, losses / n_batches)\n        }\n        log.update(\n            {\n                f\"validation/source_model_losses_with_reconstruction/{c}\": val\n                for c, val in zip(self.cache_names, losses_with_reconstruction / n_batches)\n            }\n        )\n        log.update(\n            {\n                f\"validation/source_model_losses_with_zero_ablation/{c}\": val\n                for c, val in zip(self.cache_names, losses_with_zero_ablation / n_batches)\n            }\n        )\n        log.update(self.reconstruction_score.compute())\n        wandb.log(log)\n</code></pre>"},{"location":"reference/train/sweep/","title":"Sweep","text":"<p>Sweep.</p>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.run_training_pipeline","title":"<code>run_training_pipeline(hyperparameters, source_model, autoencoder, source_data, run_name)</code>","text":"<p>Run the training pipeline for the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <code>source_model</code> <code>HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]</code> <p>The source model.</p> required <code>autoencoder</code> <code>LitSparseAutoencoder</code> <p>The sparse autoencoder.</p> required <code>source_data</code> <code>SourceDataset</code> <p>The source data.</p> required <code>run_name</code> <code>str</code> <p>The name of the run.</p> required Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def run_training_pipeline(\n    hyperparameters: RuntimeHyperparameters,\n    source_model: HookedTransformer | DataParallelWithModelAttributes[HookedTransformer],\n    autoencoder: LitSparseAutoencoder,\n    source_data: SourceDataset,\n    run_name: str,\n) -&gt; None:\n    \"\"\"Run the training pipeline for the sparse autoencoder.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n        source_model: The source model.\n        autoencoder: The sparse autoencoder.\n        source_data: The source data.\n        run_name: The name of the run.\n    \"\"\"\n    checkpoint_path = Path(\"../../.checkpoints\")\n    checkpoint_path.mkdir(exist_ok=True)\n\n    random_seed = hyperparameters[\"random_seed\"]\n    torch.random.manual_seed(random_seed)\n\n    cache_names = hyperparameters[\"source_model\"][\"cache_names\"]\n    stop_layer = stop_layer_from_cache_names(cache_names)\n\n    pipeline = Pipeline(\n        autoencoder=autoencoder,\n        cache_names=cache_names,\n        checkpoint_directory=checkpoint_path,\n        layer=stop_layer,\n        source_data_batch_size=hyperparameters[\"pipeline\"][\"source_data_batch_size\"],\n        source_dataset=source_data,\n        source_model=source_model,\n        log_frequency=hyperparameters[\"pipeline\"][\"log_frequency\"],\n        run_name=run_name,\n        num_workers_data_loading=hyperparameters[\"pipeline\"][\"num_workers_data_loading\"],\n        n_input_features=hyperparameters[\"source_model\"][\"hook_dimension\"],\n        n_learned_features=int(\n            hyperparameters[\"autoencoder\"][\"expansion_factor\"]\n            * hyperparameters[\"source_model\"][\"hook_dimension\"]\n        ),\n    )\n\n    pipeline.run_pipeline(\n        train_batch_size=hyperparameters[\"pipeline\"][\"train_batch_size\"],\n        max_store_size=hyperparameters[\"pipeline\"][\"max_store_size\"],\n        max_activations=hyperparameters[\"pipeline\"][\"max_activations\"],\n        checkpoint_frequency=hyperparameters[\"pipeline\"][\"checkpoint_frequency\"],\n        validate_frequency=hyperparameters[\"pipeline\"][\"validation_frequency\"],\n        validation_n_activations=hyperparameters[\"pipeline\"][\"validation_n_activations\"],\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_autoencoder","title":"<code>setup_autoencoder(hyperparameters)</code>","text":"<p>Setup the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>LitSparseAutoencoder</code> <p>The initialized sparse autoencoder.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_autoencoder(\n    hyperparameters: RuntimeHyperparameters,\n) -&gt; LitSparseAutoencoder:\n    \"\"\"Setup the sparse autoencoder.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The initialized sparse autoencoder.\n    \"\"\"\n    autoencoder_input_dim: int = hyperparameters[\"source_model\"][\"hook_dimension\"]\n    expansion_factor = hyperparameters[\"autoencoder\"][\"expansion_factor\"]\n\n    config = LitSparseAutoencoderConfig(\n        n_input_features=autoencoder_input_dim,\n        n_learned_features=autoencoder_input_dim * expansion_factor,\n        n_components=len(hyperparameters[\"source_model\"][\"cache_names\"]),\n        component_names=hyperparameters[\"source_model\"][\"cache_names\"],\n        l1_coefficient=hyperparameters[\"loss\"][\"l1_coefficient\"],\n        resample_interval=hyperparameters[\"activation_resampler\"][\"resample_interval\"],\n        max_n_resamples=hyperparameters[\"activation_resampler\"][\"max_n_resamples\"],\n        resample_dead_neurons_dataset_size=hyperparameters[\"activation_resampler\"][\n            \"n_activations_activity_collate\"\n        ],\n        resample_loss_dataset_size=hyperparameters[\"activation_resampler\"][\"resample_dataset_size\"],\n        resample_threshold_is_dead_portion_fires=hyperparameters[\"activation_resampler\"][\n            \"threshold_is_dead_portion_fires\"\n        ],\n    )\n\n    return LitSparseAutoencoder(config)\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_source_data","title":"<code>setup_source_data(hyperparameters)</code>","text":"<p>Setup the source data for training.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>SourceDataset</code> <p>The initialized source dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tokenizer name is not specified, but pre_tokenized is False.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_source_data(hyperparameters: RuntimeHyperparameters) -&gt; SourceDataset:\n    \"\"\"Setup the source data for training.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The initialized source dataset.\n\n    Raises:\n        ValueError: If the tokenizer name is not specified, but pre_tokenized is False.\n    \"\"\"\n    dataset_dir = (\n        hyperparameters[\"source_data\"][\"dataset_dir\"]\n        if \"dataset_dir\" in hyperparameters[\"source_data\"]\n        else None\n    )\n\n    dataset_files = (\n        hyperparameters[\"source_data\"][\"dataset_files\"]\n        if \"dataset_files\" in hyperparameters[\"source_data\"]\n        else None\n    )\n\n    if hyperparameters[\"source_data\"][\"pre_tokenized\"]:\n        return PreTokenizedDataset(\n            context_size=hyperparameters[\"source_data\"][\"context_size\"],\n            dataset_dir=dataset_dir,\n            dataset_files=dataset_files,\n            dataset_path=hyperparameters[\"source_data\"][\"dataset_path\"],\n            dataset_column_name=hyperparameters[\"source_data\"][\"dataset_column_name\"],\n            pre_download=hyperparameters[\"source_data\"][\"pre_download\"],\n        )\n\n    if hyperparameters[\"source_data\"][\"tokenizer_name\"] is None:\n        error_message = (\n            \"If pre_tokenized is False, then tokenizer_name must be specified in the \"\n            \"hyperparameters.\"\n        )\n        raise ValueError(error_message)\n\n    tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"source_data\"][\"tokenizer_name\"])\n\n    return TextDataset(\n        context_size=hyperparameters[\"source_data\"][\"context_size\"],\n        dataset_column_name=hyperparameters[\"source_data\"][\"dataset_column_name\"],\n        dataset_dir=dataset_dir,\n        dataset_files=dataset_files,\n        dataset_path=hyperparameters[\"source_data\"][\"dataset_path\"],\n        n_processes_preprocessing=4,\n        pre_download=hyperparameters[\"source_data\"][\"pre_download\"],\n        tokenizer=tokenizer,\n    )\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_source_model","title":"<code>setup_source_model(hyperparameters)</code>","text":"<p>Setup the source model using HookedTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparameters</code> <code>RuntimeHyperparameters</code> <p>The hyperparameters dictionary.</p> required <p>Returns:</p> Type Description <code>HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]</code> <p>The initialized source model.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_source_model(\n    hyperparameters: RuntimeHyperparameters,\n) -&gt; HookedTransformer | DataParallelWithModelAttributes[HookedTransformer]:\n    \"\"\"Setup the source model using HookedTransformer.\n\n    Args:\n        hyperparameters: The hyperparameters dictionary.\n\n    Returns:\n        The initialized source model.\n    \"\"\"\n    model = HookedTransformer.from_pretrained(\n        hyperparameters[\"source_model\"][\"name\"],\n        dtype=hyperparameters[\"source_model\"][\"dtype\"],\n    )\n\n    return DataParallelWithModelAttributes(model)\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.setup_wandb","title":"<code>setup_wandb()</code>","text":"<p>Initialise wandb for experiment tracking.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def setup_wandb() -&gt; RuntimeHyperparameters:\n    \"\"\"Initialise wandb for experiment tracking.\"\"\"\n    wandb.run = None  # Fix for broken pipe bug in wandb\n    wandb.init()\n    return dict(wandb.config)  # type: ignore\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.stop_layer_from_cache_names","title":"<code>stop_layer_from_cache_names(cache_names)</code>","text":"<p>Get the stop layer from the cache names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cache_names = [\n...     \"blocks.0.hook_mlp_out\",\n...     \"blocks.1.hook_mlp_out\",\n...     \"blocks.2.hook_mlp_out\",\n...     ]\n&gt;&gt;&gt; stop_layer_from_cache_names(cache_names)\n2\n</code></pre> <pre><code>&gt;&gt;&gt; cache_names = [\n...     \"blocks.0.hook_x.0.y\",\n...     \"blocks.0.hook_x.1.y\",\n...     ]\n&gt;&gt;&gt; stop_layer_from_cache_names(cache_names)\n0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cache_names</code> <code>list[str]</code> <p>The cache names.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The stop layer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no number is found in the cache names.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def stop_layer_from_cache_names(cache_names: list[str]) -&gt; int:\n    \"\"\"Get the stop layer from the cache names.\n\n    Examples:\n        &gt;&gt;&gt; cache_names = [\n        ...     \"blocks.0.hook_mlp_out\",\n        ...     \"blocks.1.hook_mlp_out\",\n        ...     \"blocks.2.hook_mlp_out\",\n        ...     ]\n        &gt;&gt;&gt; stop_layer_from_cache_names(cache_names)\n        2\n\n        &gt;&gt;&gt; cache_names = [\n        ...     \"blocks.0.hook_x.0.y\",\n        ...     \"blocks.0.hook_x.1.y\",\n        ...     ]\n        &gt;&gt;&gt; stop_layer_from_cache_names(cache_names)\n        0\n\n    Args:\n        cache_names: The cache names.\n\n    Returns:\n        The stop layer.\n\n    Raises:\n        ValueError: If no number is found in the cache names.\n    \"\"\"\n    cache_layers: list[int] = []\n\n    first_n_in_string_regex = re.compile(r\"[0-9]+\")\n\n    for cache_name in cache_names:\n        cache_layer = first_n_in_string_regex.findall(cache_name)\n        if len(cache_layer) == 0:\n            error_message = f\"Could not find a number in the cache name {cache_name}.\"\n            raise ValueError(error_message)\n        cache_layers.append(int(cache_layer[0]))\n\n    return max(cache_layers)\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.sweep","title":"<code>sweep(sweep_config=None, sweep_id=None)</code>","text":"<p>Run the training pipeline with wandb hyperparameter sweep.</p> Warning <p>Either sweep_config or sweep_id must be specified, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>sweep_config</code> <code>SweepConfig | None</code> <p>The sweep configuration.</p> <code>None</code> <code>sweep_id</code> <code>str | None</code> <p>The sweep id for an existing sweep.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither sweep_config nor sweep_id is specified.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def sweep(\n    sweep_config: SweepConfig | None = None,\n    sweep_id: str | None = None,\n) -&gt; None:\n    \"\"\"Run the training pipeline with wandb hyperparameter sweep.\n\n    Warning:\n        Either sweep_config or sweep_id must be specified, but not both.\n\n    Args:\n        sweep_config: The sweep configuration.\n        sweep_id: The sweep id for an existing sweep.\n\n    Raises:\n        ValueError: If neither sweep_config nor sweep_id is specified.\n    \"\"\"\n    if sweep_id is not None:\n        wandb.agent(sweep_id, train, project=\"sparse-autoencoder\")\n\n    elif sweep_config is not None:\n        sweep_id = wandb.sweep(sweep_config.to_dict(), project=\"sparse-autoencoder\")\n        wandb.agent(sweep_id, train)\n\n    else:\n        error_message = \"Either sweep_config or sweep_id must be specified.\"\n        raise ValueError(error_message)\n\n    wandb.finish()\n</code></pre>"},{"location":"reference/train/sweep/#sparse_autoencoder.train.sweep.train","title":"<code>train()</code>","text":"<p>Train the sparse autoencoder using the hyperparameters from the WandB sweep.</p> Source code in <code>sparse_autoencoder/train/sweep.py</code> <pre><code>def train() -&gt; None:\n    \"\"\"Train the sparse autoencoder using the hyperparameters from the WandB sweep.\"\"\"\n    try:\n        # Set up WandB\n        hyperparameters = setup_wandb()\n        run_name: str = wandb.run.name  # type: ignore\n\n        # Set up the source model\n        source_model = setup_source_model(hyperparameters)\n\n        # Set up the autoencoder, optimizer and learning rate scheduler\n        autoencoder = setup_autoencoder(hyperparameters)\n\n        # Set up the source data\n        source_data = setup_source_data(hyperparameters)\n\n        # Run the training pipeline\n        run_training_pipeline(\n            hyperparameters=hyperparameters,\n            source_model=source_model,\n            autoencoder=autoencoder,\n            source_data=source_data,\n            run_name=run_name,\n        )\n    except Exception as _e:  # noqa: BLE001\n        # exit gracefully, so wandb logs the problem\n        print(traceback.print_exc(), file=sys.stderr)  # noqa: T201\n        sys.exit(1)\n</code></pre>"},{"location":"reference/train/sweep_config/","title":"Sweep config","text":"<p>Sweep config.</p> <p>Default hyperparameter setup for quick tuning of a sparse autoencoder.</p> Warning <p>The runtime hyperparameter classes must be manually kept in sync with the hyperparameter classes, so that static type checking works.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters","title":"<code>ActivationResamplerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Activation resampler hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass ActivationResamplerHyperparameters(NestedParameter):\n    \"\"\"Activation resampler hyperparameters.\"\"\"\n\n    resample_interval: Parameter[int] = field(\n        default=Parameter(round_to_multiple(200_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Resample interval.\"\"\"\n\n    max_n_resamples: Parameter[int] = field(default=Parameter(4))\n    \"\"\"Maximum number of resamples.\"\"\"\n\n    n_activations_activity_collate: Parameter[int] = field(\n        default=Parameter(round_to_multiple(100_000_000, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Number of steps to collate before resampling.\n\n    Number of autoencoder learned activation vectors to collate before resampling.\n    \"\"\"\n\n    resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))\n    \"\"\"Resample dataset size.\n\n    Number of autoencoder input activations to use for calculating the loss, as part of the\n    resampling process to create the reset neuron weights.\n    \"\"\"\n\n    threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Dead neuron threshold.\n\n    Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the\n    collated sample).\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.max_n_resamples","title":"<code>max_n_resamples: Parameter[int] = field(default=Parameter(4))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of resamples.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.n_activations_activity_collate","title":"<code>n_activations_activity_collate: Parameter[int] = field(default=Parameter(round_to_multiple(100000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of steps to collate before resampling.</p> <p>Number of autoencoder learned activation vectors to collate before resampling.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.resample_dataset_size","title":"<code>resample_dataset_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE * 100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample dataset size.</p> <p>Number of autoencoder input activations to use for calculating the loss, as part of the resampling process to create the reset neuron weights.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.resample_interval","title":"<code>resample_interval: Parameter[int] = field(default=Parameter(round_to_multiple(200000000, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resample interval.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires","title":"<code>threshold_is_dead_portion_fires: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead neuron threshold.</p> <p>Threshold for determining if a neuron is dead (has \"fired\" in less than this portion of the collated sample).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.ActivationResamplerRuntimeHyperparameters","title":"<code>ActivationResamplerRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Activation resampler runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class ActivationResamplerRuntimeHyperparameters(TypedDict):\n    \"\"\"Activation resampler runtime hyperparameters.\"\"\"\n\n    resample_interval: int\n    max_n_resamples: int\n    n_activations_activity_collate: int\n    resample_dataset_size: int\n    threshold_is_dead_portion_fires: float\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.AutoencoderHyperparameters","title":"<code>AutoencoderHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Sparse autoencoder hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass AutoencoderHyperparameters(NestedParameter):\n    \"\"\"Sparse autoencoder hyperparameters.\"\"\"\n\n    expansion_factor: Parameter[int] = field(default=Parameter(2))\n    \"\"\"Expansion Factor.\n\n    Size of the learned features relative to the input features. A good expansion factor to start\n    with is typically 2-4.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.AutoencoderHyperparameters.expansion_factor","title":"<code>expansion_factor: Parameter[int] = field(default=Parameter(2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion Factor.</p> <p>Size of the learned features relative to the input features. A good expansion factor to start with is typically 2-4.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.AutoencoderRuntimeHyperparameters","title":"<code>AutoencoderRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Autoencoder runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class AutoencoderRuntimeHyperparameters(TypedDict):\n    \"\"\"Autoencoder runtime hyperparameters.\"\"\"\n\n    expansion_factor: int\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters","title":"<code>Hyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Parameters</code></p> <p>Sweep Hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass Hyperparameters(Parameters):\n    \"\"\"Sweep Hyperparameters.\"\"\"\n\n    # Required parameters\n    source_data: SourceDataHyperparameters\n\n    source_model: SourceModelHyperparameters\n\n    # Optional parameters\n    activation_resampler: ActivationResamplerHyperparameters = field(\n        default=ActivationResamplerHyperparameters()\n    )\n\n    autoencoder: AutoencoderHyperparameters = field(default=AutoencoderHyperparameters())\n\n    loss: LossHyperparameters = field(default=LossHyperparameters())\n\n    optimizer: OptimizerHyperparameters = field(default=OptimizerHyperparameters())\n\n    pipeline: PipelineHyperparameters = field(default=PipelineHyperparameters())\n\n    random_seed: Parameter[int] = field(default=Parameter(49))\n    \"\"\"Random seed.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\"\"\"\n        # Check the resample dataset size &lt;= the store size (currently only works if value is used\n        # for both).\n        if (\n            self.activation_resampler.resample_dataset_size.value is not None\n            and self.pipeline.max_store_size.value is not None\n            and self.activation_resampler.resample_dataset_size.value\n            &gt; int(self.pipeline.max_store_size.value)\n        ):\n            error_message = (\n                \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n                f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n                f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n            )\n            raise ValueError(error_message)\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \"\\n    \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}(\\n    {joined_items}\\n)\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.random_seed","title":"<code>random_seed: Parameter[int] = field(default=Parameter(49))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\"\"\"\n    # Check the resample dataset size &lt;= the store size (currently only works if value is used\n    # for both).\n    if (\n        self.activation_resampler.resample_dataset_size.value is not None\n        and self.pipeline.max_store_size.value is not None\n        and self.activation_resampler.resample_dataset_size.value\n        &gt; int(self.pipeline.max_store_size.value)\n    ):\n        error_message = (\n            \"Resample dataset size must be less than or equal to the pipeline max store size. \"\n            f\"Resample dataset size: {self.activation_resampler.resample_dataset_size.value}, \"\n            f\"pipeline max store size: {self.pipeline.max_store_size.value}.\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.Hyperparameters.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \"\\n    \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}(\\n    {joined_items}\\n)\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.LossHyperparameters","title":"<code>LossHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Loss hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass LossHyperparameters(NestedParameter):\n    \"\"\"Loss hyperparameters.\"\"\"\n\n    l1_coefficient: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"L1 Penalty Coefficient.\n\n    The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.\n    The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by\n    using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good\n    starting point for the L1 coefficient is 1e-3.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.LossHyperparameters.l1_coefficient","title":"<code>l1_coefficient: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>L1 Penalty Coefficient.</p> <p>The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good starting point for the L1 coefficient is 1e-3.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.LossRuntimeHyperparameters","title":"<code>LossRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Loss runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class LossRuntimeHyperparameters(TypedDict):\n    \"\"\"Loss runtime hyperparameters.\"\"\"\n\n    l1_coefficient: float\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters","title":"<code>OptimizerHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Optimizer hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass OptimizerHyperparameters(NestedParameter):\n    \"\"\"Optimizer hyperparameters.\"\"\"\n\n    lr: Parameter[float] = field(default=Parameter(1e-3))\n    \"\"\"Learning rate.\n\n    A good starting point for the learning rate is 1e-3, but this is one of the key parameters so\n    you should probably tune it.\n    \"\"\"\n\n    adam_beta_1: Parameter[float] = field(default=Parameter(0.9))\n    \"\"\"Adam Beta 1.\n\n    The exponential decay rate for the first moment estimates (mean) of the gradient.\n    \"\"\"\n\n    adam_beta_2: Parameter[float] = field(default=Parameter(0.99))\n    \"\"\"Adam Beta 2.\n\n    The exponential decay rate for the second moment estimates (variance) of the gradient.\n    \"\"\"\n\n    adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))\n    \"\"\"Adam Weight Decay.\n\n    Weight decay (L2 penalty).\n    \"\"\"\n\n    amsgrad: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"AMSGrad.\n\n    Whether to use the AMSGrad variant of this algorithm from the paper [On the Convergence of Adam\n    and Beyond](https://arxiv.org/abs/1904.09237).\n    \"\"\"\n\n    fused: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Fused.\n\n    Whether to use a fused implementation of the optimizer (may be faster on CUDA).\n    \"\"\"\n\n    lr_scheduler: Parameter[Literal[\"reduce_on_plateau\", \"cosine_annealing\"]] | None = field(\n        default=None\n    )\n    \"\"\"Learning rate scheduler.\"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.adam_beta_1","title":"<code>adam_beta_1: Parameter[float] = field(default=Parameter(0.9))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 1.</p> <p>The exponential decay rate for the first moment estimates (mean) of the gradient.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.adam_beta_2","title":"<code>adam_beta_2: Parameter[float] = field(default=Parameter(0.99))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Beta 2.</p> <p>The exponential decay rate for the second moment estimates (variance) of the gradient.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.adam_weight_decay","title":"<code>adam_weight_decay: Parameter[float] = field(default=Parameter(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adam Weight Decay.</p> <p>Weight decay (L2 penalty).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.amsgrad","title":"<code>amsgrad: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AMSGrad.</p> <p>Whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.fused","title":"<code>fused: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fused.</p> <p>Whether to use a fused implementation of the optimizer (may be faster on CUDA).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.lr","title":"<code>lr: Parameter[float] = field(default=Parameter(0.001))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate.</p> <p>A good starting point for the learning rate is 1e-3, but this is one of the key parameters so you should probably tune it.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerHyperparameters.lr_scheduler","title":"<code>lr_scheduler: Parameter[Literal['reduce_on_plateau', 'cosine_annealing']] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate scheduler.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.OptimizerRuntimeHyperparameters","title":"<code>OptimizerRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Optimizer runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class OptimizerRuntimeHyperparameters(TypedDict):\n    \"\"\"Optimizer runtime hyperparameters.\"\"\"\n\n    lr: float\n    adam_beta_1: float\n    adam_beta_2: float\n    adam_weight_decay: float\n    amsgrad: bool\n    fused: bool\n    lr_scheduler: str | None\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters","title":"<code>PipelineHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Pipeline hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass PipelineHyperparameters(NestedParameter):\n    \"\"\"Pipeline hyperparameters.\"\"\"\n\n    log_frequency: Parameter[int] = field(default=Parameter(100))\n    \"\"\"Training log frequency.\"\"\"\n\n    source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))\n    \"\"\"Source data batch size.\"\"\"\n\n    train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))\n    \"\"\"Train batch size.\"\"\"\n\n    max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))\n    \"\"\"Max store size.\"\"\"\n\n    max_activations: Parameter[int] = field(\n        default=Parameter(round_to_multiple(2e9, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Max activations.\"\"\"\n\n    num_workers_data_loading: Parameter[int] = field(default=Parameter(0))\n    \"\"\"Number of CPU workers for data loading.\"\"\"\n\n    checkpoint_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(5e7, DEFAULT_STORE_SIZE))\n    )\n    \"\"\"Checkpoint frequency.\"\"\"\n\n    validation_frequency: Parameter[int] = field(\n        default=Parameter(round_to_multiple(1e8, DEFAULT_BATCH_SIZE))\n    )\n    \"\"\"Validation frequency.\"\"\"\n\n    validation_n_activations: Parameter[int] = field(\n        default=Parameter(DEFAULT_SOURCE_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 2)\n    )\n    \"\"\"Number of activations to use for validation.\"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.checkpoint_frequency","title":"<code>checkpoint_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(50000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoint frequency.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.log_frequency","title":"<code>log_frequency: Parameter[int] = field(default=Parameter(100))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training log frequency.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.max_activations","title":"<code>max_activations: Parameter[int] = field(default=Parameter(round_to_multiple(2000000000.0, DEFAULT_STORE_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max activations.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.max_store_size","title":"<code>max_store_size: Parameter[int] = field(default=Parameter(DEFAULT_STORE_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max store size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.num_workers_data_loading","title":"<code>num_workers_data_loading: Parameter[int] = field(default=Parameter(0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of CPU workers for data loading.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.source_data_batch_size","title":"<code>source_data_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source data batch size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.train_batch_size","title":"<code>train_batch_size: Parameter[int] = field(default=Parameter(DEFAULT_BATCH_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Train batch size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.validation_frequency","title":"<code>validation_frequency: Parameter[int] = field(default=Parameter(round_to_multiple(100000000.0, DEFAULT_BATCH_SIZE)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation frequency.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineHyperparameters.validation_n_activations","title":"<code>validation_n_activations: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_BATCH_SIZE * DEFAULT_SOURCE_CONTEXT_SIZE * 2))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of activations to use for validation.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.PipelineRuntimeHyperparameters","title":"<code>PipelineRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Pipeline runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class PipelineRuntimeHyperparameters(TypedDict):\n    \"\"\"Pipeline runtime hyperparameters.\"\"\"\n\n    log_frequency: int\n    source_data_batch_size: int\n    train_batch_size: int\n    max_store_size: int\n    max_activations: int\n    num_workers_data_loading: int\n    checkpoint_frequency: int\n    validation_frequency: int\n    validation_n_activations: int\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.RuntimeHyperparameters","title":"<code>RuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class RuntimeHyperparameters(TypedDict):\n    \"\"\"Runtime hyperparameters.\"\"\"\n\n    source_data: SourceDataRuntimeHyperparameters\n    source_model: SourceModelRuntimeHyperparameters\n    activation_resampler: ActivationResamplerRuntimeHyperparameters\n    autoencoder: AutoencoderRuntimeHyperparameters\n    loss: LossRuntimeHyperparameters\n    optimizer: OptimizerRuntimeHyperparameters\n    pipeline: PipelineRuntimeHyperparameters\n    random_seed: int\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters","title":"<code>SourceDataHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceDataHyperparameters(NestedParameter):\n    \"\"\"Source data hyperparameters.\"\"\"\n\n    dataset_path: Parameter[str]\n    \"\"\"Dataset path.\"\"\"\n\n    context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))\n    \"\"\"Context size.\"\"\"\n\n    dataset_column_name: Parameter[str] | None = field(default=Parameter(value=\"input_ids\"))\n    \"\"\"Dataset column name.\"\"\"\n\n    dataset_dir: Parameter[str] | None = field(default=None)\n    \"\"\"Dataset directory (within the HF dataset)\"\"\"\n\n    dataset_files: Parameter[list[str]] | None = field(default=None)\n    \"\"\"Dataset files (within the HF dataset).\"\"\"\n\n    pre_download: Parameter[bool] = field(default=Parameter(value=False))\n    \"\"\"Whether to pre-download the dataset.\"\"\"\n\n    pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))\n    \"\"\"If the dataset is pre-tokenized.\"\"\"\n\n    tokenizer_name: Parameter[str] | None = field(default=None)\n    \"\"\"Tokenizer name.\n\n    Only set this if the dataset is not pre-tokenized.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialisation checks.\n\n        Raises:\n            ValueError: If there is an error in the source data hyperparameters.\n        \"\"\"\n        if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n            raise ValueError(error_message)\n\n        if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n            error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n            raise ValueError(error_message)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.context_size","title":"<code>context_size: Parameter[int] = field(default=Parameter(DEFAULT_SOURCE_CONTEXT_SIZE))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Context size.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_column_name","title":"<code>dataset_column_name: Parameter[str] | None = field(default=Parameter(value='input_ids'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset column name.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_dir","title":"<code>dataset_dir: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset directory (within the HF dataset)</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_files","title":"<code>dataset_files: Parameter[list[str]] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset files (within the HF dataset).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.dataset_path","title":"<code>dataset_path: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Dataset path.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.pre_download","title":"<code>pre_download: Parameter[bool] = field(default=Parameter(value=False))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to pre-download the dataset.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.pre_tokenized","title":"<code>pre_tokenized: Parameter[bool] = field(default=Parameter(value=True))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If the dataset is pre-tokenized.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.tokenizer_name","title":"<code>tokenizer_name: Parameter[str] | None = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tokenizer name.</p> <p>Only set this if the dataset is not pre-tokenized.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataHyperparameters.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post initialisation checks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error in the source data hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post initialisation checks.\n\n    Raises:\n        ValueError: If there is an error in the source data hyperparameters.\n    \"\"\"\n    if self.pre_tokenized.value is False and not isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must be specified, when `pre_tokenized` is False.\"\n        raise ValueError(error_message)\n\n    if self.pre_tokenized.value is True and isinstance(self.tokenizer_name, Parameter):\n        error_message = \"The tokenizer name must not be set, when `pre_tokenized` is True.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceDataRuntimeHyperparameters","title":"<code>SourceDataRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Source data runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class SourceDataRuntimeHyperparameters(TypedDict):\n    \"\"\"Source data runtime hyperparameters.\"\"\"\n\n    context_size: int\n    dataset_column_name: str\n    dataset_dir: str | None\n    dataset_files: list[str] | None\n    dataset_path: str\n    pre_download: bool\n    pre_tokenized: bool\n    tokenizer_name: str | None\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters","title":"<code>SourceModelHyperparameters</code>  <code>dataclass</code>","text":"<p>             Bases: <code>NestedParameter</code></p> <p>Source model hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SourceModelHyperparameters(NestedParameter):\n    \"\"\"Source model hyperparameters.\"\"\"\n\n    name: Parameter[str]\n    \"\"\"Source model name.\"\"\"\n\n    cache_names: Parameter[list[str]]\n    \"\"\"Source model hook site.\"\"\"\n\n    hook_dimension: Parameter[int]\n    \"\"\"Source model hook point dimension.\"\"\"\n\n    dtype: Parameter[str] = field(default=Parameter(\"float32\"))\n    \"\"\"Source model dtype.\"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.cache_names","title":"<code>cache_names: Parameter[list[str]]</code>  <code>instance-attribute</code>","text":"<p>Source model hook site.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.dtype","title":"<code>dtype: Parameter[str] = field(default=Parameter('float32'))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Source model dtype.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.hook_dimension","title":"<code>hook_dimension: Parameter[int]</code>  <code>instance-attribute</code>","text":"<p>Source model hook point dimension.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelHyperparameters.name","title":"<code>name: Parameter[str]</code>  <code>instance-attribute</code>","text":"<p>Source model name.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SourceModelRuntimeHyperparameters","title":"<code>SourceModelRuntimeHyperparameters</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Source model runtime hyperparameters.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>class SourceModelRuntimeHyperparameters(TypedDict):\n    \"\"\"Source model runtime hyperparameters.\"\"\"\n\n    name: str\n    cache_names: list[str]\n    hook_dimension: int\n    dtype: str\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig","title":"<code>SweepConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>WandbSweepConfig</code></p> <p>Sweep Config.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass\nclass SweepConfig(WandbSweepConfig):\n    \"\"\"Sweep Config.\"\"\"\n\n    parameters: Hyperparameters\n\n    method: Method = Method.GRID\n\n    metric: Metric = field(default=Metric(name=\"train/loss/total_loss\"))\n</code></pre>"},{"location":"reference/train/utils/","title":"Train Utils","text":"<p>Train Utils.</p>"},{"location":"reference/train/utils/get_model_device/","title":"Get the device that the model is on","text":"<p>Get the device that the model is on.</p>"},{"location":"reference/train/utils/get_model_device/#sparse_autoencoder.train.utils.get_model_device.get_model_device","title":"<code>get_model_device(model)</code>","text":"<p>Get the device on which a PyTorch model is on.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | DataParallel | LightningModule</code> <p>The PyTorch model.</p> required <p>Returns:</p> Type Description <code>device | None</code> <p>The device ('cuda' or 'cpu') where the model is located.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has no parameters.</p> Source code in <code>sparse_autoencoder/train/utils/get_model_device.py</code> <pre><code>def get_model_device(model: Module | DataParallel | LightningModule) -&gt; torch.device | None:\n    \"\"\"Get the device on which a PyTorch model is on.\n\n    Args:\n        model: The PyTorch model.\n\n    Returns:\n        The device ('cuda' or 'cpu') where the model is located.\n\n    Raises:\n        ValueError: If the model has no parameters.\n    \"\"\"\n    # Deepspeed models already have a device property, so just return that\n    if hasattr(model, \"device\"):\n        return model.device\n\n    # Tensors for lightning should not have device set (as lightning will handle this)\n    if isinstance(model, LightningModule):\n        return None\n\n    # Check if the model has parameters\n    if len(list(model.parameters())) == 0:\n        exception_message = \"The model has no parameters.\"\n        raise ValueError(exception_message)\n\n    # Return the device of the first parameter\n    return next(model.parameters()).device\n</code></pre>"},{"location":"reference/train/utils/round_down/","title":"Round down to the nearest multiple","text":"<p>Round down to the nearest multiple.</p>"},{"location":"reference/train/utils/round_down/#sparse_autoencoder.train.utils.round_down.round_to_multiple","title":"<code>round_to_multiple(value, multiple)</code>","text":"<p>Round down to the nearest multiple.</p> <p>Helper function for creating default values.</p> Example <p>round_to_multiple(1023, 100) 1000</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int | float</code> <p>The value to round down.</p> required <code>multiple</code> <code>int</code> <p>The multiple to round down to.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The value rounded down to the nearest multiple.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>value</code> is less than <code>multiple</code>.</p> Source code in <code>sparse_autoencoder/train/utils/round_down.py</code> <pre><code>def round_to_multiple(value: int | float, multiple: int) -&gt; int:  # noqa: PYI041\n    \"\"\"Round down to the nearest multiple.\n\n    Helper function for creating default values.\n\n    Example:\n        &gt;&gt;&gt; round_to_multiple(1023, 100)\n        1000\n\n    Args:\n        value: The value to round down.\n        multiple: The multiple to round down to.\n\n    Returns:\n        The value rounded down to the nearest multiple.\n\n    Raises:\n        ValueError: If `value` is less than `multiple`.\n    \"\"\"\n    int_value = int(value)\n\n    if int_value &lt; multiple:\n        error_message = f\"{value=} must be greater than or equal to {multiple=}\"\n        raise ValueError(error_message)\n\n    return int_value - int_value % multiple\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/","title":"Wandb Sweep Config Dataclasses","text":"<p>Wandb Sweep Config Dataclasses.</p> <p>Weights &amp; Biases just provide a JSON Schema, so we've converted here to dataclasses.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Controller","title":"<code>Controller</code>  <code>dataclass</code>","text":"<p>Controller.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass Controller:\n    \"\"\"Controller.\"\"\"\n\n    type: ControllerType\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType","title":"<code>ControllerType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Controller Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ControllerType(LowercaseStrEnum):\n    \"\"\"Controller Type.\"\"\"\n\n    CLOUD = auto()\n    \"\"\"Weights &amp; Biases cloud controller.\n\n    Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all\n    communicate with the Weights &amp; Biases cloud service to coordinate the sweep.\n    \"\"\"\n\n    LOCAL = auto()\n    \"\"\"Local controller.\n\n    Manages the sweep operation locally, without the need for cloud-based coordination or external\n    services.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType.CLOUD","title":"<code>CLOUD = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weights &amp; Biases cloud controller.</p> <p>Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all communicate with the Weights &amp; Biases cloud service to coordinate the sweep.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType.LOCAL","title":"<code>LOCAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Local controller.</p> <p>Manages the sweep operation locally, without the need for cloud-based coordination or external services.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution","title":"<code>Distribution</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Sweep Distribution.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Distribution(LowercaseStrEnum):\n    \"\"\"Sweep Distribution.\"\"\"\n\n    BETA = auto()\n    \"\"\"Beta distribution.\n\n    Utilizes the Beta distribution, a family of continuous probability distributions defined on the\n    interval [0, 1], for parameter sampling.\n    \"\"\"\n\n    CATEGORICAL = auto()\n    \"\"\"Categorical distribution.\n\n    Employs a categorical distribution for discrete variable sampling, where each category has an\n    equal probability of being selected.\n    \"\"\"\n\n    CATEGORICAL_W_PROBABILITIES = auto()\n    \"\"\"Categorical distribution with probabilities.\n\n    Similar to categorical distribution but allows assigning different probabilities to each\n    category.\n    \"\"\"\n\n    CONSTANT = auto()\n    \"\"\"Constant distribution.\n\n    Uses a constant value for the parameter, ensuring it remains the same across all runs.\n    \"\"\"\n\n    INT_UNIFORM = auto()\n    \"\"\"Integer uniform distribution.\n\n    Samples integer values uniformly across a specified range.\n    \"\"\"\n\n    INV_LOG_UNIFORM = auto()\n    \"\"\"Inverse log-uniform distribution.\n\n    Samples values according to an inverse log-uniform distribution, useful for parameters that span\n    several orders of magnitude.\n    \"\"\"\n\n    INV_LOG_UNIFORM_VALUES = auto()\n    \"\"\"Inverse log-uniform values distribution.\n\n    Similar to the inverse log-uniform distribution but allows specifying exact values to be\n    sampled.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.BETA","title":"<code>BETA = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Beta distribution.</p> <p>Utilizes the Beta distribution, a family of continuous probability distributions defined on the interval [0, 1], for parameter sampling.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.CATEGORICAL","title":"<code>CATEGORICAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution.</p> <p>Employs a categorical distribution for discrete variable sampling, where each category has an equal probability of being selected.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.CATEGORICAL_W_PROBABILITIES","title":"<code>CATEGORICAL_W_PROBABILITIES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Categorical distribution with probabilities.</p> <p>Similar to categorical distribution but allows assigning different probabilities to each category.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.CONSTANT","title":"<code>CONSTANT = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant distribution.</p> <p>Uses a constant value for the parameter, ensuring it remains the same across all runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.INT_UNIFORM","title":"<code>INT_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Integer uniform distribution.</p> <p>Samples integer values uniformly across a specified range.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.INV_LOG_UNIFORM","title":"<code>INV_LOG_UNIFORM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform distribution.</p> <p>Samples values according to an inverse log-uniform distribution, useful for parameters that span several orders of magnitude.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution.INV_LOG_UNIFORM_VALUES","title":"<code>INV_LOG_UNIFORM_VALUES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inverse log-uniform values distribution.</p> <p>Similar to the inverse log-uniform distribution but allows specifying exact values to be sampled.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal","title":"<code>Goal</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Goal.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Goal(LowercaseStrEnum):\n    \"\"\"Goal.\"\"\"\n\n    MAXIMIZE = auto()\n    \"\"\"Maximization goal.\n\n    Sets the objective of the hyperparameter tuning process to maximize a specified metric.\n    \"\"\"\n\n    MINIMIZE = auto()\n    \"\"\"Minimization goal.\n\n    Aims to minimize a specified metric during the hyperparameter tuning process.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal.MAXIMIZE","title":"<code>MAXIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximization goal.</p> <p>Sets the objective of the hyperparameter tuning process to maximize a specified metric.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal.MINIMIZE","title":"<code>MINIMIZE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimization goal.</p> <p>Aims to minimize a specified metric during the hyperparameter tuning process.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping","title":"<code>HyperbandStopping</code>  <code>dataclass</code>","text":"<p>Hyperband Stopping Config.</p> <p>Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs.</p> Example <p>HyperbandStopping(type=HyperbandStoppingType.HYPERBAND) HyperbandStopping(type=hyperband)</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass HyperbandStopping:\n    \"\"\"Hyperband Stopping Config.\n\n    Speed up hyperparameter search by killing off runs that appear to have lower performance\n    than successful training runs.\n\n    Example:\n        &gt;&gt;&gt; HyperbandStopping(type=HyperbandStoppingType.HYPERBAND)\n        HyperbandStopping(type=hyperband)\n    \"\"\"\n\n    type: HyperbandStoppingType | None = HyperbandStoppingType.HYPERBAND\n\n    eta: float | None = None\n    \"\"\"ETA.\n\n    Specify the bracket multiplier schedule (default: 3).\n    \"\"\"\n\n    maxiter: int | None = None\n    \"\"\"Max Iterations.\n\n    Specify the maximum number of iterations. Note this is number of times the metric is logged, not\n    the number of activations.\n    \"\"\"\n\n    miniter: int | None = None\n    \"\"\"Min Iterations.\n\n    Set the first epoch to start trimming runs, and hyperband will automatically calculate\n    the subsequent epochs to trim runs.\n    \"\"\"\n\n    s: float | None = None\n    \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\"\n\n    strict: bool | None = None\n    \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.eta","title":"<code>eta: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ETA.</p> <p>Specify the bracket multiplier schedule (default: 3).</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.maxiter","title":"<code>maxiter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max Iterations.</p> <p>Specify the maximum number of iterations. Note this is number of times the metric is logged, not the number of activations.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.miniter","title":"<code>miniter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Min Iterations.</p> <p>Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.s","title":"<code>s: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Set the number of steps you trim runs at, working backwards from the max_iter.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.strict","title":"<code>strict: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use a more aggressive condition for termination, stops more runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStoppingType","title":"<code>HyperbandStoppingType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Hyperband Stopping Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class HyperbandStoppingType(LowercaseStrEnum):\n    \"\"\"Hyperband Stopping Type.\"\"\"\n\n    HYPERBAND = auto()\n    \"\"\"Hyperband algorithm.\n\n    Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping\n    method to efficiently tune hyperparameters.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStoppingType.HYPERBAND","title":"<code>HYPERBAND = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hyperband algorithm.</p> <p>Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping method to efficiently tune hyperparameters.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Impute","title":"<code>Impute</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Metric value to use in bayes search for runs that fail, crash, or are killed.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Impute(LowercaseStrEnum):\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\"\n\n    BEST = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ImputeWhileRunning","title":"<code>ImputeWhileRunning</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Appends a calculated metric even when epochs are in a running state.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ImputeWhileRunning(LowercaseStrEnum):\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    BEST = auto()\n    FALSE = auto()\n    LATEST = auto()\n    WORST = auto()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Kind","title":"<code>Kind</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Kind.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Kind(LowercaseStrEnum):\n    \"\"\"Kind.\"\"\"\n\n    SWEEP = auto()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method","title":"<code>Method</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Method.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Method(LowercaseStrEnum):\n    \"\"\"Method.\"\"\"\n\n    BAYES = auto()\n    \"\"\"Bayesian optimization.\n\n    Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach\n    for finding the optimal set of parameters.\n    \"\"\"\n\n    CUSTOM = auto()\n    \"\"\"Custom method.\n\n    Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the\n    sweep process.\n    \"\"\"\n\n    GRID = auto()\n    \"\"\"Grid search.\n\n    Utilizes a grid search approach for hyperparameter tuning, systematically working through\n    multiple combinations of parameter values.\n    \"\"\"\n\n    RANDOM = auto()\n    \"\"\"Random search.\n\n    Implements a random search strategy for hyperparameter tuning, exploring the parameter space\n    randomly.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.BAYES","title":"<code>BAYES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bayesian optimization.</p> <p>Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach for finding the optimal set of parameters.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.CUSTOM","title":"<code>CUSTOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom method.</p> <p>Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the sweep process.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.GRID","title":"<code>GRID = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Grid search.</p> <p>Utilizes a grid search approach for hyperparameter tuning, systematically working through multiple combinations of parameter values.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method.RANDOM","title":"<code>RANDOM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random search.</p> <p>Implements a random search strategy for hyperparameter tuning, exploring the parameter space randomly.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>Metric to optimize.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Metric:\n    \"\"\"Metric to optimize.\"\"\"\n\n    name: str\n    \"\"\"Name of metric.\"\"\"\n\n    goal: Goal | None = Goal.MINIMIZE\n\n    impute: Impute | None = None\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\"\n\n    imputewhilerunning: ImputeWhileRunning | None = None\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    target: float | None = None\n    \"\"\"The sweep will finish once any run achieves this value.\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.impute","title":"<code>impute: Impute | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metric value to use in bayes search for runs that fail, crash, or are killed</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.imputewhilerunning","title":"<code>imputewhilerunning: ImputeWhileRunning | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Appends a calculated metric even when epochs are in a running state.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of metric.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.target","title":"<code>target: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will finish once any run achieves this value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter","title":"<code>NestedParameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Nested Parameter.</p> Example <p>from dataclasses import field @dataclass(frozen=True) ... class MyNestedParameter(NestedParameter): ...     a: int = field(default=Parameter(1)) ...     b: int = field(default=Parameter(2)) MyNestedParameter().to_dict() {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass NestedParameter(ABC):  # noqa: B024 (abstract so that we can check against its type)\n    \"\"\"Nested Parameter.\n\n    Example:\n        &gt;&gt;&gt; from dataclasses import field\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class MyNestedParameter(NestedParameter):\n        ...     a: int = field(default=Parameter(1))\n        ...     b: int = field(default=Parameter(2))\n        &gt;&gt;&gt; MyNestedParameter().to_dict()\n        {'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}\n    \"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\"\"\"\n\n        def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n            \"\"\"Return dict without None values.\n\n            Args:\n                obj: The object to convert to a dict.\n\n            Returns:\n                The dict representation of the object.\n            \"\"\"\n            dict_none_removed = {}\n            dict_with_none = dict(obj)\n            for key, value in dict_with_none.items():\n                if value is not None:\n                    dict_none_removed[key] = value\n            return dict_none_removed\n\n        return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n\n    def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n        \"\"\"Return dict representation of this object.\"\"\"\n        return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter.__dict__","title":"<code>__dict__()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n    \"\"\"Return dict representation of this object.\"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\"\"\"\n\n    def dict_without_none_values(obj: Any) -&gt; dict:  # noqa: ANN401\n        \"\"\"Return dict without None values.\n\n        Args:\n            obj: The object to convert to a dict.\n\n        Returns:\n            The dict representation of the object.\n        \"\"\"\n        dict_none_removed = {}\n        dict_with_none = dict(obj)\n        for key, value in dict_with_none.items():\n            if value is not None:\n                dict_none_removed[key] = value\n        return dict_none_removed\n\n    return {\"parameters\": asdict(self, dict_factory=dict_without_none_values)}\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[ParamType]</code></p> <p>Sweep Parameter.</p> <p>https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Parameter(Generic[ParamType]):\n    \"\"\"Sweep Parameter.\n\n    https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters\n    \"\"\"\n\n    value: ParamType | None = None\n    \"\"\"Single value.\n\n    Specifies the single valid value for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    max: ParamType | None = None\n    \"\"\"Maximum value.\"\"\"\n\n    min: ParamType | None = None\n    \"\"\"Minimum value.\"\"\"\n\n    distribution: Distribution | None = None\n    \"\"\"Distribution\n\n    If not specified, will default to categorical if values is set, to int_uniform if max and min\n    are set to integers, to uniform if max and min are set to floats, or to constant if value is\n    set.\n    \"\"\"\n\n    q: float | None = None\n    \"\"\"Quantization parameter.\n\n    Quantization step size for quantized hyperparameters.\n    \"\"\"\n\n    values: list[ParamType] | None = None\n    \"\"\"Discrete values.\n\n    Specifies all valid values for this hyperparameter. Compatible with grid.\n    \"\"\"\n\n    probabilities: list[float] | None = None\n    \"\"\"Probability of each value\"\"\"\n\n    mu: float | None = None\n    \"\"\"Mean for normal or lognormal distributions\"\"\"\n\n    sigma: float | None = None\n    \"\"\"Std Dev for normal or lognormal distributions\"\"\"\n\n    @final\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this object.\"\"\"\n        items_representation = []\n        for key, value in self.__dict__.items():\n            if value is not None:\n                items_representation.append(f\"{key}={value}\")\n        joined_items = \", \".join(items_representation)\n\n        class_name = self.__class__.__name__\n\n        return f\"{class_name}({joined_items})\"\n\n    @final\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of this object.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.distribution","title":"<code>distribution: Distribution | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Distribution</p> <p>If not specified, will default to categorical if values is set, to int_uniform if max and min are set to integers, to uniform if max and min are set to floats, or to constant if value is set.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.max","title":"<code>max: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.min","title":"<code>min: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.mu","title":"<code>mu: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean for normal or lognormal distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.probabilities","title":"<code>probabilities: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Probability of each value</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.q","title":"<code>q: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantization parameter.</p> <p>Quantization step size for quantized hyperparameters.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.sigma","title":"<code>sigma: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Std Dev for normal or lognormal distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.value","title":"<code>value: ParamType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single value.</p> <p>Specifies the single valid value for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.values","title":"<code>values: list[ParamType] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Discrete values.</p> <p>Specifies all valid values for this hyperparameter. Compatible with grid.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __repr__(self) -&gt; str:\n    \"\"\"Representation of this object.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@final\ndef __str__(self) -&gt; str:\n    \"\"\"String representation of this object.\"\"\"\n    items_representation = []\n    for key, value in self.__dict__.items():\n        if value is not None:\n            items_representation.append(f\"{key}={value}\")\n    joined_items = \", \".join(items_representation)\n\n    class_name = self.__class__.__name__\n\n    return f\"{class_name}({joined_items})\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameters","title":"<code>Parameters</code>  <code>dataclass</code>","text":"<p>Parameters</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass Parameters:\n    \"\"\"Parameters\"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig","title":"<code>WandbSweepConfig</code>  <code>dataclass</code>","text":"<p>Weights &amp; Biases Sweep Configuration.</p> Example <p>config = WandbSweepConfig( ...     parameters={\"lr\": Parameter(value=1e-3)}, ...     method=Method.BAYES, ...     metric=Metric(name=\"loss\"), ...     ) print(config.to_dict()[\"parameters\"]) {'lr': {'value': 0.001}}</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass\nclass WandbSweepConfig:\n    \"\"\"Weights &amp; Biases Sweep Configuration.\n\n    Example:\n        &gt;&gt;&gt; config = WandbSweepConfig(\n        ...     parameters={\"lr\": Parameter(value=1e-3)},\n        ...     method=Method.BAYES,\n        ...     metric=Metric(name=\"loss\"),\n        ...     )\n        &gt;&gt;&gt; print(config.to_dict()[\"parameters\"])\n        {'lr': {'value': 0.001}}\n    \"\"\"\n\n    parameters: Parameters | Any\n\n    method: Method\n    \"\"\"Method (search strategy).\"\"\"\n\n    metric: Metric\n    \"\"\"Metric to optimize\"\"\"\n\n    command: list[Any] | None = None\n    \"\"\"Command used to launch the training script\"\"\"\n\n    controller: Controller | None = None\n\n    description: str | None = None\n    \"\"\"Short package description\"\"\"\n\n    earlyterminate: HyperbandStopping | None = None\n\n    entity: str | None = None\n    \"\"\"The entity for this sweep\"\"\"\n\n    imageuri: str | None = None\n    \"\"\"Sweeps on Launch will use this uri instead of a job.\"\"\"\n\n    job: str | None = None\n    \"\"\"Launch Job to run.\"\"\"\n\n    kind: Kind | None = None\n\n    name: str | None = None\n    \"\"\"The name of the sweep, displayed in the W&amp;B UI.\"\"\"\n\n    program: str | None = None\n    \"\"\"Training script to run.\"\"\"\n\n    project: str | None = None\n    \"\"\"The project for this sweep.\"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\n\n        Recursively removes all None values. Handles special cases of dataclass\n        instances and values that are `NestedParameter` instances.\n\n        Returns:\n            dict[str, Any]: The dict representation of the object.\n        \"\"\"\n\n        def recursive_format(obj: Any) -&gt; Any:  # noqa: ANN401\n            \"\"\"Recursively format the dict of hyperparameters.\"\"\"\n            # Handle dataclasses\n            if is_dataclass(obj):\n                cleaned_obj = {}\n                for parameter_name in asdict(obj):\n                    value = getattr(obj, parameter_name)\n\n                    # Remove None values.\n                    if value is None:\n                        continue\n\n                    # Nested parameters have their own `to_dict` method, which we can call.\n                    if isinstance(value, NestedParameter):\n                        cleaned_obj[parameter_name] = value.to_dict()\n                    # Otherwise recurse.\n                    else:\n                        cleaned_obj[parameter_name] = recursive_format(value)\n                return cleaned_obj\n\n            # Handle dicts\n            if isinstance(obj, dict):\n                cleaned_obj = {}\n                for key, value in obj.items():\n                    # Remove None values.\n                    if value is None:\n                        continue\n\n                    # Otherwise recurse.\n                    cleaned_obj[key] = recursive_format(value)\n                return cleaned_obj\n\n            # Handle enums\n            if isinstance(obj, Enum):\n                return obj.value\n\n            # Handle other types (e.g. float, int, str)\n            return obj\n\n        return recursive_format(self)\n\n    def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n        \"\"\"Return dict representation of this object.\"\"\"\n        return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.command","title":"<code>command: list[Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Command used to launch the training script</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.description","title":"<code>description: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Short package description</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.entity","title":"<code>entity: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entity for this sweep</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.imageuri","title":"<code>imageuri: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sweeps on Launch will use this uri instead of a job.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.job","title":"<code>job: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Launch Job to run.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.method","title":"<code>method: Method</code>  <code>instance-attribute</code>","text":"<p>Method (search strategy).</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.metric","title":"<code>metric: Metric</code>  <code>instance-attribute</code>","text":"<p>Metric to optimize</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.name","title":"<code>name: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the sweep, displayed in the W&amp;B UI.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.program","title":"<code>program: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training script to run.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.project","title":"<code>project: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The project for this sweep.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.__dict__","title":"<code>__dict__()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def __dict__(self) -&gt; dict[str, Any]:  # type: ignore[override]\n    \"\"\"Return dict representation of this object.\"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> <p>Recursively removes all None values. Handles special cases of dataclass instances and values that are <code>NestedParameter</code> instances.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The dict representation of the object.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\n\n    Recursively removes all None values. Handles special cases of dataclass\n    instances and values that are `NestedParameter` instances.\n\n    Returns:\n        dict[str, Any]: The dict representation of the object.\n    \"\"\"\n\n    def recursive_format(obj: Any) -&gt; Any:  # noqa: ANN401\n        \"\"\"Recursively format the dict of hyperparameters.\"\"\"\n        # Handle dataclasses\n        if is_dataclass(obj):\n            cleaned_obj = {}\n            for parameter_name in asdict(obj):\n                value = getattr(obj, parameter_name)\n\n                # Remove None values.\n                if value is None:\n                    continue\n\n                # Nested parameters have their own `to_dict` method, which we can call.\n                if isinstance(value, NestedParameter):\n                    cleaned_obj[parameter_name] = value.to_dict()\n                # Otherwise recurse.\n                else:\n                    cleaned_obj[parameter_name] = recursive_format(value)\n            return cleaned_obj\n\n        # Handle dicts\n        if isinstance(obj, dict):\n            cleaned_obj = {}\n            for key, value in obj.items():\n                # Remove None values.\n                if value is None:\n                    continue\n\n                # Otherwise recurse.\n                cleaned_obj[key] = recursive_format(value)\n            return cleaned_obj\n\n        # Handle enums\n        if isinstance(obj, Enum):\n            return obj.value\n\n        # Handle other types (e.g. float, int, str)\n        return obj\n\n    return recursive_format(self)\n</code></pre>"},{"location":"reference/training_runs/","title":"Training runs","text":"<p>Training runs.</p>"},{"location":"reference/training_runs/gpt2/","title":"Run an sweep on all layers of GPT2 Small","text":"<p>Run an sweep on all layers of GPT2 Small.</p> <p>Command:</p> <pre><code>git clone https://github.com/ai-safety-foundation/sparse_autoencoder.git &amp;&amp; cd sparse_autoencoder &amp;&amp;\npoetry env use python3.11 &amp;&amp; poetry install &amp;&amp;\npoetry run python sparse_autoencoder/training_runs/gpt2.py\n</code></pre>"},{"location":"reference/training_runs/gpt2/#sparse_autoencoder.training_runs.gpt2.train","title":"<code>train()</code>","text":"<p>Train.</p> Source code in <code>sparse_autoencoder/training_runs/gpt2.py</code> <pre><code>def train() -&gt; None:\n    \"\"\"Train.\"\"\"\n    sweep_config = SweepConfig(\n        parameters=Hyperparameters(\n            loss=LossHyperparameters(\n                l1_coefficient=Parameter(values=[0.0001]),\n            ),\n            optimizer=OptimizerHyperparameters(\n                lr=Parameter(value=0.0001),\n            ),\n            source_model=SourceModelHyperparameters(\n                name=Parameter(\"gpt2\"),\n                cache_names=Parameter(\n                    value=[f\"blocks.{layer}.hook_mlp_out\" for layer in range(12)]\n                ),\n                hook_dimension=Parameter(768),\n            ),\n            source_data=SourceDataHyperparameters(\n                dataset_path=Parameter(\"alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2\"),\n                context_size=Parameter(256),\n                pre_tokenized=Parameter(value=True),\n                pre_download=Parameter(value=True),\n                # Total dataset is c.7bn activations (64 files)\n                # C. 1.5TB needed to store all activations\n                dataset_files=Parameter(\n                    [f\"data/train-{str(i).zfill(5)}-of-00064.parquet\" for i in range(20)]\n                ),\n            ),\n            autoencoder=AutoencoderHyperparameters(expansion_factor=Parameter(values=[32, 64])),\n            pipeline=PipelineHyperparameters(),\n            activation_resampler=ActivationResamplerHyperparameters(\n                threshold_is_dead_portion_fires=Parameter(1e-5),\n            ),\n        ),\n        method=Method.GRID,\n    )\n\n    sweep(sweep_config=sweep_config)\n</code></pre>"},{"location":"reference/utils/","title":"Shared utils","text":"<p>Shared utils.</p>"},{"location":"reference/utils/data_parallel/","title":"Data parallel utils","text":"<p>Data parallel utils.</p>"},{"location":"reference/utils/data_parallel/#sparse_autoencoder.utils.data_parallel.DataParallelWithModelAttributes","title":"<code>DataParallelWithModelAttributes</code>","text":"<p>             Bases: <code>DataParallel[T]</code>, <code>Generic[T]</code></p> <p>Data parallel with access to underlying model attributes/methods.</p> <p>Allows access to underlying model attributes/methods, which is not possible with the default <code>DataParallel</code> class. Based on: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</p> Example <p>from sparse_autoencoder import SparseAutoencoder, SparseAutoencoderConfig model = SparseAutoencoder(SparseAutoencoderConfig( ...     n_input_features=2, ...     n_learned_features=4, ... )) distributed_model = DataParallelWithModelAttributes(model) distributed_model.config.n_learned_features 4</p> Source code in <code>sparse_autoencoder/utils/data_parallel.py</code> <pre><code>class DataParallelWithModelAttributes(DataParallel[T], Generic[T]):\n    \"\"\"Data parallel with access to underlying model attributes/methods.\n\n    Allows access to underlying model attributes/methods, which is not possible with the default\n    `DataParallel` class. Based on:\n    https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder import SparseAutoencoder, SparseAutoencoderConfig\n        &gt;&gt;&gt; model = SparseAutoencoder(SparseAutoencoderConfig(\n        ...     n_input_features=2,\n        ...     n_learned_features=4,\n        ... ))\n        &gt;&gt;&gt; distributed_model = DataParallelWithModelAttributes(model)\n        &gt;&gt;&gt; distributed_model.config.n_learned_features\n        4\n    \"\"\"\n\n    def __getattr__(self, name: str) -&gt; Any:  # noqa: ANN401\n        \"\"\"Allow access to underlying model attributes/methods.\n\n        Args:\n            name: Attribute/method name.\n\n        Returns:\n            Attribute value/method.\n        \"\"\"\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n</code></pre>"},{"location":"reference/utils/data_parallel/#sparse_autoencoder.utils.data_parallel.DataParallelWithModelAttributes.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Allow access to underlying model attributes/methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Attribute/method name.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Attribute value/method.</p> Source code in <code>sparse_autoencoder/utils/data_parallel.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:  # noqa: ANN401\n    \"\"\"Allow access to underlying model attributes/methods.\n\n    Args:\n        name: Attribute/method name.\n\n    Returns:\n        Attribute value/method.\n    \"\"\"\n    try:\n        return super().__getattr__(name)\n    except AttributeError:\n        return getattr(self.module, name)\n</code></pre>"},{"location":"reference/utils/tensor_shape/","title":"Tensor shape utilities","text":"<p>Tensor shape utilities.</p>"},{"location":"reference/utils/tensor_shape/#sparse_autoencoder.utils.tensor_shape.shape_with_optional_dimensions","title":"<code>shape_with_optional_dimensions(*shape)</code>","text":"<p>Create a shape from a tuple of optional dimensions.</p> Motivation <p>By default PyTorch tensor shapes will error if you set an axis to <code>None</code>. This allows you to set that size and then the resulting output simply removes that axis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(1, 2, 3)\n(1, 2, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(1, None, 3)\n(1, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(1, None, None)\n(1,)\n</code></pre> <pre><code>&gt;&gt;&gt; shape_with_optional_dimensions(None, None, None)\n()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*shape</code> <code>int | None</code> <p>Axis sizes, with <code>None</code> representing an optional axis.</p> <code>()</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Axis sizes.</p> Source code in <code>sparse_autoencoder/utils/tensor_shape.py</code> <pre><code>def shape_with_optional_dimensions(*shape: int | None) -&gt; tuple[int, ...]:\n    \"\"\"Create a shape from a tuple of optional dimensions.\n\n    Motivation:\n        By default PyTorch tensor shapes will error if you set an axis to `None`. This allows\n        you to set that size and then the resulting output simply removes that axis.\n\n    Examples:\n        &gt;&gt;&gt; shape_with_optional_dimensions(1, 2, 3)\n        (1, 2, 3)\n\n        &gt;&gt;&gt; shape_with_optional_dimensions(1, None, 3)\n        (1, 3)\n\n        &gt;&gt;&gt; shape_with_optional_dimensions(1, None, None)\n        (1,)\n\n        &gt;&gt;&gt; shape_with_optional_dimensions(None, None, None)\n        ()\n\n    Args:\n        *shape: Axis sizes, with `None` representing an optional axis.\n\n    Returns:\n        Axis sizes.\n    \"\"\"\n    return tuple(dimension for dimension in shape if dimension is not None)\n</code></pre>"}]}