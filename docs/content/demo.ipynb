{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder Training Demo\n",
    "\n",
    "This demo trains a sparse autoencoder on activations from a Tiny Stories 1M model.\n",
    "\n",
    "To do this we setup a *source model* (the TinyStories model) that we want to generate activations\n",
    "from, along with a *source dataset* of prompts to help generate these activations.\n",
    "\n",
    "We also setup a *sparse autoencoder model* which we'll train on these generated activations, to\n",
    "learn a sparse representation of them in higher dimensional space.\n",
    "\n",
    "Finally we'll wrap this all together in a *pipeline*, which alternates between generating\n",
    "activations (storing them in ram), and training the SAE on said activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import wandb\n",
    "\n",
    "from sparse_autoencoder import SparseAutoencoder\n",
    "from sparse_autoencoder.activation_resampler import ActivationResampler\n",
    "from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss\n",
    "from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss\n",
    "from sparse_autoencoder.loss.reducer import LossReducer\n",
    "from sparse_autoencoder.optimizer.adam_with_reset import AdamWithReset\n",
    "from sparse_autoencoder.source_data.text_dataset import GenericTextDataset\n",
    "from sparse_autoencoder.train.pipeline import Pipeline\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")  # You will need a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this library works is that you can define your own hyper-parameters and then setup the\n",
    "underlying components with them. This is extremely flexible, but to help you get started we've\n",
    "included some common ones below along with some sensible defaults. You can also easily sweep through\n",
    "multiple hyperparameters with `wandb.sweep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # Expansion factor is the number of features in the sparse representation, relative to the\n",
    "    # number of features in the original MLP layer. The original paper experimented with 1x to 256x,\n",
    "    # and we have found that 4x is a good starting point.\n",
    "    \"expansion_factor\": 4,\n",
    "    # L1 coefficient is the coefficient of the L1 regularization term (used to encourage sparsity).\n",
    "    \"l1_coefficient\": 0.001,\n",
    "    # Adam parameters (set to the default ones here)\n",
    "    \"lr\": 0.001,\n",
    "    \"adam_beta_1\": 0.9,\n",
    "    \"adam_beta_2\": 0.999,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"adam_weight_decay\": 0.0,\n",
    "    # Batch sizes\n",
    "    \"train_batch_size\": 8192,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source model is just a [TransformerLens](https://github.com/neelnanda-io/TransformerLens) model\n",
    "(see [here](https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "for a full list of supported models).\n",
    "\n",
    "In this example we're training a sparse autoencoder on the activations from the first MLP layer, so\n",
    "we'll also get some details about that hook point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Source: tiny-stories-1M, Hook: blocks.0.mlp.hook_post, Features: 256'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source model setup with TransformerLens\n",
    "src_model_name = \"tiny-stories-1M\"\n",
    "src_model = HookedTransformer.from_pretrained(src_model_name, dtype=\"float32\")\n",
    "\n",
    "# Details about the activations we'll train the sparse autoencoder on\n",
    "src_model_activation_hook_point = \"blocks.0.mlp.hook_post\"\n",
    "src_model_activation_layer = 0\n",
    "src_d_mlp: int = src_model.cfg.d_mlp  # type: ignore (TransformerLens typing is currently broken)\n",
    "\n",
    "f\"Source: {src_model_name}, Hook: {src_model_activation_hook_point}, Features: {src_d_mlp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then setup the sparse autoencoder. The default model (`SparseAutoencoder`) is setup as per\n",
    "the original Anthropic paper [Towards Monosemanticity: Decomposing Language Models With Dictionary\n",
    "Learning ](https://transformer-circuits.pub/2023/monosemantic-features/index.html).\n",
    "\n",
    "However it's just a standard PyTorch model, so you can create your own model instead if you want to\n",
    "use a different architecture. To do this you just need to extend the `AbstractAutoencoder`, and\n",
    "optionally the underlying `AbstractEncoder`, `AbstractDecoder` and `AbstractOuterBias`. See these\n",
    "classes (which are fully documented) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (_pre_encoder_bias): TiedBias(position=pre_encoder)\n",
       "  (_encoder): LinearEncoder(\n",
       "    in_features=256, out_features=1024\n",
       "    (activation_function): ReLU()\n",
       "  )\n",
       "  (_decoder): UnitNormDecoder(in_features=1024, out_features=256)\n",
       "  (_post_decoder_bias): TiedBias(position=post_decoder)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expansion_factor = hyperparameters[\"expansion_factor\"]\n",
    "autoencoder = SparseAutoencoder(\n",
    "    n_input_features=src_d_mlp,  # size of the activations we are autoencoding\n",
    "    n_learned_features=int(src_d_mlp * expansion_factor),  # size of SAE\n",
    "    geometric_median_dataset=torch.zeros(src_d_mlp),  # this is used to initialize the tied bias\n",
    ").to(device)\n",
    "autoencoder  # Print the model (it's pretty straightforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to setup an Optimizer and Loss function. In this case we'll also use the standard\n",
    "approach from the original Anthropic paper. However you can create your own loss functions and\n",
    "optimizers by extending `AbstractLoss` and `AbstractOptimizerWithReset` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LossReducer(\n",
       "  (0): LearnedActivationsL1Loss(l1_coefficient=0.001)\n",
       "  (1): MSEReconstructionLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a loss reducer, which simply adds up the losses from the underlying loss functions.\n",
    "loss = LossReducer(\n",
    "    LearnedActivationsL1Loss(\n",
    "        l1_coefficient=hyperparameters[\"l1_coefficient\"],\n",
    "    ),\n",
    "    MSEReconstructionLoss(),\n",
    ")\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamWithReset (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamWithReset(\n",
    "    params=autoencoder.parameters(),\n",
    "    named_parameters=autoencoder.named_parameters(),\n",
    "    lr=hyperparameters[\"lr\"],\n",
    "    betas=(hyperparameters[\"adam_beta_1\"], hyperparameters[\"adam_beta_2\"]),\n",
    "    eps=hyperparameters[\"adam_epsilon\"],\n",
    "    weight_decay=hyperparameters[\"adam_weight_decay\"],\n",
    ")\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll initialise an activation resampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_resampler = ActivationResampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a dataset of tokenized prompts, to be used in generating activations (which are in turn\n",
    "used to train the SAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer: PreTrainedTokenizerBase = src_model.tokenizer  # type: ignore\n",
    "source_data = GenericTextDataset(tokenizer=tokenizer, dataset_path=\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialise [wandb](https://wandb.ai/site), the pipeline will automatically log all metrics to\n",
    "wandb. However, we should pass in a dictionary with all of our hyperaparameters so they're on \n",
    "wandb. \n",
    "\n",
    "We strongly encourage users to make use of wandb in order to understand the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.cache/wandb/run-20231121_190511-5pvpkttg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/5pvpkttg' target=\"_blank\">pious-yogurt-57</a></strong> to <a href='https://wandb.ai/alan-cooney/sparse-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alan-cooney/sparse-autoencoder' target=\"_blank\">https://wandb.ai/alan-cooney/sparse-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/5pvpkttg' target=\"_blank\">https://wandb.ai/alan-cooney/sparse-autoencoder/runs/5pvpkttg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/5pvpkttg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x33f921550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\".cache/\").mkdir(exist_ok=True)\n",
    "wandb.init(\n",
    "    project=\"sparse-autoencoder\",\n",
    "    dir=\".cache\",\n",
    "    config=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d946c771e94b64b3c4a382b997a3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Activations trained on:   0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a88fef33-195d-468c-b527-20ad75eccf2d)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/util/retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mraise\u001b[39;00m reraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    471\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/util/util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m     39\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     cache_name\u001b[39m=\u001b[39msrc_model_activation_hook_point,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     layer\u001b[39m=\u001b[39msrc_model_activation_layer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     source_data_batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pipeline\u001b[39m.\u001b[39;49mrun_pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_batch_size\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     max_store_size\u001b[39m=\u001b[39;49m\u001b[39m1_000_000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Sizes for demo purposes (you probably want to scale these by 10x)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     max_activations\u001b[39m=\u001b[39;49m\u001b[39m10_000_000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     resample_frequency\u001b[39m=\u001b[39;49m\u001b[39m2_500_000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/abstract_pipeline.py:186\u001b[0m, in \u001b[0;36mAbstractPipeline.run_pipeline\u001b[0;34m(self, train_batch_size, max_store_size, max_activations, resample_frequency, validate_frequency, checkpoint_frequency)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, max_activations, store_size):\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Generate\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     progress_bar\u001b[39m.\u001b[39mset_postfix({\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgenerate\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m--> 186\u001b[0m     activation_store: TensorActivationStore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_activations(store_size)\n\u001b[1;32m    188\u001b[0m     \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     progress_bar\u001b[39m.\u001b[39mset_postfix({\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/pipeline.py:45\u001b[0m, in \u001b[0;36mPipeline.generate_activations\u001b[0;34m(self, store_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m# Loop through the dataloader until the store reaches the desired size\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_data:\n\u001b[1;32m     46\u001b[0m         input_ids: BatchTokenizedPrompts \u001b[39m=\u001b[39;49m batch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(source_model_device)\n\u001b[1;32m     47\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_model\u001b[39m.\u001b[39;49mforward(input_ids, stop_at_layer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)  \u001b[39m# type: ignore (TLens is typed incorrectly)\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/abstract_pipeline.py:262\u001b[0m, in \u001b[0;36mAbstractPipeline.stateful_dataloader_iterable\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstateful_dataloader_iterable\u001b[39m(\n\u001b[1;32m    229\u001b[0m     dataloader: DataLoader[TorchTokenizedPrompts]\n\u001b[1;32m    230\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[TorchTokenizedPrompts]:\n\u001b[1;32m    231\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a stateful dataloader iterable.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m    Create an iterable that maintains it's position in the dataloader between loops.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39m        Stateful iterable over the data in the dataloader.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[39myield from\u001b[39;00m dataloader\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:1379\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[39myield\u001b[39;00m formatter\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1377\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m \u001b[39mfor\u001b[39;49;00m key, example \u001b[39min\u001b[39;49;00m ex_iterable:\n\u001b[1;32m   1380\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures:\n\u001b[1;32m   1381\u001b[0m         \u001b[39m# `IterableDataset` automatically fills missing columns with None.\u001b[39;49;00m\n\u001b[1;32m   1382\u001b[0m         \u001b[39m# This is done with `_apply_feature_types_on_example`.\u001b[39;49;00m\n\u001b[1;32m   1383\u001b[0m         example \u001b[39m=\u001b[39;49m _apply_feature_types_on_example(\n\u001b[1;32m   1384\u001b[0m             example, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures, token_per_repo_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_token_per_repo_id\n\u001b[1;32m   1385\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:982\u001b[0m, in \u001b[0;36mBufferShuffledExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[39m# this is the shuffle buffer that we keep in memory\u001b[39;00m\n\u001b[1;32m    981\u001b[0m mem_buffer \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 982\u001b[0m \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mex_iterable:\n\u001b[1;32m    983\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(mem_buffer) \u001b[39m==\u001b[39;49m buffer_size:  \u001b[39m# if the buffer is full, pick and example from it\u001b[39;49;00m\n\u001b[1;32m    984\u001b[0m         i \u001b[39m=\u001b[39;49m \u001b[39mnext\u001b[39;49m(indices_iterator)\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:678\u001b[0m, in \u001b[0;36mMappedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[39myield from\u001b[39;00m ArrowExamplesIterable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_arrow, {})\n\u001b[1;32m    677\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter()\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:693\u001b[0m, in \u001b[0;36mMappedExamplesIterable._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     format_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatched:\n\u001b[0;32m--> 693\u001b[0m     \u001b[39mfor\u001b[39;49;00m key, example \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m    694\u001b[0m         \u001b[39m# If `batched`, first build the batch, if `batch_size` is None or <=0, then the batch is the whole dataset\u001b[39;49;00m\n\u001b[1;32m    695\u001b[0m         iterator_batch \u001b[39m=\u001b[39;49m (\n\u001b[1;32m    696\u001b[0m             iterator\n\u001b[1;32m    697\u001b[0m             \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m\n\u001b[1;32m    698\u001b[0m             \u001b[39melse\u001b[39;49;00m islice(iterator, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m         key_examples_list \u001b[39m=\u001b[39;49m [(key, example)] \u001b[39m+\u001b[39;49m \u001b[39mlist\u001b[39;49m(iterator_batch)\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:1114\u001b[0m, in \u001b[0;36mTypedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1112\u001b[0m     \u001b[39m# Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     \u001b[39mfor\u001b[39;49;00m key, example \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mex_iterable:\n\u001b[1;32m   1115\u001b[0m         \u001b[39myield\u001b[39;49;00m key, _apply_feature_types_on_example(\n\u001b[1;32m   1116\u001b[0m             example, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures, token_per_repo_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_per_repo_id\n\u001b[1;32m   1117\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:320\u001b[0m, in \u001b[0;36mShuffledDataSourcesArrowExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m kwargs_with_shuffled_shards \u001b[39m=\u001b[39m _shuffle_gen_kwargs(rng, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[1;32m    319\u001b[0m formatter \u001b[39m=\u001b[39m PythonFormatter()\n\u001b[0;32m--> 320\u001b[0m \u001b[39mfor\u001b[39;49;00m key, pa_table \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_tables_fn(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_with_shuffled_shards):\n\u001b[1;32m    321\u001b[0m     \u001b[39mfor\u001b[39;49;00m pa_subtable \u001b[39min\u001b[39;49;00m pa_table\u001b[39m.\u001b[39;49mto_reader(max_chunksize\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mARROW_READER_BATCH_SIZE_IN_DATASET_ITER):\n\u001b[1;32m    322\u001b[0m         formatted_batch \u001b[39m=\u001b[39;49m formatter\u001b[39m.\u001b[39;49mformat_batch(pa_subtable)\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/packaged_modules/parquet/parquet.py:87\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     85\u001b[0m parquet_file \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39mParquetFile(f)\n\u001b[1;32m     86\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mfor\u001b[39;49;00m batch_idx, record_batch \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(\n\u001b[1;32m     88\u001b[0m         parquet_file\u001b[39m.\u001b[39;49miter_batches(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbatch_size, columns\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mcolumns)\n\u001b[1;32m     89\u001b[0m     ):\n\u001b[1;32m     90\u001b[0m         pa_table \u001b[39m=\u001b[39;49m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_batches([record_batch])\n\u001b[1;32m     91\u001b[0m         \u001b[39m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m         \u001b[39m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m         \u001b[39m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/pyarrow/_parquet.pyx:1366\u001b[0m, in \u001b[0;36miter_batches\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/pyarrow/types.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib._datatype_to_pep3118\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:333\u001b[0m, in \u001b[0;36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mfor\u001b[39;00m retry \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_retries \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    332\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m         out \u001b[39m=\u001b[39m read(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    334\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mexcept\u001b[39;00m (ClientError, \u001b[39mTimeoutError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/fsspec/spec.py:1856\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1854\u001b[0m     \u001b[39m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1856\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache\u001b[39m.\u001b[39;49m_fetch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc \u001b[39m+\u001b[39;49m length)\n\u001b[1;32m   1857\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n\u001b[1;32m   1858\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/fsspec/caching.py:189\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    187\u001b[0m     part \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, end \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize)\n\u001b[0;32m--> 189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetcher(start, end)  \u001b[39m# new block replaces old\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m=\u001b[39m start\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache)\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:444\u001b[0m, in \u001b[0;36mHfFileSystemFile._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    433\u001b[0m headers \u001b[39m=\u001b[39m {\n\u001b[1;32m    434\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrange\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbytes=\u001b[39m\u001b[39m{\u001b[39;00mstart\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mend\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    435\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39m_api\u001b[39m.\u001b[39m_build_hf_headers(),\n\u001b[1;32m    436\u001b[0m }\n\u001b[1;32m    437\u001b[0m url \u001b[39m=\u001b[39m hf_hub_url(\n\u001b[1;32m    438\u001b[0m     repo_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolved_path\u001b[39m.\u001b[39mrepo_id,\n\u001b[1;32m    439\u001b[0m     revision\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolved_path\u001b[39m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39mendpoint,\n\u001b[1;32m    443\u001b[0m )\n\u001b[0;32m--> 444\u001b[0m r \u001b[39m=\u001b[39m http_backoff(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    445\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:267\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    266\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    503\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a88fef33-195d-468c-b527-20ad75eccf2d)')"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    cache_name=src_model_activation_hook_point,\n",
    "    layer=src_model_activation_layer,\n",
    "    source_model=src_model,\n",
    "    autoencoder=autoencoder,\n",
    "    source_dataset=source_data,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    activation_resampler=activation_resampler,\n",
    "    source_data_batch_size=8,\n",
    ")\n",
    "\n",
    "pipeline.run_pipeline(\n",
    "    train_batch_size=int(hyperparameters[\"train_batch_size\"]),\n",
    "    max_store_size=1_000_000,\n",
    "    # Sizes for demo purposes (you probably want to scale these by 10x)\n",
    "    max_activations=10_000_000,\n",
    "    resample_frequency=2_500_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98980f3ab4a42ec831c6f4b0eefa4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LearnedActivationsL1Loss</td><td></td></tr><tr><td>LossReducer</td><td></td></tr><tr><td>MSEReconstructionLoss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LearnedActivationsL1Loss</td><td>0.00027</td></tr><tr><td>LossReducer</td><td>0.01715</td></tr><tr><td>MSEReconstructionLoss</td><td>0.01688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-donkey-54</strong> at: <a href='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/1teoia5b' target=\"_blank\">https://wandb.ai/alan-cooney/sparse-autoencoder/runs/1teoia5b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.cache/wandb/run-20231120_211221-1teoia5b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Advice\n",
    "\n",
    "-- Unfinished --\n",
    "\n",
    "- Check recovery loss is low while sparsity is low as well (<20 L1) usually.\n",
    "- Can't be sure features are useful until you dig into them more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "-- Unfinished --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
