{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ai-safety-foundation/sparse_autoencoder/blob/main/docs/content/demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Demo\n",
    "\n",
    "This is a quick start demo to get training a SAE right away. All you need to do is choose a few\n",
    "hyperparameters (like the model to train on), and then set it off.\n",
    "\n",
    "In this demo we'll train a sparse autoencoder on all MLP layer outputs in GPT-2 small (effectively\n",
    "training an SAE on each layer in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab  # noqa: F401 # type: ignore\n",
    "\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "#  Install if in Colab\n",
    "if in_colab:\n",
    "    %pip install sparse_autoencoder transformer_lens transformers wandb\n",
    "\n",
    "# Otherwise enable hot reloading in dev mode\n",
    "if not in_colab:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sparse_autoencoder import (\n",
    "    ActivationResamplerHyperparameters,\n",
    "    AutoencoderHyperparameters,\n",
    "    Hyperparameters,\n",
    "    LossHyperparameters,\n",
    "    Method,\n",
    "    OptimizerHyperparameters,\n",
    "    Parameter,\n",
    "    PipelineHyperparameters,\n",
    "    SourceDataHyperparameters,\n",
    "    SourceModelHyperparameters,\n",
    "    SweepConfig,\n",
    "    sweep,\n",
    ")\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"demo.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize any hyperparameters you want below (by default we're sweeping over l1 coefficient and\n",
    "learning rate).\n",
    "\n",
    "Note we are using the RANDOM sweep approach (try random combinations of hyperparameters), which\n",
    "works surprisingly well but will need to be stopped at some point (as otherwise it will continue\n",
    "forever). If you want to run pre-defined runs consider using `Parameter(values=[0.01, 0.05...])` for\n",
    "example rather than `Parameter(max=0.03, min=0.008)` for each parameter you are sweeping over. You\n",
    "can then set the strategy to `Method.GRID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt_small_mlp_layers(\n",
    "    expansion_factor: int = 4,\n",
    "    n_layers: int = 12,\n",
    ") -> None:\n",
    "    \"\"\"Run a new sweep experiment on GPT 2 Small's MLP layers.\n",
    "\n",
    "    Args:\n",
    "        expansion_factor: Expansion factor for the autoencoder.\n",
    "        n_layers: Number of layers to train on. Max is 12.\n",
    "\n",
    "    \"\"\"\n",
    "    sweep_config = SweepConfig(\n",
    "        parameters=Hyperparameters(\n",
    "            loss=LossHyperparameters(\n",
    "                l1_coefficient=Parameter(max=0.03, min=0.008),\n",
    "            ),\n",
    "            optimizer=OptimizerHyperparameters(\n",
    "                lr=Parameter(max=0.001, min=0.00001),\n",
    "            ),\n",
    "            source_model=SourceModelHyperparameters(\n",
    "                name=Parameter(\"gpt2\"),\n",
    "                cache_names=Parameter(\n",
    "                    [f\"blocks.{layer}.hook_mlp_out\" for layer in range(n_layers)]\n",
    "                ),\n",
    "                hook_dimension=Parameter(768),\n",
    "            ),\n",
    "            source_data=SourceDataHyperparameters(\n",
    "                dataset_path=Parameter(\"alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2\"),\n",
    "                context_size=Parameter(256),\n",
    "                pre_tokenized=Parameter(value=True),\n",
    "                pre_download=Parameter(value=False),  # Default to streaming the dataset\n",
    "            ),\n",
    "            autoencoder=AutoencoderHyperparameters(\n",
    "                expansion_factor=Parameter(value=expansion_factor),\n",
    "                type=Parameter(\"tanh_encoder\"),\n",
    "            ),\n",
    "            pipeline=PipelineHyperparameters(\n",
    "                max_activations=Parameter(1_000_000_000),\n",
    "                checkpoint_frequency=Parameter(100_000_000),\n",
    "                validation_frequency=Parameter(100_000_000),\n",
    "                max_store_size=Parameter(100_000),\n",
    "                source_data_batch_size=Parameter(16),\n",
    "                train_batch_size=Parameter(8192)\n",
    "            ),\n",
    "            activation_resampler=ActivationResamplerHyperparameters(\n",
    "                resample_interval=Parameter(200_000_000),\n",
    "                n_activations_activity_collate=Parameter(100_000_000),\n",
    "                threshold_is_dead_portion_fires=Parameter(1e-6),\n",
    "                max_n_resamples=Parameter(4),\n",
    "                resample_dataset_size=Parameter(81920),\n",
    "            ),\n",
    "        ),\n",
    "        method=Method.RANDOM,\n",
    "    )\n",
    "\n",
    "    sweep(sweep_config=sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start a sweep with just one agent (the current machine). If you have multiple GPUs, it\n",
    "will use them automatically. Similarly it will work on Apple silicon devices by automatically using MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 00f0cyw4\n",
      "Sweep URL: https://wandb.ai/hufy-dev/sparse-autoencoder/sweeps/00f0cyw4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dsm2zn3g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_resampler: {'max_n_resamples': 4, 'n_activations_activity_collate': 100000000, 'resample_dataset_size': 409600, 'resample_interval': 200000000, 'threshold_is_dead_portion_fires': 1e-06}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tautoencoder: {'expansion_factor': 4, 'type': 'tanh_encoder'}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: {'l1_coefficient': 0.02815741075080031}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: {'adam_beta_1': 0.9, 'adam_beta_2': 0.99, 'adam_weight_decay': 0, 'amsgrad': False, 'fused': False, 'lr': 0.0008266150398536362}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpipeline: {'checkpoint_frequency': 100000000, 'log_frequency': 100, 'max_activations': 1000000000, 'max_store_size': 500000, 'num_workers_data_loading': 0, 'source_data_batch_size': 8, 'train_batch_size': 4096, 'validation_frequency': 100000000, 'validation_n_activations': 8192}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_seed: 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsource_data: {'context_size': 256, 'dataset_column_name': 'input_ids', 'dataset_path': 'alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2', 'pre_download': False, 'pre_tokenized': True}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsource_model: {'cache_names': ['blocks.0.hook_mlp_out', 'blocks.1.hook_mlp_out', 'blocks.2.hook_mlp_out', 'blocks.3.hook_mlp_out', 'blocks.4.hook_mlp_out', 'blocks.5.hook_mlp_out', 'blocks.6.hook_mlp_out', 'blocks.7.hook_mlp_out', 'blocks.8.hook_mlp_out', 'blocks.9.hook_mlp_out', 'blocks.10.hook_mlp_out', 'blocks.11.hook_mlp_out'], 'dtype': 'float32', 'hook_dimension': 768, 'name': 'gpt2'}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhufy-dev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hufy/sparse_autoencoder/docs/content/wandb/run-20240331_183640-dsm2zn3g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hufy-dev/sparse-autoencoder/runs/dsm2zn3g' target=\"_blank\">royal-sweep-1</a></strong> to <a href='https://wandb.ai/hufy-dev/sparse-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/hufy-dev/sparse-autoencoder/sweeps/00f0cyw4' target=\"_blank\">https://wandb.ai/hufy-dev/sparse-autoencoder/sweeps/00f0cyw4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hufy-dev/sparse-autoencoder' target=\"_blank\">https://wandb.ai/hufy-dev/sparse-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/hufy-dev/sparse-autoencoder/sweeps/00f0cyw4' target=\"_blank\">https://wandb.ai/hufy-dev/sparse-autoencoder/sweeps/00f0cyw4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hufy-dev/sparse-autoencoder/runs/dsm2zn3g' target=\"_blank\">https://wandb.ai/hufy-dev/sparse-autoencoder/runs/dsm2zn3g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3246f01db11415ba8f4caa918c7c4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a149677f8b3940c9b5cec4cd51ee6f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5663216e6f2429ea586688694d7fb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Activations trained on:   0%|          | 0/1000000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.0.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.0.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.1.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.1.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.2.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.2.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.3.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.3.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.4.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.4.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.5.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.5.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.6.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.6.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.7.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.7.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.8.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.8.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.9.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.9.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.10.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.10.hook_mlp_out': ...})` instead.\n",
      "/home/hufy/.cache/pypoetry/virtualenvs/sparse-autoencoder-PMXxRQv7-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: You called `self.log('train/neuron_activity/blocks.11.hook_mlp_out', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'train/neuron_activity/blocks.11.hook_mlp_out': ...})` instead.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_gpt_small_mlp_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to speed things up? You can trivially add extra machines to the sweep, each of which will peel\n",
    "of some runs from the sweep agent (stored on Wandb). To do this, on another machine simply run:\n",
    "\n",
    "```bash\n",
    "pip install sparse_autoencoder\n",
    "join-sae-sweep --id=SWEEP_ID_SHOWN_ON_WANDB\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31186ba1239ad81afeb3c631b4833e71f34259d3b92eebb37a9091b916e08620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
