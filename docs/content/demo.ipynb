{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ai-safety-foundation/sparse_autoencoder/blob/main/docs/content/demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Demo\n",
    "\n",
    "This is a quick start demo to get training a SAE right away. All you need to do is choose a few\n",
    "hyperparameters (like the model to train on), and then set it off.\n",
    "\n",
    "In this demo we'll train a sparse autoencoder on all MLP layer outputs in GPT-2 small (effectively\n",
    "training an SAE on each layer in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab  # noqa: F401 # type: ignore\n",
    "\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "#  Install if in Colab\n",
    "if in_colab:\n",
    "    %pip install sparse_autoencoder transformer_lens transformers wandb\n",
    "\n",
    "# Otherwise enable hot reloading in dev mode\n",
    "if not in_colab:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sparse_autoencoder import (\n",
    "    ActivationResamplerHyperparameters,\n",
    "    AutoencoderHyperparameters,\n",
    "    Hyperparameters,\n",
    "    LossHyperparameters,\n",
    "    Method,\n",
    "    OptimizerHyperparameters,\n",
    "    Parameter,\n",
    "    PipelineHyperparameters,\n",
    "    SourceDataHyperparameters,\n",
    "    SourceModelHyperparameters,\n",
    "    SweepConfig,\n",
    "    sweep,\n",
    ")\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"demo.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize any hyperparameters you want below (by default we're sweeping over l1 coefficient and\n",
    "learning rate).\n",
    "\n",
    "Note we are using the RANDOM sweep approach (try random combinations of hyperparameters), which\n",
    "works surprisingly well but will need to be stopped at some point (as otherwise it will continue\n",
    "forever). If you want to run pre-defined runs consider using `Parameter(values=[0.01, 0.05...])` for\n",
    "example rather than `Parameter(max=0.03, min=0.008)` for each parameter you are sweeping over. You\n",
    "can then set the strategy to `Method.GRID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt_small_mlp_layers(\n",
    "    expansion_factor: int = 4,\n",
    "    n_layers: int = 12,\n",
    ") -> None:\n",
    "    \"\"\"Run a new sweep experiment on GPT 2 Small's MLP layers.\n",
    "\n",
    "    Args:\n",
    "        expansion_factor: Expansion factor for the autoencoder.\n",
    "        n_layers: Number of layers to train on. Max is 12.\n",
    "\n",
    "    \"\"\"\n",
    "    sweep_config = SweepConfig(\n",
    "        parameters=Hyperparameters(\n",
    "            loss=LossHyperparameters(\n",
    "                l1_coefficient=Parameter(max=0.03, min=0.008),\n",
    "            ),\n",
    "            optimizer=OptimizerHyperparameters(\n",
    "                lr=Parameter(max=0.001, min=0.00001),\n",
    "            ),\n",
    "            source_model=SourceModelHyperparameters(\n",
    "                name=Parameter(\"gpt2\"),\n",
    "                cache_names=Parameter(\n",
    "                    [f\"blocks.{layer}.hook_mlp_out\" for layer in range(n_layers)]\n",
    "                ),\n",
    "                hook_dimension=Parameter(768),\n",
    "            ),\n",
    "            source_data=SourceDataHyperparameters(\n",
    "                dataset_path=Parameter(\"alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2\"),\n",
    "                context_size=Parameter(256),\n",
    "                pre_tokenized=Parameter(value=True),\n",
    "                pre_download=Parameter(value=False),  # Default to streaming the dataset\n",
    "            ),\n",
    "            autoencoder=AutoencoderHyperparameters(\n",
    "                expansion_factor=Parameter(value=expansion_factor)\n",
    "            ),\n",
    "            pipeline=PipelineHyperparameters(\n",
    "                max_activations=Parameter(1_000_000_000),\n",
    "                checkpoint_frequency=Parameter(100_000_000),\n",
    "                validation_frequency=Parameter(100_000_000),\n",
    "                max_store_size=Parameter(1_000_000),\n",
    "            ),\n",
    "            activation_resampler=ActivationResamplerHyperparameters(\n",
    "                resample_interval=Parameter(200_000_000),\n",
    "                n_activations_activity_collate=Parameter(100_000_000),\n",
    "                threshold_is_dead_portion_fires=Parameter(1e-6),\n",
    "                max_n_resamples=Parameter(4),\n",
    "            ),\n",
    "        ),\n",
    "        method=Method.RANDOM,\n",
    "    )\n",
    "\n",
    "    sweep(sweep_config=sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start a sweep with just one agent (the current machine). If you have multiple GPUs, it\n",
    "will use them automatically. Similarly it will work on Apple silicon devices by automatically using MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gpt_small_mlp_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to speed things up? You can trivially add extra machines to the sweep, each of which will peel\n",
    "of some runs from the sweep agent (stored on Wandb). To do this, on another machine simply run:\n",
    "\n",
    "```bash\n",
    "pip install sparse_autoencoder\n",
    "join-sae-sweep --id=SWEEP_ID_SHOWN_ON_WANDB\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31186ba1239ad81afeb3c631b4833e71f34259d3b92eebb37a9091b916e08620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
