{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process datasets\n",
    "\n",
    "When training a sparse autoencoder (SAE) often you want to use a text dataset such as [The\n",
    "Pile](https://huggingface.co/datasets/monology/pile-uncopyrighted). \n",
    "\n",
    "The `TextDataset` class can\n",
    "pre-process this for you on the fly (i.e. tokenize and split into `context_size` chunks of tokens),\n",
    "so that you can get started right away. However, if you're experimenting a lot, it can be nicer to\n",
    "run this once and then save the resulting dataset to HuggingFace. You can then use\n",
    "`PreTokenizedDataset` to load this directly, saving you from running this pre-processing every time\n",
    "you use it.\n",
    "\n",
    "The following code shows you how to do this, and is also used to upload a set of commonly used\n",
    "datasets for SAE training to [Alan Cooney's HuggingFace hub](https://huggingface.co/alancooney)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you will also need to login to HuggingFace via the CLI:\n",
    "\n",
    "```shell\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sparse_autoencoder import TextDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload helper\n",
    "\n",
    "Here we define a helper function to upload multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetToPreprocess:\n",
    "    \"\"\"Dataset to preprocess info.\"\"\"\n",
    "\n",
    "    source_path: str\n",
    "    \"\"\"Source path from HF (e.g. `roneneldan/TinyStories`).\"\"\"\n",
    "\n",
    "    tokenizer_name: str\n",
    "    \"\"\"HF tokenizer name (e.g. `gpt2`).\"\"\"\n",
    "\n",
    "    data_dir: str | None = None\n",
    "    \"\"\"Data directory to download from the source dataset.\"\"\"\n",
    "\n",
    "    data_files: list[str] | None = None\n",
    "    \"\"\"Data files to download from the source dataset.\"\"\"\n",
    "\n",
    "    hugging_face_username: str = \"alancooney\"\n",
    "    \"\"\"HF username for the upload.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def source_alias(self) -> str:\n",
    "        \"\"\"Create a source alias for the destination dataset name.\n",
    "\n",
    "        Returns:\n",
    "            The modified source path as source alias.\n",
    "        \"\"\"\n",
    "        return self.source_path.replace(\"/\", \"-\")\n",
    "\n",
    "    @property\n",
    "    def tokenizer_alias(self) -> str:\n",
    "        \"\"\"Create a tokenizer alias for the destination dataset name.\n",
    "\n",
    "        Returns:\n",
    "            The modified tokenizer name as tokenizer alias.\n",
    "        \"\"\"\n",
    "        return self.tokenizer_name.replace(\"/\", \"-\")\n",
    "\n",
    "    @property\n",
    "    def destination_repo_name(self) -> str:\n",
    "        \"\"\"Destination repo name.\n",
    "\n",
    "        Returns:\n",
    "            The destination repo name.\n",
    "        \"\"\"\n",
    "        return f\"sae-{self.source_alias}-tokenizer-{self.tokenizer_alias}\"\n",
    "\n",
    "    @property\n",
    "    def destination_repo_id(self) -> str:\n",
    "        \"\"\"Destination repo ID.\n",
    "\n",
    "        Returns:\n",
    "            The destination repo ID.\n",
    "        \"\"\"\n",
    "        return f\"{self.hugging_face_username}/{self.destination_repo_name}\"\n",
    "\n",
    "\n",
    "def upload_datasets(datasets_to_preprocess: list[DatasetToPreprocess]) -> None:\n",
    "    \"\"\"Upload datasets to HF.\n",
    "\n",
    "    Warning:\n",
    "        Assumes you have already created the corresponding repos on HF.\n",
    "\n",
    "    Args:\n",
    "        datasets_to_preprocess: List of datasets to preprocess.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the repo doesn't exist.\n",
    "    \"\"\"\n",
    "    repositories_updating = [dataset.destination_repo_id for dataset in datasets_to_preprocess]\n",
    "    print(\"Updating repositories:\\n\" \"\\n\".join(repositories_updating))\n",
    "\n",
    "    for dataset in datasets_to_preprocess:\n",
    "        print(\"Processing dataset: \", dataset.source_path)\n",
    "\n",
    "        # Preprocess\n",
    "        tokenizer = AutoTokenizer.from_pretrained(dataset.tokenizer_name)\n",
    "        text_dataset = TextDataset(\n",
    "            dataset_path=dataset.source_path,\n",
    "            tokenizer=tokenizer,\n",
    "            pre_download=True,  # Must be true to upload after pre-processing, to the hub.\n",
    "            dataset_files=dataset.data_files,\n",
    "            dataset_dir=dataset.data_dir,\n",
    "        )\n",
    "        print(\"Size: \", text_dataset.dataset.size_in_bytes)\n",
    "        print(\"Info: \", text_dataset.dataset.info)\n",
    "\n",
    "        # Upload\n",
    "        text_dataset.push_to_hugging_face_hub(repo_id=dataset.destination_repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2\n",
      "Processing dataset:  monology/pile-uncopyrighted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466ec98b85f04861a4e4de4aa2b23fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b3a53a3dc44e078e2e3eb1a4703c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b0a2a5492445a4aa79702b772dd0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94c8266b9ff447eb631a06c33763ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5721665024e642edb2fcbbeda7f9c0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5899215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets: list[DatasetToPreprocess] = [\n",
    "    DatasetToPreprocess(\n",
    "        source_path=\"roneneldan/TinyStories\",\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        # Get the newer versions (Generated with GPT-4 only)\n",
    "        data_files=[\"TinyStoriesV2-GPT4-train.txt\", \"TinyStoriesV2-GPT4-valid.txt\"],\n",
    "    ),\n",
    "    DatasetToPreprocess(\n",
    "        source_path=\"monology/pile-uncopyrighted\",\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        # Get just the first few (each file is 11GB so this should be enough for a large dataset)\n",
    "        data_files=[\"00.jsonl.zst\"],\n",
    "        data_dir=\"train\",\n",
    "    ),\n",
    "    DatasetToPreprocess(\n",
    "        source_path=\"monology/pile-uncopyrighted\",\n",
    "        tokenizer_name=\"EleutherAI/gpt-neox-20b\",\n",
    "        data_files=[\"00.jsonl.zst\", \"01.jsonl.zst\"],\n",
    "        data_dir=\"train\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "upload_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a dataset is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33df17be57f045959cf133f77f964e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673b26788d6e4757889ee358bc633dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 tokens: They went back to the leopard cage. They saw the same leopard on the same rock. It was still sleeping and did not move. Ben and Lily were very sad. They thought the leopard was boring and did not like them. They were about to leave, when they heard a loud roar. They turned around and saw another leopard coming out of a cave. It was bigger and had more spots than the first leopard. It looked at Ben and Lily and roared again. Ben and Lily were scared and excited at the same time. They had never heard a leopard roar before. They saw the first leopard wake up and look at the second leopard. They saw the second leopard jump on the rock and nuzzle the first leopard. They realized they were a mommy leopard and a daddy leopard. They saw them play and lick and cuddle. Ben and Lily smiled and clapped. They were happy to see the leopards. They thought the leopards were amazing and did like them. They watched them for a long time, until mom said it was time to go home. They waved goodbye to the leopards and thanked them for the show. They got in the car and told mom all about the le\n",
      "256 tokens: Lily sighed and said, \"OK, OK. I'll make a deal with you. If you take the pill, I'll let you keep the teddy bear. And I'll make you another surprise tomorrow. How about that?\" Max thought for a moment and said, \"OK, OK. I'll take the pill. But only if you give me the teddy bear. And the surprise.\" Lily said, \"Deal. Here, take the pill. And here's the teddy bear. And I'll think of a new surprise for you tomorrow. I promise.\" Max took the pill and swallowed it. He said, \"Thank you, Lily. You are the best sister ever. I love you.\" Lily said, \"I love you too, Max. Now, let's cuddle with the teddy bear and take a nap. Maybe you'll feel better when you wake up.\" They hugged each other and the teddy bear and fell asleep. Mommy came in and saw them. She smiled and said, \"What a creative and sweet girl you are, Lily. And what a brave and good boy you are, Max. I'm so proud of you both.\" She kissed them and covered them with a blanket. She said, \"Sweet dreams\n",
      "256 tokens: They walked to the door and entered the aeroplane. They found their seats and put their backpacks under them. They buckled their belts and listened to a voice that told them what to do. Tom looked out the window and saw the wing and the sky. He felt the aeroplane move and heard a loud noise. He closed his eyes and squeezed his mum's hand. His mum said, \"We are taking off, Tom. We are going to fly. Open your eyes and see.\" Tom opened his eyes and saw the ground get smaller and smaller. He saw the clouds and the sun. He saw other aeroplanes and birds. He felt the aeroplane go up and down and left and right. He felt a tickle in his tummy and a smile on his face. He said, \"Mum, this is amazing. I love flying. Thank you for taking me.\" His mum said, \"You're welcome, Tom. I'm proud of you. You didn't resist, Tom. You tried something new. And now you're having fun.\" They hugged and looked out the window. They saw a rainbow and a fair. Tom said, \"Look, mum, a fair. Can we go there when we land?\" His\n"
     ]
    }
   ],
   "source": [
    "downloaded_dataset = load_dataset(\n",
    "    \"alancooney/sae-roneneldan-TinyStories-tokenizer-gpt2\", streaming=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "i = 0\n",
    "first_k = 3\n",
    "for data_item in iter(downloaded_dataset[\"train\"]):  # type:ignore\n",
    "    # Get just the first few\n",
    "    i += 1\n",
    "    if i >= first_k:\n",
    "        break\n",
    "\n",
    "    # Print the decoded items\n",
    "    input_ids = data_item[\"input_ids\"]\n",
    "    decoded = tokenizer.decode(input_ids)\n",
    "    print(f\"{len(input_ids)} tokens: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
