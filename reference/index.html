
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Sparse Autoencoder for Mechanistic Interpretability">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.5">
    
    
      
        <title>Sparse Autoencoder Library - Sparse Autoencoder</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../css/material_extra.css">
    
      <link rel="stylesheet" href="../css/custom_formatting.css">
    
      <link rel="stylesheet" href="../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#sparse-autoencoder-library" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Sparse Autoencoder" class="md-header__button md-logo" aria-label="Sparse Autoencoder" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sparse Autoencoder
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sparse Autoencoder Library
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="amber"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ai-safety-foundation/sparse_autoencoder" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ai-safety-foundation/sparse_autoencoder
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Sparse Autoencoder" class="md-nav__button md-logo" aria-label="Sparse Autoencoder" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sparse Autoencoder
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ai-safety-foundation/sparse_autoencoder" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ai-safety-foundation/sparse_autoencoder
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../demo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demo
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre-process-datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Source dataset pre-processing
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="activation_resampler/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    activation_resampler
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            activation_resampler
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_resampler/activation_resampler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    activation_resampler
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="activation_resampler/utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1_2" id="__nav_4_1_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1_2">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_resampler/utils/component_slice_tensor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    component_slice_tensor
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="activation_store/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    activation_store
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            activation_store
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/base_store/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base_store
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/tensor_store/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tensor_store
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="autoencoder/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    autoencoder
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            autoencoder
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="autoencoder/components/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    components
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_3_1" id="__nav_4_3_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3_1">
            <span class="md-nav__icon md-icon"></span>
            components
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/linear_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linear_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/tied_bias/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tied_bias
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/unit_norm_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    unit_norm_decoder
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    types
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="loss/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    loss
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            loss
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/abstract_loss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_loss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/decoded_activations_l2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decoded_activations_l2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/learned_activations_l1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    learned_activations_l1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/reducer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reducer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    metrics
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            metrics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/abstract_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/metrics_container/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    metrics_container
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/train/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    train
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_5_3" id="__nav_4_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5_3">
            <span class="md-nav__icon md-icon"></span>
            train
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/abstract_train_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_train_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/capacity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    capacity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/feature_density/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    feature_density
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/l0_norm_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    l0_norm_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/neuron_activity_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    neuron_activity_metric
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_5_4" id="__nav_4_5_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5_4">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/utils/add_component_axis_if_missing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    add_component_axis_if_missing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/utils/find_metric_result/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    find_metric_result
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/validate/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    validate
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_5_5" id="__nav_4_5_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5_5">
            <span class="md-nav__icon md-icon"></span>
            validate
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/validate/abstract_validate_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_validate_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/validate/model_reconstruction_score/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    model_reconstruction_score
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="optimizer/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    optimizer
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_6">
            <span class="md-nav__icon md-icon"></span>
            optimizer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizer/abstract_optimizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_optimizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizer/adam_with_reset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    adam_with_reset
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="source_data/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    source_data
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_7" id="__nav_4_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_7">
            <span class="md-nav__icon md-icon"></span>
            source_data
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/abstract_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/mock_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mock_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/pretokenized_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pretokenized_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/text_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_dataset
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="source_model/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    source_model
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_8">
            <span class="md-nav__icon md-icon"></span>
            source_model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/replace_activations_hook/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    replace_activations_hook
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/reshape_activations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reshape_activations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/store_activations_hook/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    store_activations_hook
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/zero_ablate_hook/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_ablate_hook
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tensor_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tensor_types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="train/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    train
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_10" id="__nav_4_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_10">
            <span class="md-nav__icon md-icon"></span>
            train
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/join_sweep/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    join_sweep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/sweep/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sweep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/sweep_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sweep_config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="train/utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_10_5" id="__nav_4_10_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_10_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_10_5">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/utils/get_model_device/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    get_model_device
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/utils/round_down/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    round_down
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/utils/wandb_sweep_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wandb_sweep_types
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_11" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="training_runs/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    training_runs
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_11" id="__nav_4_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_11">
            <span class="md-nav__icon md-icon"></span>
            training_runs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="training_runs/gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_12" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_12" id="__nav_4_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_12">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/tensor_shape/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tensor_shape
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../citation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Citation
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      sparse_autoencoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler" class="md-nav__link">
    <span class="md-ellipsis">
      ActivationResampler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ActivationResampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.neuron_activity_window_end" class="md-nav__link">
    <span class="md-ellipsis">
      neuron_activity_window_end
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.neuron_activity_window_start" class="md-nav__link">
    <span class="md-ellipsis">
      neuron_activity_window_start
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities" class="md-nav__link">
    <span class="md-ellipsis">
      assign_sampling_probabilities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations" class="md-nav__link">
    <span class="md-ellipsis">
      compute_loss_and_get_activations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.renormalize_and_scale" class="md-nav__link">
    <span class="md-ellipsis">
      renormalize_and_scale
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.resample_dead_neurons" class="md-nav__link">
    <span class="md-ellipsis">
      resample_dead_neurons
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.sample_input" class="md-nav__link">
    <span class="md-ellipsis">
      sample_input
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.step_resampler" class="md-nav__link">
    <span class="md-ellipsis">
      step_resampler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResamplerHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      ActivationResamplerHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ActivationResamplerHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResamplerHyperparameters.max_n_resamples" class="md-nav__link">
    <span class="md-ellipsis">
      max_n_resamples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResamplerHyperparameters.n_activations_activity_collate" class="md-nav__link">
    <span class="md-ellipsis">
      n_activations_activity_collate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResamplerHyperparameters.resample_dataset_size" class="md-nav__link">
    <span class="md-ellipsis">
      resample_dataset_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResamplerHyperparameters.resample_interval" class="md-nav__link">
    <span class="md-ellipsis">
      resample_interval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires" class="md-nav__link">
    <span class="md-ellipsis">
      threshold_is_dead_portion_fires
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset" class="md-nav__link">
    <span class="md-ellipsis">
      AdamWithReset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdamWithReset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.parameter_names" class="md-nav__link">
    <span class="md-ellipsis">
      parameter_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state" class="md-nav__link">
    <span class="md-ellipsis">
      reset_neurons_state
    </span>
  </a>
  
    <nav class="md-nav" aria-label="reset_neurons_state">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this" class="md-nav__link">
    <span class="md-ellipsis">
      ... train the model and then resample some dead neurons, then do this ...
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated" class="md-nav__link">
    <span class="md-ellipsis">
      Reset the optimizer state for parameters that have been updated
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_state_all_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_state_all_parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.AutoencoderHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      AutoencoderHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AutoencoderHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AutoencoderHyperparameters.expansion_factor" class="md-nav__link">
    <span class="md-ellipsis">
      expansion_factor
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      CapacityMetric
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CapacityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric.calculate" class="md-nav__link">
    <span class="md-ellipsis">
      calculate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric.capacities" class="md-nav__link">
    <span class="md-ellipsis">
      capacities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric.wandb_capacities_histogram" class="md-nav__link">
    <span class="md-ellipsis">
      wandb_capacities_histogram
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Controller" class="md-nav__link">
    <span class="md-ellipsis">
      Controller
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.ControllerType" class="md-nav__link">
    <span class="md-ellipsis">
      ControllerType
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ControllerType">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ControllerType.CLOUD" class="md-nav__link">
    <span class="md-ellipsis">
      CLOUD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ControllerType.LOCAL" class="md-nav__link">
    <span class="md-ellipsis">
      LOCAL
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Distribution
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Distribution">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.BETA" class="md-nav__link">
    <span class="md-ellipsis">
      BETA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.CATEGORICAL" class="md-nav__link">
    <span class="md-ellipsis">
      CATEGORICAL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.CATEGORICAL_W_PROBABILITIES" class="md-nav__link">
    <span class="md-ellipsis">
      CATEGORICAL_W_PROBABILITIES
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.CONSTANT" class="md-nav__link">
    <span class="md-ellipsis">
      CONSTANT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.INT_UNIFORM" class="md-nav__link">
    <span class="md-ellipsis">
      INT_UNIFORM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.INV_LOG_UNIFORM" class="md-nav__link">
    <span class="md-ellipsis">
      INV_LOG_UNIFORM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Distribution.INV_LOG_UNIFORM_VALUES" class="md-nav__link">
    <span class="md-ellipsis">
      INV_LOG_UNIFORM_VALUES
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Goal" class="md-nav__link">
    <span class="md-ellipsis">
      Goal
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Goal">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Goal.MAXIMIZE" class="md-nav__link">
    <span class="md-ellipsis">
      MAXIMIZE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Goal.MINIMIZE" class="md-nav__link">
    <span class="md-ellipsis">
      MINIMIZE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping" class="md-nav__link">
    <span class="md-ellipsis">
      HyperbandStopping
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HyperbandStopping">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.eta" class="md-nav__link">
    <span class="md-ellipsis">
      eta
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.maxiter" class="md-nav__link">
    <span class="md-ellipsis">
      maxiter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.miniter" class="md-nav__link">
    <span class="md-ellipsis">
      miniter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.s" class="md-nav__link">
    <span class="md-ellipsis">
      s
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.strict" class="md-nav__link">
    <span class="md-ellipsis">
      strict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.__repr__" class="md-nav__link">
    <span class="md-ellipsis">
      __repr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStopping.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStoppingType" class="md-nav__link">
    <span class="md-ellipsis">
      HyperbandStoppingType
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HyperbandStoppingType">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.HyperbandStoppingType.HYPERBAND" class="md-nav__link">
    <span class="md-ellipsis">
      HYPERBAND
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      Hyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Hyperparameters.random_seed" class="md-nav__link">
    <span class="md-ellipsis">
      random_seed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Hyperparameters.__post_init__" class="md-nav__link">
    <span class="md-ellipsis">
      __post_init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Hyperparameters.__repr__" class="md-nav__link">
    <span class="md-ellipsis">
      __repr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Hyperparameters.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Impute" class="md-nav__link">
    <span class="md-ellipsis">
      Impute
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.ImputeWhileRunning" class="md-nav__link">
    <span class="md-ellipsis">
      ImputeWhileRunning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Kind" class="md-nav__link">
    <span class="md-ellipsis">
      Kind
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss" class="md-nav__link">
    <span class="md-ellipsis">
      L2ReconstructionLoss
    </span>
  </a>
  
    <nav class="md-nav" aria-label="L2ReconstructionLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log" class="md-nav__link">
    <span class="md-ellipsis">
      Outputs both loss and metrics to log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss.log_name" class="md-nav__link">
    <span class="md-ellipsis">
      log_name
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss" class="md-nav__link">
    <span class="md-ellipsis">
      LearnedActivationsL1Loss
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LearnedActivationsL1Loss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log" class="md-nav__link">
    <span class="md-ellipsis">
      Returns loss and metrics to log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="md-nav__link">
    <span class="md-ellipsis">
      l1_coefficient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="md-nav__link">
    <span class="md-ellipsis">
      extra_repr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.log_name" class="md-nav__link">
    <span class="md-ellipsis">
      log_name
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.scalar_loss_with_log" class="md-nav__link">
    <span class="md-ellipsis">
      scalar_loss_with_log
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      LossHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LossHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossHyperparameters.l1_coefficient" class="md-nav__link">
    <span class="md-ellipsis">
      l1_coefficient
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer" class="md-nav__link">
    <span class="md-ellipsis">
      LossReducer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LossReducer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__dir__" class="md-nav__link">
    <span class="md-ellipsis">
      __dir__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__getitem__" class="md-nav__link">
    <span class="md-ellipsis">
      __getitem__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__iter__" class="md-nav__link">
    <span class="md-ellipsis">
      __iter__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.log_name" class="md-nav__link">
    <span class="md-ellipsis">
      log_name
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReductionType" class="md-nav__link">
    <span class="md-ellipsis">
      LossReductionType
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Method" class="md-nav__link">
    <span class="md-ellipsis">
      Method
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Method">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Method.BAYES" class="md-nav__link">
    <span class="md-ellipsis">
      BAYES
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Method.CUSTOM" class="md-nav__link">
    <span class="md-ellipsis">
      CUSTOM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Method.GRID" class="md-nav__link">
    <span class="md-ellipsis">
      GRID
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Method.RANDOM" class="md-nav__link">
    <span class="md-ellipsis">
      RANDOM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric" class="md-nav__link">
    <span class="md-ellipsis">
      Metric
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Metric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric.impute" class="md-nav__link">
    <span class="md-ellipsis">
      impute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric.imputewhilerunning" class="md-nav__link">
    <span class="md-ellipsis">
      imputewhilerunning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric.name" class="md-nav__link">
    <span class="md-ellipsis">
      name
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric.target" class="md-nav__link">
    <span class="md-ellipsis">
      target
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric.__repr__" class="md-nav__link">
    <span class="md-ellipsis">
      __repr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Metric.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.NestedParameter" class="md-nav__link">
    <span class="md-ellipsis">
      NestedParameter
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NestedParameter">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.NestedParameter.__dict__" class="md-nav__link">
    <span class="md-ellipsis">
      __dict__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.NestedParameter.to_dict" class="md-nav__link">
    <span class="md-ellipsis">
      to_dict
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      OptimizerHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OptimizerHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.adam_beta_1" class="md-nav__link">
    <span class="md-ellipsis">
      adam_beta_1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.adam_beta_2" class="md-nav__link">
    <span class="md-ellipsis">
      adam_beta_2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.adam_weight_decay" class="md-nav__link">
    <span class="md-ellipsis">
      adam_weight_decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.amsgrad" class="md-nav__link">
    <span class="md-ellipsis">
      amsgrad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.fused" class="md-nav__link">
    <span class="md-ellipsis">
      fused
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.lr" class="md-nav__link">
    <span class="md-ellipsis">
      lr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.OptimizerHyperparameters.lr_scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      lr_scheduler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Parameter">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.distribution" class="md-nav__link">
    <span class="md-ellipsis">
      distribution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.max" class="md-nav__link">
    <span class="md-ellipsis">
      max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.min" class="md-nav__link">
    <span class="md-ellipsis">
      min
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.mu" class="md-nav__link">
    <span class="md-ellipsis">
      mu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.probabilities" class="md-nav__link">
    <span class="md-ellipsis">
      probabilities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.q" class="md-nav__link">
    <span class="md-ellipsis">
      q
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.sigma" class="md-nav__link">
    <span class="md-ellipsis">
      sigma
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.value" class="md-nav__link">
    <span class="md-ellipsis">
      value
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.values" class="md-nav__link">
    <span class="md-ellipsis">
      values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.__repr__" class="md-nav__link">
    <span class="md-ellipsis">
      __repr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Parameter.__str__" class="md-nav__link">
    <span class="md-ellipsis">
      __str__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.activation_resampler" class="md-nav__link">
    <span class="md-ellipsis">
      activation_resampler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      autoencoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.cache_names" class="md-nav__link">
    <span class="md-ellipsis">
      cache_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.layer" class="md-nav__link">
    <span class="md-ellipsis">
      layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.log_frequency" class="md-nav__link">
    <span class="md-ellipsis">
      log_frequency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.loss" class="md-nav__link">
    <span class="md-ellipsis">
      loss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.metrics" class="md-nav__link">
    <span class="md-ellipsis">
      metrics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.n_components" class="md-nav__link">
    <span class="md-ellipsis">
      n_components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.n_input_features" class="md-nav__link">
    <span class="md-ellipsis">
      n_input_features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.n_learned_features" class="md-nav__link">
    <span class="md-ellipsis">
      n_learned_features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.progress_bar" class="md-nav__link">
    <span class="md-ellipsis">
      progress_bar
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.source_data" class="md-nav__link">
    <span class="md-ellipsis">
      source_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.source_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      source_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.source_model" class="md-nav__link">
    <span class="md-ellipsis">
      source_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.total_activations_trained_on" class="md-nav__link">
    <span class="md-ellipsis">
      total_activations_trained_on
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.generate_activations" class="md-nav__link">
    <span class="md-ellipsis">
      generate_activations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.run_pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      run_pipeline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.save_checkpoint" class="md-nav__link">
    <span class="md-ellipsis">
      save_checkpoint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.train_autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      train_autoencoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.update_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      update_parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.validate_sae" class="md-nav__link">
    <span class="md-ellipsis">
      validate_sae
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      PipelineHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PipelineHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.checkpoint_frequency" class="md-nav__link">
    <span class="md-ellipsis">
      checkpoint_frequency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.log_frequency" class="md-nav__link">
    <span class="md-ellipsis">
      log_frequency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.max_activations" class="md-nav__link">
    <span class="md-ellipsis">
      max_activations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.max_store_size" class="md-nav__link">
    <span class="md-ellipsis">
      max_store_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.num_workers_data_loading" class="md-nav__link">
    <span class="md-ellipsis">
      num_workers_data_loading
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.source_data_batch_size" class="md-nav__link">
    <span class="md-ellipsis">
      source_data_batch_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.train_batch_size" class="md-nav__link">
    <span class="md-ellipsis">
      train_batch_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.validation_frequency" class="md-nav__link">
    <span class="md-ellipsis">
      validation_frequency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PipelineHyperparameters.validation_n_activations" class="md-nav__link">
    <span class="md-ellipsis">
      validation_n_activations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.PreTokenizedDataset" class="md-nav__link">
    <span class="md-ellipsis">
      PreTokenizedDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PreTokenizedDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PreTokenizedDataset.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PreTokenizedDataset.preprocess" class="md-nav__link">
    <span class="md-ellipsis">
      preprocess
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      SourceDataHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SourceDataHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.context_size" class="md-nav__link">
    <span class="md-ellipsis">
      context_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_column_name" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_column_name
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_dir" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_dir
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_files" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_files
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_path" class="md-nav__link">
    <span class="md-ellipsis">
      dataset_path
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.pre_download" class="md-nav__link">
    <span class="md-ellipsis">
      pre_download
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.pre_tokenized" class="md-nav__link">
    <span class="md-ellipsis">
      pre_tokenized
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.tokenizer_name" class="md-nav__link">
    <span class="md-ellipsis">
      tokenizer_name
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceDataHyperparameters.__post_init__" class="md-nav__link">
    <span class="md-ellipsis">
      __post_init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceModelHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      SourceModelHyperparameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SourceModelHyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceModelHyperparameters.cache_names" class="md-nav__link">
    <span class="md-ellipsis">
      cache_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceModelHyperparameters.dtype" class="md-nav__link">
    <span class="md-ellipsis">
      dtype
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceModelHyperparameters.hook_dimension" class="md-nav__link">
    <span class="md-ellipsis">
      hook_dimension
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceModelHyperparameters.name" class="md-nav__link">
    <span class="md-ellipsis">
      name
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SourceModelRuntimeHyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      SourceModelRuntimeHyperparameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      SparseAutoencoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SparseAutoencoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.config" class="md-nav__link">
    <span class="md-ellipsis">
      config
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.decoder" class="md-nav__link">
    <span class="md-ellipsis">
      decoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.encoder" class="md-nav__link">
    <span class="md-ellipsis">
      encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      geometric_median_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.post_decoder_bias" class="md-nav__link">
    <span class="md-ellipsis">
      post_decoder_bias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias" class="md-nav__link">
    <span class="md-ellipsis">
      pre_encoder_bias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.reset_optimizer_parameter_details" class="md-nav__link">
    <span class="md-ellipsis">
      reset_optimizer_parameter_details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.tied_bias" class="md-nav__link">
    <span class="md-ellipsis">
      tied_bias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.get_single_component_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      get_single_component_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      initialize_tied_parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.load" class="md-nav__link">
    <span class="md-ellipsis">
      load
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.load_from_hugging_face" class="md-nav__link">
    <span class="md-ellipsis">
      load_from_hugging_face
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.load_from_wandb" class="md-nav__link">
    <span class="md-ellipsis">
      load_from_wandb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.post_backwards_hook" class="md-nav__link">
    <span class="md-ellipsis">
      post_backwards_hook
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.reset_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.save" class="md-nav__link">
    <span class="md-ellipsis">
      save
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.save_to_hugging_face" class="md-nav__link">
    <span class="md-ellipsis">
      save_to_hugging_face
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.save_to_wandb" class="md-nav__link">
    <span class="md-ellipsis">
      save_to_wandb
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoderConfig" class="md-nav__link">
    <span class="md-ellipsis">
      SparseAutoencoderConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SparseAutoencoderConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoderConfig.n_components" class="md-nav__link">
    <span class="md-ellipsis">
      n_components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoderConfig.n_input_features" class="md-nav__link">
    <span class="md-ellipsis">
      n_input_features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoderConfig.n_learned_features" class="md-nav__link">
    <span class="md-ellipsis">
      n_learned_features
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SweepConfig" class="md-nav__link">
    <span class="md-ellipsis">
      SweepConfig
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore" class="md-nav__link">
    <span class="md-ellipsis">
      TensorActivationStore
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorActivationStore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.current_activations_stored_per_component" class="md-nav__link">
    <span class="md-ellipsis">
      current_activations_stored_per_component
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.max_items" class="md-nav__link">
    <span class="md-ellipsis">
      max_items
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.n_components" class="md-nav__link">
    <span class="md-ellipsis">
      n_components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__getitem__" class="md-nav__link">
    <span class="md-ellipsis">
      __getitem__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__sizeof__" class="md-nav__link">
    <span class="md-ellipsis">
      __sizeof__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.append" class="md-nav__link">
    <span class="md-ellipsis">
      append
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.empty" class="md-nav__link">
    <span class="md-ellipsis">
      empty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.extend" class="md-nav__link">
    <span class="md-ellipsis">
      extend
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.shuffle" class="md-nav__link">
    <span class="md-ellipsis">
      shuffle
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset" class="md-nav__link">
    <span class="md-ellipsis">
      TextDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset.preprocess" class="md-nav__link">
    <span class="md-ellipsis">
      preprocess
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset.push_to_hugging_face_hub" class="md-nav__link">
    <span class="md-ellipsis">
      push_to_hugging_face_hub
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      TrainBatchFeatureDensityMetric
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TrainBatchFeatureDensityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate" class="md-nav__link">
    <span class="md-ellipsis">
      calculate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density" class="md-nav__link">
    <span class="md-ellipsis">
      feature_density
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram" class="md-nav__link">
    <span class="md-ellipsis">
      wandb_feature_density_histogram
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.sweep" class="md-nav__link">
    <span class="md-ellipsis">
      sweep
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="sparse-autoencoder-library">Sparse Autoencoder Library<a class="headerlink" href="#sparse-autoencoder-library" title="Permanent link"></a></h1>


<div class="doc doc-object doc-module">



<a id="sparse_autoencoder"></a>
  <div class="doc doc-contents first">
  
      <p>Sparse Autoencoder Library.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.ActivationResampler" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>ActivationResampler</code>


<a href="#sparse_autoencoder.ActivationResampler" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">

  
      <p>Activation resampler.</p>
<p>Collates the number of times each neuron fires over a set number of learned activation vectors,
and then provides the parameters necessary to reset any dead neurons.</p>

<details class="motivation" open>
  <summary>Motivation</summary>
  <p>Over the course of training, a subset of autoencoder neurons will have zero activity across
a large number of datapoints. The authors of <em>Towards Monosemanticity: Decomposing Language
Models With Dictionary Learning</em> found that resampling these dead neurons during training
improves the number of likely-interpretable features (i.e., those in the high density
cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket
Hypothesis and increase the number of chances the network has to find promising feature
directions.</p>
<p>An interesting nuance around dead neurons involves the ultralow density cluster. They found
that if we increase the number of training steps then networks will kill off more of these
ultralow density neurons. This reinforces the use of the high density cluster as a useful
metric because there can exist neurons that are de facto dead but will not appear to be when
looking at the number of dead neurons alone.</p>
<p>This approach is designed to seed new features to fit inputs where the current autoencoder
performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled
neuron will only fire weakly for inputs similar to the one used for its reinitialization.
This was done to minimize interference with the rest of the network.</p>
</details>
<details class="warning" open>
  <summary>Warning</summary>
  <p>The optimizer should be reset after applying this function, as the Adam state will be
incorrect for the modified weights and biases.</p>
</details>
<details class="warning" open>
  <summary>Warning</summary>
  <p>This approach is also known to create sudden loss spikes, and resampling too frequently
causes training to diverge.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ActivationResampler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation resampler.</span>

<span class="sd">    Collates the number of times each neuron fires over a set number of learned activation vectors,</span>
<span class="sd">    and then provides the parameters necessary to reset any dead neurons.</span>

<span class="sd">    Motivation:</span>
<span class="sd">        Over the course of training, a subset of autoencoder neurons will have zero activity across</span>
<span class="sd">        a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language</span>
<span class="sd">        Models With Dictionary Learning* found that resampling these dead neurons during training</span>
<span class="sd">        improves the number of likely-interpretable features (i.e., those in the high density</span>
<span class="sd">        cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket</span>
<span class="sd">        Hypothesis and increase the number of chances the network has to find promising feature</span>
<span class="sd">        directions.</span>

<span class="sd">        An interesting nuance around dead neurons involves the ultralow density cluster. They found</span>
<span class="sd">        that if we increase the number of training steps then networks will kill off more of these</span>
<span class="sd">        ultralow density neurons. This reinforces the use of the high density cluster as a useful</span>
<span class="sd">        metric because there can exist neurons that are de facto dead but will not appear to be when</span>
<span class="sd">        looking at the number of dead neurons alone.</span>

<span class="sd">        This approach is designed to seed new features to fit inputs where the current autoencoder</span>
<span class="sd">        performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled</span>
<span class="sd">        neuron will only fire weakly for inputs similar to the one used for its reinitialization.</span>
<span class="sd">        This was done to minimize interference with the rest of the network.</span>

<span class="sd">    Warning:</span>
<span class="sd">        The optimizer should be reset after applying this function, as the Adam state will be</span>
<span class="sd">        incorrect for the modified weights and biases.</span>

<span class="sd">    Warning:</span>
<span class="sd">        This approach is also known to create sudden loss spikes, and resampling too frequently</span>
<span class="sd">        causes training to diverge.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_activations_seen_since_last_resample</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of activations since we last resampled.&quot;&quot;&quot;</span>

    <span class="n">_collated_neuron_activity</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Collated neuron activity, over the current data collection window.&quot;&quot;&quot;</span>

    <span class="n">_threshold_is_dead_portion_fires</span><span class="p">:</span> <span class="nb">float</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Threshold for determining if a neuron has fired (or is dead).&quot;&quot;&quot;</span>

    <span class="n">_max_n_resamples</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum number of times that resampling should be performed.&quot;&quot;&quot;</span>

    <span class="n">_n_activations_collated_since_last_resample</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of activations collated since we last resampled.</span>

<span class="sd">    Number of vectors used to collate neuron activity, over the current collation window.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_n_components</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of components.&quot;&quot;&quot;</span>

    <span class="n">_n_times_resampled</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of times that resampling has been performed.&quot;&quot;&quot;</span>

    <span class="n">neuron_activity_window_end</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;End of the window for collecting neuron activity.&quot;&quot;&quot;</span>

    <span class="n">neuron_activity_window_start</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Start of the window for collecting neuron activity.&quot;&quot;&quot;</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_learned_features</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">n_components</span><span class="p">:</span> <span class="n">NonNegativeInt</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">resample_interval</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">200_000_000</span><span class="p">,</span>
        <span class="n">max_n_resamples</span><span class="p">:</span> <span class="n">NonNegativeInt</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">n_activations_activity_collate</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">100_000_000</span><span class="p">,</span>
        <span class="n">resample_dataset_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">819_200</span><span class="p">,</span>
        <span class="n">threshold_is_dead_portion_fires</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Field</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ge</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">le</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the activation resampler.</span>

<span class="sd">        Defaults to values used in the Anthropic Towards Monosemanticity paper.</span>

<span class="sd">        Args:</span>
<span class="sd">            n_learned_features: Number of learned features</span>
<span class="sd">            n_components: Number of components that the SAE is being trained on.</span>
<span class="sd">            resample_interval: Interval in number of autoencoder input activation vectors trained</span>
<span class="sd">                on, before resampling.</span>
<span class="sd">            max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.</span>
<span class="sd">                Set to inf if you want to have no limit.</span>
<span class="sd">            n_activations_activity_collate: Number of autoencoder learned activation vectors to</span>
<span class="sd">                collate before resampling (the activation resampler will start collecting on vector</span>
<span class="sd">                $\text{resample_interval} - \text{n_steps_collate}$).</span>
<span class="sd">            resample_dataset_size: Number of autoencoder input activations to use for calculating</span>
<span class="sd">                the loss, as part of the resampling process to create the reset neuron weights.</span>
<span class="sd">            threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has</span>
<span class="sd">                &quot;fired&quot; in less than this portion of the collated sample).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If any of the arguments are invalid (e.g. negative integers).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">n_activations_activity_collate</span> <span class="o">&gt;</span> <span class="n">resample_interval</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;Number of steps to collate must be less than or equal to the resample interval.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span> <span class="o">=</span> <span class="n">resample_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_start</span> <span class="o">=</span> <span class="n">resample_interval</span> <span class="o">-</span> <span class="n">n_activations_activity_collate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_n_resamples</span> <span class="o">=</span> <span class="n">max_n_resamples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span> <span class="o">=</span> <span class="n">resample_dataset_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_threshold_is_dead_portion_fires</span> <span class="o">=</span> <span class="n">threshold_is_dead_portion_fires</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">n_components</span>

    <span class="k">def</span> <span class="nf">_get_dead_neuron_indices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">)]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Identify the indices of neurons that are dead.</span>

<span class="sd">        Identifies any neurons that have fired less than the threshold portion of the collated</span>
<span class="sd">        sample size.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; resampler = ActivationResampler(n_learned_features=6, n_components=2)</span>
<span class="sd">            &gt;&gt;&gt; resampler._collated_neuron_activity = torch.tensor(</span>
<span class="sd">            ...     [[1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 0]]</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; resampler._get_dead_neuron_indices()</span>
<span class="sd">            [tensor([2, 3]), tensor([5])]</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of dead neuron indices for each component.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If no neuron activity has been collated yet.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check we have already collated some neuron activity</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Cannot get dead neuron indices without neuron activity.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Find any neurons that fire less than the threshold portion of times</span>
        <span class="n">threshold_is_dead_n_fires</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_activations_collated_since_last_resample</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_threshold_is_dead_portion_fires</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold_is_dead_n_fires</span><span class="p">)[</span>
                <span class="mi">0</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">component_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_loss_and_get_activations</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LossInputActivationsTuple</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the loss on a random subset of inputs.</span>

<span class="sd">        Motivation:</span>
<span class="sd">            Helps find input vectors that have high SAE loss, so that we can resample dead neurons</span>
<span class="sd">            in a way that improves performance on these specific input vectors.</span>

<span class="sd">        Args:</span>
<span class="sd">            store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of loss per item, and all input activations.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of items in the store is less than the number of inputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">loss_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">input_activations_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
                <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
            <span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>
            <span class="n">n_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span>
            <span class="n">n_batches_required</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_inputs</span> <span class="o">//</span> <span class="n">train_batch_size</span>
            <span class="n">model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)):</span>
                <span class="n">input_activations_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">source_activations</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
                <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source_activations</span><span class="p">)</span>
                <span class="n">loss_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">loss_fn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                        <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">n_batches_required</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="n">loss_per_item</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loss_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
            <span class="n">input_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_activations_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>

            <span class="c1"># Check we generated enough data</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_inputs</span><span class="p">:</span>
                <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot get </span><span class="si">{</span><span class="n">n_inputs</span><span class="si">}</span><span class="s2"> items from the store, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;as only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">)</span><span class="si">}</span><span class="s2"> were available.&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">LossInputActivationsTuple</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">assign_sampling_probabilities</span><span class="p">(</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Assign the sampling probabilities for each input activations vector.</span>

<span class="sd">        Assign each input vector a probability of being picked that is proportional to the square of</span>
<span class="sd">        the autoencoder&#39;s loss on that input.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)</span>
<span class="sd">            tensor([0.0700, 0.2900, 0.6400])</span>

<span class="sd">            &gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])</span>
<span class="sd">            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)</span>
<span class="sd">            tensor([[0.0700, 0.0700],</span>
<span class="sd">                    [0.2900, 0.2900],</span>
<span class="sd">                    [0.6400, 0.6400]])</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: Loss per item.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor of probabilities for each item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">square_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">square_loss</span> <span class="o">/</span> <span class="n">square_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sample_input</span><span class="p">(</span>
        <span class="n">probabilities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)],</span>
        <span class="n">input_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">n_samples</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample an input vector based on the provided probabilities.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; probabilities = torch.tensor([[0.1], [0.2], [0.7]])</span>
<span class="sd">            &gt;&gt;&gt; input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])</span>
<span class="sd">            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(</span>
<span class="sd">            ...     probabilities, input_activations, [2]</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; sampled_input[0].tolist()</span>
<span class="sd">            [[5.0, 6.0], [3.0, 4.0]]</span>

<span class="sd">        Args:</span>
<span class="sd">            probabilities: Probabilities for each input.</span>
<span class="sd">            input_activations: Input activation vectors.</span>
<span class="sd">            n_samples: Number of samples to take (number of dead neurons).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Sampled input activation vector.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of samples is greater than the number of input activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sampled_inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
            <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">component_n_samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">component_probabilities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_component_slice_tensor</span><span class="p">(</span>
                <span class="n">input_tensor</span><span class="o">=</span><span class="n">probabilities</span><span class="p">,</span>
                <span class="n">n_dim_with_component</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">component_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">component_input_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
                <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">get_component_slice_tensor</span><span class="p">(</span>
                <span class="n">input_tensor</span><span class="o">=</span><span class="n">input_activations</span><span class="p">,</span>
                <span class="n">n_dim_with_component</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="n">component_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">component_n_samples</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">component_input_activations</span><span class="p">):</span>
                <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot sample </span><span class="si">{</span><span class="n">component_n_samples</span><span class="si">}</span><span class="s2"> inputs from &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">component_input_activations</span><span class="p">)</span><span class="si">}</span><span class="s2"> input activations.&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

            <span class="c1"># Handle the 0 dead neurons case</span>
            <span class="k">if</span> <span class="n">component_n_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">sampled_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                        <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">component_input_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">component_input_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">component_input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Handle the 1+ dead neuron case</span>
            <span class="n">component_sample_indices</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
                <span class="n">component_probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">component_n_samples</span>
            <span class="p">)</span>
            <span class="n">sampled_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">component_input_activations</span><span class="p">[</span><span class="n">component_sample_indices</span><span class="p">,</span> <span class="p">:])</span>

        <span class="k">return</span> <span class="n">sampled_inputs</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">renormalize_and_scale</span><span class="p">(</span>
        <span class="n">sampled_input</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)],</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
        <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Renormalize and scale the resampled dictionary vectors.</span>

<span class="sd">        Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
<span class="sd">        neurons times 0.2.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; from torch.nn import Parameter</span>
<span class="sd">            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])</span>
<span class="sd">            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])</span>
<span class="sd">            &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))</span>
<span class="sd">            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(</span>
<span class="sd">            ...     sampled_input,</span>
<span class="sd">            ...     neuron_activity,</span>
<span class="sd">            ...     encoder_weight</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; rescaled_input.round(decimals=1)</span>
<span class="sd">            tensor([[0.2000, 0.2000]])</span>

<span class="sd">        Args:</span>
<span class="sd">            sampled_input: Tensor of the sampled input activation.</span>
<span class="sd">            neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">            encoder_weight: Tensor of encoder weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Rescaled sampled input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If there are no alive neurons.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">alive_neuron_mask</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; learned_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="c1"># Check there is at least one alive neuron</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;No alive neurons found.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Handle no dead neurons</span>
        <span class="n">n_dead_neurons</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_dead_neurons</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampled_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

        <span class="c1"># Calculate the average norm of the encoder weights for alive neurons.</span>
        <span class="n">detached_encoder_weight</span> <span class="o">=</span> <span class="n">encoder_weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># Don&#39;t track gradients</span>
        <span class="n">alive_encoder_weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ALIVE_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">detached_encoder_weight</span><span class="p">[</span><span class="n">alive_neuron_mask</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">average_alive_norm</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">SINGLE_ITEM</span><span class="p">]</span> <span class="o">=</span> <span class="n">alive_encoder_weights</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
        <span class="c1"># neurons times 0.2.</span>
        <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">renormalized_input</span> <span class="o">*</span> <span class="p">(</span><span class="n">average_alive_norm</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">        Returns:</span>
<span class="sd">            For each component that the SAE is being trained on, the indices of dead neurons and the</span>
<span class="sd">            updates for the encoder and decoder weights and biases.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">parameter_update_results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">dead_neuron_indices</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
                <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">)]</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dead_neuron_indices</span><span class="p">()</span>

            <span class="c1"># Compute the loss for the current model on a random subset of inputs and get the</span>
            <span class="c1"># activations.</span>
            <span class="n">loss_per_item</span><span class="p">,</span> <span class="n">input_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_and_get_activations</span><span class="p">(</span>
                <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Assign each input vector a probability of being picked that is proportional to the</span>
            <span class="c1"># square of the autoencoder&#39;s loss on that input.</span>
            <span class="n">sample_probabilities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
                <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">)</span>

            <span class="c1"># For each dead neuron sample an input according to these probabilities.</span>
            <span class="n">sampled_input</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
                <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">(</span>
                <span class="n">sample_probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">dead</span><span class="p">)</span> <span class="k">for</span> <span class="n">dead</span> <span class="ow">in</span> <span class="n">dead_neuron_indices</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">component_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">):</span>
                <span class="c1"># Renormalize each input vector to have unit L2 norm and set this to be the</span>
                <span class="c1"># dictionary vector for the dead autoencoder neuron.</span>
                <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
                    <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">dead_decoder_weight_updates</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                    <span class="n">renormalized_input</span><span class="p">,</span> <span class="s2">&quot;dead_neuron input_feature -&gt; input_feature dead_neuron&quot;</span>
                <span class="p">)</span>

                <span class="c1"># For the corresponding encoder vector, renormalize the input vector to equal the</span>
                <span class="c1"># average norm of the encoder weights for alive neurons times 0.2. Set the</span>
                <span class="c1"># corresponding encoder bias element to zero.</span>
                <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
                    <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">get_component_slice_tensor</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span>

                <span class="n">rescaled_sampled_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">renormalize_and_scale</span><span class="p">(</span>
                    <span class="n">sampled_input</span><span class="o">=</span><span class="n">sampled_input</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                    <span class="n">neuron_activity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                    <span class="n">encoder_weight</span><span class="o">=</span><span class="n">encoder_weight</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">dead_encoder_bias_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                    <span class="n">dead_neuron_indices</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">parameter_update_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">ParameterUpdateResults</span><span class="p">(</span>
                        <span class="n">dead_neuron_indices</span><span class="o">=</span><span class="n">dead_neuron_indices</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                        <span class="n">dead_encoder_weight_updates</span><span class="o">=</span><span class="n">rescaled_sampled_input</span><span class="p">,</span>
                        <span class="n">dead_encoder_bias_updates</span><span class="o">=</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
                        <span class="n">dead_decoder_weight_updates</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="n">parameter_update_results</span>

    <span class="k">def</span> <span class="nf">step_resampler</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_neuron_activity</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
        <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Step the resampler, collating neuron activity and resampling if necessary.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_neuron_activity: Number of times each neuron fired in the current batch.</span>
<span class="sd">            activation_store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Parameter update results (for each component that the SAE is being trained on) if</span>
<span class="sd">            resampling is due. Otherwise None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update the counter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_times_resampled</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_n_resamples</span><span class="p">:</span>
            <span class="c1"># Collate neuron activity, if in the data collection window. For example in the</span>
            <span class="c1"># Anthropic Towards Monosemanticity paper, the window started collecting at 100m</span>
            <span class="c1"># activations and stopped at 200m (and then repeated this again a few times until the</span>
            <span class="c1"># max times to resample was hit).</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_start</span><span class="p">:</span>
                <span class="n">detached_neuron_activity</span> <span class="o">=</span> <span class="n">batch_neuron_activity</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">detached_neuron_activity</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_activations_collated_since_last_resample</span> <span class="o">+=</span> <span class="n">train_batch_size</span>

            <span class="c1"># Check if we should resample.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span><span class="p">:</span>
                <span class="c1"># Get resampled dictionary vectors</span>
                <span class="n">resample_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_dead_neurons</span><span class="p">(</span>
                    <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                    <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                    <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># Update counters</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_activations_collated_since_last_resample</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_times_resampled</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Reset the collated neuron activity</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

                <span class="k">return</span> <span class="n">resample_res</span>

        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a string representation of the activation resampler.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;ActivationResampler(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;n_components=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;neuron_activity_window_start=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;neuron_activity_window_end=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;max_resamples=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_n_resamples</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;resample_dataset_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;dead_neuron_threshold=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_threshold_is_dead_portion_fires</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResampler.neuron_activity_window_end" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">neuron_activity_window_end</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">resample_interval</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.neuron_activity_window_end" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>End of the window for collecting neuron activity.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResampler.neuron_activity_window_start" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">neuron_activity_window_start</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">resample_interval</span> <span class="o">-</span> <span class="n">n_activations_activity_collate</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.neuron_activity_window_start" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Start of the window for collecting neuron activity.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">resample_interval</span><span class="o">=</span><span class="mi">200000000</span><span class="p">,</span> <span class="n">max_n_resamples</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_activations_activity_collate</span><span class="o">=</span><span class="mi">100000000</span><span class="p">,</span> <span class="n">resample_dataset_size</span><span class="o">=</span><span class="mi">819200</span><span class="p">,</span> <span class="n">threshold_is_dead_portion_fires</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ActivationResampler.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the activation resampler.</p>
<p>Defaults to values used in the Anthropic Towards Monosemanticity paper.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>n_learned_features</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of learned features</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_components</code></td>
          <td>
                <code><span title="pydantic.NonNegativeInt">NonNegativeInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of components that the SAE is being trained on.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>resample_interval</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Interval in number of autoencoder input activation vectors trained
on, before resampling.</p>
            </div>
          </td>
          <td>
                <code>200000000</code>
          </td>
        </tr>
        <tr>
          <td><code>max_n_resamples</code></td>
          <td>
                <code><span title="pydantic.NonNegativeInt">NonNegativeInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum number of resamples to perform throughout the entire pipeline.
Set to inf if you want to have no limit.</p>
            </div>
          </td>
          <td>
                <code>4</code>
          </td>
        </tr>
        <tr>
          <td><code>n_activations_activity_collate</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of autoencoder learned activation vectors to
collate before resampling (the activation resampler will start collecting on vector
<span class="arithmatex">\(\text{resample_interval} - \text{n_steps_collate}\)</span>).</p>
            </div>
          </td>
          <td>
                <code>100000000</code>
          </td>
        </tr>
        <tr>
          <td><code>resample_dataset_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of autoencoder input activations to use for calculating
the loss, as part of the resampling process to create the reset neuron weights.</p>
            </div>
          </td>
          <td>
                <code>819200</code>
          </td>
        </tr>
        <tr>
          <td><code>threshold_is_dead_portion_fires</code></td>
          <td>
                <code><span title="typing.Annotated">Annotated</span>[float, <span title="pydantic.Field">Field</span>(strict=True, ge=0, le=1)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Threshold for determining if a neuron is dead (has
"fired" in less than this portion of the collated sample).</p>
            </div>
          </td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If any of the arguments are invalid (e.g. negative integers).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">n_learned_features</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">n_components</span><span class="p">:</span> <span class="n">NonNegativeInt</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">resample_interval</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">200_000_000</span><span class="p">,</span>
    <span class="n">max_n_resamples</span><span class="p">:</span> <span class="n">NonNegativeInt</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_activations_activity_collate</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">100_000_000</span><span class="p">,</span>
    <span class="n">resample_dataset_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">819_200</span><span class="p">,</span>
    <span class="n">threshold_is_dead_portion_fires</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Field</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ge</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">le</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the activation resampler.</span>

<span class="sd">    Defaults to values used in the Anthropic Towards Monosemanticity paper.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_learned_features: Number of learned features</span>
<span class="sd">        n_components: Number of components that the SAE is being trained on.</span>
<span class="sd">        resample_interval: Interval in number of autoencoder input activation vectors trained</span>
<span class="sd">            on, before resampling.</span>
<span class="sd">        max_n_resamples: Maximum number of resamples to perform throughout the entire pipeline.</span>
<span class="sd">            Set to inf if you want to have no limit.</span>
<span class="sd">        n_activations_activity_collate: Number of autoencoder learned activation vectors to</span>
<span class="sd">            collate before resampling (the activation resampler will start collecting on vector</span>
<span class="sd">            $\text{resample_interval} - \text{n_steps_collate}$).</span>
<span class="sd">        resample_dataset_size: Number of autoencoder input activations to use for calculating</span>
<span class="sd">            the loss, as part of the resampling process to create the reset neuron weights.</span>
<span class="sd">        threshold_is_dead_portion_fires: Threshold for determining if a neuron is dead (has</span>
<span class="sd">            &quot;fired&quot; in less than this portion of the collated sample).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If any of the arguments are invalid (e.g. negative integers).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n_activations_activity_collate</span> <span class="o">&gt;</span> <span class="n">resample_interval</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Number of steps to collate must be less than or equal to the resample interval.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span> <span class="o">=</span> <span class="n">resample_interval</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_start</span> <span class="o">=</span> <span class="n">resample_interval</span> <span class="o">-</span> <span class="n">n_activations_activity_collate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_n_resamples</span> <span class="o">=</span> <span class="n">max_n_resamples</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span> <span class="o">=</span> <span class="n">resample_dataset_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_threshold_is_dead_portion_fires</span> <span class="o">=</span> <span class="n">threshold_is_dead_portion_fires</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">n_components</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.__str__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__str__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ActivationResampler.__str__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Return a string representation of the activation resampler.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return a string representation of the activation resampler.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;ActivationResampler(&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;n_components=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;neuron_activity_window_start=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;neuron_activity_window_end=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;max_resamples=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_n_resamples</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;resample_dataset_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;dead_neuron_threshold=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_threshold_is_dead_portion_fires</span><span class="si">}</span><span class="s2">)&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.assign_sampling_probabilities" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Assign the sampling probabilities for each input activations vector.</p>
<p>Assign each input vector a probability of being picked that is proportional to the square of
the autoencoder's loss on that input.</p>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ActivationResampler</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([0.0700, 0.2900, 0.6400])</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ActivationResampler</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[0.0700, 0.0700],</span>
<span class="go">        [0.2900, 0.2900],</span>
<span class="go">        [0.6400, 0.6400]])</span>
</code></pre></div>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>loss</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss per item.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tensor of probabilities for each item.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">assign_sampling_probabilities</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Assign the sampling probabilities for each input activations vector.</span>

<span class="sd">    Assign each input vector a probability of being picked that is proportional to the square of</span>
<span class="sd">    the autoencoder&#39;s loss on that input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)</span>
<span class="sd">        tensor([0.0700, 0.2900, 0.6400])</span>

<span class="sd">        &gt;&gt;&gt; loss = torch.tensor([[1.0, 2], [2, 4], [3, 6]])</span>
<span class="sd">        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=2)</span>
<span class="sd">        tensor([[0.0700, 0.0700],</span>
<span class="sd">                [0.2900, 0.2900],</span>
<span class="sd">                [0.6400, 0.6400]])</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: Loss per item.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor of probabilities for each item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">square_loss</span> <span class="o">/</span> <span class="n">square_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">compute_loss_and_get_activations</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Compute the loss on a random subset of inputs.</p>

<details class="motivation" open>
  <summary>Motivation</summary>
  <p>Helps find input vectors that have high SAE loss, so that we can resample dead neurons
in a way that improves performance on these specific input vectors.</p>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a> | <span title="torch.nn.parallel.DataParallel">DataParallel</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss_fn</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size (also used for resampling).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.LossInputActivationsTuple" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.LossInputActivationsTuple">LossInputActivationsTuple</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tuple of loss per item, and all input activations.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the number of items in the store is less than the number of inputs</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_loss_and_get_activations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LossInputActivationsTuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the loss on a random subset of inputs.</span>

<span class="sd">    Motivation:</span>
<span class="sd">        Helps find input vectors that have high SAE loss, so that we can resample dead neurons</span>
<span class="sd">        in a way that improves performance on these specific input vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">        store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of loss per item, and all input activations.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of items in the store is less than the number of inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">loss_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">input_activations_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
            <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>
        <span class="n">n_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span>
        <span class="n">n_batches_required</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_inputs</span> <span class="o">//</span> <span class="n">train_batch_size</span>
        <span class="n">model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)):</span>
            <span class="n">input_activations_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">source_activations</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
            <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source_activations</span><span class="p">)</span>
            <span class="n">loss_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">loss_fn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                    <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">n_batches_required</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">loss_per_item</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loss_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
        <span class="n">input_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_activations_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>

        <span class="c1"># Check we generated enough data</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_inputs</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot get </span><span class="si">{</span><span class="n">n_inputs</span><span class="si">}</span><span class="s2"> items from the store, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;as only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">)</span><span class="si">}</span><span class="s2"> were available.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">LossInputActivationsTuple</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.renormalize_and_scale" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">renormalize_and_scale</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.renormalize_and_scale" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Renormalize and scale the resampled dictionary vectors.</p>
<p>Renormalize the input vector to equal the average norm of the encoder weights for alive
neurons times 0.2.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>from torch.nn import Parameter
_seed = torch.manual_seed(0)  # For reproducibility in example
sampled_input = torch.tensor([[3.0, 4.0]])
neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])
encoder_weight = Parameter(torch.ones((6, 2)))
rescaled_input = ActivationResampler.renormalize_and_scale(
...     sampled_input,
...     neuron_activity,
...     encoder_weight
... )
rescaled_input.round(decimals=1)
tensor([[0.2000, 0.2000]])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sampled_input</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE">DEAD_FEATURE</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor of the sampled input activation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity</code></td>
          <td>
                <code><span title="jaxtyping.Int64">Int64</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor representing the number of times each neuron fired.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>encoder_weight</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor of encoder weights.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE">DEAD_FEATURE</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Rescaled sampled input.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there are no alive neurons.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">renormalize_and_scale</span><span class="p">(</span>
    <span class="n">sampled_input</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)],</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
    <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Renormalize and scale the resampled dictionary vectors.</span>

<span class="sd">    Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
<span class="sd">    neurons times 0.2.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from torch.nn import Parameter</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])</span>
<span class="sd">        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])</span>
<span class="sd">        &gt;&gt;&gt; encoder_weight = Parameter(torch.ones((6, 2)))</span>
<span class="sd">        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(</span>
<span class="sd">        ...     sampled_input,</span>
<span class="sd">        ...     neuron_activity,</span>
<span class="sd">        ...     encoder_weight</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rescaled_input.round(decimals=1)</span>
<span class="sd">        tensor([[0.2000, 0.2000]])</span>

<span class="sd">    Args:</span>
<span class="sd">        sampled_input: Tensor of the sampled input activation.</span>
<span class="sd">        neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">        encoder_weight: Tensor of encoder weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Rescaled sampled input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If there are no alive neurons.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alive_neuron_mask</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; learned_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="c1"># Check there is at least one alive neuron</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;No alive neurons found.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Handle no dead neurons</span>
    <span class="n">n_dead_neurons</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n_dead_neurons</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampled_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="c1"># Calculate the average norm of the encoder weights for alive neurons.</span>
    <span class="n">detached_encoder_weight</span> <span class="o">=</span> <span class="n">encoder_weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># Don&#39;t track gradients</span>
    <span class="n">alive_encoder_weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ALIVE_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">detached_encoder_weight</span><span class="p">[</span><span class="n">alive_neuron_mask</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">average_alive_norm</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">SINGLE_ITEM</span><span class="p">]</span> <span class="o">=</span> <span class="n">alive_encoder_weights</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
    <span class="c1"># neurons times 0.2.</span>
    <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">renormalized_input</span> <span class="o">*</span> <span class="p">(</span><span class="n">average_alive_norm</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.resample_dead_neurons" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">resample_dead_neurons</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ActivationResampler.resample_dead_neurons" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Resample dead neurons.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a> | <span title="torch.nn.parallel.DataParallel">DataParallel</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss_fn</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size (also used for resampling).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults">ParameterUpdateResults</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>For each component that the SAE is being trained on, the indices of dead neurons and the</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults">ParameterUpdateResults</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>updates for the encoder and decoder weights and biases.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">    Returns:</span>
<span class="sd">        For each component that the SAE is being trained on, the indices of dead neurons and the</span>
<span class="sd">        updates for the encoder and decoder weights and biases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parameter_update_results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">dead_neuron_indices</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
            <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">)]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dead_neuron_indices</span><span class="p">()</span>

        <span class="c1"># Compute the loss for the current model on a random subset of inputs and get the</span>
        <span class="c1"># activations.</span>
        <span class="n">loss_per_item</span><span class="p">,</span> <span class="n">input_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_and_get_activations</span><span class="p">(</span>
            <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
            <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
            <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Assign each input vector a probability of being picked that is proportional to the</span>
        <span class="c1"># square of the autoencoder&#39;s loss on that input.</span>
        <span class="n">sample_probabilities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss_per_item</span><span class="p">)</span>

        <span class="c1"># For each dead neuron sample an input according to these probabilities.</span>
        <span class="n">sampled_input</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
            <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">(</span>
            <span class="n">sample_probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">dead</span><span class="p">)</span> <span class="k">for</span> <span class="n">dead</span> <span class="ow">in</span> <span class="n">dead_neuron_indices</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">component_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">):</span>
            <span class="c1"># Renormalize each input vector to have unit L2 norm and set this to be the</span>
            <span class="c1"># dictionary vector for the dead autoencoder neuron.</span>
            <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
                <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">dead_decoder_weight_updates</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                <span class="n">renormalized_input</span><span class="p">,</span> <span class="s2">&quot;dead_neuron input_feature -&gt; input_feature dead_neuron&quot;</span>
            <span class="p">)</span>

            <span class="c1"># For the corresponding encoder vector, renormalize the input vector to equal the</span>
            <span class="c1"># average norm of the encoder weights for alive neurons times 0.2. Set the</span>
            <span class="c1"># corresponding encoder bias element to zero.</span>
            <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
                <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">get_component_slice_tensor</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span>

            <span class="n">rescaled_sampled_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">renormalize_and_scale</span><span class="p">(</span>
                <span class="n">sampled_input</span><span class="o">=</span><span class="n">sampled_input</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                <span class="n">neuron_activity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                <span class="n">encoder_weight</span><span class="o">=</span><span class="n">encoder_weight</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">dead_encoder_bias_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                <span class="n">dead_neuron_indices</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">parameter_update_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ParameterUpdateResults</span><span class="p">(</span>
                    <span class="n">dead_neuron_indices</span><span class="o">=</span><span class="n">dead_neuron_indices</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span>
                    <span class="n">dead_encoder_weight_updates</span><span class="o">=</span><span class="n">rescaled_sampled_input</span><span class="p">,</span>
                    <span class="n">dead_encoder_bias_updates</span><span class="o">=</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
                    <span class="n">dead_decoder_weight_updates</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">parameter_update_results</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.sample_input" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">sample_input</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.sample_input" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Sample an input vector based on the provided probabilities.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>probabilities = torch.tensor([[0.1], [0.2], [0.7]])
input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])
_seed = torch.manual_seed(0)  # For reproducibility in example
sampled_input = ActivationResampler.sample_input(
...     probabilities, input_activations, [2]
... )
sampled_input[0].tolist()
[[5.0, 6.0], [3.0, 4.0]]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>probabilities</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Probabilities for each input.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>input_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Input activation vectors.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_samples</code></td>
          <td>
                <code>list[int]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of samples to take (number of dead neurons).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE">DEAD_FEATURE</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sampled input activation vector.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the number of samples is greater than the number of input activations.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_input</span><span class="p">(</span>
    <span class="n">probabilities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)],</span>
    <span class="n">input_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample an input vector based on the provided probabilities.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; probabilities = torch.tensor([[0.1], [0.2], [0.7]])</span>
<span class="sd">        &gt;&gt;&gt; input_activations = torch.tensor([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]])</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(</span>
<span class="sd">        ...     probabilities, input_activations, [2]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; sampled_input[0].tolist()</span>
<span class="sd">        [[5.0, 6.0], [3.0, 4.0]]</span>

<span class="sd">    Args:</span>
<span class="sd">        probabilities: Probabilities for each input.</span>
<span class="sd">        input_activations: Input activation vectors.</span>
<span class="sd">        n_samples: Number of samples to take (number of dead neurons).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sampled input activation vector.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of samples is greater than the number of input activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sampled_inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
        <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">DEAD_FEATURE</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">component_n_samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">component_probabilities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_component_slice_tensor</span><span class="p">(</span>
            <span class="n">input_tensor</span><span class="o">=</span><span class="n">probabilities</span><span class="p">,</span>
            <span class="n">n_dim_with_component</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">component_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">component_input_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">get_component_slice_tensor</span><span class="p">(</span>
            <span class="n">input_tensor</span><span class="o">=</span><span class="n">input_activations</span><span class="p">,</span>
            <span class="n">n_dim_with_component</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">component_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">component_n_samples</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">component_input_activations</span><span class="p">):</span>
            <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot sample </span><span class="si">{</span><span class="n">component_n_samples</span><span class="si">}</span><span class="s2"> inputs from &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">component_input_activations</span><span class="p">)</span><span class="si">}</span><span class="s2"> input activations.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

        <span class="c1"># Handle the 0 dead neurons case</span>
        <span class="k">if</span> <span class="n">component_n_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">sampled_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">component_input_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">component_input_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">component_input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="k">continue</span>

        <span class="c1"># Handle the 1+ dead neuron case</span>
        <span class="n">component_sample_indices</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
            <span class="n">component_probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">component_n_samples</span>
        <span class="p">)</span>
        <span class="n">sampled_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">component_input_activations</span><span class="p">[</span><span class="n">component_sample_indices</span><span class="p">,</span> <span class="p">:])</span>

    <span class="k">return</span> <span class="n">sampled_inputs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.ActivationResampler.step_resampler" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">step_resampler</span><span class="p">(</span><span class="n">batch_neuron_activity</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ActivationResampler.step_resampler" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Step the resampler, collating neuron activity and resampling if necessary.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch_neuron_activity</code></td>
          <td>
                <code><span title="jaxtyping.Int64">Int64</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each neuron fired in the current batch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>activation_store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a> | <span title="torch.nn.parallel.DataParallel">DataParallel</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss_fn</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size (also used for resampling).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults">ParameterUpdateResults</a>] | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Parameter update results (for each component that the SAE is being trained on) if</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults">ParameterUpdateResults</a>] | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>resampling is due. Otherwise None.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">step_resampler</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch_neuron_activity</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Step the resampler, collating neuron activity and resampling if necessary.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_neuron_activity: Number of times each neuron fired in the current batch.</span>
<span class="sd">        activation_store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Parameter update results (for each component that the SAE is being trained on) if</span>
<span class="sd">        resampling is due. Otherwise None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Update the counter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_times_resampled</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_n_resamples</span><span class="p">:</span>
        <span class="c1"># Collate neuron activity, if in the data collection window. For example in the</span>
        <span class="c1"># Anthropic Towards Monosemanticity paper, the window started collecting at 100m</span>
        <span class="c1"># activations and stopped at 200m (and then repeated this again a few times until the</span>
        <span class="c1"># max times to resample was hit).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_start</span><span class="p">:</span>
            <span class="n">detached_neuron_activity</span> <span class="o">=</span> <span class="n">batch_neuron_activity</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">detached_neuron_activity</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_activations_collated_since_last_resample</span> <span class="o">+=</span> <span class="n">train_batch_size</span>

        <span class="c1"># Check if we should resample.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neuron_activity_window_end</span><span class="p">:</span>
            <span class="c1"># Get resampled dictionary vectors</span>
            <span class="n">resample_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_dead_neurons</span><span class="p">(</span>
                <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Update counters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_activations_seen_since_last_resample</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_activations_collated_since_last_resample</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_times_resampled</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Reset the collated neuron activity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collated_neuron_activity</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="k">return</span> <span class="n">resample_res</span>

    <span class="k">return</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.ActivationResamplerHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>ActivationResamplerHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResamplerHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Activation resampler hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ActivationResamplerHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation resampler hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">resample_interval</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mi">200_000_000</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">))</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample interval.&quot;&quot;&quot;</span>

    <span class="n">max_n_resamples</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum number of resamples.&quot;&quot;&quot;</span>

    <span class="n">n_activations_activity_collate</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mi">100_000_000</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">))</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of steps to collate before resampling.</span>

<span class="sd">    Number of autoencoder learned activation vectors to collate before resampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">resample_dataset_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_BATCH_SIZE</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample dataset size.</span>

<span class="sd">    Number of autoencoder input activations to use for calculating the loss, as part of the</span>
<span class="sd">    resampling process to create the reset neuron weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">threshold_is_dead_portion_fires</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dead neuron threshold.</span>

<span class="sd">    Threshold for determining if a neuron is dead (has &quot;fired&quot; in less than this portion of the</span>
<span class="sd">    collated sample).</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResamplerHyperparameters.max_n_resamples" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">max_n_resamples</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResamplerHyperparameters.max_n_resamples" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Maximum number of resamples.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResamplerHyperparameters.n_activations_activity_collate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_activations_activity_collate</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mi">100000000</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">)))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResamplerHyperparameters.n_activations_activity_collate" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of steps to collate before resampling.</p>
<p>Number of autoencoder learned activation vectors to collate before resampling.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResamplerHyperparameters.resample_dataset_size" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">resample_dataset_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_BATCH_SIZE</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResamplerHyperparameters.resample_dataset_size" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Resample dataset size.</p>
<p>Number of autoencoder input activations to use for calculating the loss, as part of the
resampling process to create the reset neuron weights.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResamplerHyperparameters.resample_interval" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">resample_interval</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mi">200000000</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">)))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResamplerHyperparameters.resample_interval" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Resample interval.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">threshold_is_dead_portion_fires</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResamplerHyperparameters.threshold_is_dead_portion_fires" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Dead neuron threshold.</p>
<p>Threshold for determining if a neuron is dead (has "fired" in less than this portion of the
collated sample).</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.AdamWithReset" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>AdamWithReset</code>


<a href="#sparse_autoencoder.AdamWithReset" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.optim.Adam">Adam</span></code>, <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset" href="optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset">AbstractOptimizerWithReset</a></code></p>

  
      <p>Adam Optimizer with a reset method.</p>
<p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when
manually editing the model parameters during training (e.g. when resampling dead neurons). This
is because Adam maintains running averages of the gradients and the squares of gradients, which
will be incorrect if the parameters are changed.</p>
<p>Otherwise this is the same as the standard Adam optimizer.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">AdamWithReset</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adam Optimizer with a reset method.</span>

<span class="sd">    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when</span>
<span class="sd">    manually editing the model parameters during training (e.g. when resampling dead neurons). This</span>
<span class="sd">    is because Adam maintains running averages of the gradients and the squares of gradients, which</span>
<span class="sd">    will be incorrect if the parameters are changed.</span>

<span class="sd">    Otherwise this is the same as the standard Adam optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">parameter_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parameter Names.</span>

<span class="sd">    The names of the parameters, so that we can find them later when resetting the state.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_has_components_dim</span><span class="p">:</span> <span class="nb">bool</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Whether the parameters have a components dimension.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># (extending existing implementation)</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">params_t</span><span class="p">,</span>
        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">SINGLE_ITEM</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">betas</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">foreach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">named_parameters</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]],</span>
        <span class="n">has_components_dim</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the optimizer.</span>

<span class="sd">        Warning:</span>
<span class="sd">            Named parameters must be with default settings (remove duplicates and not recursive).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (</span>
<span class="sd">            ...     SparseAutoencoder, SparseAutoencoderConfig</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; model = SparseAutoencoder(</span>
<span class="sd">            ...        SparseAutoencoderConfig(</span>
<span class="sd">            ...             n_input_features=5,</span>
<span class="sd">            ...             n_learned_features=10,</span>
<span class="sd">            ...             n_components=2</span>
<span class="sd">            ...         )</span>
<span class="sd">            ...    )</span>
<span class="sd">            &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">            ...     model.parameters(),</span>
<span class="sd">            ...     named_parameters=model.named_parameters(),</span>
<span class="sd">            ...     has_components_dim=True,</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_state_all_parameters()</span>

<span class="sd">        Args:</span>
<span class="sd">            params: Iterable of parameters to optimize or dicts defining parameter groups.</span>
<span class="sd">            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a</span>
<span class="sd">                float LR unless specifying fused=True or capturable=True.</span>
<span class="sd">            betas: Coefficients used for computing running averages of gradient and its square.</span>
<span class="sd">            eps: Term added to the denominator to improve numerical stability.</span>
<span class="sd">            weight_decay: Weight decay (L2 penalty).</span>
<span class="sd">            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper &quot;On the</span>
<span class="sd">                Convergence of Adam and Beyond&quot;.</span>
<span class="sd">            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used</span>
<span class="sd">                over the for-loop implementation on CUDA if more performant. Note that foreach uses</span>
<span class="sd">                more peak memory.</span>
<span class="sd">            maximize: If True, maximizes the parameters based on the objective, instead of</span>
<span class="sd">                minimizing.</span>
<span class="sd">            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair</span>
<span class="sd">                ungraphed performance.</span>
<span class="sd">            differentiable: Whether autograd should occur through the optimizer step in training.</span>
<span class="sd">                Setting to True can impair performance.</span>
<span class="sd">            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,</span>
<span class="sd">                torch.float32, torch.float16, and torch.bfloat16.</span>
<span class="sd">            named_parameters: An iterator over the named parameters of the model. This is used to</span>
<span class="sd">                find the parameters when resetting their state. You should set this as</span>
<span class="sd">                `model.named_parameters()`.</span>
<span class="sd">            has_components_dim: If the parameters have a components dimension (i.e. if you are</span>
<span class="sd">                training an SAE on more than one component).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of parameter names does not match the number of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialise the parent class (note we repeat the parameter names so that type hints work).</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span>
            <span class="n">foreach</span><span class="o">=</span><span class="n">foreach</span><span class="p">,</span>
            <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">,</span>
            <span class="n">capturable</span><span class="o">=</span><span class="n">capturable</span><span class="p">,</span>
            <span class="n">differentiable</span><span class="o">=</span><span class="n">differentiable</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="n">fused</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span> <span class="o">=</span> <span class="n">has_components_dim</span>

        <span class="c1"># Store the names of the parameters, so that we can find them later when resetting the</span>
        <span class="c1"># state.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;The number of parameter names does not match the number of parameters. &quot;</span>
                <span class="s2">&quot;If using model.named_parameters() make sure remove_duplicates is True &quot;</span>
                <span class="s2">&quot;and recursive is False (the default settings).&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_state_all_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the state for all parameters.</span>

<span class="sd">        Iterates over all parameters and resets both the running averages of the gradients and the</span>
<span class="sd">        squares of gradients.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Iterate over every parameter</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="c1"># Get the state</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

                <span class="c1"># Check if state is initialized</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Reset running averages</span>
                <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

                <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
                <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
                    <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_neurons_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parameter</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">,</span>
        <span class="n">neuron_indices</span><span class="p">:</span> <span class="n">Int</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">)],</span>
        <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">component_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the state for specific neurons, on a specific parameter.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (</span>
<span class="sd">            ...     SparseAutoencoder, SparseAutoencoderConfig</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; model = SparseAutoencoder(</span>
<span class="sd">            ...        SparseAutoencoderConfig(</span>
<span class="sd">            ...             n_input_features=5,</span>
<span class="sd">            ...             n_learned_features=10,</span>
<span class="sd">            ...             n_components=2</span>
<span class="sd">            ...         )</span>
<span class="sd">            ...    )</span>
<span class="sd">            &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">            ...     model.parameters(),</span>
<span class="sd">            ...     named_parameters=model.named_parameters(),</span>
<span class="sd">            ...     has_components_dim=True,</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...</span>
<span class="sd">            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</span>
<span class="sd">            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(</span>
<span class="sd">            ...     model.decoder.weight,</span>
<span class="sd">            ...     dead_neurons_indices,</span>
<span class="sd">            ...     axis=1</span>
<span class="sd">            ... )</span>

<span class="sd">        Args:</span>
<span class="sd">            parameter: The parameter to be reset. Examples from the standard sparse autoencoder</span>
<span class="sd">                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,</span>
<span class="sd">            neuron_indices: The indices of the neurons to reset.</span>
<span class="sd">            axis: The axis of the state values to reset (i.e. the input/output features axis, as</span>
<span class="sd">                we&#39;re resetting all input/output features for a specific dead neuron).</span>
<span class="sd">            component_idx: The component index of the state values to reset.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the parameter has a components dimension, but has_components_dim is</span>
<span class="sd">                False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the state of the parameter</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

        <span class="c1"># If the number of dimensions is 3, we definitely have a components dimension. If 2, we may</span>
        <span class="c1"># do (as the bias has 2 dimensions with components, but the weight has 2 dimensions without</span>
        <span class="c1"># components).</span>
        <span class="n">definitely_has_components_dimension</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span>
            <span class="ow">and</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">definitely_has_components_dimension</span>
        <span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;The parameter has a components dimension, but has_components_dim is False. &quot;</span>
                <span class="s2">&quot;This should not happen.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Check if state is initialized</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Check there are any neurons to reset</span>
        <span class="k">if</span> <span class="n">neuron_indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Move the neuron indices to the correct device</span>
        <span class="n">neuron_indices</span> <span class="o">=</span> <span class="n">neuron_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Reset running averages for the specified neurons</span>
        <span class="k">if</span> <span class="s2">&quot;exp_avg&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">][</span><span class="n">component_idx</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">][</span><span class="n">component_idx</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
        <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">][</span><span class="n">component_idx</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.AdamWithReset.parameter_names" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">parameter_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_value</span><span class="p">)</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.AdamWithReset.parameter_names" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Parameter Names.</p>
<p>The names of the parameters, so that we can find them later when resetting the state.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.AdamWithReset.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">foreach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">capturable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">differentiable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fused</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">named_parameters</span><span class="p">,</span> <span class="n">has_components_dim</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.AdamWithReset.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the optimizer.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>Named parameters must be with default settings (remove duplicates and not recursive).</p>
</details>
<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
from sparse_autoencoder.autoencoder.model import (
...     SparseAutoencoder, SparseAutoencoderConfig
... )
model = SparseAutoencoder(
...        SparseAutoencoderConfig(
...             n_input_features=5,
...             n_learned_features=10,
...             n_components=2
...         )
...    )
optimizer = AdamWithReset(
...     model.parameters(),
...     named_parameters=model.named_parameters(),
...     has_components_dim=True,
... )
optimizer.reset_state_all_parameters()</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>params</code></td>
          <td>
                <code><span title="torch.optim.optimizer.params_t">params_t</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Iterable of parameters to optimize or dicts defining parameter groups.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>lr</code></td>
          <td>
                <code>float | <span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.SINGLE_ITEM" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.SINGLE_ITEM">SINGLE_ITEM</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a
float LR unless specifying fused=True or capturable=True.</p>
            </div>
          </td>
          <td>
                <code>0.001</code>
          </td>
        </tr>
        <tr>
          <td><code>betas</code></td>
          <td>
                <code>tuple[float, float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Coefficients used for computing running averages of gradient and its square.</p>
            </div>
          </td>
          <td>
                <code>(0.9, 0.999)</code>
          </td>
        </tr>
        <tr>
          <td><code>eps</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Term added to the denominator to improve numerical stability.</p>
            </div>
          </td>
          <td>
                <code>1e-08</code>
          </td>
        </tr>
        <tr>
          <td><code>weight_decay</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Weight decay (L2 penalty).</p>
            </div>
          </td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
        <tr>
          <td><code>amsgrad</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to use the AMSGrad variant of this algorithm from the paper "On the
Convergence of Adam and Beyond".</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>foreach</code></td>
          <td>
                <code>bool | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether foreach implementation of optimizer is used. If None, foreach is used
over the for-loop implementation on CUDA if more performant. Note that foreach uses
more peak memory.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>maximize</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If True, maximizes the parameters based on the objective, instead of
minimizing.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>capturable</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether this instance is safe to capture in a CUDA graph. True can impair
ungraphed performance.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>differentiable</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether autograd should occur through the optimizer step in training.
Setting to True can impair performance.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>fused</code></td>
          <td>
                <code>bool | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64,
torch.float32, torch.float16, and torch.bfloat16.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>named_parameters</code></td>
          <td>
                <code><span title="collections.abc.Iterator">Iterator</span>[tuple[str, <span title="torch.nn.parameter.Parameter">Parameter</span>]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>An iterator over the named parameters of the model. This is used to
find the parameters when resetting their state. You should set this as
<code>model.named_parameters()</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>has_components_dim</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the parameters have a components dimension (i.e. if you are
training an SAE on more than one component).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the number of parameter names does not match the number of parameters.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># (extending existing implementation)</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">params_t</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">SINGLE_ITEM</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">betas</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">foreach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">named_parameters</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]],</span>
    <span class="n">has_components_dim</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the optimizer.</span>

<span class="sd">    Warning:</span>
<span class="sd">        Named parameters must be with default settings (remove duplicates and not recursive).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (</span>
<span class="sd">        ...     SparseAutoencoder, SparseAutoencoderConfig</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; model = SparseAutoencoder(</span>
<span class="sd">        ...        SparseAutoencoderConfig(</span>
<span class="sd">        ...             n_input_features=5,</span>
<span class="sd">        ...             n_learned_features=10,</span>
<span class="sd">        ...             n_components=2</span>
<span class="sd">        ...         )</span>
<span class="sd">        ...    )</span>
<span class="sd">        &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">        ...     model.parameters(),</span>
<span class="sd">        ...     named_parameters=model.named_parameters(),</span>
<span class="sd">        ...     has_components_dim=True,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_state_all_parameters()</span>

<span class="sd">    Args:</span>
<span class="sd">        params: Iterable of parameters to optimize or dicts defining parameter groups.</span>
<span class="sd">        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a</span>
<span class="sd">            float LR unless specifying fused=True or capturable=True.</span>
<span class="sd">        betas: Coefficients used for computing running averages of gradient and its square.</span>
<span class="sd">        eps: Term added to the denominator to improve numerical stability.</span>
<span class="sd">        weight_decay: Weight decay (L2 penalty).</span>
<span class="sd">        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper &quot;On the</span>
<span class="sd">            Convergence of Adam and Beyond&quot;.</span>
<span class="sd">        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used</span>
<span class="sd">            over the for-loop implementation on CUDA if more performant. Note that foreach uses</span>
<span class="sd">            more peak memory.</span>
<span class="sd">        maximize: If True, maximizes the parameters based on the objective, instead of</span>
<span class="sd">            minimizing.</span>
<span class="sd">        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair</span>
<span class="sd">            ungraphed performance.</span>
<span class="sd">        differentiable: Whether autograd should occur through the optimizer step in training.</span>
<span class="sd">            Setting to True can impair performance.</span>
<span class="sd">        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,</span>
<span class="sd">            torch.float32, torch.float16, and torch.bfloat16.</span>
<span class="sd">        named_parameters: An iterator over the named parameters of the model. This is used to</span>
<span class="sd">            find the parameters when resetting their state. You should set this as</span>
<span class="sd">            `model.named_parameters()`.</span>
<span class="sd">        has_components_dim: If the parameters have a components dimension (i.e. if you are</span>
<span class="sd">            training an SAE on more than one component).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of parameter names does not match the number of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialise the parent class (note we repeat the parameter names so that type hints work).</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span>
        <span class="n">foreach</span><span class="o">=</span><span class="n">foreach</span><span class="p">,</span>
        <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">,</span>
        <span class="n">capturable</span><span class="o">=</span><span class="n">capturable</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="o">=</span><span class="n">differentiable</span><span class="p">,</span>
        <span class="n">fused</span><span class="o">=</span><span class="n">fused</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span> <span class="o">=</span> <span class="n">has_components_dim</span>

    <span class="c1"># Store the names of the parameters, so that we can find them later when resetting the</span>
    <span class="c1"># state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;The number of parameter names does not match the number of parameters. &quot;</span>
            <span class="s2">&quot;If using model.named_parameters() make sure remove_duplicates is True &quot;</span>
            <span class="s2">&quot;and recursive is False (the default settings).&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.AdamWithReset.reset_neurons_state" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">reset_neurons_state</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">component_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset the state for specific neurons, on a specific parameter.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
from sparse_autoencoder.autoencoder.model import (
...     SparseAutoencoder, SparseAutoencoderConfig
... )
model = SparseAutoencoder(
...        SparseAutoencoderConfig(
...             n_input_features=5,
...             n_learned_features=10,
...             n_components=2
...         )
...    )
optimizer = AdamWithReset(
...     model.parameters(),
...     named_parameters=model.named_parameters(),
...     has_components_dim=True,
... )</p>
<h4 id="sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this">... train the model and then resample some dead neurons, then do this ...<a class="headerlink" href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this" title="Permanent link"></a></h4>
<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>
<h4 id="sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated">Reset the optimizer state for parameters that have been updated<a class="headerlink" href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated" title="Permanent link"></a></h4>
<p>optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)
optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)
optimizer.reset_neurons_state(
...     model.decoder.weight,
...     dead_neurons_indices,
...     axis=1
... )</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>parameter</code></td>
          <td>
                <code><span title="torch.nn.parameter.Parameter">Parameter</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The parameter to be reset. Examples from the standard sparse autoencoder
implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>,</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_indices</code></td>
          <td>
                <code><span title="jaxtyping.Int">Int</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE_IDX" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE_IDX">LEARNT_FEATURE_IDX</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The indices of the neurons to reset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>axis</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The axis of the state values to reset (i.e. the input/output features axis, as
we're resetting all input/output features for a specific dead neuron).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The component index of the state values to reset.</p>
            </div>
          </td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the parameter has a components dimension, but has_components_dim is
False.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_neurons_state</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">parameter</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">,</span>
    <span class="n">neuron_indices</span><span class="p">:</span> <span class="n">Int</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE_IDX</span><span class="p">)],</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">component_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the state for specific neurons, on a specific parameter.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import (</span>
<span class="sd">        ...     SparseAutoencoder, SparseAutoencoderConfig</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; model = SparseAutoencoder(</span>
<span class="sd">        ...        SparseAutoencoderConfig(</span>
<span class="sd">        ...             n_input_features=5,</span>
<span class="sd">        ...             n_learned_features=10,</span>
<span class="sd">        ...             n_components=2</span>
<span class="sd">        ...         )</span>
<span class="sd">        ...    )</span>
<span class="sd">        &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">        ...     model.parameters(),</span>
<span class="sd">        ...     named_parameters=model.named_parameters(),</span>
<span class="sd">        ...     has_components_dim=True,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...</span>
<span class="sd">        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</span>
<span class="sd">        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.weight, dead_neurons_indices, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(model.encoder.bias, dead_neurons_indices, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(</span>
<span class="sd">        ...     model.decoder.weight,</span>
<span class="sd">        ...     dead_neurons_indices,</span>
<span class="sd">        ...     axis=1</span>
<span class="sd">        ... )</span>

<span class="sd">    Args:</span>
<span class="sd">        parameter: The parameter to be reset. Examples from the standard sparse autoencoder</span>
<span class="sd">            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,</span>
<span class="sd">        neuron_indices: The indices of the neurons to reset.</span>
<span class="sd">        axis: The axis of the state values to reset (i.e. the input/output features axis, as</span>
<span class="sd">            we&#39;re resetting all input/output features for a specific dead neuron).</span>
<span class="sd">        component_idx: The component index of the state values to reset.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the parameter has a components dimension, but has_components_dim is</span>
<span class="sd">            False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the state of the parameter</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

    <span class="c1"># If the number of dimensions is 3, we definitely have a components dimension. If 2, we may</span>
    <span class="c1"># do (as the bias has 2 dimensions with components, but the weight has 2 dimensions without</span>
    <span class="c1"># components).</span>
    <span class="n">definitely_has_components_dimension</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span>
        <span class="ow">and</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">definitely_has_components_dimension</span>
    <span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;The parameter has a components dimension, but has_components_dim is False. &quot;</span>
            <span class="s2">&quot;This should not happen.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Check if state is initialized</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Check there are any neurons to reset</span>
    <span class="k">if</span> <span class="n">neuron_indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Move the neuron indices to the correct device</span>
    <span class="n">neuron_indices</span> <span class="o">=</span> <span class="n">neuron_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Reset running averages for the specified neurons</span>
    <span class="k">if</span> <span class="s2">&quot;exp_avg&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">][</span><span class="n">component_idx</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">][</span><span class="n">component_idx</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
    <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_components_dim</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">][</span><span class="n">component_idx</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.AdamWithReset.reset_state_all_parameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">reset_state_all_parameters</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.AdamWithReset.reset_state_all_parameters" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset the state for all parameters.</p>
<p>Iterates over all parameters and resets both the running averages of the gradients and the
squares of gradients.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_state_all_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the state for all parameters.</span>

<span class="sd">    Iterates over all parameters and resets both the running averages of the gradients and the</span>
<span class="sd">    squares of gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Iterate over every parameter</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="c1"># Get the state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

            <span class="c1"># Check if state is initialized</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="c1"># Reset running averages</span>
            <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
            <span class="n">exp_avg</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
            <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.AutoencoderHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>AutoencoderHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.AutoencoderHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Sparse autoencoder hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">AutoencoderHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse autoencoder hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">expansion_factor</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expansion Factor.</span>

<span class="sd">    Size of the learned features relative to the input features. A good expansion factor to start</span>
<span class="sd">    with is typically 2-4.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.AutoencoderHyperparameters.expansion_factor" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">expansion_factor</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.AutoencoderHyperparameters.expansion_factor" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Expansion Factor.</p>
<p>Size of the learned features relative to the input features. A good expansion factor to start
with is typically 2-4.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.CapacityMetric" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>CapacityMetric</code>


<a href="#sparse_autoencoder.CapacityMetric" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric" href="metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric">AbstractTrainMetric</a></code></p>

  
      <p>Capacities Metrics for Learned Features.</p>
<p>Measure the capacity of a set of features as defined in <a href="https://arxiv.org/pdf/2210.01892.pdf">Polysemanticity and Capacity in Neural
Networks</a>.</p>
<p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.
Formally it's the ratio of the squared dot product of a feature with itself to the sum of its
squared dot products of all features.</p>
<p>If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is
1/n.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CapacityMetric</span><span class="p">(</span><span class="n">AbstractTrainMetric</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Capacities Metrics for Learned Features.</span>

<span class="sd">    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural</span>
<span class="sd">    Networks](https://arxiv.org/pdf/2210.01892.pdf).</span>

<span class="sd">    Capacity is intuitively measuring the &#39;proportion of a dimension&#39; assigned to a feature.</span>
<span class="sd">    Formally it&#39;s the ratio of the squared dot product of a feature with itself to the sum of its</span>
<span class="sd">    squared dot products of all features.</span>

<span class="sd">    If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is</span>
<span class="sd">    1/n.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">capacities</span><span class="p">(</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate capacities.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])</span>
<span class="sd">            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)</span>
<span class="sd">            &gt;&gt;&gt; orthogonal_caps</span>
<span class="sd">            tensor([[1., 1., 1.]])</span>

<span class="sd">        Args:</span>
<span class="sd">            features: A collection of features.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A 1D tensor of capacities, where each element is the capacity of the corresponding</span>
<span class="sd">            feature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">squared_dot_products</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="n">features</span><span class="p">,</span>
                <span class="n">features</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;batch_1 </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">                    batch_2 </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">                    -&gt; </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> batch_1 batch_2&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">**</span> <span class="mi">2</span>
        <span class="p">)</span>

        <span class="n">sum_of_sq_dot</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">squared_dot_products</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">diagonal</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span>
            <span class="n">squared_dot_products</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">diagonal</span> <span class="o">/</span> <span class="n">sum_of_sq_dot</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">wandb_capacities_histogram</span><span class="p">(</span>
        <span class="n">capacities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the capacities.</span>

<span class="sd">        This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;capacities_histogram&quot;:</span>
<span class="sd">        wandb_capacities_histogram(capacities)})`.</span>

<span class="sd">        Args:</span>
<span class="sd">            capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">np_capacities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">capacities</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">np_histograms</span> <span class="o">=</span> <span class="p">[</span><span class="n">histogram</span><span class="p">(</span><span class="n">capacity</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">capacity</span> <span class="ow">in</span> <span class="n">np_capacities</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="n">np_histogram</span><span class="p">)</span> <span class="k">for</span> <span class="n">np_histogram</span> <span class="ow">in</span> <span class="n">np_histograms</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the capacities for a training batch.&quot;&quot;&quot;</span>
        <span class="n">train_batch_capacities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacities</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span><span class="p">)</span>

        <span class="n">histograms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wandb_capacities_histogram</span><span class="p">(</span><span class="n">train_batch_capacities</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">MetricResult</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;capacities&quot;</span><span class="p">,</span>
                <span class="n">component_wise_values</span><span class="o">=</span><span class="n">histograms</span><span class="p">,</span>
                <span class="n">location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
                <span class="n">aggregate_approach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t aggregate histograms</span>
            <span class="p">)</span>
        <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.CapacityMetric.calculate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">calculate</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.CapacityMetric.calculate" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the capacities for a training batch.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the capacities for a training batch.&quot;&quot;&quot;</span>
    <span class="n">train_batch_capacities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacities</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span><span class="p">)</span>

    <span class="n">histograms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wandb_capacities_histogram</span><span class="p">(</span><span class="n">train_batch_capacities</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="n">MetricResult</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;capacities&quot;</span><span class="p">,</span>
            <span class="n">component_wise_values</span><span class="o">=</span><span class="n">histograms</span><span class="p">,</span>
            <span class="n">location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
            <span class="n">aggregate_approach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t aggregate histograms</span>
        <span class="p">)</span>
    <span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.CapacityMetric.capacities" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">capacities</span><span class="p">(</span><span class="n">features</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.CapacityMetric.capacities" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate capacities.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])
orthogonal_caps = CapacityMetric.capacities(orthogonal_features)
orthogonal_caps
tensor([[1., 1., 1.]])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>features</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A collection of features.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>feature.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">capacities</span><span class="p">(</span>
    <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate capacities.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; orthogonal_features = torch.tensor([[[1., 0., 0.]], [[0., 1., 0.]], [[0., 0., 1.]]])</span>
<span class="sd">        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)</span>
<span class="sd">        &gt;&gt;&gt; orthogonal_caps</span>
<span class="sd">        tensor([[1., 1., 1.]])</span>

<span class="sd">    Args:</span>
<span class="sd">        features: A collection of features.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1D tensor of capacities, where each element is the capacity of the corresponding</span>
<span class="sd">        feature.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">squared_dot_products</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="n">features</span><span class="p">,</span>
            <span class="n">features</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;batch_1 </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">                batch_2 </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">                -&gt; </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> batch_1 batch_2&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>

    <span class="n">sum_of_sq_dot</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">squared_dot_products</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">diagonal</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span>
        <span class="n">squared_dot_products</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">diagonal</span> <span class="o">/</span> <span class="n">sum_of_sq_dot</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.CapacityMetric.wandb_capacities_histogram" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">wandb_capacities_histogram</span><span class="p">(</span><span class="n">capacities</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.CapacityMetric.wandb_capacities_histogram" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Create a W&amp;B histogram of the capacities.</p>
<p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({"capacities_histogram":
wandb_capacities_histogram(capacities)})</code>.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>capacities</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Capacity of each feature. Can be calculated using :func:<code>calc_capacities</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<span title="wandb.Histogram">Histogram</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">wandb_capacities_histogram</span><span class="p">(</span>
    <span class="n">capacities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the capacities.</span>

<span class="sd">    This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;capacities_histogram&quot;:</span>
<span class="sd">    wandb_capacities_histogram(capacities)})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">np_capacities</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">capacities</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">np_histograms</span> <span class="o">=</span> <span class="p">[</span><span class="n">histogram</span><span class="p">(</span><span class="n">capacity</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">capacity</span> <span class="ow">in</span> <span class="n">np_capacities</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="n">np_histogram</span><span class="p">)</span> <span class="k">for</span> <span class="n">np_histogram</span> <span class="ow">in</span> <span class="n">np_histograms</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Controller" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Controller</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.Controller" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">

  
      <p>Controller.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Controller</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Controller.&quot;&quot;&quot;</span>

    <span class="nb">type</span><span class="p">:</span> <span class="n">ControllerType</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.ControllerType" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>ControllerType</code>


<a href="#sparse_autoencoder.ControllerType" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Controller Type.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ControllerType</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Controller Type.&quot;&quot;&quot;</span>

    <span class="n">CLOUD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Weights &amp; Biases cloud controller.</span>

<span class="sd">    Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all</span>
<span class="sd">    communicate with the Weights &amp; Biases cloud service to coordinate the sweep.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">LOCAL</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Local controller.</span>

<span class="sd">    Manages the sweep operation locally, without the need for cloud-based coordination or external</span>
<span class="sd">    services.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ControllerType.CLOUD" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">CLOUD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ControllerType.CLOUD" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Weights &amp; Biases cloud controller.</p>
<p>Utilizes Weights &amp; Biases as the sweep controller, enabling launching of multiple nodes that all
communicate with the Weights &amp; Biases cloud service to coordinate the sweep.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.ControllerType.LOCAL" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">LOCAL</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.ControllerType.LOCAL" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Local controller.</p>
<p>Manages the sweep operation locally, without the need for cloud-based coordination or external
services.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Distribution" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Distribution</code>


<a href="#sparse_autoencoder.Distribution" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Sweep Distribution.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Distribution</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sweep Distribution.&quot;&quot;&quot;</span>

    <span class="n">BETA</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Beta distribution.</span>

<span class="sd">    Utilizes the Beta distribution, a family of continuous probability distributions defined on the</span>
<span class="sd">    interval [0, 1], for parameter sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CATEGORICAL</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Categorical distribution.</span>

<span class="sd">    Employs a categorical distribution for discrete variable sampling, where each category has an</span>
<span class="sd">    equal probability of being selected.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CATEGORICAL_W_PROBABILITIES</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Categorical distribution with probabilities.</span>

<span class="sd">    Similar to categorical distribution but allows assigning different probabilities to each</span>
<span class="sd">    category.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CONSTANT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constant distribution.</span>

<span class="sd">    Uses a constant value for the parameter, ensuring it remains the same across all runs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">INT_UNIFORM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Integer uniform distribution.</span>

<span class="sd">    Samples integer values uniformly across a specified range.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">INV_LOG_UNIFORM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Inverse log-uniform distribution.</span>

<span class="sd">    Samples values according to an inverse log-uniform distribution, useful for parameters that span</span>
<span class="sd">    several orders of magnitude.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">INV_LOG_UNIFORM_VALUES</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Inverse log-uniform values distribution.</span>

<span class="sd">    Similar to the inverse log-uniform distribution but allows specifying exact values to be</span>
<span class="sd">    sampled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.BETA" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">BETA</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.BETA" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Beta distribution.</p>
<p>Utilizes the Beta distribution, a family of continuous probability distributions defined on the
interval [0, 1], for parameter sampling.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.CATEGORICAL" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">CATEGORICAL</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.CATEGORICAL" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Categorical distribution.</p>
<p>Employs a categorical distribution for discrete variable sampling, where each category has an
equal probability of being selected.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.CATEGORICAL_W_PROBABILITIES" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">CATEGORICAL_W_PROBABILITIES</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.CATEGORICAL_W_PROBABILITIES" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Categorical distribution with probabilities.</p>
<p>Similar to categorical distribution but allows assigning different probabilities to each
category.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.CONSTANT" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">CONSTANT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.CONSTANT" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Constant distribution.</p>
<p>Uses a constant value for the parameter, ensuring it remains the same across all runs.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.INT_UNIFORM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">INT_UNIFORM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.INT_UNIFORM" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Integer uniform distribution.</p>
<p>Samples integer values uniformly across a specified range.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.INV_LOG_UNIFORM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">INV_LOG_UNIFORM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.INV_LOG_UNIFORM" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Inverse log-uniform distribution.</p>
<p>Samples values according to an inverse log-uniform distribution, useful for parameters that span
several orders of magnitude.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Distribution.INV_LOG_UNIFORM_VALUES" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">INV_LOG_UNIFORM_VALUES</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Distribution.INV_LOG_UNIFORM_VALUES" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Inverse log-uniform values distribution.</p>
<p>Similar to the inverse log-uniform distribution but allows specifying exact values to be
sampled.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Goal" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Goal</code>


<a href="#sparse_autoencoder.Goal" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Goal.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Goal</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Goal.&quot;&quot;&quot;</span>

    <span class="n">MAXIMIZE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximization goal.</span>

<span class="sd">    Sets the objective of the hyperparameter tuning process to maximize a specified metric.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">MINIMIZE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Minimization goal.</span>

<span class="sd">    Aims to minimize a specified metric during the hyperparameter tuning process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Goal.MAXIMIZE" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">MAXIMIZE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Goal.MAXIMIZE" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Maximization goal.</p>
<p>Sets the objective of the hyperparameter tuning process to maximize a specified metric.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Goal.MINIMIZE" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">MINIMIZE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Goal.MINIMIZE" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Minimization goal.</p>
<p>Aims to minimize a specified metric during the hyperparameter tuning process.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.HyperbandStopping" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>HyperbandStopping</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStopping" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">

  
      <p>Hyperband Stopping Config.</p>
<p>Speed up hyperparameter search by killing off runs that appear to have lower performance
than successful training runs.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>HyperbandStopping(type=HyperbandStoppingType.HYPERBAND)
HyperbandStopping(type=hyperband)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">HyperbandStopping</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hyperband Stopping Config.</span>

<span class="sd">    Speed up hyperparameter search by killing off runs that appear to have lower performance</span>
<span class="sd">    than successful training runs.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; HyperbandStopping(type=HyperbandStoppingType.HYPERBAND)</span>
<span class="sd">        HyperbandStopping(type=hyperband)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">type</span><span class="p">:</span> <span class="n">HyperbandStoppingType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">HyperbandStoppingType</span><span class="o">.</span><span class="n">HYPERBAND</span>

    <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ETA.</span>

<span class="sd">    Specify the bracket multiplier schedule (default: 3).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Max Iterations.</span>

<span class="sd">    Specify the maximum number of iterations. Note this is number of times the metric is logged, not</span>
<span class="sd">    the number of activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">miniter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Min Iterations.</span>

<span class="sd">    Set the first epoch to start trimming runs, and hyperband will automatically calculate</span>
<span class="sd">    the subsequent epochs to trim runs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">s</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set the number of steps you trim runs at, working backwards from the max_iter.&quot;&quot;&quot;</span>

    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use a more aggressive condition for termination, stops more runs.&quot;&quot;&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
        <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

        <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.HyperbandStopping.eta" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStopping.eta" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>ETA.</p>
<p>Specify the bracket multiplier schedule (default: 3).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.HyperbandStopping.maxiter" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStopping.maxiter" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Max Iterations.</p>
<p>Specify the maximum number of iterations. Note this is number of times the metric is logged, not
the number of activations.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.HyperbandStopping.miniter" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">miniter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStopping.miniter" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Min Iterations.</p>
<p>Set the first epoch to start trimming runs, and hyperband will automatically calculate
the subsequent epochs to trim runs.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.HyperbandStopping.s" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">s</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStopping.s" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Set the number of steps you trim runs at, working backwards from the max_iter.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.HyperbandStopping.strict" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStopping.strict" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Use a more aggressive condition for termination, stops more runs.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.HyperbandStopping.__repr__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.HyperbandStopping.__repr__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.HyperbandStopping.__str__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__str__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.HyperbandStopping.__str__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>String representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
    <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

    <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.HyperbandStoppingType" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>HyperbandStoppingType</code>


<a href="#sparse_autoencoder.HyperbandStoppingType" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Hyperband Stopping Type.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">HyperbandStoppingType</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hyperband Stopping Type.&quot;&quot;&quot;</span>

    <span class="n">HYPERBAND</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hyperband algorithm.</span>

<span class="sd">    Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping</span>
<span class="sd">    method to efficiently tune hyperparameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.HyperbandStoppingType.HYPERBAND" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">HYPERBAND</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.HyperbandStoppingType.HYPERBAND" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Hyperband algorithm.</p>
<p>Implements the Hyperband stopping algorithm, an adaptive resource allocation and early-stopping
method to efficiently tune hyperparameters.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Hyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Hyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.Hyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.Parameters" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameters">Parameters</a></code></p>

  
      <p>Sweep Hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Hyperparameters</span><span class="p">(</span><span class="n">Parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sweep Hyperparameters.&quot;&quot;&quot;</span>

    <span class="c1"># Required parameters</span>
    <span class="n">source_data</span><span class="p">:</span> <span class="n">SourceDataHyperparameters</span>

    <span class="n">source_model</span><span class="p">:</span> <span class="n">SourceModelHyperparameters</span>

    <span class="c1"># Optional parameters</span>
    <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">ActivationResamplerHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">ActivationResamplerHyperparameters</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">AutoencoderHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">AutoencoderHyperparameters</span><span class="p">())</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">LossHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">LossHyperparameters</span><span class="p">())</span>

    <span class="n">optimizer</span><span class="p">:</span> <span class="n">OptimizerHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">OptimizerHyperparameters</span><span class="p">())</span>

    <span class="n">pipeline</span><span class="p">:</span> <span class="n">PipelineHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">PipelineHyperparameters</span><span class="p">())</span>

    <span class="n">random_seed</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">49</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Random seed.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Post initialisation checks.&quot;&quot;&quot;</span>
        <span class="c1"># Check the resample dataset size &lt;= the store size (currently only works if value is used</span>
        <span class="c1"># for both).</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dataset_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">max_store_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dataset_size</span><span class="o">.</span><span class="n">value</span>
            <span class="o">&gt;</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">max_store_size</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;Resample dataset size must be less than or equal to the pipeline max store size. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Resample dataset size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dataset_size</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;pipeline max store size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">max_store_size</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
        <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">    &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

        <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="se">\n</span><span class="s2">    </span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="se">\n</span><span class="s2">)&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Hyperparameters.random_seed" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">random_seed</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">49</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Hyperparameters.random_seed" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Random seed.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Hyperparameters.__post_init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">__post_init__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Hyperparameters.__post_init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Post initialisation checks.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Post initialisation checks.&quot;&quot;&quot;</span>
    <span class="c1"># Check the resample dataset size &lt;= the store size (currently only works if value is used</span>
    <span class="c1"># for both).</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dataset_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">max_store_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dataset_size</span><span class="o">.</span><span class="n">value</span>
        <span class="o">&gt;</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">max_store_size</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Resample dataset size must be less than or equal to the pipeline max store size. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Resample dataset size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dataset_size</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;pipeline max store size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">max_store_size</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Hyperparameters.__repr__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Hyperparameters.__repr__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Hyperparameters.__str__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__str__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Hyperparameters.__str__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>String representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
    <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">    &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

    <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="se">\n</span><span class="s2">    </span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="se">\n</span><span class="s2">)&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Impute" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Impute</code>


<a href="#sparse_autoencoder.Impute" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Metric value to use in bayes search for runs that fail, crash, or are killed.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Impute</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Metric value to use in bayes search for runs that fail, crash, or are killed.&quot;&quot;&quot;</span>

    <span class="n">BEST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">LATEST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">WORST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.ImputeWhileRunning" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>ImputeWhileRunning</code>


<a href="#sparse_autoencoder.ImputeWhileRunning" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Appends a calculated metric even when epochs are in a running state.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ImputeWhileRunning</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Appends a calculated metric even when epochs are in a running state.&quot;&quot;&quot;</span>

    <span class="n">BEST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">FALSE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">LATEST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">WORST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Kind" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Kind</code>


<a href="#sparse_autoencoder.Kind" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Kind.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Kind</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Kind.&quot;&quot;&quot;</span>

    <span class="n">SWEEP</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.L2ReconstructionLoss" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>L2ReconstructionLoss</code>


<a href="#sparse_autoencoder.L2ReconstructionLoss" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>L2 Reconstruction loss.</p>
<p>L2 reconstruction loss is calculated as the sum squared error between each each input vector
and it's corresponding decoded vector. The original paper found that models trained with some
loss functions such as cross-entropy loss generally prefer to represent features
polysemantically, whereas models trained with L2 may achieve the same loss for both
polysemantic and monosemantic representations of true features.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
loss = L2ReconstructionLoss()
input_activations = torch.tensor([[5.0, 4], [3.0, 4]])
output_activations = torch.tensor([[1.0, 5], [1.0, 5]])
unused_activations = torch.zeros_like(input_activations)</p>
<h3 id="sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log">Outputs both loss and metrics to log<a class="headerlink" href="#sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log" title="Permanent link"></a></h3>
<p>loss.forward(input_activations, unused_activations, output_activations)
tensor([8.5000, 2.5000])</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">L2ReconstructionLoss</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L2 Reconstruction loss.</span>

<span class="sd">    L2 reconstruction loss is calculated as the sum squared error between each each input vector</span>
<span class="sd">    and it&#39;s corresponding decoded vector. The original paper found that models trained with some</span>
<span class="sd">    loss functions such as cross-entropy loss generally prefer to represent features</span>
<span class="sd">    polysemantically, whereas models trained with L2 may achieve the same loss for both</span>
<span class="sd">    polysemantic and monosemantic representations of true features.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; loss = L2ReconstructionLoss()</span>
<span class="sd">        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])</span>
<span class="sd">        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])</span>
<span class="sd">        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)</span>
<span class="sd">        &gt;&gt;&gt; # Outputs both loss and metrics to log</span>
<span class="sd">        &gt;&gt;&gt; loss.forward(input_activations, unused_activations, output_activations)</span>
<span class="sd">        tensor([8.5000, 2.5000])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_reduction</span><span class="p">:</span> <span class="n">LossReductionType</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MSE reduction type.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the L2 reconstruction loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            reduction: MSE reduction type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the loss module for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;l2_reconstruction_loss&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>  <span class="c1"># noqa: ARG002</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span>
        <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]</span>
        <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the L2 reconstruction loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">square_error_loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

        <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduction</span><span class="p">:</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">square_error_loss</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.L2ReconstructionLoss.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.L2ReconstructionLoss.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the L2 reconstruction loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>reduction</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>MSE reduction type.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN">MEAN</span></code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the L2 reconstruction loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction: MSE reduction type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reduction</span> <span class="o">=</span> <span class="n">reduction</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.L2ReconstructionLoss.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.L2ReconstructionLoss.forward" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the L2 reconstruction loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)] | <span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span>
    <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]</span>
    <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the L2 reconstruction loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_error_loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

    <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduction</span><span class="p">:</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">square_error_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.L2ReconstructionLoss.log_name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">log_name</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.L2ReconstructionLoss.log_name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Log name.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the loss module for logging.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the loss module for logging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;l2_reconstruction_loss&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.LearnedActivationsL1Loss" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>LearnedActivationsL1Loss</code>


<a href="#sparse_autoencoder.LearnedActivationsL1Loss" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>Learned activations L1 (absolute error) loss.</p>
<p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this
multiplied by the l1_coefficient (designed to encourage sparsity).</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>l1_loss = LearnedActivationsL1Loss(0.1)
learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])
unused_activations = torch.zeros_like(learned_activations)</p>
<h3 id="sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log">Returns loss and metrics to log<a class="headerlink" href="#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log" title="Permanent link"></a></h3>
<p>l1_loss.forward(unused_activations, learned_activations, unused_activations)[0]
tensor(0.5000)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LearnedActivationsL1Loss</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this</span>
<span class="sd">    multiplied by the l1_coefficient (designed to encourage sparsity).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)</span>
<span class="sd">        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])</span>
<span class="sd">        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)</span>
<span class="sd">        &gt;&gt;&gt; # Returns loss and metrics to log</span>
<span class="sd">        &gt;&gt;&gt; l1_loss.forward(unused_activations, learned_activations, unused_activations)[0]</span>
<span class="sd">        tensor(0.5000)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L1 coefficient.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the loss module for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;learned_activations_l1_loss_penalty&quot;</span>

    <span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">:</span> <span class="n">PositiveFloat</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the absolute error loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">                approximate guide if you use e.g. 2x this number of tokens you might consider using</span>
<span class="sd">                0.5x the l1 coefficient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="n">l1_coefficient</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_l1_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>  <span class="c1"># noqa: ARG002</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>  <span class="c1"># noqa: ARG002s</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_L1LossAndPenalty</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of itemwise absolute loss, and itemwise absolute loss multiplied by the l1</span>
<span class="sd">            coefficient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Absolute loss is the summed absolute value of the learned activations (i.e. over the</span>
        <span class="c1"># learned feature axis).</span>
        <span class="n">itemwise_absolute_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">itemwise_absolute_loss_penalty</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span>

        <span class="k">return</span> <span class="n">_L1LossAndPenalty</span><span class="p">(</span>
            <span class="n">itemwise_absolute_loss</span><span class="o">=</span><span class="n">itemwise_absolute_loss</span><span class="p">,</span>
            <span class="n">itemwise_absolute_loss_penalty</span><span class="o">=</span><span class="n">itemwise_absolute_loss_penalty</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span>
            <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
        <span class="p">)</span><span class="o">.</span><span class="n">itemwise_absolute_loss_penalty</span>

    <span class="c1"># Override to add both the loss and the penalty to the log</span>
    <span class="k">def</span> <span class="nf">scalar_loss_with_log</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">batch_reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
        <span class="n">component_reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LossResultWithMetrics</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Scalar L1 loss (reduced across the batch and component axis) with logging.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>
<span class="sd">            batch_reduction: Batch reduction type. Typically you would choose LossReductionType.MEAN</span>
<span class="sd">                to make the loss independent of the batch size.</span>
<span class="sd">            component_reduction: Component reduction type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log</span>
<span class="sd">                (loss before and after the l1 coefficient).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If batch_reduction is LossReductionType.NONE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">itemwise_absolute_loss</span><span class="p">,</span> <span class="n">itemwise_absolute_loss_penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span>
            <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
        <span class="p">)</span>

        <span class="k">match</span> <span class="n">batch_reduction</span><span class="p">:</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
                <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss_penalty</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
                <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss_penalty</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
                <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Batch reduction type NONE not supported.&quot;</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Create the log</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">MetricResult</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                <span class="n">postfix</span><span class="o">=</span><span class="s2">&quot;learned_activations_l1&quot;</span><span class="p">,</span>
                <span class="n">component_wise_values</span><span class="o">=</span><span class="n">batch_scalar_loss</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">batch_scalar_loss</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="k">else</span> <span class="n">batch_scalar_loss</span><span class="p">,</span>
                <span class="n">location</span><span class="o">=</span><span class="n">MetricLocation</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">MetricResult</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                <span class="n">postfix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_name</span><span class="p">(),</span>
                <span class="n">component_wise_values</span><span class="o">=</span><span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="k">else</span> <span class="n">batch_scalar_loss_penalty</span><span class="p">,</span>
                <span class="n">location</span><span class="o">=</span><span class="n">MetricLocation</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">]</span>

        <span class="k">match</span> <span class="n">component_reduction</span><span class="p">:</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
                <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
                <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">return</span> <span class="n">LossResultWithMetrics</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">batch_scalar_loss_penalty</span><span class="p">,</span> <span class="n">loss_metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extra representation string.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;l1_coefficient=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]</span> <span class="o">=</span> <span class="n">l1_coefficient</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>L1 coefficient.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">l1_coefficient</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the absolute error loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>l1_coefficient</code></td>
          <td>
                <code><span title="pydantic.PositiveFloat">PositiveFloat</span> | <span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>L1 coefficient. The original paper experimented with L1 coefficients of
[0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an
approximate guide if you use e.g. 2x this number of tokens you might consider using
0.5x the l1 coefficient.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">:</span> <span class="n">PositiveFloat</span> <span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the absolute error loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">            approximate guide if you use e.g. 2x this number of tokens you might consider using</span>
<span class="sd">            0.5x the l1 coefficient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="n">l1_coefficient</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">extra_repr</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Extra representation string.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extra representation string.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;l1_coefficient=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.forward" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Learned activations L1 (absolute error) loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span>
        <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
    <span class="p">)</span><span class="o">.</span><span class="n">itemwise_absolute_loss_penalty</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.log_name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">log_name</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.log_name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Log name.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the loss module for logging.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the loss module for logging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;learned_activations_l1_loss_penalty&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.scalar_loss_with_log" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">scalar_loss_with_log</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">batch_reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span> <span class="n">component_reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.scalar_loss_with_log" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Scalar L1 loss (reduced across the batch and component axis) with logging.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch_reduction</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch reduction type. Typically you would choose LossReductionType.MEAN
to make the loss independent of the batch size.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN">MEAN</span></code>
          </td>
        </tr>
        <tr>
          <td><code>component_reduction</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Component reduction type.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.loss.abstract_loss.LossReductionType.NONE">NONE</span></code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossResultWithMetrics" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossResultWithMetrics">LossResultWithMetrics</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log
(loss before and after the l1 coefficient).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If batch_reduction is LossReductionType.NONE.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">scalar_loss_with_log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">batch_reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
    <span class="n">component_reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LossResultWithMetrics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Scalar L1 loss (reduced across the batch and component axis) with logging.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>
<span class="sd">        batch_reduction: Batch reduction type. Typically you would choose LossReductionType.MEAN</span>
<span class="sd">            to make the loss independent of the batch size.</span>
<span class="sd">        component_reduction: Component reduction type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log</span>
<span class="sd">            (loss before and after the l1 coefficient).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If batch_reduction is LossReductionType.NONE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">itemwise_absolute_loss</span><span class="p">,</span> <span class="n">itemwise_absolute_loss_penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span>
        <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
    <span class="p">)</span>

    <span class="k">match</span> <span class="n">batch_reduction</span><span class="p">:</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
            <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss_penalty</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
            <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">itemwise_absolute_loss_penalty</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Batch reduction type NONE not supported.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Create the log</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">MetricResult</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
            <span class="n">postfix</span><span class="o">=</span><span class="s2">&quot;learned_activations_l1&quot;</span><span class="p">,</span>
            <span class="n">component_wise_values</span><span class="o">=</span><span class="n">batch_scalar_loss</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_scalar_loss</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="n">batch_scalar_loss</span><span class="p">,</span>
            <span class="n">location</span><span class="o">=</span><span class="n">MetricLocation</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">MetricResult</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
            <span class="n">postfix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_name</span><span class="p">(),</span>
            <span class="n">component_wise_values</span><span class="o">=</span><span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="n">batch_scalar_loss_penalty</span><span class="p">,</span>
            <span class="n">location</span><span class="o">=</span><span class="n">MetricLocation</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">]</span>

    <span class="k">match</span> <span class="n">component_reduction</span><span class="p">:</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
            <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
            <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">return</span> <span class="n">LossResultWithMetrics</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">batch_scalar_loss_penalty</span><span class="p">,</span> <span class="n">loss_metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.LossHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>LossHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.LossHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Loss hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LossHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">l1_coefficient</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L1 Penalty Coefficient.</span>

<span class="sd">    The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.</span>
<span class="sd">    The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by</span>
<span class="sd">    using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good</span>
<span class="sd">    starting point for the L1 coefficient is 1e-3.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.LossHyperparameters.l1_coefficient" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">l1_coefficient</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.001</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.LossHyperparameters.l1_coefficient" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>L1 Penalty Coefficient.</p>
<p>The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.
The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by
using more features, or using a lower L1 coefficient. If your expansion factor is 2, then a good
starting point for the L1 coefficient is 1e-3.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.LossReducer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>LossReducer</code>


<a href="#sparse_autoencoder.LossReducer" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>Loss reducer.</p>
<p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to
nn.Sequential.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss
from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss
LossReducer(
...     L2ReconstructionLoss(),
...     LearnedActivationsL1Loss(0.001),
... )
LossReducer(
  (0): L2ReconstructionLoss()
  (1): LearnedActivationsL1Loss(l1_coefficient=0.001)
)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LossReducer</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss reducer.</span>

<span class="sd">    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to</span>
<span class="sd">    nn.Sequential.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss</span>
<span class="sd">        &gt;&gt;&gt; LossReducer(</span>
<span class="sd">        ...     L2ReconstructionLoss(),</span>
<span class="sd">        ...     LearnedActivationsL1Loss(0.001),</span>
<span class="sd">        ... )</span>
<span class="sd">        LossReducer(</span>
<span class="sd">          (0): L2ReconstructionLoss()</span>
<span class="sd">          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)</span>
<span class="sd">        )</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_modules</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;AbstractLoss&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Children loss modules.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the loss module for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;total_loss&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">loss_modules</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the loss reducer.</span>

<span class="sd">        Args:</span>
<span class="sd">            *loss_modules: Loss modules to reduce.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the loss reducer has no loss modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_modules</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss_module</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Loss reducer must have at least one loss module.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reduce loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Mean loss across the batch, summed across the loss modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_modules_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;module train_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">loss_module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">all_modules_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Dir dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractLoss</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get item dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">AbstractLoss</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterator dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.__dir__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__dir__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.__dir__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Dir dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dir dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.__getitem__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LossReducer.__getitem__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Get item dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractLoss</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get item dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">loss_modules</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LossReducer.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the loss reducer.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>*loss_modules</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss modules to reduce.</p>
            </div>
          </td>
          <td>
                <code>()</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the loss reducer has no loss modules.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">loss_modules</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the loss reducer.</span>

<span class="sd">    Args:</span>
<span class="sd">        *loss_modules: Loss modules to reduce.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the loss reducer has no loss modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_modules</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss_module</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Loss reducer must have at least one loss module.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.__iter__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__iter__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.__iter__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Iterator dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">AbstractLoss</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterator dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.__len__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.__len__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Length dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LossReducer.forward" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Reduce loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Mean loss across the batch, summed across the loss modules.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Mean loss across the batch, summed across the loss modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_modules_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;module train_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">loss_module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">all_modules_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.LossReducer.log_name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">log_name</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.log_name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Log name.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the loss module for logging.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the loss module for logging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;total_loss&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.LossReductionType" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>LossReductionType</code>


<a href="#sparse_autoencoder.LossReductionType" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Loss reduction type.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LossReductionType</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss reduction type.&quot;&quot;&quot;</span>

    <span class="n">MEAN</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>

    <span class="n">SUM</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span>

    <span class="n">NONE</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Method" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Method</code>


<a href="#sparse_autoencoder.Method" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Method.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Method</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Method.&quot;&quot;&quot;</span>

    <span class="n">BAYES</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Bayesian optimization.</span>

<span class="sd">    Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach</span>
<span class="sd">    for finding the optimal set of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CUSTOM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom method.</span>

<span class="sd">    Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the</span>
<span class="sd">    sweep process.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">GRID</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Grid search.</span>

<span class="sd">    Utilizes a grid search approach for hyperparameter tuning, systematically working through</span>
<span class="sd">    multiple combinations of parameter values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">RANDOM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Random search.</span>

<span class="sd">    Implements a random search strategy for hyperparameter tuning, exploring the parameter space</span>
<span class="sd">    randomly.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Method.BAYES" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">BAYES</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Method.BAYES" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Bayesian optimization.</p>
<p>Employs Bayesian optimization for hyperparameter tuning, a probabilistic model-based approach
for finding the optimal set of parameters.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Method.CUSTOM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">CUSTOM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Method.CUSTOM" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Custom method.</p>
<p>Allows for a user-defined custom method for hyperparameter tuning, providing flexibility in the
sweep process.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Method.GRID" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">GRID</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Method.GRID" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Grid search.</p>
<p>Utilizes a grid search approach for hyperparameter tuning, systematically working through
multiple combinations of parameter values.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Method.RANDOM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">RANDOM</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Method.RANDOM" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Random search.</p>
<p>Implements a random search strategy for hyperparameter tuning, exploring the parameter space
randomly.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Metric" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Metric</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.Metric" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">

  
      <p>Metric to optimize.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Metric</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Metric to optimize.&quot;&quot;&quot;</span>

    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Name of metric.&quot;&quot;&quot;</span>

    <span class="n">goal</span><span class="p">:</span> <span class="n">Goal</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">Goal</span><span class="o">.</span><span class="n">MINIMIZE</span>

    <span class="n">impute</span><span class="p">:</span> <span class="n">Impute</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Metric value to use in bayes search for runs that fail, crash, or are killed&quot;&quot;&quot;</span>

    <span class="n">imputewhilerunning</span><span class="p">:</span> <span class="n">ImputeWhileRunning</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Appends a calculated metric even when epochs are in a running state.&quot;&quot;&quot;</span>

    <span class="n">target</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The sweep will finish once any run achieves this value.&quot;&quot;&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
        <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

        <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Metric.impute" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">impute</span><span class="p">:</span> <span class="n">Impute</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Metric.impute" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Metric value to use in bayes search for runs that fail, crash, or are killed</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Metric.imputewhilerunning" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">imputewhilerunning</span><span class="p">:</span> <span class="n">ImputeWhileRunning</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Metric.imputewhilerunning" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Appends a calculated metric even when epochs are in a running state.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Metric.name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">name</span><span class="p">:</span> <span class="nb">str</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Metric.name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Name of metric.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Metric.target" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">target</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Metric.target" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>The sweep will finish once any run achieves this value.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Metric.__repr__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Metric.__repr__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Metric.__str__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__str__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Metric.__str__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>String representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
    <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

    <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.NestedParameter" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>NestedParameter</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.NestedParameter" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Nested Parameter.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>from dataclasses import field
@dataclass(frozen=True)
... class MyNestedParameter(NestedParameter):
...     a: int = field(default=Parameter(1))
...     b: int = field(default=Parameter(2))
MyNestedParameter().to_dict()
{'parameters': {'a': {'value': 1}, 'b': {'value': 2}}}</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NestedParameter</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>  <span class="c1"># noqa: B024 (abstract so that we can check against it&#39;s type)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Nested Parameter.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from dataclasses import field</span>
<span class="sd">        &gt;&gt;&gt; @dataclass(frozen=True)</span>
<span class="sd">        ... class MyNestedParameter(NestedParameter):</span>
<span class="sd">        ...     a: int = field(default=Parameter(1))</span>
<span class="sd">        ...     b: int = field(default=Parameter(2))</span>
<span class="sd">        &gt;&gt;&gt; MyNestedParameter().to_dict()</span>
<span class="sd">        {&#39;parameters&#39;: {&#39;a&#39;: {&#39;value&#39;: 1}, &#39;b&#39;: {&#39;value&#39;: 2}}}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return dict representation of this object.&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">dict_without_none_values</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>  <span class="c1"># noqa: ANN401</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Return dict without None values.</span>

<span class="sd">            Args:</span>
<span class="sd">                obj: The object to convert to a dict.</span>

<span class="sd">            Returns:</span>
<span class="sd">                The dict representation of the object.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">dict_none_removed</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">dict_with_none</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">dict_with_none</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">dict_none_removed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">return</span> <span class="n">dict_none_removed</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dict_factory</span><span class="o">=</span><span class="n">dict_without_none_values</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">__dict__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return dict representation of this object.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.NestedParameter.__dict__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="vm">__dict__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.NestedParameter.__dict__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Return dict representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__dict__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return dict representation of this object.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.NestedParameter.to_dict" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">to_dict</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.NestedParameter.to_dict" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Return dict representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return dict representation of this object.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">dict_without_none_values</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>  <span class="c1"># noqa: ANN401</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return dict without None values.</span>

<span class="sd">        Args:</span>
<span class="sd">            obj: The object to convert to a dict.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The dict representation of the object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dict_none_removed</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">dict_with_none</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">dict_with_none</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dict_none_removed</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="n">dict_none_removed</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dict_factory</span><span class="o">=</span><span class="n">dict_without_none_values</span><span class="p">)}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.OptimizerHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>OptimizerHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Optimizer hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">OptimizerHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimizer hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">lr</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learning rate.</span>

<span class="sd">    A good starting point for the learning rate is 1e-3, but this is one of the key parameters so</span>
<span class="sd">    you should probably tune it.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">adam_beta_1</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.9</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adam Beta 1.</span>

<span class="sd">    The exponential decay rate for the first moment estimates (mean) of the gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">adam_beta_2</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.99</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adam Beta 2.</span>

<span class="sd">    The exponential decay rate for the second moment estimates (variance) of the gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">adam_weight_decay</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adam Weight Decay.</span>

<span class="sd">    Weight decay (L2 penalty).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">amsgrad</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;AMSGrad.</span>

<span class="sd">    Whether to use the AMSGrad variant of this algorithm from the paper [On the Convergence of Adam</span>
<span class="sd">    and Beyond](https://arxiv.org/abs/1904.09237).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fused</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fused.</span>

<span class="sd">    Whether to use a fused implementation of the optimizer (may be faster on CUDA).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;reduce_on_plateau&quot;</span><span class="p">,</span> <span class="s2">&quot;cosine_annealing&quot;</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learning rate scheduler.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.adam_beta_1" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">adam_beta_1</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.9</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.adam_beta_1" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Adam Beta 1.</p>
<p>The exponential decay rate for the first moment estimates (mean) of the gradient.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.adam_beta_2" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">adam_beta_2</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.99</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.adam_beta_2" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Adam Beta 2.</p>
<p>The exponential decay rate for the second moment estimates (variance) of the gradient.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.adam_weight_decay" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">adam_weight_decay</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.adam_weight_decay" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Adam Weight Decay.</p>
<p>Weight decay (L2 penalty).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.amsgrad" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">amsgrad</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.amsgrad" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>AMSGrad.</p>
<p>Whether to use the AMSGrad variant of this algorithm from the paper <a href="https://arxiv.org/abs/1904.09237">On the Convergence of Adam
and Beyond</a>.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.fused" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">fused</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.fused" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Fused.</p>
<p>Whether to use a fused implementation of the optimizer (may be faster on CUDA).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.lr" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">lr</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mf">0.001</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.lr" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Learning rate.</p>
<p>A good starting point for the learning rate is 1e-3, but this is one of the key parameters so
you should probably tune it.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.OptimizerHyperparameters.lr_scheduler" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;reduce_on_plateau&#39;</span><span class="p">,</span> <span class="s1">&#39;cosine_annealing&#39;</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.OptimizerHyperparameters.lr_scheduler" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Learning rate scheduler.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Parameter" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Parameter</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="typing.Generic">Generic</span>[<span title="sparse_autoencoder.train.utils.wandb_sweep_types.ParamType">ParamType</span>]</code></p>

  
      <p>Sweep Parameter.</p>
<p><a href="https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters">https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters</a></p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Parameter</span><span class="p">(</span><span class="n">Generic</span><span class="p">[</span><span class="n">ParamType</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sweep Parameter.</span>

<span class="sd">    https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">value</span><span class="p">:</span> <span class="n">ParamType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Single value.</span>

<span class="sd">    Specifies the single valid value for this hyperparameter. Compatible with grid.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">max</span><span class="p">:</span> <span class="n">ParamType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum value.&quot;&quot;&quot;</span>

    <span class="nb">min</span><span class="p">:</span> <span class="n">ParamType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Minimum value.&quot;&quot;&quot;</span>

    <span class="n">distribution</span><span class="p">:</span> <span class="n">Distribution</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Distribution</span>

<span class="sd">    If not specified, will default to categorical if values is set, to int_uniform if max and min</span>
<span class="sd">    are set to integers, to uniform if max and min are set to floats, or to constant if value is</span>
<span class="sd">    set.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">q</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantization parameter.</span>

<span class="sd">    Quantization step size for quantized hyperparameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">values</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParamType</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Discrete values.</span>

<span class="sd">    Specifies all valid values for this hyperparameter. Compatible with grid.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">probabilities</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Probability of each value&quot;&quot;&quot;</span>

    <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mean for normal or lognormal distributions&quot;&quot;&quot;</span>

    <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Std Dev for normal or lognormal distributions&quot;&quot;&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
        <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

        <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.distribution" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">distribution</span><span class="p">:</span> <span class="n">Distribution</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.distribution" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Distribution</p>
<p>If not specified, will default to categorical if values is set, to int_uniform if max and min
are set to integers, to uniform if max and min are set to floats, or to constant if value is
set.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.max" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="nb">max</span><span class="p">:</span> <span class="n">ParamType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.max" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Maximum value.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.min" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="nb">min</span><span class="p">:</span> <span class="n">ParamType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.min" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Minimum value.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.mu" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">mu</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.mu" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Mean for normal or lognormal distributions</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.probabilities" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">probabilities</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.probabilities" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Probability of each value</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.q" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">q</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.q" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Quantization parameter.</p>
<p>Quantization step size for quantized hyperparameters.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.sigma" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.sigma" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Std Dev for normal or lognormal distributions</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.value" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">value</span><span class="p">:</span> <span class="n">ParamType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.value" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Single value.</p>
<p>Specifies the single valid value for this hyperparameter. Compatible with grid.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Parameter.values" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">values</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParamType</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Parameter.values" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Discrete values.</p>
<p>Specifies all valid values for this hyperparameter. Compatible with grid.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Parameter.__repr__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__repr__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Parameter.__repr__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Representation of this object.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Parameter.__str__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__str__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Parameter.__str__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>String representation of this object.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String representation of this object.&quot;&quot;&quot;</span>
    <span class="n">items_representation</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">items_representation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">joined_items</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">items_representation</span><span class="p">)</span>

    <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">joined_items</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.Pipeline" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>Pipeline</code>


<a href="#sparse_autoencoder.Pipeline" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">

  
      <p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p>
<p>Includes all the key functionality to train a sparse autoencoder, with a specific set of
    hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Pipeline</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pipeline for training a Sparse Autoencoder on TransformerLens activations.</span>

<span class="sd">    Includes all the key functionality to train a sparse autoencoder, with a specific set of</span>
<span class="sd">        hyperparameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">ActivationResampler</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation resampler to use.&quot;&quot;&quot;</span>

    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse autoencoder to train.&quot;&quot;&quot;</span>

    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of input features in the sparse autoencoder.&quot;&quot;&quot;</span>

    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of learned features in the sparse autoencoder.&quot;&quot;&quot;</span>

    <span class="n">cache_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Names of the cache hook points to use in the source model.&quot;&quot;&quot;</span>

    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to stope the source model at (if we don&#39;t need activations after this layer).&quot;&quot;&quot;</span>

    <span class="n">log_frequency</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Frequency at which to log metrics (in steps).&quot;&quot;&quot;</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss function to use.&quot;&quot;&quot;</span>

    <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Metrics to use.&quot;&quot;&quot;</span>

    <span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimizer to use.&quot;&quot;&quot;</span>

    <span class="n">progress_bar</span><span class="p">:</span> <span class="n">tqdm</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Progress bar for the pipeline.&quot;&quot;&quot;</span>

    <span class="n">source_data</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterable over the source data.&quot;&quot;&quot;</span>

    <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source dataset to generate activation data from (tokenized prompts).&quot;&quot;&quot;</span>

    <span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">HookedTransformer</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model to get activations from.&quot;&quot;&quot;</span>

    <span class="n">total_activations_trained_on</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Total number of activations trained on state.&quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_components</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of source model components the SAE is trained on.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span><span class="p">)</span>

    <span class="nd">@final</span>
    <span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">ActivationResampler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
        <span class="n">cache_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">layer</span><span class="p">:</span> <span class="n">NonNegativeInt</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">,</span>
        <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span><span class="p">,</span>
        <span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">HookedTransformer</span><span class="p">],</span>
        <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">run_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sparse_autoencoder&quot;</span><span class="p">,</span>
        <span class="n">checkpoint_directory</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">DEFAULT_CHECKPOINT_DIRECTORY</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_frequency</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span> <span class="o">=</span> <span class="n">default_metrics</span><span class="p">,</span>
        <span class="n">num_workers_data_loading</span><span class="p">:</span> <span class="n">NonNegativeInt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">source_data_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_resampler: Activation resampler to use.</span>
<span class="sd">            autoencoder: Sparse autoencoder to train.</span>
<span class="sd">            cache_names: Names of the cache hook points to use in the source model.</span>
<span class="sd">            layer: Layer to stope the source model at (if we don&#39;t need activations after this</span>
<span class="sd">                layer).</span>
<span class="sd">            loss: Loss function to use.</span>
<span class="sd">            optimizer: Optimizer to use.</span>
<span class="sd">            source_dataset: Source dataset to get data from.</span>
<span class="sd">            source_model: Source model to get activations from.</span>
<span class="sd">            n_input_features: Number of input features in the sparse autoencoder.</span>
<span class="sd">            n_learned_features: Number of learned features in the sparse autoencoder.</span>
<span class="sd">            run_name: Name of the run for saving checkpoints.</span>
<span class="sd">            checkpoint_directory: Directory to save checkpoints to.</span>
<span class="sd">            lr_scheduler: Learning rate scheduler to use.</span>
<span class="sd">            log_frequency: Frequency at which to log metrics (in steps)</span>
<span class="sd">            metrics: Metrics to use.</span>
<span class="sd">            num_workers_data_loading: Number of CPU workers for the dataloader.</span>
<span class="sd">            source_data_batch_size: Batch size for the source data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="o">=</span> <span class="n">activation_resampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span> <span class="o">=</span> <span class="n">autoencoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span> <span class="o">=</span> <span class="n">cache_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">=</span> <span class="n">checkpoint_directory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span> <span class="o">=</span> <span class="n">log_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_name</span> <span class="o">=</span> <span class="n">run_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">=</span> <span class="n">source_data_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span> <span class="o">=</span> <span class="n">source_dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span> <span class="o">=</span> <span class="n">source_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>

        <span class="c1"># Create a stateful iterator</span>
        <span class="n">source_dataloader</span> <span class="o">=</span> <span class="n">source_dataset</span><span class="o">.</span><span class="n">get_dataloader</span><span class="p">(</span>
            <span class="n">source_data_batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers_data_loading</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="nf">generate_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store_size</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorActivationStore</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate activations.</span>

<span class="sd">        Args:</span>
<span class="sd">            store_size: Number of activations to generate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Activation store for the train section.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the store size is not divisible by the batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check the store size is divisible by the batch size</span>
        <span class="k">if</span> <span class="n">store_size</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Store size must be divisible by the batch size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span><span class="si">}</span><span class="s2">), &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">store_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Setup the store</span>
        <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>
        <span class="n">store</span> <span class="o">=</span> <span class="n">TensorActivationStore</span><span class="p">(</span>
            <span class="n">store_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span>
        <span class="p">)</span>

        <span class="c1"># Add the hook to the model (will automatically store the activations every time the model</span>
        <span class="c1"># runs)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">cache_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span><span class="p">):</span>
            <span class="n">hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">store_activations_hook</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">,</span> <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">add_hook</span><span class="p">(</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>

        <span class="c1"># Loop through the dataloader until the store reaches the desired size</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">store_size</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">)</span>
                <span class="n">input_ids</span><span class="p">:</span> <span class="n">Int</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">SOURCE_DATA_BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">POSITION</span><span class="p">)]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span>
                    <span class="s2">&quot;input_ids&quot;</span>
                <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">stop_at_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>  <span class="c1"># type: ignore (TLens is typed incorrectly)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
        <span class="n">store</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">store</span>

    <span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
    <span class="k">def</span> <span class="nf">train_autoencoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Train the sparse autoencoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_store: Activation store from the generate section.</span>
<span class="sd">            train_batch_size: Train batch size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of times each neuron fired, for each component.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">autoencoder_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">)</span>

        <span class="n">activations_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">activation_store</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">learned_activations_fired_count</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">store_batch</span> <span class="ow">in</span> <span class="n">activations_dataloader</span><span class="p">:</span>
            <span class="c1"># Zero the gradients</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Move the batch to the device (in place)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">store_batch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">autoencoder_device</span><span class="p">)</span>

            <span class="c1"># Forward pass</span>
            <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Get loss &amp; metrics</span>
            <span class="n">metrics</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">total_loss</span><span class="p">,</span> <span class="n">loss_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">scalar_loss_with_log</span><span class="p">(</span>
                <span class="n">batch</span><span class="p">,</span>
                <span class="n">learned_activations</span><span class="p">,</span>
                <span class="n">reconstructed_activations</span><span class="p">,</span>
                <span class="n">component_reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">loss_metrics</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">:</span>
                    <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span>
                        <span class="n">TrainMetricData</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">calculated</span><span class="p">)</span>

            <span class="c1"># Store count of how many neurons have fired</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">fired</span> <span class="o">=</span> <span class="n">learned_activations</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="n">learned_activations_fired_count</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">fired</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

            <span class="c1"># Backwards pass</span>
            <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">post_backwards_hook</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Log training metrics</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span> <span class="o">+=</span> <span class="n">train_batch_size</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span> <span class="o">/</span> <span class="n">train_batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span>
                <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">log</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">metric_result</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
                    <span class="n">log</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric_result</span><span class="o">.</span><span class="n">wandb_log</span><span class="p">)</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="n">log</span><span class="p">,</span>
                    <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span><span class="p">,</span>
                    <span class="n">commit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">learned_activations_fired_count</span>

    <span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_updates</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update the parameters of the model from the results of the resampler.</span>

<span class="sd">        Args:</span>
<span class="sd">            parameter_updates: Parameter updates from the resampler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">component_parameter_update</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameter_updates</span><span class="p">):</span>
            <span class="c1"># Update the weights and biases</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
                <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_encoder_weight_updates</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_bias</span><span class="p">(</span>
                <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
                <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Reset the optimizer</span>
            <span class="k">for</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">reset_optimizer_parameter_details</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">reset_neurons_state</span><span class="p">(</span>
                    <span class="n">parameter</span><span class="o">=</span><span class="n">parameter</span><span class="p">,</span>
                    <span class="n">neuron_indices</span><span class="o">=</span><span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                    <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
                    <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="nf">validate_sae</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_n_activations</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get validation metrics.</span>

<span class="sd">        Args:</span>
<span class="sd">            validation_n_activations: Number of activations to use for validation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">losses_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">validation_n_activations</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>

        <span class="c1"># Create the metric data stores</span>
        <span class="n">losses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">losses_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">source_model_device</span>
        <span class="p">)</span>
        <span class="n">losses_with_reconstruction</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">losses_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">source_model_device</span><span class="p">)</span>
        <span class="n">losses_with_zero_ablation</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">losses_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">source_model_device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">cache_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">)</span>

                <span class="n">input_ids</span><span class="p">:</span> <span class="n">Int</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">SOURCE_DATA_BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">POSITION</span><span class="p">)]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span>
                    <span class="s2">&quot;input_ids&quot;</span>
                <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>

                <span class="c1"># Run a forward pass with and without the replaced activations</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
                <span class="n">replacement_hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">replace_activations_hook</span><span class="p">,</span>
                    <span class="n">sparse_autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">,</span>
                    <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
                    <span class="n">n_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
                    <span class="n">loss_with_reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
                        <span class="n">input_ids</span><span class="p">,</span>
                        <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                        <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[</span>
                            <span class="p">(</span>
                                <span class="n">cache_name</span><span class="p">,</span>
                                <span class="n">replacement_hook</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">],</span>
                    <span class="p">)</span>
                    <span class="n">loss_with_zero_ablation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
                        <span class="n">input_ids</span><span class="p">,</span>
                        <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                        <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">zero_ablate_hook</span><span class="p">)],</span>
                    <span class="p">)</span>

                    <span class="n">losses</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                    <span class="n">losses_with_reconstruction</span><span class="p">[</span>
                        <span class="n">batch_idx</span><span class="p">,</span> <span class="n">component_idx</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">loss_with_reconstruction</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                    <span class="n">losses_with_zero_ablation</span><span class="p">[</span>
                        <span class="n">batch_idx</span><span class="p">,</span> <span class="n">component_idx</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">loss_with_zero_ablation</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Log</span>
        <span class="n">validation_data</span> <span class="o">=</span> <span class="n">ValidationMetricData</span><span class="p">(</span>
            <span class="n">source_model_loss</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span>
            <span class="n">source_model_loss_with_reconstruction</span><span class="o">=</span><span class="n">losses_with_reconstruction</span><span class="p">,</span>
            <span class="n">source_model_loss_with_zero_ablation</span><span class="o">=</span><span class="n">losses_with_zero_ablation</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">validation_metrics</span><span class="p">:</span>
            <span class="n">log</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">metric_result</span> <span class="ow">in</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">):</span>
                <span class="n">log</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric_result</span><span class="o">.</span><span class="n">wandb_log</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">is_final</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the model as a checkpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            is_final: Whether this is the final checkpoint.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Path to the saved checkpoint.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">run_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="s1">&#39;final&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">is_final</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># Wandb</span>
        <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">save_to_wandb</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># Local</span>
        <span class="n">local_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.pt&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">local_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">local_path</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="nf">run_pipeline</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">max_store_size</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">max_activations</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">validation_n_activations</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">validate_frequency</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpoint_frequency</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run the full training pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_batch_size: Train batch size.</span>
<span class="sd">            max_store_size: Maximum size of the activation store.</span>
<span class="sd">            max_activations: Maximum total number of activations to train on (the original paper</span>
<span class="sd">                used 8bn, although others have had success with 100m+).</span>
<span class="sd">            validation_n_activations: Number of activations to use for validation.</span>
<span class="sd">            validate_frequency: Frequency at which to get validation metrics.</span>
<span class="sd">            checkpoint_frequency: Frequency at which to save a checkpoint.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">last_validated</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">last_checkpoint</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the source model to evaluation (inference) mode</span>

        <span class="c1"># Get the store size</span>
        <span class="n">store_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">max_store_size</span> <span class="o">-</span> <span class="n">max_store_size</span> <span class="o">%</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Activations trained on&quot;</span><span class="p">,</span>
            <span class="n">total</span><span class="o">=</span><span class="n">max_activations</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_activations</span><span class="p">,</span> <span class="n">store_size</span><span class="p">):</span>
                <span class="c1"># Generate</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span><span class="p">})</span>
                <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_activations</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

                <span class="c1"># Update the counters</span>
                <span class="n">n_activation_vectors_in_store</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
                <span class="n">last_validated</span> <span class="o">+=</span> <span class="n">n_activation_vectors_in_store</span>
                <span class="n">last_checkpoint</span> <span class="o">+=</span> <span class="n">n_activation_vectors_in_store</span>

                <span class="c1"># Train</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;train&quot;</span><span class="p">})</span>
                <span class="n">batch_neuron_activity</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span>
                    <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
                <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_autoencoder</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>

                <span class="c1"># Resample dead neurons (if needed)</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;resample&quot;</span><span class="p">})</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Get the updates</span>
                    <span class="n">parameter_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">step_resampler</span><span class="p">(</span>
                        <span class="n">batch_neuron_activity</span><span class="o">=</span><span class="n">batch_neuron_activity</span><span class="p">,</span>
                        <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                        <span class="n">autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">,</span>
                        <span class="n">loss_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                        <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="k">if</span> <span class="n">parameter_updates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                                <span class="p">{</span>
                                    <span class="s2">&quot;resample/dead_neurons&quot;</span><span class="p">:</span> <span class="p">[</span>
                                        <span class="nb">len</span><span class="p">(</span><span class="n">update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">)</span>
                                        <span class="k">for</span> <span class="n">update</span> <span class="ow">in</span> <span class="n">parameter_updates</span>
                                    <span class="p">]</span>
                                <span class="p">},</span>
                                <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="p">)</span>

                        <span class="c1"># Update the parameters</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">update_parameters</span><span class="p">(</span><span class="n">parameter_updates</span><span class="p">)</span>

                <span class="c1"># Get validation metrics (if needed)</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;validate&quot;</span><span class="p">})</span>
                <span class="k">if</span> <span class="n">validate_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_validated</span> <span class="o">&gt;=</span> <span class="n">validate_frequency</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">validate_sae</span><span class="p">(</span><span class="n">validation_n_activations</span><span class="p">)</span>
                    <span class="n">last_validated</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="c1"># Checkpoint (if needed)</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">})</span>
                <span class="k">if</span> <span class="n">checkpoint_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_checkpoint</span> <span class="o">&gt;=</span> <span class="n">checkpoint_frequency</span><span class="p">:</span>
                    <span class="n">last_checkpoint</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

                <span class="c1"># Update the progress bar</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

        <span class="c1"># Save the final checkpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">is_final</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.activation_resampler" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">activation_resampler</span><span class="p">:</span> <span class="n">ActivationResampler</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">activation_resampler</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.activation_resampler" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Activation resampler to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.autoencoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">]</span> <span class="o">=</span> <span class="n">autoencoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.autoencoder" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Sparse autoencoder to train.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.cache_names" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">cache_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_names</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.cache_names" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Names of the cache hook points to use in the source model.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.layer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">layer</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.layer" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Layer to stope the source model at (if we don't need activations after this layer).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.log_frequency" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">log_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">log_frequency</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.log_frequency" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Frequency at which to log metrics (in steps).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.loss" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span> <span class="o">=</span> <span class="n">loss</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.loss" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Loss function to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.metrics" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span> <span class="o">=</span> <span class="n">metrics</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.metrics" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Metrics to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.n_components" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_components</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.n_components" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of source model components the SAE is trained on.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.n_input_features" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_input_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.n_input_features" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of input features in the sparse autoencoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.n_learned_features" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_learned_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.n_learned_features" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of learned features in the sparse autoencoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.optimizer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span> <span class="o">=</span> <span class="n">optimizer</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.optimizer" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Optimizer to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.progress_bar" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">progress_bar</span><span class="p">:</span> <span class="n">tqdm</span> <span class="o">|</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.progress_bar" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Progress bar for the pipeline.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.source_data" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">source_data</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.source_data" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Iterable over the source data.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.source_dataset" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span> <span class="o">=</span> <span class="n">source_dataset</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.source_dataset" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source dataset to generate activation data from (tokenized prompts).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.source_model" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">HookedTransformer</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_model</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.source_model" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source model to get activations from.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.Pipeline.total_activations_trained_on" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">total_activations_trained_on</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.total_activations_trained_on" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Total number of activations trained on state.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">activation_resampler</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">cache_names</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">source_dataset</span><span class="p">,</span> <span class="n">source_model</span><span class="p">,</span> <span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">run_name</span><span class="o">=</span><span class="s1">&#39;sparse_autoencoder&#39;</span><span class="p">,</span> <span class="n">checkpoint_directory</span><span class="o">=</span><span class="n">DEFAULT_CHECKPOINT_DIRECTORY</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_frequency</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">default_metrics</span><span class="p">,</span> <span class="n">num_workers_data_loading</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">source_data_batch_size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the pipeline.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_resampler</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler">ActivationResampler</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation resampler to use.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a> | <span title="torch.nn.parallel.DataParallel">DataParallel</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder to train.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>cache_names</code></td>
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Names of the cache hook points to use in the source model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>layer</code></td>
          <td>
                <code><span title="pydantic.NonNegativeInt">NonNegativeInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Layer to stope the source model at (if we don't need activations after this
layer).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function to use.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>optimizer</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset" href="optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset">AbstractOptimizerWithReset</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optimizer to use.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_dataset</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source dataset to get data from.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_model</code></td>
          <td>
                <code><span title="transformer_lens.HookedTransformer">HookedTransformer</span> | <span title="torch.nn.parallel.DataParallel">DataParallel</span>[<span title="transformer_lens.HookedTransformer">HookedTransformer</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source model to get activations from.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_input_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of input features in the sparse autoencoder.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_learned_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of learned features in the sparse autoencoder.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>run_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the run for saving checkpoints.</p>
            </div>
          </td>
          <td>
                <code>&#39;sparse_autoencoder&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>checkpoint_directory</code></td>
          <td>
                <code><span title="pathlib.Path">Path</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Directory to save checkpoints to.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.train.pipeline.DEFAULT_CHECKPOINT_DIRECTORY">DEFAULT_CHECKPOINT_DIRECTORY</span></code>
          </td>
        </tr>
        <tr>
          <td><code>lr_scheduler</code></td>
          <td>
                <code><span title="torch.optim.lr_scheduler.LRScheduler">LRScheduler</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learning rate scheduler to use.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>log_frequency</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to log metrics (in steps)</p>
            </div>
          </td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>metrics</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.metrics_container.MetricsContainer" href="metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer">MetricsContainer</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Metrics to use.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.metrics_container.default_metrics" href="metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.default_metrics">default_metrics</a></code>
          </td>
        </tr>
        <tr>
          <td><code>num_workers_data_loading</code></td>
          <td>
                <code><span title="pydantic.NonNegativeInt">NonNegativeInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of CPU workers for the dataloader.</p>
            </div>
          </td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>source_data_batch_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch size for the source data.</p>
            </div>
          </td>
          <td>
                <code>12</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">ActivationResampler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">SparseAutoencoder</span><span class="p">],</span>
    <span class="n">cache_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">layer</span><span class="p">:</span> <span class="n">NonNegativeInt</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">,</span>
    <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span><span class="p">,</span>
    <span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="n">DataParallel</span><span class="p">[</span><span class="n">HookedTransformer</span><span class="p">],</span>
    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">run_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sparse_autoencoder&quot;</span><span class="p">,</span>
    <span class="n">checkpoint_directory</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">DEFAULT_CHECKPOINT_DIRECTORY</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">log_frequency</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span> <span class="o">=</span> <span class="n">default_metrics</span><span class="p">,</span>
    <span class="n">num_workers_data_loading</span><span class="p">:</span> <span class="n">NonNegativeInt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">source_data_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the pipeline.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_resampler: Activation resampler to use.</span>
<span class="sd">        autoencoder: Sparse autoencoder to train.</span>
<span class="sd">        cache_names: Names of the cache hook points to use in the source model.</span>
<span class="sd">        layer: Layer to stope the source model at (if we don&#39;t need activations after this</span>
<span class="sd">            layer).</span>
<span class="sd">        loss: Loss function to use.</span>
<span class="sd">        optimizer: Optimizer to use.</span>
<span class="sd">        source_dataset: Source dataset to get data from.</span>
<span class="sd">        source_model: Source model to get activations from.</span>
<span class="sd">        n_input_features: Number of input features in the sparse autoencoder.</span>
<span class="sd">        n_learned_features: Number of learned features in the sparse autoencoder.</span>
<span class="sd">        run_name: Name of the run for saving checkpoints.</span>
<span class="sd">        checkpoint_directory: Directory to save checkpoints to.</span>
<span class="sd">        lr_scheduler: Learning rate scheduler to use.</span>
<span class="sd">        log_frequency: Frequency at which to log metrics (in steps)</span>
<span class="sd">        metrics: Metrics to use.</span>
<span class="sd">        num_workers_data_loading: Number of CPU workers for the dataloader.</span>
<span class="sd">        source_data_batch_size: Batch size for the source data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="o">=</span> <span class="n">activation_resampler</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span> <span class="o">=</span> <span class="n">autoencoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span> <span class="o">=</span> <span class="n">cache_names</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">=</span> <span class="n">checkpoint_directory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span> <span class="o">=</span> <span class="n">log_frequency</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">run_name</span> <span class="o">=</span> <span class="n">run_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">=</span> <span class="n">source_data_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span> <span class="o">=</span> <span class="n">source_dataset</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span> <span class="o">=</span> <span class="n">source_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>

    <span class="c1"># Create a stateful iterator</span>
    <span class="n">source_dataloader</span> <span class="o">=</span> <span class="n">source_dataset</span><span class="o">.</span><span class="n">get_dataloader</span><span class="p">(</span>
        <span class="n">source_data_batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers_data_loading</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.generate_activations" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">generate_activations</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.generate_activations" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Generate activations.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>store_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of activations to generate.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.tensor_store.TensorActivationStore" href="activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore">TensorActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store for the train section.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the store size is not divisible by the batch size.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="nf">generate_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store_size</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorActivationStore</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate activations.</span>

<span class="sd">    Args:</span>
<span class="sd">        store_size: Number of activations to generate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Activation store for the train section.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the store size is not divisible by the batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check the store size is divisible by the batch size</span>
    <span class="k">if</span> <span class="n">store_size</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Store size must be divisible by the batch size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span><span class="si">}</span><span class="s2">), &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">store_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Setup the store</span>
    <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">TensorActivationStore</span><span class="p">(</span>
        <span class="n">store_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span>
    <span class="p">)</span>

    <span class="c1"># Add the hook to the model (will automatically store the activations every time the model</span>
    <span class="c1"># runs)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">cache_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span><span class="p">):</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">store_activations_hook</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">,</span> <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">add_hook</span><span class="p">(</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>

    <span class="c1"># Loop through the dataloader until the store reaches the desired size</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">store_size</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">)</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">Int</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">SOURCE_DATA_BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">POSITION</span><span class="p">)]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span>
                <span class="s2">&quot;input_ids&quot;</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">stop_at_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>  <span class="c1"># type: ignore (TLens is typed incorrectly)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
    <span class="n">store</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">store</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.run_pipeline" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">run_pipeline</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">max_store_size</span><span class="p">,</span> <span class="n">max_activations</span><span class="p">,</span> <span class="n">validation_n_activations</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">validate_frequency</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">checkpoint_frequency</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.run_pipeline" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Run the full training pipeline.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_store_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum size of the activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_activations</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum total number of activations to train on (the original paper
used 8bn, although others have had success with 100m+).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>validation_n_activations</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of activations to use for validation.</p>
            </div>
          </td>
          <td>
                <code>1024</code>
          </td>
        </tr>
        <tr>
          <td><code>validate_frequency</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to get validation metrics.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>checkpoint_frequency</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to save a checkpoint.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="nf">run_pipeline</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">max_store_size</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">max_activations</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">validation_n_activations</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="n">validate_frequency</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">checkpoint_frequency</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run the full training pipeline.</span>

<span class="sd">    Args:</span>
<span class="sd">        train_batch_size: Train batch size.</span>
<span class="sd">        max_store_size: Maximum size of the activation store.</span>
<span class="sd">        max_activations: Maximum total number of activations to train on (the original paper</span>
<span class="sd">            used 8bn, although others have had success with 100m+).</span>
<span class="sd">        validation_n_activations: Number of activations to use for validation.</span>
<span class="sd">        validate_frequency: Frequency at which to get validation metrics.</span>
<span class="sd">        checkpoint_frequency: Frequency at which to save a checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">last_validated</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">last_checkpoint</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the source model to evaluation (inference) mode</span>

    <span class="c1"># Get the store size</span>
    <span class="n">store_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">max_store_size</span> <span class="o">-</span> <span class="n">max_store_size</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span>
        <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Activations trained on&quot;</span><span class="p">,</span>
        <span class="n">total</span><span class="o">=</span><span class="n">max_activations</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_activations</span><span class="p">,</span> <span class="n">store_size</span><span class="p">):</span>
            <span class="c1"># Generate</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span><span class="p">})</span>
            <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_activations</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

            <span class="c1"># Update the counters</span>
            <span class="n">n_activation_vectors_in_store</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
            <span class="n">last_validated</span> <span class="o">+=</span> <span class="n">n_activation_vectors_in_store</span>
            <span class="n">last_checkpoint</span> <span class="o">+=</span> <span class="n">n_activation_vectors_in_store</span>

            <span class="c1"># Train</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;train&quot;</span><span class="p">})</span>
            <span class="n">batch_neuron_activity</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span>
                <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_autoencoder</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>

            <span class="c1"># Resample dead neurons (if needed)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;resample&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Get the updates</span>
                <span class="n">parameter_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">step_resampler</span><span class="p">(</span>
                    <span class="n">batch_neuron_activity</span><span class="o">=</span><span class="n">batch_neuron_activity</span><span class="p">,</span>
                    <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                    <span class="n">autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">,</span>
                    <span class="n">loss_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                    <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">parameter_updates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                            <span class="p">{</span>
                                <span class="s2">&quot;resample/dead_neurons&quot;</span><span class="p">:</span> <span class="p">[</span>
                                    <span class="nb">len</span><span class="p">(</span><span class="n">update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">)</span>
                                    <span class="k">for</span> <span class="n">update</span> <span class="ow">in</span> <span class="n">parameter_updates</span>
                                <span class="p">]</span>
                            <span class="p">},</span>
                            <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span>

                    <span class="c1"># Update the parameters</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">update_parameters</span><span class="p">(</span><span class="n">parameter_updates</span><span class="p">)</span>

            <span class="c1"># Get validation metrics (if needed)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;validate&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="n">validate_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_validated</span> <span class="o">&gt;=</span> <span class="n">validate_frequency</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">validate_sae</span><span class="p">(</span><span class="n">validation_n_activations</span><span class="p">)</span>
                <span class="n">last_validated</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Checkpoint (if needed)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="n">checkpoint_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_checkpoint</span> <span class="o">&gt;=</span> <span class="n">checkpoint_frequency</span><span class="p">:</span>
                <span class="n">last_checkpoint</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

            <span class="c1"># Update the progress bar</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

    <span class="c1"># Save the final checkpoint</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">is_final</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.save_checkpoint" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">save_checkpoint</span><span class="p">(</span><span class="o">*</span><span class="p">,</span> <span class="n">is_final</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.save_checkpoint" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Save the model as a checkpoint.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>is_final</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether this is the final checkpoint.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="pathlib.Path">Path</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path to the saved checkpoint.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">is_final</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save the model as a checkpoint.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_final: Whether this is the final checkpoint.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Path to the saved checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">run_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="s1">&#39;final&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">is_final</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># Wandb</span>
    <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">save_to_wandb</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># Local</span>
    <span class="n">local_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.pt&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">local_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">local_path</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.train_autoencoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">train_autoencoder</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.train_autoencoder" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Train the sparse autoencoder.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.tensor_store.TensorActivationStore" href="activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore">TensorActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store from the generate section.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Int64">Int64</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each neuron fired, for each component.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="k">def</span> <span class="nf">train_autoencoder</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Int64</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train the sparse autoencoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_store: Activation store from the generate section.</span>
<span class="sd">        train_batch_size: Train batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Number of times each neuron fired, for each component.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">autoencoder_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">)</span>

    <span class="n">activations_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">activation_store</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">learned_activations_fired_count</span><span class="p">:</span> <span class="n">Int64</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">store_batch</span> <span class="ow">in</span> <span class="n">activations_dataloader</span><span class="p">:</span>
        <span class="c1"># Zero the gradients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Move the batch to the device (in place)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">store_batch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">autoencoder_device</span><span class="p">)</span>

        <span class="c1"># Forward pass</span>
        <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="c1"># Get loss &amp; metrics</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_loss</span><span class="p">,</span> <span class="n">loss_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">scalar_loss_with_log</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span>
            <span class="n">learned_activations</span><span class="p">,</span>
            <span class="n">reconstructed_activations</span><span class="p">,</span>
            <span class="n">component_reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">loss_metrics</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">:</span>
                <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span>
                    <span class="n">TrainMetricData</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">calculated</span><span class="p">)</span>

        <span class="c1"># Store count of how many neurons have fired</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fired</span> <span class="o">=</span> <span class="n">learned_activations</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="n">learned_activations_fired_count</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">fired</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

        <span class="c1"># Backwards pass</span>
        <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">post_backwards_hook</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Log training metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span> <span class="o">+=</span> <span class="n">train_batch_size</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span> <span class="o">/</span> <span class="n">train_batch_size</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span>
            <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">log</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">metric_result</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric_result</span><span class="o">.</span><span class="n">wandb_log</span><span class="p">)</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">log</span><span class="p">,</span>
                <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_activations_trained_on</span><span class="p">,</span>
                <span class="n">commit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">learned_activations_fired_count</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.update_parameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">update_parameters</span><span class="p">(</span><span class="n">parameter_updates</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.update_parameters" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Update the parameters of the model from the results of the resampler.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>parameter_updates</code></td>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults" href="activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ParameterUpdateResults">ParameterUpdateResults</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Parameter updates from the resampler.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_updates</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ParameterUpdateResults</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update the parameters of the model from the results of the resampler.</span>

<span class="sd">    Args:</span>
<span class="sd">        parameter_updates: Parameter updates from the resampler.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">component_parameter_update</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameter_updates</span><span class="p">):</span>
        <span class="c1"># Update the weights and biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
            <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_encoder_weight_updates</span><span class="p">,</span>
            <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_bias</span><span class="p">(</span>
            <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
            <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
            <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
            <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Reset the optimizer</span>
        <span class="k">for</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">reset_optimizer_parameter_details</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">reset_neurons_state</span><span class="p">(</span>
                <span class="n">parameter</span><span class="o">=</span><span class="n">parameter</span><span class="p">,</span>
                <span class="n">neuron_indices</span><span class="o">=</span><span class="n">component_parameter_update</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.Pipeline.validate_sae" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">validate_sae</span><span class="p">(</span><span class="n">validation_n_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.validate_sae" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Get validation metrics.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>validation_n_activations</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of activations to use for validation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="nf">validate_sae</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_n_activations</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get validation metrics.</span>

<span class="sd">    Args:</span>
<span class="sd">        validation_n_activations: Number of activations to use for validation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">losses_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">validation_n_activations</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>

    <span class="c1"># Create the metric data stores</span>
    <span class="n">losses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="n">losses_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">source_model_device</span>
    <span class="p">)</span>
    <span class="n">losses_with_reconstruction</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">losses_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">source_model_device</span><span class="p">)</span>
    <span class="n">losses_with_zero_ablation</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">losses_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">source_model_device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">component_idx</span><span class="p">,</span> <span class="n">cache_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_names</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">)</span>

            <span class="n">input_ids</span><span class="p">:</span> <span class="n">Int</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">SOURCE_DATA_BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">POSITION</span><span class="p">)]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span>
                <span class="s2">&quot;input_ids&quot;</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>

            <span class="c1"># Run a forward pass with and without the replaced activations</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
            <span class="n">replacement_hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">replace_activations_hook</span><span class="p">,</span>
                <span class="n">sparse_autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">component_idx</span><span class="o">=</span><span class="n">component_idx</span><span class="p">,</span>
                <span class="n">n_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
                <span class="n">loss_with_reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[</span>
                        <span class="p">(</span>
                            <span class="n">cache_name</span><span class="p">,</span>
                            <span class="n">replacement_hook</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">],</span>
                <span class="p">)</span>
                <span class="n">loss_with_zero_ablation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                    <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">zero_ablate_hook</span><span class="p">)],</span>
                <span class="p">)</span>

                <span class="n">losses</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="n">losses_with_reconstruction</span><span class="p">[</span>
                    <span class="n">batch_idx</span><span class="p">,</span> <span class="n">component_idx</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">loss_with_reconstruction</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="n">losses_with_zero_ablation</span><span class="p">[</span>
                    <span class="n">batch_idx</span><span class="p">,</span> <span class="n">component_idx</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">loss_with_zero_ablation</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Log</span>
    <span class="n">validation_data</span> <span class="o">=</span> <span class="n">ValidationMetricData</span><span class="p">(</span>
        <span class="n">source_model_loss</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span>
        <span class="n">source_model_loss_with_reconstruction</span><span class="o">=</span><span class="n">losses_with_reconstruction</span><span class="p">,</span>
        <span class="n">source_model_loss_with_zero_ablation</span><span class="o">=</span><span class="n">losses_with_zero_ablation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">validation_metrics</span><span class="p">:</span>
        <span class="n">log</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">metric_result</span> <span class="ow">in</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">):</span>
            <span class="n">log</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric_result</span><span class="o">.</span><span class="n">wandb_log</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.PipelineHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>PipelineHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Pipeline hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">PipelineHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pipeline hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">log_frequency</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Training log frequency.&quot;&quot;&quot;</span>

    <span class="n">source_data_batch_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_SOURCE_BATCH_SIZE</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source data batch size.&quot;&quot;&quot;</span>

    <span class="n">train_batch_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_BATCH_SIZE</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train batch size.&quot;&quot;&quot;</span>

    <span class="n">max_store_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_STORE_SIZE</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Max store size.&quot;&quot;&quot;</span>

    <span class="n">max_activations</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mf">2e9</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">))</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Max activations.&quot;&quot;&quot;</span>

    <span class="n">num_workers_data_loading</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of CPU workers for data loading.&quot;&quot;&quot;</span>

    <span class="n">checkpoint_frequency</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mf">5e7</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">))</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checkpoint frequency.&quot;&quot;&quot;</span>

    <span class="n">validation_frequency</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mf">1e8</span><span class="p">,</span> <span class="n">DEFAULT_BATCH_SIZE</span><span class="p">))</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Validation frequency.&quot;&quot;&quot;</span>

    <span class="n">validation_n_activations</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_SOURCE_BATCH_SIZE</span> <span class="o">*</span> <span class="n">DEFAULT_SOURCE_CONTEXT_SIZE</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of activations to use for validation.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.checkpoint_frequency" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">checkpoint_frequency</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mf">50000000.0</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">)))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.checkpoint_frequency" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Checkpoint frequency.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.log_frequency" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">log_frequency</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.log_frequency" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Training log frequency.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.max_activations" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">max_activations</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mf">2000000000.0</span><span class="p">,</span> <span class="n">DEFAULT_STORE_SIZE</span><span class="p">)))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.max_activations" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Max activations.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.max_store_size" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">max_store_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_STORE_SIZE</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.max_store_size" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Max store size.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.num_workers_data_loading" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">num_workers_data_loading</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.num_workers_data_loading" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of CPU workers for data loading.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.source_data_batch_size" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">source_data_batch_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_SOURCE_BATCH_SIZE</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.source_data_batch_size" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source data batch size.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.train_batch_size" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">train_batch_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_BATCH_SIZE</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.train_batch_size" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Train batch size.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.validation_frequency" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">validation_frequency</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">round_to_multiple</span><span class="p">(</span><span class="mf">100000000.0</span><span class="p">,</span> <span class="n">DEFAULT_BATCH_SIZE</span><span class="p">)))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.validation_frequency" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Validation frequency.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.PipelineHyperparameters.validation_n_activations" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">validation_n_activations</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_SOURCE_BATCH_SIZE</span> <span class="o">*</span> <span class="n">DEFAULT_SOURCE_CONTEXT_SIZE</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.PipelineHyperparameters.validation_n_activations" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of activations to use for validation.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.PreTokenizedDataset" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>PreTokenizedDataset</code>


<a href="#sparse_autoencoder.PreTokenizedDataset" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a>[dict]</code></p>

  
      <p>General Pre-Tokenized Dataset from Hugging Face.</p>
<p>Can be used for various datasets available on Hugging Face.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">PreTokenizedDataset</span><span class="p">(</span><span class="n">SourceDataset</span><span class="p">[</span><span class="nb">dict</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;General Pre-Tokenized Dataset from Hugging Face.</span>

<span class="sd">    Can be used for various datasets available on Hugging Face.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">        The method splits each pre-tokenized item based on the context size.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_batch: A batch of source data.</span>
<span class="sd">            context_size: The context size to use for tokenized prompts.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tokenized prompts.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the context size is larger than the tokenized prompt size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokenized_prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset_column_name</span><span class="p">]</span>

        <span class="c1"># Check the context size is not too large</span>
        <span class="k">if</span> <span class="n">context_size</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The context size (</span><span class="si">{</span><span class="n">context_size</span><span class="si">}</span><span class="s2">) is larger than the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;tokenized prompt size (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Chunk each tokenized prompt into blocks of context_size,</span>
        <span class="c1"># discarding the last block if too small.</span>
        <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">tokenized_prompts</span><span class="p">:</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
            <span class="p">]</span>
            <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataset_files</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">dataset_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">pre_download</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize a pre-tokenized dataset from Hugging Face.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset_path: The path to the dataset on Hugging Face (e.g.</span>
<span class="sd">                `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</span>
<span class="sd">            context_size: The context size for tokenized prompts.</span>
<span class="sd">            buffer_size: The buffer size to use when shuffling the dataset when streaming. When</span>
<span class="sd">                streaming a dataset, this just pre-downloads at least `buffer_size` items and then</span>
<span class="sd">                shuffles just that buffer. Note that the generated activations should also be</span>
<span class="sd">                shuffled before training the sparse autoencoder, so a large buffer may not be</span>
<span class="sd">                strictly necessary here. Note also that this is the number of items in the dataset</span>
<span class="sd">                (e.g. number of prompts) and is typically significantly less than the number of</span>
<span class="sd">                tokenized prompts once the preprocessing function has been applied.</span>
<span class="sd">            dataset_dir: Defining the `data_dir` of the dataset configuration.</span>
<span class="sd">            dataset_files: Path(s) to source data file(s).</span>
<span class="sd">            dataset_split: Dataset split (e.g. `train`).</span>
<span class="sd">            dataset_column_name: The column name for the tokenized prompts.</span>
<span class="sd">            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.</span>
<span class="sd">                tokenizing prompts).</span>
<span class="sd">            pre_download: Whether to pre-download the whole dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
            <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
            <span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span>
            <span class="n">dataset_files</span><span class="o">=</span><span class="n">dataset_files</span><span class="p">,</span>
            <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
            <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
            <span class="n">dataset_column_name</span><span class="o">=</span><span class="n">dataset_column_name</span><span class="p">,</span>
            <span class="n">pre_download</span><span class="o">=</span><span class="n">pre_download</span><span class="p">,</span>
            <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.PreTokenizedDataset.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dataset_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dataset_files</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dataset_split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">dataset_column_name</span><span class="o">=</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">pre_download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.PreTokenizedDataset.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize a pre-tokenized dataset from Hugging Face.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataset_path</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The path to the dataset on Hugging Face (e.g.
`alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The context size for tokenized prompts.</p>
            </div>
          </td>
          <td>
                <code>256</code>
          </td>
        </tr>
        <tr>
          <td><code>buffer_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The buffer size to use when shuffling the dataset when streaming. When
streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then
shuffles just that buffer. Note that the generated activations should also be
shuffled before training the sparse autoencoder, so a large buffer may not be
strictly necessary here. Note also that this is the number of items in the dataset
(e.g. number of prompts) and is typically significantly less than the number of
tokenized prompts once the preprocessing function has been applied.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_dir</code></td>
          <td>
                <code>str | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Defining the <code>data_dir</code> of the dataset configuration.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_files</code></td>
          <td>
                <code>str | <span title="collections.abc.Sequence">Sequence</span>[str] | <span title="collections.abc.Mapping">Mapping</span>[str, str | <span title="collections.abc.Sequence">Sequence</span>[str]] | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path(s) to source data file(s).</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_split</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dataset split (e.g. <code>train</code>).</p>
            </div>
          </td>
          <td>
                <code>&#39;train&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_column_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The column name for the tokenized prompts.</p>
            </div>
          </td>
          <td>
                <code>&#39;input_ids&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>preprocess_batch_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The batch size to use just for preprocessing the dataset (e.g.
tokenizing prompts).</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>pre_download</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to pre-download the whole dataset.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dataset_files</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
    <span class="n">dataset_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
    <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">pre_download</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize a pre-tokenized dataset from Hugging Face.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset_path: The path to the dataset on Hugging Face (e.g.</span>
<span class="sd">            `alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2).</span>
<span class="sd">        context_size: The context size for tokenized prompts.</span>
<span class="sd">        buffer_size: The buffer size to use when shuffling the dataset when streaming. When</span>
<span class="sd">            streaming a dataset, this just pre-downloads at least `buffer_size` items and then</span>
<span class="sd">            shuffles just that buffer. Note that the generated activations should also be</span>
<span class="sd">            shuffled before training the sparse autoencoder, so a large buffer may not be</span>
<span class="sd">            strictly necessary here. Note also that this is the number of items in the dataset</span>
<span class="sd">            (e.g. number of prompts) and is typically significantly less than the number of</span>
<span class="sd">            tokenized prompts once the preprocessing function has been applied.</span>
<span class="sd">        dataset_dir: Defining the `data_dir` of the dataset configuration.</span>
<span class="sd">        dataset_files: Path(s) to source data file(s).</span>
<span class="sd">        dataset_split: Dataset split (e.g. `train`).</span>
<span class="sd">        dataset_column_name: The column name for the tokenized prompts.</span>
<span class="sd">        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.</span>
<span class="sd">            tokenizing prompts).</span>
<span class="sd">        pre_download: Whether to pre-download the whole dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
        <span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span>
        <span class="n">dataset_files</span><span class="o">=</span><span class="n">dataset_files</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
        <span class="n">dataset_column_name</span><span class="o">=</span><span class="n">dataset_column_name</span><span class="p">,</span>
        <span class="n">pre_download</span><span class="o">=</span><span class="n">pre_download</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.PreTokenizedDataset.preprocess" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">preprocess</span><span class="p">(</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.PreTokenizedDataset.preprocess" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Preprocess a batch of prompts.</p>
<p>The method splits each pre-tokenized item based on the context size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_batch</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A batch of source data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The context size to use for tokenized prompts.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts">TokenizedPrompts</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tokenized prompts.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the context size is larger than the tokenized prompt size.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">    The method splits each pre-tokenized item based on the context size.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_batch: A batch of source data.</span>
<span class="sd">        context_size: The context size to use for tokenized prompts.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tokenized prompts.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the context size is larger than the tokenized prompt size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tokenized_prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset_column_name</span><span class="p">]</span>

    <span class="c1"># Check the context size is not too large</span>
    <span class="k">if</span> <span class="n">context_size</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The context size (</span><span class="si">{</span><span class="n">context_size</span><span class="si">}</span><span class="s2">) is larger than the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;tokenized prompt size (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">).&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Chunk each tokenized prompt into blocks of context_size,</span>
    <span class="c1"># discarding the last block if too small.</span>
    <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">tokenized_prompts</span><span class="p">:</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
        <span class="p">]</span>
        <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.SourceDataHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>SourceDataHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Source data hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SourceDataHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source data hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">dataset_path</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dataset path.&quot;&quot;&quot;</span>

    <span class="n">context_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_SOURCE_CONTEXT_SIZE</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context size.&quot;&quot;&quot;</span>

    <span class="n">dataset_column_name</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&quot;input_ids&quot;</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dataset column name.&quot;&quot;&quot;</span>

    <span class="n">dataset_dir</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dataset directory (within the HF dataset)&quot;&quot;&quot;</span>

    <span class="n">dataset_files</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dataset files (within the HF dataset).&quot;&quot;&quot;</span>

    <span class="n">pre_download</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Whether to pre-download the dataset.&quot;&quot;&quot;</span>

    <span class="n">pre_tokenized</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;If the dataset is pre-tokenized.&quot;&quot;&quot;</span>

    <span class="n">tokenizer_name</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tokenizer name.</span>

<span class="sd">    Only set this if the dataset is not pre-tokenized.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Post initialisation checks.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If there is an error in the source data hyperparameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_tokenized</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;The tokenizer name must be specified, when `pre_tokenized` is False.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_tokenized</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;The tokenizer name must not be set, when `pre_tokenized` is True.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.context_size" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">context_size</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">DEFAULT_SOURCE_CONTEXT_SIZE</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.context_size" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Context size.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.dataset_column_name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">dataset_column_name</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;input_ids&#39;</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_column_name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Dataset column name.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.dataset_dir" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">dataset_dir</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_dir" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Dataset directory (within the HF dataset)</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.dataset_files" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">dataset_files</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_files" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Dataset files (within the HF dataset).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.dataset_path" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">dataset_path</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.dataset_path" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Dataset path.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.pre_download" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">pre_download</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.pre_download" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Whether to pre-download the dataset.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.pre_tokenized" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">pre_tokenized</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.pre_tokenized" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>If the dataset is pre-tokenized.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.tokenizer_name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">tokenizer_name</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceDataHyperparameters.tokenizer_name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Tokenizer name.</p>
<p>Only set this if the dataset is not pre-tokenized.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SourceDataHyperparameters.__post_init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">__post_init__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.SourceDataHyperparameters.__post_init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Post initialisation checks.</p>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there is an error in the source data hyperparameters.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Post initialisation checks.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If there is an error in the source data hyperparameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_tokenized</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;The tokenizer name must be specified, when `pre_tokenized` is False.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_tokenized</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;The tokenizer name must not be set, when `pre_tokenized` is True.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.SourceModelHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>SourceModelHyperparameters</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.SourceModelHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.NestedParameter">NestedParameter</a></code></p>

  
      <p>Source model hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SourceModelHyperparameters</span><span class="p">(</span><span class="n">NestedParameter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">name</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model name.&quot;&quot;&quot;</span>

    <span class="n">cache_names</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model hook site.&quot;&quot;&quot;</span>

    <span class="n">hook_dimension</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model hook point dimension.&quot;&quot;&quot;</span>

    <span class="n">dtype</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model dtype.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceModelHyperparameters.cache_names" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">cache_names</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceModelHyperparameters.cache_names" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source model hook site.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceModelHyperparameters.dtype" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">dtype</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Parameter</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceModelHyperparameters.dtype" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source model dtype.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceModelHyperparameters.hook_dimension" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">hook_dimension</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceModelHyperparameters.hook_dimension" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source model hook point dimension.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SourceModelHyperparameters.name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">name</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SourceModelHyperparameters.name" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Source model name.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.SourceModelRuntimeHyperparameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>SourceModelRuntimeHyperparameters</code>


<a href="#sparse_autoencoder.SourceModelRuntimeHyperparameters" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="typing.TypedDict">TypedDict</span></code></p>

  
      <p>Source model runtime hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SourceModelRuntimeHyperparameters</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model runtime hyperparameters.&quot;&quot;&quot;</span>

    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">cache_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">hook_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.SparseAutoencoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>SparseAutoencoder</code>


<a href="#sparse_autoencoder.SparseAutoencoder" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Sparse Autoencoder Model.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse Autoencoder Model.&quot;&quot;&quot;</span>

    <span class="n">config</span><span class="p">:</span> <span class="n">SparseAutoencoderConfig</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model config.&quot;&quot;&quot;</span>

    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimated Geometric Median of the Dataset.</span>

<span class="sd">    Used for initialising :attr:`tied_bias`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tied_bias</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Parameter</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tied Bias Parameter.</span>

<span class="sd">    The same bias is used pre-encoder and post-decoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pre_encoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pre-Encoder Bias.&quot;&quot;&quot;</span>

    <span class="n">encoder</span><span class="p">:</span> <span class="n">LinearEncoder</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encoder.&quot;&quot;&quot;</span>

    <span class="n">decoder</span><span class="p">:</span> <span class="n">UnitNormDecoder</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decoder.&quot;&quot;&quot;</span>

    <span class="n">post_decoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Post-Decoder Bias.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">SparseAutoencoderConfig</span><span class="p">,</span>
        <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Model config.</span>
<span class="sd">            geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
        <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
        <span class="n">tied_bias_shape</span> <span class="o">=</span> <span class="n">shape_with_optional_dimensions</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">geometric_median_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tied_bias_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Initialize the tied bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">tied_bias_shape</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

        <span class="c1"># Initialize the components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_encoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span>
            <span class="n">input_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span>
            <span class="n">learnt_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span>
            <span class="n">learnt_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span>
            <span class="n">decoded_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">post_decoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ForwardPassResult</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of learned activations and decoded activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_encoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
        <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_decoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ForwardPassResult</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
        <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reset_optimizer_parameter_details</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">ResetOptimizerParameterDetails</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset optimizer parameter details.</span>

<span class="sd">        Details of the parameters that should be reset in the optimizer, when resetting</span>
<span class="sd">        dictionary vectors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of tuples of the form `(parameter, axis)`, where `parameter` is the parameter to</span>
<span class="sd">            reset (e.g. encoder.weight), and `axis` is the axis of the parameter to reset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">reset_optimizer_parameter_details</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">reset_optimizer_parameter_details</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">post_backwards_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Hook to be called after each learning step.</span>

<span class="sd">        This can be used to e.g. constrain weights to unit norm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">constrain_weights_unit_norm</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="nf">get_single_component_state_dict</span><span class="p">(</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">SparseAutoencoderState</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">:</span> <span class="n">NonNegativeInt</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the state dict for a single component.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: Sparse Autoencoder state.</span>
<span class="sd">            component_idx: Index of the component to get the state dict for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            State dict for the component.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the state dict doesn&#39;t contain a components dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check the state has a components dimension</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;Trying to load a single component from the state dict, but the state dict &quot;</span>
                <span class="s2">&quot;doesn&#39;t contain a components dimension.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Return the state dict for the component</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the model config and state dict to a file.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_path: Path to save the model to.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">file_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">SparseAutoencoderState</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">file_path</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
        <span class="n">file_path</span><span class="p">:</span> <span class="n">FILE_LIKE</span><span class="p">,</span>
        <span class="n">component_idx</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseAutoencoder&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the model from a file.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_path: Path to load the model from.</span>
<span class="sd">            component_idx: If loading a state dict from a model that has been trained on multiple</span>
<span class="sd">                components (e.g. all MLP layers) you may want to to load just one component. In this</span>
<span class="sd">                case you can set `component_idx` to the index of the component to load. Note you</span>
<span class="sd">                should not set this if you want to load a state dict from a model that has been</span>
<span class="sd">                trained on a single component (or if you want to load all components).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The loaded model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the file</span>
        <span class="n">serialized_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">SparseAutoencoderState</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">serialized_state</span><span class="p">)</span>

        <span class="c1"># Initialise the model</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">SparseAutoencoderConfig</span><span class="p">(</span>
            <span class="n">n_input_features</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span>
            <span class="n">n_learned_features</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span> <span class="k">if</span> <span class="n">component_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">SparseAutoencoder</span><span class="o">.</span><span class="n">get_single_component_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">component_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">state</span><span class="o">.</span><span class="n">state_dict</span>
        <span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">SparseAutoencoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">save_to_wandb</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">artifact_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">directory</span><span class="p">:</span> <span class="n">DirectoryPath</span> <span class="o">=</span> <span class="n">DEFAULT_TMP_DIR</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the model to wandb.</span>

<span class="sd">        Args:</span>
<span class="sd">            artifact_name: A human-readable name for this artifact, which is how you can identify</span>
<span class="sd">                this artifact in the UI or reference it in use_artifact calls. Names can contain</span>
<span class="sd">                letters, numbers, underscores, hyphens, and dots. The name must be unique across a</span>
<span class="sd">                project. Example: &quot;sweep_name 1e9 activations&quot;.</span>
<span class="sd">            directory: Directory to save the model to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the wandb artifact.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If wandb is not initialised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Save the file</span>
        <span class="n">directory</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">file_name</span> <span class="o">=</span> <span class="n">artifact_name</span> <span class="o">+</span> <span class="s2">&quot;.pt&quot;</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">/</span> <span class="n">file_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

        <span class="c1"># Upload to wandb</span>
        <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Trying to save the model to wandb, but wandb is not initialised.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
        <span class="n">artifact</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Artifact</span><span class="p">(</span>
            <span class="n">artifact_name</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Sparse Autoencoder model state, created with `sparse_autoencoder`.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">artifact</span><span class="o">.</span><span class="n">add_file</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">file_path</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sae-model-state.pt&quot;</span><span class="p">)</span>
        <span class="n">artifact</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">artifact</span><span class="p">)</span>
        <span class="n">artifact</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">artifact</span><span class="o">.</span><span class="n">source_qualified_name</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_from_wandb</span><span class="p">(</span>
        <span class="n">wandb_artifact_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">component_idx</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseAutoencoder&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the model from wandb.</span>

<span class="sd">        Args:</span>
<span class="sd">            wandb_artifact_name: Name of the wandb artifact to load the model from (e.g.</span>
<span class="sd">                &quot;username/project/artifact_name:version&quot;).</span>
<span class="sd">            component_idx: If loading a state dict from a model that has been trained on multiple</span>
<span class="sd">                components (e.g. all MLP layers) you may want to to load just one component. In this</span>
<span class="sd">                case you can set `component_idx` to the index of the component to load. Note you</span>
<span class="sd">                should not set this if you want to load a state dict from a model that has been</span>
<span class="sd">                trained on a single component (or if you want to load all components).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The loaded model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">api</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Api</span><span class="p">()</span>
        <span class="n">artifact</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">artifact</span><span class="p">(</span><span class="n">wandb_artifact_name</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
        <span class="n">download_path</span> <span class="o">=</span> <span class="n">artifact</span><span class="o">.</span><span class="n">download</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">SparseAutoencoder</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">download_path</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;sae-model-state.pt&quot;</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_to_hugging_face</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">directory</span><span class="p">:</span> <span class="n">DirectoryPath</span> <span class="o">=</span> <span class="n">DEFAULT_TMP_DIR</span><span class="p">,</span>
        <span class="n">hf_access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the model to Hugging Face.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_name: Name of the file (e.g. &quot;model-something.pt&quot;).</span>
<span class="sd">            repo_id: ID of the repo to save the model to.</span>
<span class="sd">            directory: Directory to save the model to.</span>
<span class="sd">            hf_access_token: Hugging Face access token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Save the file</span>
        <span class="n">directory</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">/</span> <span class="n">file_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

        <span class="c1"># Upload to Hugging Face</span>
        <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">hf_access_token</span><span class="p">)</span>
        <span class="n">api</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span>
            <span class="n">path_or_fileobj</span><span class="o">=</span><span class="n">file_path</span><span class="p">,</span>
            <span class="n">path_in_repo</span><span class="o">=</span><span class="n">file_name</span><span class="p">,</span>
            <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
            <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_from_hugging_face</span><span class="p">(</span>
        <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">component_idx</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseAutoencoder&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the model from Hugging Face.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_name: File name of the .pt state file.</span>
<span class="sd">            repo_id: ID of the repo to load the model from.</span>
<span class="sd">            component_idx: If loading a state dict from a model that has been trained on multiple</span>
<span class="sd">                components (e.g. all MLP layers) you may want to to load just one component. In this</span>
<span class="sd">                case you can set `component_idx` to the index of the component to load. Note you</span>
<span class="sd">                should not set this if you want to load a state dict from a model that has been</span>
<span class="sd">                trained on a single component (or if you want to load all components).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The loaded model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">local_file</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
            <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
            <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
            <span class="n">filename</span><span class="o">=</span><span class="n">file_name</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="s2">&quot;main&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">SparseAutoencoder</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">local_file</span><span class="p">),</span> <span class="n">component_idx</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.config" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">config</span><span class="p">:</span> <span class="n">SparseAutoencoderConfig</span> <span class="o">=</span> <span class="n">config</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.config" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Model config.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.decoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">decoder</span><span class="p">:</span> <span class="n">UnitNormDecoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span><span class="n">learnt_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">decoded_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.decoder" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Decoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.encoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">encoder</span><span class="p">:</span> <span class="n">LinearEncoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.encoder" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Encoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Estimated Geometric Median of the Dataset.</p>
<p>Used for initialising :attr:<code>tied_bias</code>.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.post_decoder_bias" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">post_decoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.post_decoder_bias" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Post-Decoder Bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.pre_encoder_bias" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">pre_encoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Pre-Encoder Bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.reset_optimizer_parameter_details" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">reset_optimizer_parameter_details</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ResetOptimizerParameterDetails</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.reset_optimizer_parameter_details" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset optimizer parameter details.</p>
<p>Details of the parameters that should be reset in the optimizer, when resetting
dictionary vectors.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails" href="autoencoder/types/#sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails">ResetOptimizerParameterDetails</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>List of tuples of the form <code>(parameter, axis)</code>, where <code>parameter</code> is the parameter to</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails" href="autoencoder/types/#sparse_autoencoder.autoencoder.types.ResetOptimizerParameterDetails">ResetOptimizerParameterDetails</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>reset (e.g. encoder.weight), and <code>axis</code> is the axis of the parameter to reset.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoder.tied_bias" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">tied_bias</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">tied_bias_shape</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.tied_bias" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Tied Bias Parameter.</p>
<p>The same bias is used pre-encoder and post-decoder.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">geometric_median_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Sparse Autoencoder Model.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>config</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoderConfig" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderConfig">SparseAutoencoderConfig</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Model config.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>geometric_median_dataset</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)] | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Estimated geometric median of the dataset.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">SparseAutoencoderConfig</span><span class="p">,</span>
    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">    Args:</span>
<span class="sd">        config: Model config.</span>
<span class="sd">        geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
    <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
    <span class="n">tied_bias_shape</span> <span class="o">=</span> <span class="n">shape_with_optional_dimensions</span><span class="p">(</span>
        <span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">geometric_median_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tied_bias_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Initialize the tied bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">tied_bias_shape</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

    <span class="c1"># Initialize the components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pre_encoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span>
        <span class="n">input_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span>
        <span class="n">learnt_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span>
        <span class="n">learnt_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span>
        <span class="n">decoded_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">post_decoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.forward" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT_OPTIONAL">COMPONENT_OPTIONAL</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.ForwardPassResult" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.ForwardPassResult">ForwardPassResult</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of learned activations and decoded activations.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT_OPTIONAL</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)</span>
    <span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ForwardPassResult</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of learned activations and decoded activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_encoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
    <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_decoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ForwardPassResult</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.get_single_component_state_dict" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">get_single_component_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.get_single_component_state_dict" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Get the state dict for a single component.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoderState" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoderState">SparseAutoencoderState</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse Autoencoder state.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code><span title="pydantic.NonNegativeInt">NonNegativeInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Index of the component to get the state dict for.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>dict[str, <span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>State dict for the component.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the state dict doesn't contain a components dimension.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="nd">@validate_call</span>
<span class="k">def</span> <span class="nf">get_single_component_state_dict</span><span class="p">(</span>
    <span class="n">state</span><span class="p">:</span> <span class="n">SparseAutoencoderState</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">:</span> <span class="n">NonNegativeInt</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the state dict for a single component.</span>

<span class="sd">    Args:</span>
<span class="sd">        state: Sparse Autoencoder state.</span>
<span class="sd">        component_idx: Index of the component to get the state dict for.</span>

<span class="sd">    Returns:</span>
<span class="sd">        State dict for the component.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the state dict doesn&#39;t contain a components dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check the state has a components dimension</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Trying to load a single component from the state dict, but the state dict &quot;</span>
            <span class="s2">&quot;doesn&#39;t contain a components dimension.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Return the state dict for the component</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">initialize_tied_parameters</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the tied parameters.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
    <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.load" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">load</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">component_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.load" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Load the model from a file.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>file_path</code></td>
          <td>
                <code><span title="torch.serialization.FILE_LIKE">FILE_LIKE</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path to load the model from.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If loading a state dict from a model that has been trained on multiple
components (e.g. all MLP layers) you may want to to load just one component. In this
case you can set <code>component_idx</code> to the index of the component to load. Note you
should not set this if you want to load a state dict from a model that has been
trained on a single component (or if you want to load all components).</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The loaded model.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
    <span class="n">file_path</span><span class="p">:</span> <span class="n">FILE_LIKE</span><span class="p">,</span>
    <span class="n">component_idx</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseAutoencoder&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load the model from a file.</span>

<span class="sd">    Args:</span>
<span class="sd">        file_path: Path to load the model from.</span>
<span class="sd">        component_idx: If loading a state dict from a model that has been trained on multiple</span>
<span class="sd">            components (e.g. all MLP layers) you may want to to load just one component. In this</span>
<span class="sd">            case you can set `component_idx` to the index of the component to load. Note you</span>
<span class="sd">            should not set this if you want to load a state dict from a model that has been</span>
<span class="sd">            trained on a single component (or if you want to load all components).</span>

<span class="sd">    Returns:</span>
<span class="sd">        The loaded model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the file</span>
    <span class="n">serialized_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">SparseAutoencoderState</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">serialized_state</span><span class="p">)</span>

    <span class="c1"># Initialise the model</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">SparseAutoencoderConfig</span><span class="p">(</span>
        <span class="n">n_input_features</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_input_features</span><span class="p">,</span>
        <span class="n">n_learned_features</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_components</span> <span class="k">if</span> <span class="n">component_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">SparseAutoencoder</span><span class="o">.</span><span class="n">get_single_component_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">component_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">state</span><span class="o">.</span><span class="n">state_dict</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SparseAutoencoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.load_from_hugging_face" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">load_from_hugging_face</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="n">repo_id</span><span class="p">,</span> <span class="n">component_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.load_from_hugging_face" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Load the model from Hugging Face.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>file_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>File name of the .pt state file.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>repo_id</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>ID of the repo to load the model from.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If loading a state dict from a model that has been trained on multiple
components (e.g. all MLP layers) you may want to to load just one component. In this
case you can set <code>component_idx</code> to the index of the component to load. Note you
should not set this if you want to load a state dict from a model that has been
trained on a single component (or if you want to load all components).</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The loaded model.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">load_from_hugging_face</span><span class="p">(</span>
    <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">component_idx</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseAutoencoder&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load the model from Hugging Face.</span>

<span class="sd">    Args:</span>
<span class="sd">        file_name: File name of the .pt state file.</span>
<span class="sd">        repo_id: ID of the repo to load the model from.</span>
<span class="sd">        component_idx: If loading a state dict from a model that has been trained on multiple</span>
<span class="sd">            components (e.g. all MLP layers) you may want to to load just one component. In this</span>
<span class="sd">            case you can set `component_idx` to the index of the component to load. Note you</span>
<span class="sd">            should not set this if you want to load a state dict from a model that has been</span>
<span class="sd">            trained on a single component (or if you want to load all components).</span>

<span class="sd">    Returns:</span>
<span class="sd">        The loaded model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">local_file</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
        <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">filename</span><span class="o">=</span><span class="n">file_name</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="s2">&quot;main&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">SparseAutoencoder</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">local_file</span><span class="p">),</span> <span class="n">component_idx</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.load_from_wandb" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">load_from_wandb</span><span class="p">(</span><span class="n">wandb_artifact_name</span><span class="p">,</span> <span class="n">component_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.load_from_wandb" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Load the model from wandb.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>wandb_artifact_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the wandb artifact to load the model from (e.g.
"username/project/artifact_name:version").</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If loading a state dict from a model that has been trained on multiple
components (e.g. all MLP layers) you may want to to load just one component. In this
case you can set <code>component_idx</code> to the index of the component to load. Note you
should not set this if you want to load a state dict from a model that has been
trained on a single component (or if you want to load all components).</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The loaded model.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">load_from_wandb</span><span class="p">(</span>
    <span class="n">wandb_artifact_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">component_idx</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseAutoencoder&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load the model from wandb.</span>

<span class="sd">    Args:</span>
<span class="sd">        wandb_artifact_name: Name of the wandb artifact to load the model from (e.g.</span>
<span class="sd">            &quot;username/project/artifact_name:version&quot;).</span>
<span class="sd">        component_idx: If loading a state dict from a model that has been trained on multiple</span>
<span class="sd">            components (e.g. all MLP layers) you may want to to load just one component. In this</span>
<span class="sd">            case you can set `component_idx` to the index of the component to load. Note you</span>
<span class="sd">            should not set this if you want to load a state dict from a model that has been</span>
<span class="sd">            trained on a single component (or if you want to load all components).</span>

<span class="sd">    Returns:</span>
<span class="sd">        The loaded model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">api</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Api</span><span class="p">()</span>
    <span class="n">artifact</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">artifact</span><span class="p">(</span><span class="n">wandb_artifact_name</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
    <span class="n">download_path</span> <span class="o">=</span> <span class="n">artifact</span><span class="o">.</span><span class="n">download</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">SparseAutoencoder</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">download_path</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;sae-model-state.pt&quot;</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.post_backwards_hook" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">post_backwards_hook</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.post_backwards_hook" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Hook to be called after each learning step.</p>
<p>This can be used to e.g. constrain weights to unit norm.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">post_backwards_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook to be called after each learning step.</span>

<span class="sd">    This can be used to e.g. constrain weights to unit norm.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">constrain_weights_unit_norm</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.reset_parameters" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.reset_parameters" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset the parameters.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.save" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">save</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.save" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Save the model config and state dict to a file.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>file_path</code></td>
          <td>
                <code><span title="pathlib.Path">Path</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path to save the model to.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save the model config and state dict to a file.</span>

<span class="sd">    Args:</span>
<span class="sd">        file_path: Path to save the model to.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">file_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">SparseAutoencoderState</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">file_path</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.save_to_hugging_face" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">save_to_hugging_face</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="n">repo_id</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">DEFAULT_TMP_DIR</span><span class="p">,</span> <span class="n">hf_access_token</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.save_to_hugging_face" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Save the model to Hugging Face.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>file_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the file (e.g. "model-something.pt").</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>repo_id</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>ID of the repo to save the model to.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>directory</code></td>
          <td>
                <code><span title="pydantic.DirectoryPath">DirectoryPath</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Directory to save the model to.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.autoencoder.model.DEFAULT_TMP_DIR">DEFAULT_TMP_DIR</span></code>
          </td>
        </tr>
        <tr>
          <td><code>hf_access_token</code></td>
          <td>
                <code>str | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Hugging Face access token.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_to_hugging_face</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">DirectoryPath</span> <span class="o">=</span> <span class="n">DEFAULT_TMP_DIR</span><span class="p">,</span>
    <span class="n">hf_access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save the model to Hugging Face.</span>

<span class="sd">    Args:</span>
<span class="sd">        file_name: Name of the file (e.g. &quot;model-something.pt&quot;).</span>
<span class="sd">        repo_id: ID of the repo to save the model to.</span>
<span class="sd">        directory: Directory to save the model to.</span>
<span class="sd">        hf_access_token: Hugging Face access token.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Save the file</span>
    <span class="n">directory</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">/</span> <span class="n">file_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

    <span class="c1"># Upload to Hugging Face</span>
    <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">hf_access_token</span><span class="p">)</span>
    <span class="n">api</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span>
        <span class="n">path_or_fileobj</span><span class="o">=</span><span class="n">file_path</span><span class="p">,</span>
        <span class="n">path_in_repo</span><span class="o">=</span><span class="n">file_name</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
        <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.SparseAutoencoder.save_to_wandb" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">save_to_wandb</span><span class="p">(</span><span class="n">artifact_name</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">DEFAULT_TMP_DIR</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.save_to_wandb" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Save the model to wandb.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>artifact_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A human-readable name for this artifact, which is how you can identify
this artifact in the UI or reference it in use_artifact calls. Names can contain
letters, numbers, underscores, hyphens, and dots. The name must be unique across a
project. Example: "sweep_name 1e9 activations".</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>directory</code></td>
          <td>
                <code><span title="pydantic.DirectoryPath">DirectoryPath</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Directory to save the model to.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.autoencoder.model.DEFAULT_TMP_DIR">DEFAULT_TMP_DIR</span></code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the wandb artifact.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If wandb is not initialised.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_to_wandb</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">artifact_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">DirectoryPath</span> <span class="o">=</span> <span class="n">DEFAULT_TMP_DIR</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save the model to wandb.</span>

<span class="sd">    Args:</span>
<span class="sd">        artifact_name: A human-readable name for this artifact, which is how you can identify</span>
<span class="sd">            this artifact in the UI or reference it in use_artifact calls. Names can contain</span>
<span class="sd">            letters, numbers, underscores, hyphens, and dots. The name must be unique across a</span>
<span class="sd">            project. Example: &quot;sweep_name 1e9 activations&quot;.</span>
<span class="sd">        directory: Directory to save the model to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the wandb artifact.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If wandb is not initialised.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Save the file</span>
    <span class="n">directory</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="n">artifact_name</span> <span class="o">+</span> <span class="s2">&quot;.pt&quot;</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">directory</span> <span class="o">/</span> <span class="n">file_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

    <span class="c1"># Upload to wandb</span>
    <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Trying to save the model to wandb, but wandb is not initialised.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
    <span class="n">artifact</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Artifact</span><span class="p">(</span>
        <span class="n">artifact_name</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Sparse Autoencoder model state, created with `sparse_autoencoder`.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">artifact</span><span class="o">.</span><span class="n">add_file</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">file_path</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sae-model-state.pt&quot;</span><span class="p">)</span>
    <span class="n">artifact</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">artifact</span><span class="p">)</span>
    <span class="n">artifact</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">artifact</span><span class="o">.</span><span class="n">source_qualified_name</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.SparseAutoencoderConfig" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>SparseAutoencoderConfig</code>


<a href="#sparse_autoencoder.SparseAutoencoderConfig" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pydantic.BaseModel">BaseModel</span></code></p>

  
      <p>SAE model config.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SparseAutoencoderConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SAE model config.&quot;&quot;&quot;</span>

    <span class="n">n_input_features</span><span class="p">:</span> <span class="n">PositiveInt</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of input features.</span>

<span class="sd">    E.g. `d_mlp` if training on MLP activations from TransformerLens).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_learned_features</span><span class="p">:</span> <span class="n">PositiveInt</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of learned features.</span>

<span class="sd">    The initial paper experimented with 1 to 256 times the number of input features, and primarily</span>
<span class="sd">    used a multiple of 8.&quot;&quot;&quot;</span>

    <span class="n">n_components</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of source model components the SAE is trained on.&quot;&quot;</span>

<span class="sd">    This is useful if you want to train the SAE on several components of the source model at once.</span>
<span class="sd">    If `None`, the SAE is assumed to be trained on just one component (in this case the model won&#39;t</span>
<span class="sd">    contain a component axis in any of the parameters).</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoderConfig.n_components" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_components</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoderConfig.n_components" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of source model components the SAE is trained on.""</p>
<p>This is useful if you want to train the SAE on several components of the source model at once.
If <code>None</code>, the SAE is assumed to be trained on just one component (in this case the model won't
contain a component axis in any of the parameters).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoderConfig.n_input_features" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_input_features</span><span class="p">:</span> <span class="n">PositiveInt</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoderConfig.n_input_features" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of input features.</p>
<p>E.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.SparseAutoencoderConfig.n_learned_features" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_learned_features</span><span class="p">:</span> <span class="n">PositiveInt</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoderConfig.n_learned_features" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of learned features.</p>
<p>The initial paper experimented with 1 to 256 times the number of input features, and primarily
used a multiple of 8.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.SweepConfig" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>SweepConfig</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#sparse_autoencoder.SweepConfig" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig" href="train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig">WandbSweepConfig</a></code></p>

  
      <p>Sweep Config.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/sweep_config.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SweepConfig</span><span class="p">(</span><span class="n">WandbSweepConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sweep Config.&quot;&quot;&quot;</span>

    <span class="n">parameters</span><span class="p">:</span> <span class="n">Hyperparameters</span>

    <span class="n">method</span><span class="p">:</span> <span class="n">Method</span> <span class="o">=</span> <span class="n">Method</span><span class="o">.</span><span class="n">GRID</span>

    <span class="n">metric</span><span class="p">:</span> <span class="n">Metric</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">Metric</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;train/loss/total_loss&quot;</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.TensorActivationStore" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>TensorActivationStore</code>


<a href="#sparse_autoencoder.TensorActivationStore" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>Tensor Activation Store.</p>
<p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation
vectors to be stored to be known in advance. Multiprocess safe.</p>
<p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with
additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p>
<p>Examples:
Create an empty activation dataset:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=100, n_components=2)
</code></pre>
<p>Add a single activation vector to the dataset (for a component):</p>
<pre><code>&gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)
&gt;&gt;&gt; store.append(torch.randn(100), component_idx=1)
&gt;&gt;&gt; len(store)
1
</code></pre>
<p>Add a [batch, neurons] activation tensor to the dataset:</p>
<pre><code>&gt;&gt;&gt; store.empty()
&gt;&gt;&gt; batch = torch.randn(10, 100)
&gt;&gt;&gt; store.extend(batch, component_idx=0)
&gt;&gt;&gt; store.extend(batch, component_idx=1)
&gt;&gt;&gt; len(store)
10
</code></pre>
<p>Shuffle the dataset <strong>before passing it to the DataLoader</strong>:</p>
<pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument
</code></pre>
<p>Use the dataloader to iterate over the dataset:</p>
<pre><code>&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)
&gt;&gt;&gt; next_item = next(iter(loader))
&gt;&gt;&gt; next_item.shape
torch.Size([2, 2, 100])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TensorActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tensor Activation Store.</span>

<span class="sd">    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation</span>
<span class="sd">    vectors to be stored to be known in advance. Multiprocess safe.</span>

<span class="sd">    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with</span>
<span class="sd">    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).</span>

<span class="sd">    Examples:</span>
<span class="sd">    Create an empty activation dataset:</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, n_neurons=100, n_components=2)</span>

<span class="sd">    Add a single activation vector to the dataset (for a component):</span>

<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=1)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        1</span>

<span class="sd">    Add a [batch, neurons] activation tensor to the dataset:</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(batch, component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(batch, component_idx=1)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        10</span>

<span class="sd">    Shuffle the dataset **before passing it to the DataLoader**:</span>

<span class="sd">        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument</span>

<span class="sd">    Use the dataloader to iterate over the dataset:</span>

<span class="sd">        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)</span>
<span class="sd">        &gt;&gt;&gt; next_item = next(iter(loader))</span>
<span class="sd">        &gt;&gt;&gt; next_item.shape</span>
<span class="sd">        torch.Size([2, 2, 100])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_data</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ITEMS</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Underlying Tensor Data Store.&quot;&quot;&quot;</span>

    <span class="n">_items_stored</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of items stored.&quot;&quot;&quot;</span>

    <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum Number of Items to Store.&quot;&quot;&quot;</span>

    <span class="n">_n_components</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of components&quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_components</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of components.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">current_activations_stored_per_component</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of activations stored per component.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span>

    <span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_items</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">n_neurons</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">n_components</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the Tensor Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_items: Maximum number of items to store per component (individual activation</span>
<span class="sd">                vectors).</span>
<span class="sd">            n_neurons: Number of neurons in each activation vector.</span>
<span class="sd">            n_components: Number of components to store (i.e. number of source models).</span>
<span class="sd">            device: Device to store the activation vectors on.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_items</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Returns the number of activation vectors per component in the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            2</span>

<span class="sd">        Returns:</span>
<span class="sd">            The number of activation vectors in the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Min as this is the amount of activations that can be fetched by get_item</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_activations_stored_per_component</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)</span>
<span class="sd">            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100</span>
<span class="sd">            800</span>

<span class="sd">        Returns:</span>
<span class="sd">            The size of the underlying tensor in bytes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="nb">slice</span> <span class="o">|</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ANY</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)</span>
<span class="sd">            &gt;&gt;&gt; store[1, 0]</span>
<span class="sd">            tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([0.]), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([1.]), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([2.]), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">        &gt;&gt;&gt; [store[i, 0].item() for i in range(3)]</span>
<span class="sd">        [0.0, 2.0, 1.0]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Generate a permutation of the indices for the active data</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

        <span class="c1"># Use this permutation to shuffle the active data in-place</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">],</span> <span class="n">component_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a single item to the store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store[1, 0]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            item: The item to append to the dataset.</span>
<span class="sd">            component_idx: The component index to append the item to.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If there is no space remaining.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check we have space</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">StoreFullError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span> <span class="n">component_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)],</span>
        <span class="n">component_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a batch to the store.</span>

<span class="sd">        Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The batch to append to the dataset.</span>
<span class="sd">            component_idx: The component index to append the batch to.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If there is no space remaining.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check we have space</span>
        <span class="n">n_activation_tensors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_activation_tensors</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">StoreFullError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">n_activation_tensors</span><span class="p">,</span>
            <span class="n">component_idx</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">n_activation_tensors</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We don&#39;t need to zero the data, just reset the number of items stored</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.TensorActivationStore.current_activations_stored_per_component" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">current_activations_stored_per_component</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.TensorActivationStore.current_activations_stored_per_component" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of activations stored per component.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.TensorActivationStore.max_items" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.TensorActivationStore.max_items" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Maximum Number of Items to Store.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sparse_autoencoder.TensorActivationStore.n_components" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <code class="highlight language-python"><span class="n">n_components</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.TensorActivationStore.n_components" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of components.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.__getitem__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__getitem__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">TensorActivationStore</span><span class="p">(</span><span class="n">max_items</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">component_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">component_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="go">tensor([1., 1., 1., 1., 1.])</span>
</code></pre></div>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>index</code></td>
          <td>
                <code>tuple[int, ...] | slice | int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The index of the tensor to fetch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.ANY" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.ANY">ANY</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="nb">slice</span> <span class="o">|</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">ANY</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=5, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store[1, 0]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">max_items</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the Tensor Activation Store.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>max_items</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum number of items to store per component (individual activation
vectors).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_neurons</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of neurons in each activation vector.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_components</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of components to store (i.e. number of source models).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="torch.device">device</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Device to store the activation vectors on.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span>
<span class="normal">99</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">max_items</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">n_neurons</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">n_components</span><span class="p">:</span> <span class="n">PositiveInt</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the Tensor Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        max_items: Maximum number of items to store per component (individual activation</span>
<span class="sd">            vectors).</span>
<span class="sd">        n_neurons: Number of neurons in each activation vector.</span>
<span class="sd">        n_components: Number of components to store (i.e. number of source models).</span>
<span class="sd">        device: Device to store the activation vectors on.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">n_components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_items</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.__len__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__len__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>
<p>Returns the number of activation vectors per component in the dataset.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)
store.append(torch.randn(100), component_idx=0)
store.append(torch.randn(100), component_idx=0)
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of activation vectors in the dataset.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Returns the number of activation vectors per component in the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, n_neurons=100, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100), component_idx=0)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">    Returns:</span>
<span class="sd">        The number of activation vectors in the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Min as this is the amount of activations that can be fetched by get_item</span>
    <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_activations_stored_per_component</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.__sizeof__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">__sizeof__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__sizeof__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Sizeof Dunder Method.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)
store.<strong>sizeof</strong>() # Pre-allocated tensor of 2x100
800</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The size of the underlying tensor in bytes.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, n_neurons=100, n_components=1)</span>
<span class="sd">        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100</span>
<span class="sd">        800</span>

<span class="sd">    Returns:</span>
<span class="sd">        The size of the underlying tensor in bytes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.append" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.append" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Add a single item to the store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)
store.append(torch.zeros(5), component_idx=0)
store.append(torch.ones(5), component_idx=0)
store[1, 0]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>item</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The item to append to the dataset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The component index to append the item to.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>IndexError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there is no space remaining.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">],</span> <span class="n">component_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a single item to the store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; store[1, 0]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        item: The item to append to the dataset.</span>
<span class="sd">        component_idx: The component index to append the item to.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If there is no space remaining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check we have space</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StoreFullError</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">],</span> <span class="n">component_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.empty" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.empty" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Empty the store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)
store.extend(torch.zeros(2, 5), component_idx=0)
len(store)
2
store.empty()
len(store)
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>
<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We don&#39;t need to zero the data, just reset the number of items stored</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.extend" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">component_idx</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.extend" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Add a batch to the store.</p>
<p>Examples:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)
store.extend(torch.zeros(2, 5), component_idx=0)
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE">INPUT_OUTPUT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The batch to append to the dataset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>component_idx</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The component index to append the batch to.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>IndexError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there is no space remaining.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="p">)],</span>
    <span class="n">component_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a batch to the store.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=5, n_components=1)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(2, 5), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batch to append to the dataset.</span>
<span class="sd">        component_idx: The component index to append the batch to.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If there is no space remaining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check we have space</span>
    <span class="n">n_activation_tensors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_activation_tensors</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StoreFullError</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">n_activation_tensors</span><span class="p">,</span>
        <span class="n">component_idx</span><span class="p">,</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_items_stored</span><span class="p">[</span><span class="n">component_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">n_activation_tensors</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TensorActivationStore.shuffle" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">shuffle</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.shuffle" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Shuffle the Data In-Place.</p>
<p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
_seed = torch.manual_seed(42)
store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)
store.append(torch.tensor([0.]), component_idx=0)
store.append(torch.tensor([1.]), component_idx=0)
store.append(torch.tensor([2.]), component_idx=0)
store.shuffle()
[store[i, 0].item() for i in range(3)]
[0.0, 2.0, 1.0]</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, n_neurons=1, n_components=1)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([0.]), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([1.]), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([2.]), component_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">    &gt;&gt;&gt; [store[i, 0].item() for i in range(3)]</span>
<span class="sd">    [0.0, 2.0, 1.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate a permutation of the indices for the active data</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

    <span class="c1"># Use this permutation to shuffle the active data in-place</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.TextDataset" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>TextDataset</code>


<a href="#sparse_autoencoder.TextDataset" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch" href="source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch">GenericTextDataBatch</a>]</code></p>

  
      <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">TextDataset</span><span class="p">(</span><span class="n">SourceDataset</span><span class="p">[</span><span class="n">GenericTextDataBatch</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generic Text Dataset for any text-based dataset from Hugging Face.&quot;&quot;&quot;</span>

    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_batch</span><span class="p">:</span> <span class="n">GenericTextDataBatch</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_batch: A batch of source data, including &#39;text&#39; with a list of strings.</span>
<span class="sd">            context_size: Context size for tokenized prompts.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tokenized prompts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>

        <span class="n">tokenized_prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.</span>
        <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset_column_name</span><span class="p">]):</span>  <span class="c1"># type: ignore</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
            <span class="p">]</span>
            <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>

    <span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataset_files</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">dataset_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
        <span class="n">n_processes_preprocessing</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">pre_download</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize a generic text dataset from Hugging Face.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset_path: Path to the dataset on Hugging Face (e.g. `&#39;monology/pile-uncopyright&#39;`).</span>
<span class="sd">            tokenizer: Tokenizer to process text data.</span>
<span class="sd">            buffer_size: The buffer size to use when shuffling the dataset when streaming. When</span>
<span class="sd">                streaming a dataset, this just pre-downloads at least `buffer_size` items and then</span>
<span class="sd">                shuffles just that buffer. Note that the generated activations should also be</span>
<span class="sd">                shuffled before training the sparse autoencoder, so a large buffer may not be</span>
<span class="sd">                strictly necessary here. Note also that this is the number of items in the dataset</span>
<span class="sd">                (e.g. number of prompts) and is typically significantly less than the number of</span>
<span class="sd">                tokenized prompts once the preprocessing function has been applied.</span>
<span class="sd">            context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">                a context size of 250.</span>
<span class="sd">            dataset_dir: Defining the `data_dir` of the dataset configuration.</span>
<span class="sd">            dataset_files: Path(s) to source data file(s).</span>
<span class="sd">            dataset_split: Dataset split (e.g., &#39;train&#39;).</span>
<span class="sd">            dataset_column_name: The column name for the prompts.</span>
<span class="sd">            n_processes_preprocessing: Number of processes to use for preprocessing.</span>
<span class="sd">            preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).</span>
<span class="sd">            pre_download: Whether to pre-download the whole dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
            <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
            <span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span>
            <span class="n">dataset_files</span><span class="o">=</span><span class="n">dataset_files</span><span class="p">,</span>
            <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
            <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
            <span class="n">dataset_column_name</span><span class="o">=</span><span class="n">dataset_column_name</span><span class="p">,</span>
            <span class="n">n_processes_preprocessing</span><span class="o">=</span><span class="n">n_processes_preprocessing</span><span class="p">,</span>
            <span class="n">pre_download</span><span class="o">=</span><span class="n">pre_download</span><span class="p">,</span>
            <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="nf">push_to_hugging_face_hub</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">commit_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Upload preprocessed dataset using sparse_autoencoder.&quot;</span><span class="p">,</span>
        <span class="n">max_shard_size</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_shards</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;main&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">private</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Share preprocessed dataset to Hugging Face hub.</span>

<span class="sd">        Motivation:</span>
<span class="sd">            Pre-processing a dataset can be time-consuming, so it is useful to be able to share the</span>
<span class="sd">            pre-processed dataset with others. This function allows you to do that by pushing the</span>
<span class="sd">            pre-processed dataset to the Hugging Face hub.</span>

<span class="sd">        Warning:</span>
<span class="sd">            You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)</span>
<span class="sd">            to use this.</span>

<span class="sd">        Warning:</span>
<span class="sd">            This will only work if the dataset is not streamed (i.e. if `pre_download=True` when</span>
<span class="sd">            initializing the dataset).</span>

<span class="sd">        Args:</span>
<span class="sd">            repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).</span>
<span class="sd">            commit_message: Commit message.</span>
<span class="sd">            max_shard_size: Maximum shard size (e.g. `&#39;500MB&#39;`). Should not be set if `n_shards`</span>
<span class="sd">                is set.</span>
<span class="sd">            n_shards: Number of shards to split the dataset into. A high number is recommended</span>
<span class="sd">                here to allow for flexible distributed training of SAEs across nodes (where e.g.</span>
<span class="sd">                each node fetches it&#39;s own shard).</span>
<span class="sd">            revision: Branch to push to.</span>
<span class="sd">            private: Whether to save the dataset privately.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the dataset is streamed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;Cannot share a streamed dataset to Hugging Face. &quot;</span>
                <span class="s2">&quot;Please use `pre_download=True` when initializing the dataset.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>
            <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
            <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
            <span class="n">max_shard_size</span><span class="o">=</span><span class="n">max_shard_size</span><span class="p">,</span>
            <span class="n">num_shards</span><span class="o">=</span><span class="n">n_shards</span><span class="p">,</span>
            <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TextDataset.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dataset_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dataset_files</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dataset_split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">dataset_column_name</span><span class="o">=</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="n">n_processes_preprocessing</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">pre_download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TextDataset.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize a generic text dataset from Hugging Face.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataset_path</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path to the dataset on Hugging Face (e.g. <code>'monology/pile-uncopyright'</code>).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>tokenizer</code></td>
          <td>
                <code><span title="transformers.PreTrainedTokenizerBase">PreTrainedTokenizerBase</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tokenizer to process text data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>buffer_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The buffer size to use when shuffling the dataset when streaming. When
streaming a dataset, this just pre-downloads at least <code>buffer_size</code> items and then
shuffles just that buffer. Note that the generated activations should also be
shuffled before training the sparse autoencoder, so a large buffer may not be
strictly necessary here. Note also that this is the number of items in the dataset
(e.g. number of prompts) and is typically significantly less than the number of
tokenized prompts once the preprocessing function has been applied.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The context size to use when returning a list of tokenized prompts.
<em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em> used
a context size of 250.</p>
            </div>
          </td>
          <td>
                <code>256</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_dir</code></td>
          <td>
                <code>str | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Defining the <code>data_dir</code> of the dataset configuration.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_files</code></td>
          <td>
                <code>str | <span title="collections.abc.Sequence">Sequence</span>[str] | <span title="collections.abc.Mapping">Mapping</span>[str, str | <span title="collections.abc.Sequence">Sequence</span>[str]] | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path(s) to source data file(s).</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_split</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dataset split (e.g., 'train').</p>
            </div>
          </td>
          <td>
                <code>&#39;train&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_column_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The column name for the prompts.</p>
            </div>
          </td>
          <td>
                <code>&#39;input_ids&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>n_processes_preprocessing</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of processes to use for preprocessing.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>preprocess_batch_size</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch size for preprocessing (tokenizing prompts).</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>pre_download</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to pre-download the whole dataset.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arbitrary_types_allowed&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dataset_files</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
    <span class="n">dataset_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
    <span class="n">n_processes_preprocessing</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">pre_download</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize a generic text dataset from Hugging Face.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset_path: Path to the dataset on Hugging Face (e.g. `&#39;monology/pile-uncopyright&#39;`).</span>
<span class="sd">        tokenizer: Tokenizer to process text data.</span>
<span class="sd">        buffer_size: The buffer size to use when shuffling the dataset when streaming. When</span>
<span class="sd">            streaming a dataset, this just pre-downloads at least `buffer_size` items and then</span>
<span class="sd">            shuffles just that buffer. Note that the generated activations should also be</span>
<span class="sd">            shuffled before training the sparse autoencoder, so a large buffer may not be</span>
<span class="sd">            strictly necessary here. Note also that this is the number of items in the dataset</span>
<span class="sd">            (e.g. number of prompts) and is typically significantly less than the number of</span>
<span class="sd">            tokenized prompts once the preprocessing function has been applied.</span>
<span class="sd">        context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">            a context size of 250.</span>
<span class="sd">        dataset_dir: Defining the `data_dir` of the dataset configuration.</span>
<span class="sd">        dataset_files: Path(s) to source data file(s).</span>
<span class="sd">        dataset_split: Dataset split (e.g., &#39;train&#39;).</span>
<span class="sd">        dataset_column_name: The column name for the prompts.</span>
<span class="sd">        n_processes_preprocessing: Number of processes to use for preprocessing.</span>
<span class="sd">        preprocess_batch_size: Batch size for preprocessing (tokenizing prompts).</span>
<span class="sd">        pre_download: Whether to pre-download the whole dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
        <span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span>
        <span class="n">dataset_files</span><span class="o">=</span><span class="n">dataset_files</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
        <span class="n">dataset_column_name</span><span class="o">=</span><span class="n">dataset_column_name</span><span class="p">,</span>
        <span class="n">n_processes_preprocessing</span><span class="o">=</span><span class="n">n_processes_preprocessing</span><span class="p">,</span>
        <span class="n">pre_download</span><span class="o">=</span><span class="n">pre_download</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TextDataset.preprocess" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">preprocess</span><span class="p">(</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TextDataset.preprocess" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Preprocess a batch of prompts.</p>
<p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_batch</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch" href="source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch">GenericTextDataBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A batch of source data, including 'text' with a list of strings.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Context size for tokenized prompts.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts">TokenizedPrompts</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tokenized prompts.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_batch</span><span class="p">:</span> <span class="n">GenericTextDataBatch</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_batch: A batch of source data, including &#39;text&#39; with a list of strings.</span>
<span class="sd">        context_size: Context size for tokenized prompts.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tokenized prompts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>

    <span class="n">tokenized_prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.</span>
    <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset_column_name</span><span class="p">]):</span>  <span class="c1"># type: ignore</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
        <span class="p">]</span>
        <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TextDataset.push_to_hugging_face_hub" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">push_to_hugging_face_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="s1">&#39;Upload preprocessed dataset using sparse_autoencoder.&#39;</span><span class="p">,</span> <span class="n">max_shard_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_shards</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s1">&#39;main&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">private</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TextDataset.push_to_hugging_face_hub" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Share preprocessed dataset to Hugging Face hub.</p>

<details class="motivation" open>
  <summary>Motivation</summary>
  <p>Pre-processing a dataset can be time-consuming, so it is useful to be able to share the
pre-processed dataset with others. This function allows you to do that by pushing the
pre-processed dataset to the Hugging Face hub.</p>
</details>
<details class="warning" open>
  <summary>Warning</summary>
  <p>You must be logged into HuggingFace (e.g with <code>huggingface-cli login</code> from the terminal)
to use this.</p>
</details>
<details class="warning" open>
  <summary>Warning</summary>
  <p>This will only work if the dataset is not streamed (i.e. if <code>pre_download=True</code> when
initializing the dataset).</p>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>repo_id</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Hugging Face repo ID to save the dataset to (e.g. <code>username/dataset_name</code>).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>commit_message</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Commit message.</p>
            </div>
          </td>
          <td>
                <code>&#39;Upload preprocessed dataset using sparse_autoencoder.&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>max_shard_size</code></td>
          <td>
                <code>str | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum shard size (e.g. <code>'500MB'</code>). Should not be set if <code>n_shards</code>
is set.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>n_shards</code></td>
          <td>
                <code><span title="pydantic.PositiveInt">PositiveInt</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of shards to split the dataset into. A high number is recommended
here to allow for flexible distributed training of SAEs across nodes (where e.g.
each node fetches it's own shard).</p>
            </div>
          </td>
          <td>
                <code>64</code>
          </td>
        </tr>
        <tr>
          <td><code>revision</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Branch to push to.</p>
            </div>
          </td>
          <td>
                <code>&#39;main&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>private</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to save the dataset privately.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>TypeError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the dataset is streamed.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="nf">push_to_hugging_face_hub</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">commit_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Upload preprocessed dataset using sparse_autoencoder.&quot;</span><span class="p">,</span>
    <span class="n">max_shard_size</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_shards</span><span class="p">:</span> <span class="n">PositiveInt</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;main&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">private</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Share preprocessed dataset to Hugging Face hub.</span>

<span class="sd">    Motivation:</span>
<span class="sd">        Pre-processing a dataset can be time-consuming, so it is useful to be able to share the</span>
<span class="sd">        pre-processed dataset with others. This function allows you to do that by pushing the</span>
<span class="sd">        pre-processed dataset to the Hugging Face hub.</span>

<span class="sd">    Warning:</span>
<span class="sd">        You must be logged into HuggingFace (e.g with `huggingface-cli login` from the terminal)</span>
<span class="sd">        to use this.</span>

<span class="sd">    Warning:</span>
<span class="sd">        This will only work if the dataset is not streamed (i.e. if `pre_download=True` when</span>
<span class="sd">        initializing the dataset).</span>

<span class="sd">    Args:</span>
<span class="sd">        repo_id: Hugging Face repo ID to save the dataset to (e.g. `username/dataset_name`).</span>
<span class="sd">        commit_message: Commit message.</span>
<span class="sd">        max_shard_size: Maximum shard size (e.g. `&#39;500MB&#39;`). Should not be set if `n_shards`</span>
<span class="sd">            is set.</span>
<span class="sd">        n_shards: Number of shards to split the dataset into. A high number is recommended</span>
<span class="sd">            here to allow for flexible distributed training of SAEs across nodes (where e.g.</span>
<span class="sd">            each node fetches it&#39;s own shard).</span>
<span class="sd">        revision: Branch to push to.</span>
<span class="sd">        private: Whether to save the dataset privately.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dataset is streamed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Cannot share a streamed dataset to Hugging Face. &quot;</span>
            <span class="s2">&quot;Please use `pre_download=True` when initializing the dataset.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
        <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
        <span class="n">max_shard_size</span><span class="o">=</span><span class="n">max_shard_size</span><span class="p">,</span>
        <span class="n">num_shards</span><span class="o">=</span><span class="n">n_shards</span><span class="p">,</span>
        <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="sparse_autoencoder.TrainBatchFeatureDensityMetric" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>TrainBatchFeatureDensityMetric</code>


<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric" href="metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric">AbstractTrainMetric</a></code></p>

  
      <p>Train batch feature density.</p>
<p>Percentage of samples in which each feature was active (i.e. the neuron has "fired"), in a
training batch.</p>
<p>Generally we want a small number of features to be active in each batch, so average feature
density should be low. By contrast if the average feature density is high, it means that the
features are not sparse enough.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>This is not the same as the feature density of the entire training set. It's main use is
tracking the progress of training.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TrainBatchFeatureDensityMetric</span><span class="p">(</span><span class="n">AbstractTrainMetric</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train batch feature density.</span>

<span class="sd">    Percentage of samples in which each feature was active (i.e. the neuron has &quot;fired&quot;), in a</span>
<span class="sd">    training batch.</span>

<span class="sd">    Generally we want a small number of features to be active in each batch, so average feature</span>
<span class="sd">    density should be low. By contrast if the average feature density is high, it means that the</span>
<span class="sd">    features are not sparse enough.</span>

<span class="sd">    Warning:</span>
<span class="sd">        This is not the same as the feature density of the entire training set. It&#39;s main use is</span>
<span class="sd">        tracking the progress of training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span>

    <span class="nd">@validate_call</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">threshold</span><span class="p">:</span> <span class="n">NonNegativeFloat</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the train batch feature density metric.</span>

<span class="sd">        Args:</span>
<span class="sd">            threshold: Threshold for considering a feature active (i.e. the neuron has &quot;fired&quot;).</span>
<span class="sd">                This should be close to zero.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">feature_density</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Count how many times each feature was active.</span>

<span class="sd">        Percentage of samples in which each feature was active (i.e. the neuron has &quot;fired&quot;).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; activations = torch.tensor([[[0.5, 0.5, 0.0]], [[0.5, 0.0, 0.0001]]])</span>
<span class="sd">            &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()</span>
<span class="sd">            [[1.0, 0.5, 0.0]]</span>

<span class="sd">        Args:</span>
<span class="sd">            activations: Sample of cached activations (the Autoencoder&#39;s learned features).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of times each feature was active in a sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">has_fired</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span>  <span class="c1"># Move to float so it can be averaged</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
            <span class="n">has_fired</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">                -&gt; </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">wandb_feature_density_histogram</span><span class="p">(</span>
        <span class="n">feature_density</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the feature density.</span>

<span class="sd">        This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;feature_density_histogram&quot;:</span>
<span class="sd">        wandb_feature_density_histogram(feature_density)})`.</span>

<span class="sd">        Args:</span>
<span class="sd">            feature_density: Number of times each feature was active in a sample. Can be calculated</span>
<span class="sd">                using :func:`feature_activity_count`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">numpy_feature_density</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">feature_density</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">np_histograms</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">histogram</span><span class="p">(</span><span class="n">component_feature_density</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">component_feature_density</span> <span class="ow">in</span> <span class="n">numpy_feature_density</span>
        <span class="p">]</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="n">np_histogram</span><span class="p">)</span> <span class="k">for</span> <span class="n">np_histogram</span> <span class="ow">in</span> <span class="n">np_histograms</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the train batch feature density metrics.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Train metric data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary with the train batch feature density metric, and a histogram of the feature</span>
<span class="sd">            density.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_batch_feature_density</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_density</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span><span class="p">)</span>

        <span class="n">component_wise_histograms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wandb_feature_density_histogram</span><span class="p">(</span>
            <span class="n">train_batch_feature_density</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">MetricResult</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;feature_density&quot;</span><span class="p">,</span>
                <span class="n">component_wise_values</span><span class="o">=</span><span class="n">component_wise_histograms</span><span class="p">,</span>
                <span class="n">location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
                <span class="n">aggregate_approach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t aggregate the histograms</span>
            <span class="p">)</span>
        <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the train batch feature density metric.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>threshold</code></td>
          <td>
                <code><span title="pydantic.NonNegativeFloat">NonNegativeFloat</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Threshold for considering a feature active (i.e. the neuron has "fired").
This should be close to zero.</p>
            </div>
          </td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@validate_call</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="n">NonNegativeFloat</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the train batch feature density metric.</span>

<span class="sd">    Args:</span>
<span class="sd">        threshold: Threshold for considering a feature active (i.e. the neuron has &quot;fired&quot;).</span>
<span class="sd">            This should be close to zero.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">calculate</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the train batch feature density metrics.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.train.abstract_train_metric.TrainMetricData" href="metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.TrainMetricData">TrainMetricData</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train metric data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.abstract_metric.MetricResult" href="metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.MetricResult">MetricResult</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dictionary with the train batch feature density metric, and a histogram of the feature</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.abstract_metric.MetricResult" href="metrics/abstract_metric/#sparse_autoencoder.metrics.abstract_metric.MetricResult">MetricResult</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>density.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">MetricResult</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the train batch feature density metrics.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: Train metric data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary with the train batch feature density metric, and a histogram of the feature</span>
<span class="sd">        density.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_batch_feature_density</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_density</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span><span class="p">)</span>

    <span class="n">component_wise_histograms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wandb_feature_density_histogram</span><span class="p">(</span>
        <span class="n">train_batch_feature_density</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="n">MetricResult</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;feature_density&quot;</span><span class="p">,</span>
            <span class="n">component_wise_values</span><span class="o">=</span><span class="n">component_wise_histograms</span><span class="p">,</span>
            <span class="n">location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
            <span class="n">aggregate_approach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t aggregate the histograms</span>
        <span class="p">)</span>
    <span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">feature_density</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Count how many times each feature was active.</p>
<p>Percentage of samples in which each feature was active (i.e. the neuron has "fired").</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
activations = torch.tensor([[[0.5, 0.5, 0.0]], [[0.5, 0.0, 0.0001]]])
TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()
[[1.0, 0.5, 0.0]]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.BATCH" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH">BATCH</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sample of cached activations (the Autoencoder's learned features).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each feature was active in a sample.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">feature_density</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count how many times each feature was active.</span>

<span class="sd">    Percentage of samples in which each feature was active (i.e. the neuron has &quot;fired&quot;).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; activations = torch.tensor([[[0.5, 0.5, 0.0]], [[0.5, 0.0, 0.0001]]])</span>
<span class="sd">        &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()</span>
<span class="sd">        [[1.0, 0.5, 0.0]]</span>

<span class="sd">    Args:</span>
<span class="sd">        activations: Sample of cached activations (the Autoencoder&#39;s learned features).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Number of times each feature was active in a sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">has_fired</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span>  <span class="c1"># Move to float so it can be averaged</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
        <span class="n">has_fired</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">BATCH</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">            -&gt; </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">wandb_feature_density_histogram</span><span class="p">(</span><span class="n">feature_density</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram" class="headerlink" title="Permanent link"></a></h3>


  <div class="doc doc-contents ">
  
      <p>Create a W&amp;B histogram of the feature density.</p>
<p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({"feature_density_histogram":
wandb_feature_density_histogram(feature_density)})</code>.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>feature_density</code></td>
          <td>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.names" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.names">names</a>(<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.COMPONENT" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.COMPONENT">COMPONENT</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE" href="tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE">LEARNT_FEATURE</a>)]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each feature was active in a sample. Can be calculated
using :func:<code>feature_activity_count</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>list[<span title="wandb.Histogram">Histogram</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">wandb_feature_density_histogram</span><span class="p">(</span>
    <span class="n">feature_density</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the feature density.</span>

<span class="sd">    This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;feature_density_histogram&quot;:</span>
<span class="sd">    wandb_feature_density_histogram(feature_density)})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_density: Number of times each feature was active in a sample. Can be calculated</span>
<span class="sd">            using :func:`feature_activity_count`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">numpy_feature_density</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">names</span><span class="p">(</span><span class="n">Axis</span><span class="o">.</span><span class="n">COMPONENT</span><span class="p">,</span> <span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">feature_density</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">np_histograms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">histogram</span><span class="p">(</span><span class="n">component_feature_density</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">component_feature_density</span> <span class="ow">in</span> <span class="n">numpy_feature_density</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="n">np_histogram</span><span class="p">)</span> <span class="k">for</span> <span class="n">np_histogram</span> <span class="ow">in</span> <span class="n">np_histograms</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>



<div class="doc doc-object doc-function">



<h2 id="sparse_autoencoder.sweep" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>          <code class="highlight language-python"><span class="n">sweep</span><span class="p">(</span><span class="n">sweep_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sweep_id</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.sweep" class="headerlink" title="Permanent link"></a></h2>


  <div class="doc doc-contents ">
  
      <p>Run the training pipeline with wandb hyperparameter sweep.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>Either sweep_config or sweep_id must be specified, but not both.</p>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sweep_config</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.sweep_config.SweepConfig" href="train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig">SweepConfig</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The sweep configuration.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>sweep_id</code></td>
          <td>
                <code>str | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The sweep id for an existing sweep.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If neither sweep_config nor sweep_id is specified.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/sweep.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sweep</span><span class="p">(</span>
    <span class="n">sweep_config</span><span class="p">:</span> <span class="n">SweepConfig</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sweep_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run the training pipeline with wandb hyperparameter sweep.</span>

<span class="sd">    Warning:</span>
<span class="sd">        Either sweep_config or sweep_id must be specified, but not both.</span>

<span class="sd">    Args:</span>
<span class="sd">        sweep_config: The sweep configuration.</span>
<span class="sd">        sweep_id: The sweep id for an existing sweep.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If neither sweep_config nor sweep_id is specified.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sweep_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">agent</span><span class="p">(</span><span class="n">sweep_id</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">project</span><span class="o">=</span><span class="s2">&quot;sparse-autoencoder&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">sweep_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sweep_id</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">sweep</span><span class="p">(</span><span class="n">sweep_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span> <span class="n">project</span><span class="o">=</span><span class="s2">&quot;sparse-autoencoder&quot;</span><span class="p">)</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">agent</span><span class="p">(</span><span class="n">sweep_id</span><span class="p">,</span> <span class="n">train</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Either sweep_config or sweep_id must be specified.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.action.edit"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c18c5fb9.min.js"></script>
      
        <script src="../javascript/custom_formatting.js"></script>
      
        <script src="../javascript/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>