<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Home - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Home";
        var mkdocs_page_input_path = "reference/index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Home</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_resampler</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="activation_resampler/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="activation_resampler/abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">autoencoder</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="autoencoder/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="autoencoder/components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="autoencoder/components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="autoencoder/components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="autoencoder/model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="metrics/abstract_metric/">abstract_metric</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">optimizer</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="optimizer/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="optimizer/abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="optimizer/adam_with_reset/">adam_with_reset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_data</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="source_data/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="source_data/abstract_dataset/">abstract_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="source_data/pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="source_data/random_int/">random_int</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="source_data/text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
      <li class="breadcrumb-item active">Home</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder"></a>
  <div class="doc doc-contents first">
  
      <p>Sparse Autoencoder Library.</p>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h2 id="sparse_autoencoder.LossLogType" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">LossLogType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">str</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module-attribute"><code>module-attribute</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>Loss log dict.</p>
  </div>

</div>


<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.AbstractLoss" class="doc doc-heading">
          <code>AbstractLoss</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code>, <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Abstract loss interface.</p>
<p>Interface for implementing batch itemwise loss functions.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AbstractLoss</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract loss interface.</span>

<span class="sd">    Interface for implementing batch itemwise loss functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_modules</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;AbstractLoss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore[assignment] (narrowing)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Children loss modules.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Batch itemwise loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">batch_scalar_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ItemTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Batch scalar loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>
<span class="sd">            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">                make the loss independent of the batch size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss for the batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">itemwise_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>

        <span class="k">match</span> <span class="n">reduction</span><span class="p">:</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">itemwise_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">itemwise_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">batch_scalar_loss_with_log</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">,</span> <span class="n">LossLogType</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Batch scalar loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>
<span class="sd">            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">                make the loss independent of the batch size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of the batch scalar loss and a dict of any properties to log.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">children_loss_scalars</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">LossLogType</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># If the loss module has children (e.g. it is a reducer):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">child_loss</span><span class="p">,</span> <span class="n">child_metrics</span> <span class="o">=</span> <span class="n">loss_module</span><span class="o">.</span><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span>
                    <span class="n">source_activations</span><span class="p">,</span>
                    <span class="n">learned_activations</span><span class="p">,</span>
                    <span class="n">decoded_activations</span><span class="p">,</span>
                    <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">children_loss_scalars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_loss</span><span class="p">)</span>
                <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">child_metrics</span><span class="p">)</span>

            <span class="c1"># Get the total loss &amp; metric</span>
            <span class="n">current_module_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">children_loss_scalars</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Otherwise if it is a leaf loss module:</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_module_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_scalar_loss</span><span class="p">(</span>
                <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span>
            <span class="p">)</span>

        <span class="c1"># Add in the current loss module&#39;s metric</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">metrics</span><span class="p">[</span><span class="n">class_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_module_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">current_module_loss</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">,</span> <span class="n">LossLogType</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Batch scalar loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>
<span class="sd">            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">                make the loss independent of the batch size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of the batch scalar loss and a dict of any properties to log.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span>
            <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AbstractLoss.__call__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Batch scalar loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
            <li>
              <b><code>reduction</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>, default:
                      <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN">MEAN</a></code>
)
              –
              <div class="doc-md-description">
                <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to
make the loss independent of the batch size.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.ItemTensor" href="tensor_types/#sparse_autoencoder.tensor_types.ItemTensor">ItemTensor</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossLogType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossLogType">LossLogType</a>]</code>
            –
            <div class="doc-md-description">
              <p>Tuple of the batch scalar loss and a dict of any properties to log.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">,</span> <span class="n">LossLogType</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch scalar loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>
<span class="sd">        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">            make the loss independent of the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of the batch scalar loss and a dict of any properties to log.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span>
        <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AbstractLoss.batch_scalar_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">batch_scalar_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Batch scalar loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
            <li>
              <b><code>reduction</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>, default:
                      <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN">MEAN</a></code>
)
              –
              <div class="doc-md-description">
                <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to
make the loss independent of the batch size.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.ItemTensor" href="tensor_types/#sparse_autoencoder.tensor_types.ItemTensor">ItemTensor</a></code>
            –
            <div class="doc-md-description">
              <p>Loss for the batch.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">batch_scalar_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ItemTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch scalar loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>
<span class="sd">        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">            make the loss independent of the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss for the batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">itemwise_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>

    <span class="k">match</span> <span class="n">reduction</span><span class="p">:</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">itemwise_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">itemwise_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AbstractLoss.batch_scalar_loss_with_log" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Batch scalar loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
            <li>
              <b><code>reduction</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>, default:
                      <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN">MEAN</a></code>
)
              –
              <div class="doc-md-description">
                <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to
make the loss independent of the batch size.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.ItemTensor" href="tensor_types/#sparse_autoencoder.tensor_types.ItemTensor">ItemTensor</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossLogType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossLogType">LossLogType</a>]</code>
            –
            <div class="doc-md-description">
              <p>Tuple of the batch scalar loss and a dict of any properties to log.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">batch_scalar_loss_with_log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">,</span> <span class="n">LossLogType</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch scalar loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>
<span class="sd">        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">            make the loss independent of the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of the batch scalar loss and a dict of any properties to log.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">children_loss_scalars</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">LossLogType</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># If the loss module has children (e.g. it is a reducer):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">child_loss</span><span class="p">,</span> <span class="n">child_metrics</span> <span class="o">=</span> <span class="n">loss_module</span><span class="o">.</span><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span>
                <span class="n">source_activations</span><span class="p">,</span>
                <span class="n">learned_activations</span><span class="p">,</span>
                <span class="n">decoded_activations</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">children_loss_scalars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_loss</span><span class="p">)</span>
            <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">child_metrics</span><span class="p">)</span>

        <span class="c1"># Get the total loss &amp; metric</span>
        <span class="n">current_module_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">children_loss_scalars</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Otherwise if it is a leaf loss module:</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">current_module_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_scalar_loss</span><span class="p">(</span>
            <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span>
        <span class="p">)</span>

    <span class="c1"># Add in the current loss module&#39;s metric</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">class_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_module_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">current_module_loss</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AbstractLoss.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Batch itemwise loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
            –
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch itemwise loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.ActivationStore" class="doc doc-heading">
          <code>ActivationStore</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.utils.data.Dataset">Dataset</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a>]</code>, <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Activation Store Abstract Class.</p>
<p>Extends the <code>torch.utils.data.Dataset</code> class to provide an activation store, with additional
:meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which should typically be
non-blocking). The resulting activation store can be used with a <code>torch.utils.data.DataLoader</code>
to iterate over the dataset.</p>
<p>Extend this class if you want to create a new activation store (noting you also need to create
<code>__getitem__</code> and <code>__len__</code> methods from the underlying <code>torch.utils.data.Dataset</code> class).</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
class MyActivationStore(ActivationStore):
...     def <strong>init</strong>(self):
...         super().<strong>init</strong>()
...         self._data = [] # In this example, we just store in a list
...
...     def append(self, item) -&gt; None:
...         self._data.append(item)
...
...     def extend(self, batch):
...         self._data.extend(batch)
...
...     def empty(self):
...         self._data = []
...
...     def <strong>getitem</strong>(self, index: int):
...         return self._data[index]
...
...     def <strong>len</strong>(self) -&gt; int:
...         return len(self._data)
...
store = MyActivationStore()
store.append(torch.randn(100))
print(len(store))
1</p>
</blockquote>
</blockquote>
</blockquote>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ActivationStore</span><span class="p">(</span><span class="n">Dataset</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">],</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation Store Abstract Class.</span>

<span class="sd">    Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional</span>
<span class="sd">    :meth:`append` and :meth:`extend` methods (the latter of which should typically be</span>
<span class="sd">    non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader`</span>
<span class="sd">    to iterate over the dataset.</span>

<span class="sd">    Extend this class if you want to create a new activation store (noting you also need to create</span>
<span class="sd">    `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class).</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; class MyActivationStore(ActivationStore):</span>
<span class="sd">    ...     def __init__(self):</span>
<span class="sd">    ...         super().__init__()</span>
<span class="sd">    ...         self._data = [] # In this example, we just store in a list</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def append(self, item) -&gt; None:</span>
<span class="sd">    ...         self._data.append(item)</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def extend(self, batch):</span>
<span class="sd">    ...         self._data.extend(batch)</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def empty(self):</span>
<span class="sd">    ...         self._data = []</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def __getitem__(self, index: int):</span>
<span class="sd">    ...         return self._data[index]</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def __len__(self) -&gt; int:</span>
<span class="sd">    ...         return len(self._data)</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; store = MyActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a Single Item to the Store.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a Batch to the Store.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the Store.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the Length of the Store.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get an Item from the Store.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Optional shuffle method.&quot;&quot;&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">fill_with_test_data</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fill the store with test data.</span>

<span class="sd">        For use when testing your code, to ensure it works with a real activation store.</span>

<span class="sd">        Warning:</span>
<span class="sd">            You may want to use `torch.seed(0)` to make the random data deterministic, if your test</span>
<span class="sd">            requires inspecting the data itself.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore</span>
<span class="sd">            &gt;&gt;&gt; store = TensorActivationStore(max_items=16*16, num_neurons=256)</span>
<span class="sd">            &gt;&gt;&gt; store.fill_with_test_data()</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            256</span>
<span class="sd">            &gt;&gt;&gt; store[0].shape</span>
<span class="sd">            torch.Size([256])</span>

<span class="sd">        Args:</span>
<span class="sd">            num_batches: Number of batches to fill the store with.</span>
<span class="sd">            batch_size: Number of items per batch.</span>
<span class="sd">            input_features: Number of input features per item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get an Item from the Store.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get an Item from the Store.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get the Length of the Store.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the Length of the Store.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Add a Single Item to the Store.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a Single Item to the Store.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Empty the Store.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the Store.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Add a Batch to the Store.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a Batch to the Store.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.fill_with_test_data" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">fill_with_test_data</span><span class="p">(</span><span class="n">num_batches</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Fill the store with test data.</p>
<p>For use when testing your code, to ensure it works with a real activation store.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>You may want to use <code>torch.seed(0)</code> to make the random data deterministic, if your test
requires inspecting the data itself.</p>
</details>
<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore
store = TensorActivationStore(max_items=16*16, num_neurons=256)
store.fill_with_test_data()
len(store)
256
store[0].shape
torch.Size([256])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_batches</code></b>
                  (<code>int</code>, default:
                      <code>16</code>
)
              –
              <div class="doc-md-description">
                <p>Number of batches to fill the store with.</p>
              </div>
            </li>
            <li>
              <b><code>batch_size</code></b>
                  (<code>int</code>, default:
                      <code>16</code>
)
              –
              <div class="doc-md-description">
                <p>Number of items per batch.</p>
              </div>
            </li>
            <li>
              <b><code>input_features</code></b>
                  (<code>int</code>, default:
                      <code>256</code>
)
              –
              <div class="doc-md-description">
                <p>Number of input features per item.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">fill_with_test_data</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fill the store with test data.</span>

<span class="sd">    For use when testing your code, to ensure it works with a real activation store.</span>

<span class="sd">    Warning:</span>
<span class="sd">        You may want to use `torch.seed(0)` to make the random data deterministic, if your test</span>
<span class="sd">        requires inspecting the data itself.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=16*16, num_neurons=256)</span>
<span class="sd">        &gt;&gt;&gt; store.fill_with_test_data()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        256</span>
<span class="sd">        &gt;&gt;&gt; store[0].shape</span>
<span class="sd">        torch.Size([256])</span>

<span class="sd">    Args:</span>
<span class="sd">        num_batches: Number of batches to fill the store with.</span>
<span class="sd">        batch_size: Number of items per batch.</span>
<span class="sd">        input_features: Number of input features per item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationStore.shuffle" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">shuffle</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Optional shuffle method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/base_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optional shuffle method.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.DiskActivationStore" class="doc doc-heading">
          <code>DiskActivationStore</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>Disk Activation Store.</p>
<p>Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up
activation vectors and then write them to the disk in batches.</p>
<p>Multiprocess safe (supports writing from multiple GPU workers).</p>
<p>Warning:
Unless you want to keep and use existing .pt files in the storage directory when initialized,
set <code>empty_dir</code> to <code>True</code>.</p>
<p>Note also that :meth:<code>close</code> must be called to ensure all activation vectors are written to disk
after the last batch has been added to the store.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DiskActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Disk Activation Store.</span>

<span class="sd">    Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up</span>
<span class="sd">    activation vectors and then write them to the disk in batches.</span>

<span class="sd">    Multiprocess safe (supports writing from multiple GPU workers).</span>

<span class="sd">    Warning:</span>
<span class="sd">    Unless you want to keep and use existing .pt files in the storage directory when initialized,</span>
<span class="sd">    set `empty_dir` to `True`.</span>

<span class="sd">    Note also that :meth:`close` must be called to ensure all activation vectors are written to disk</span>
<span class="sd">    after the last batch has been added to the store.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_storage_path</span><span class="p">:</span> <span class="n">Path</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Path to the Directory where the Activation Vectors are Stored.&quot;&quot;&quot;</span>

    <span class="n">_cache</span><span class="p">:</span> <span class="n">ListProxy</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Cache for Activation Vectors.</span>

<span class="sd">    Activation vectors are buffered in memory until the cache is full, at which point they are</span>
<span class="sd">    written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_cache_lock</span><span class="p">:</span> <span class="n">Lock</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lock for the Cache.&quot;&quot;&quot;</span>

    <span class="n">_max_cache_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum Number of Activation Vectors to cache in Memory.&quot;&quot;&quot;</span>

    <span class="n">_thread_pool</span><span class="p">:</span> <span class="n">ThreadPoolExecutor</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Threadpool for non-blocking writes to the file system.&quot;&quot;&quot;</span>

    <span class="n">_disk_n_activation_vectors</span><span class="p">:</span> <span class="n">ValueProxy</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length of the Store (on disk).</span>

<span class="sd">    Minus 1 signifies not calculated yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">storage_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">DEFAULT_DISK_ACTIVATION_STORE_PATH</span><span class="p">,</span>
        <span class="n">max_cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">empty_dir</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Disk Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            storage_path: Path to the directory where the activation vectors will be stored.</span>
<span class="sd">            max_cache_size: The maximum number of activation vectors to cache in memory before</span>
<span class="sd">                writing to disk. Note this is only followed approximately.</span>
<span class="sd">            num_workers: Number of CPU workers to use for non-blocking writes to the file system (so</span>
<span class="sd">                that the model can keep running whilst it writes the previous activations to disk).</span>
<span class="sd">                This should be less than the number of CPU cores available. You don&#39;t need multiple</span>
<span class="sd">                GPUs to take advantage of this feature.</span>
<span class="sd">            empty_dir: Whether to empty the directory before writing. Generally you want to set this</span>
<span class="sd">                to `True` as otherwise the directory may contain stale activation vectors from</span>
<span class="sd">                previous runs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Setup the storage directory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">=</span> <span class="n">storage_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Setup the Cache</span>
        <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span> <span class="o">=</span> <span class="n">max_cache_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Empty the directory if needed</span>
        <span class="k">if</span> <span class="n">empty_dir</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>

        <span class="c1"># Create a threadpool for non-blocking writes to the cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span> <span class="o">=</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_write_to_disk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Write the contents of the queue to disk.</span>

<span class="sd">        Args:</span>
<span class="sd">            wait_for_max: Whether to wait until the cache is full before writing to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
            <span class="c1"># Check we have enough items</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="n">size_to_get</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">wait_for_max</span> <span class="ow">and</span> <span class="n">size_to_get</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="c1"># Get the activations from the cache and delete them</span>
            <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">size_to_get</span><span class="p">]</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">size_to_get</span><span class="p">]</span>

            <span class="c1"># Update the length cache</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>

        <span class="n">stacked_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>

        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="fm">__len__</span><span class="si">}</span><span class="s2">.pt&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">stacked_activations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">/</span> <span class="n">filename</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a Single Item to the Store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; future.result()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        1</span>

<span class="sd">        Args:</span>
<span class="sd">            item: Activation vector to add to the store.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Future that completes when the activation vector has queued to be written to disk, and</span>
<span class="sd">            if needed, written to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

            <span class="c1"># Write to disk if needed</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a Batch to the Store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))</span>
<span class="sd">        &gt;&gt;&gt; future.result()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        10</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: Batch of activation vectors to add to the store.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Future that completes when the activation vectors have queued to be written to disk, and</span>
<span class="sd">            if needed, written to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">=</span> <span class="n">resize_to_list_vectors</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>

            <span class="c1"># Write to disk if needed</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>

    <span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">        This should be called after the last batch has been added to the store. It will wait for</span>
<span class="sd">        all activation vectors to be written to disk.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        1</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_all_filenames</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Path</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a List of All Activation Vector Filenames.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*.pt&quot;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the Store.</span>

<span class="sd">        Warning:</span>
<span class="sd">        This will delete all .pt files in the top level of the storage directory.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; future.result()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        1</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
            <span class="n">file</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Find the file containing the activation vector</span>
        <span class="n">file_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span>
        <span class="n">file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">file_index</span><span class="si">}</span><span class="s2">.pt&quot;</span>

        <span class="c1"># Load the file and return the activation vector</span>
        <span class="n">activation_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">activation_vectors</span><span class="p">[</span><span class="n">index</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Calculate the length if not cached</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
                <span class="n">cache_size</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">cache_size</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
        <span class="c1"># Shutdown the thread pool after everything is complete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__del__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__del__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Delete Dunder Method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
    <span class="c1"># Shutdown the thread pool after everything is complete</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>index</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The index of the tensor to fetch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
            –
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Find the file containing the activation vector</span>
    <span class="n">file_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span>
    <span class="n">file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">file_index</span><span class="si">}</span><span class="s2">.pt&quot;</span>

    <span class="c1"># Load the file and return the activation vector</span>
    <span class="n">activation_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">activation_vectors</span><span class="p">[</span><span class="n">index</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="n">DEFAULT_DISK_ACTIVATION_STORE_PATH</span><span class="p">,</span> <span class="n">max_cache_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">empty_dir</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Disk Activation Store.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>storage_path</code></b>
                  (<code><span title="pathlib.Path">Path</span></code>, default:
                      <code><span title="sparse_autoencoder.activation_store.disk_store.DEFAULT_DISK_ACTIVATION_STORE_PATH">DEFAULT_DISK_ACTIVATION_STORE_PATH</span></code>
)
              –
              <div class="doc-md-description">
                <p>Path to the directory where the activation vectors will be stored.</p>
              </div>
            </li>
            <li>
              <b><code>max_cache_size</code></b>
                  (<code>int</code>, default:
                      <code>10000</code>
)
              –
              <div class="doc-md-description">
                <p>The maximum number of activation vectors to cache in memory before
writing to disk. Note this is only followed approximately.</p>
              </div>
            </li>
            <li>
              <b><code>num_workers</code></b>
                  (<code>int</code>, default:
                      <code>6</code>
)
              –
              <div class="doc-md-description">
                <p>Number of CPU workers to use for non-blocking writes to the file system (so
that the model can keep running whilst it writes the previous activations to disk).
This should be less than the number of CPU cores available. You don't need multiple
GPUs to take advantage of this feature.</p>
              </div>
            </li>
            <li>
              <b><code>empty_dir</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to empty the directory before writing. Generally you want to set this
to <code>True</code> as otherwise the directory may contain stale activation vectors from
previous runs.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">storage_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">DEFAULT_DISK_ACTIVATION_STORE_PATH</span><span class="p">,</span>
    <span class="n">max_cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">empty_dir</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Disk Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        storage_path: Path to the directory where the activation vectors will be stored.</span>
<span class="sd">        max_cache_size: The maximum number of activation vectors to cache in memory before</span>
<span class="sd">            writing to disk. Note this is only followed approximately.</span>
<span class="sd">        num_workers: Number of CPU workers to use for non-blocking writes to the file system (so</span>
<span class="sd">            that the model can keep running whilst it writes the previous activations to disk).</span>
<span class="sd">            This should be less than the number of CPU cores available. You don&#39;t need multiple</span>
<span class="sd">            GPUs to take advantage of this feature.</span>
<span class="sd">        empty_dir: Whether to empty the directory before writing. Generally you want to set this</span>
<span class="sd">            to `True` as otherwise the directory may contain stale activation vectors from</span>
<span class="sd">            previous runs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Setup the storage directory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">=</span> <span class="n">storage_path</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Setup the Cache</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span> <span class="o">=</span> <span class="n">max_cache_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Empty the directory if needed</span>
    <span class="k">if</span> <span class="n">empty_dir</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>

    <span class="c1"># Create a threadpool for non-blocking writes to the cache</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span> <span class="o">=</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
print(len(store))
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate the length if not cached</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
            <span class="n">cache_size</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">cache_size</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Add a Single Item to the Store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
future = store.append(torch.randn(100))
future.result()
print(len(store))
1</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>item</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>)
              –
              <div class="doc-md-description">
                <p>Activation vector to add to the store.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
            –
            <div class="doc-md-description">
              <p>Future that completes when the activation vector has queued to be written to disk, and</p>
            </div>
          </li>
          <li>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
            –
            <div class="doc-md-description">
              <p>if needed, written to disk.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a Single Item to the Store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; future.result()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>

<span class="sd">    Args:</span>
<span class="sd">        item: Activation vector to add to the store.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future that completes when the activation vector has queued to be written to disk, and</span>
<span class="sd">        if needed, written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

        <span class="c1"># Write to disk if needed</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Empty the Store.</p>
<p>Warning:
This will delete all .pt files in the top level of the storage directory.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
future = store.append(torch.randn(100))
future.result()
print(len(store))
1</p>
<p>store.empty()
print(len(store))
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the Store.</span>

<span class="sd">    Warning:</span>
<span class="sd">    This will delete all .pt files in the top level of the storage directory.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; future.result()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>

<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
        <span class="n">file</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Add a Batch to the Store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=10, empty_dir=True)
future = store.extend(torch.randn(10, 100))
future.result()
print(len(store))
10</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SourceModelActivations" href="tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations">SourceModelActivations</a></code>)
              –
              <div class="doc-md-description">
                <p>Batch of activation vectors to add to the store.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
            –
            <div class="doc-md-description">
              <p>Future that completes when the activation vectors have queued to be written to disk, and</p>
            </div>
          </li>
          <li>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
            –
            <div class="doc-md-description">
              <p>if needed, written to disk.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a Batch to the Store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))</span>
<span class="sd">    &gt;&gt;&gt; future.result()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    10</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: Batch of activation vectors to add to the store.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future that completes when the activation vectors have queued to be written to disk, and</span>
<span class="sd">        if needed, written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">=</span> <span class="n">resize_to_list_vectors</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>

        <span class="c1"># Write to disk if needed</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">wait_for_writes_to_complete</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Wait for Writes to Complete.</p>
<p>This should be called after the last batch has been added to the store. It will wait for
all activation vectors to be written to disk.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
future = store.append(torch.randn(100))
store.wait_for_writes_to_complete()
print(len(store))
1</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">    This should be called after the last batch has been added to the store. It will wait for</span>
<span class="sd">    all activation vectors to be written to disk.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.LearnedActivationsL1Loss" class="doc doc-heading">
          <code>LearnedActivationsL1Loss</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>Learned activations L1 (absolute error) loss.</p>
<p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this
multiplied by the l1_coefficient (designed to encourage sparsity).</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>l1_loss = LearnedActivationsL1Loss(0.1)
learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])
unused_activations = torch.zeros_like(learned_activations)</p>
<h3 id="sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log">Returns loss and metrics to log</h3>
<p>l1_loss(unused_activations, learned_activations, unused_activations)
(tensor(0.5000), {'LearnedActivationsL1Loss': 0.5})</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LearnedActivationsL1Loss</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this</span>
<span class="sd">    multiplied by the l1_coefficient (designed to encourage sparsity).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)</span>
<span class="sd">        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])</span>
<span class="sd">        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)</span>
<span class="sd">        &gt;&gt;&gt; # Returns loss and metrics to log</span>
<span class="sd">        &gt;&gt;&gt; l1_loss(unused_activations, learned_activations, unused_activations)</span>
<span class="sd">        (tensor(0.5000), {&#39;LearnedActivationsL1Loss&#39;: 0.5})</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L1 coefficient.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the absolute error loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">                approximate guide if you use e.g. 2x this number of tokens you might consider using</span>
<span class="sd">                0.5x the l1 coefficient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="n">l1_coefficient</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">absolute_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">absolute_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extra representation string.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;l1_coefficient=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">l1_coefficient</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>L1 coefficient.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">l1_coefficient</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the absolute error loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>l1_coefficient</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>L1 coefficient. The original paper experimented with L1 coefficients of
[0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an
approximate guide if you use e.g. 2x this number of tokens you might consider using
0.5x the l1 coefficient.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the absolute error loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">            approximate guide if you use e.g. 2x this number of tokens you might consider using</span>
<span class="sd">            0.5x the l1 coefficient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="n">l1_coefficient</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extra_repr</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Extra representation string.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extra representation string.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;l1_coefficient=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Learned activations L1 (absolute error) loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
            –
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">absolute_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">absolute_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.ListActivationStore" class="doc doc-heading">
          <code>ListActivationStore</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>List Activation Store.</p>
<p>Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick
experiments where you don't want to calculate how much memory you need in advance.</p>
<p>Multiprocess safe if the <code>multiprocessing_enabled</code> argument is set to <code>True</code>. This works in two
ways:</p>
<ol>
<li>The list of activation vectors is stored in a multiprocessing manager, which allows multiple
    processes (typically multiple GPUs) to read/write to the list.</li>
<li>The <code>extend</code> method is non-blocking, and uses a threadpool to write to the list in the
    background, which allows the main process to continue working even if there is just one GPU.</li>
</ol>
<p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with
additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p>
<p>Note that the built-in :meth:<code>shuffle</code> method is much faster than using the <code>shuffle</code> argument
on <code>torch.utils.data.DataLoader</code>. You should therefore call this method before passing the
dataset to the loader and then set the DataLoader <code>shuffle</code> argument to <code>False</code>.</p>
<p>Examples:
Create an empty activation dataset:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; store = ListActivationStore()
</code></pre>
<p>Add a single activation vector to the dataset (this is blocking):</p>
<pre><code>&gt;&gt;&gt; store.append(torch.randn(100))
&gt;&gt;&gt; len(store)
1
</code></pre>
<p>Add a batch of activation vectors to the dataset (non-blocking):</p>
<pre><code>&gt;&gt;&gt; batch = torch.randn(10, 100)
&gt;&gt;&gt; store.extend(batch)
&gt;&gt;&gt; len(store)
11
</code></pre>
<p>Shuffle the dataset <strong>before passing it to the DataLoader</strong>:</p>
<pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument
&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)
</code></pre>
<p>Use the dataloader to iterate over the dataset:</p>
<pre><code>&gt;&gt;&gt; next_item = next(iter(loader))
&gt;&gt;&gt; next_item.shape
torch.Size([2, 100])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ListActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;List Activation Store.</span>

<span class="sd">    Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick</span>
<span class="sd">    experiments where you don&#39;t want to calculate how much memory you need in advance.</span>

<span class="sd">    Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two</span>
<span class="sd">    ways:</span>

<span class="sd">    1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple</span>
<span class="sd">        processes (typically multiple GPUs) to read/write to the list.</span>
<span class="sd">    2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the</span>
<span class="sd">        background, which allows the main process to continue working even if there is just one GPU.</span>

<span class="sd">    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with</span>
<span class="sd">    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).</span>

<span class="sd">    Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument</span>
<span class="sd">    on `torch.utils.data.DataLoader`. You should therefore call this method before passing the</span>
<span class="sd">    dataset to the loader and then set the DataLoader `shuffle` argument to `False`.</span>

<span class="sd">    Examples:</span>
<span class="sd">    Create an empty activation dataset:</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>

<span class="sd">    Add a single activation vector to the dataset (this is blocking):</span>

<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        1</span>

<span class="sd">    Add a batch of activation vectors to the dataset (non-blocking):</span>

<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(batch)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        11</span>

<span class="sd">    Shuffle the dataset **before passing it to the DataLoader**:</span>

<span class="sd">        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument</span>
<span class="sd">        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)</span>

<span class="sd">    Use the dataloader to iterate over the dataset:</span>

<span class="sd">        &gt;&gt;&gt; next_item = next(iter(loader))</span>
<span class="sd">        &gt;&gt;&gt; next_item.shape</span>
<span class="sd">        torch.Size([2, 100])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">|</span> <span class="n">ListProxy</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Underlying List Data Store.&quot;&quot;&quot;</span>

    <span class="n">_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Device to Store the Activation Vectors On.&quot;&quot;&quot;</span>

    <span class="n">_pool</span><span class="p">:</span> <span class="n">ProcessPoolExecutor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiprocessing Pool.&quot;&quot;&quot;</span>

    <span class="n">_pool_exceptions</span><span class="p">:</span> <span class="n">ListProxy</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="ne">Exception</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pool Exceptions.</span>

<span class="sd">    Used to keep track of exceptions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_pool_futures</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Future</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pool Futures.</span>

<span class="sd">    Used to keep track of processes running in the pool.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">multiprocessing_enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the List Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Data to initialize the dataset with.</span>
<span class="sd">            device: Device to store the activation vectors on.</span>
<span class="sd">            max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.</span>
<span class="sd">                Default is the number of cores you have.</span>
<span class="sd">            multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU</span>
<span class="sd">                workers. This creates significant overhead, so you should only enable it if you have</span>
<span class="sd">                multiple GPUs (and experiment with enabling/disabling it).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Default to empty</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># If multiprocessing is enabled, use a multiprocessing manager to create a shared list</span>
        <span class="c1"># between processes. Otherwise, just use a normal list.</span>
        <span class="k">if</span> <span class="n">multiprocessing_enabled</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span> <span class="o">=</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">max_workers</span><span class="p">)</span>
            <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Device for storing the activation vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Returns the number of activation vectors in the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">        Returns the size of the dataset in bytes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The list of tensors is really a list of pointers to tensors, so we need to account for</span>
        <span class="c1"># this as well as the size of the tensors themselves.</span>
        <span class="n">list_of_pointers_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span>

        <span class="c1"># Handle 0 items</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">list_of_pointers_size</span>

        <span class="c1"># Otherwise, get the size of the first tensor</span>
        <span class="n">first_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">first_tensor_size</span> <span class="o">=</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
        <span class="n">num_tensors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
        <span class="n">total_tensors_size</span> <span class="o">=</span> <span class="n">first_tensor_size</span> <span class="o">*</span> <span class="n">num_tensors</span>

        <span class="k">return</span> <span class="n">total_tensors_size</span> <span class="o">+</span> <span class="n">list_of_pointers_size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; store[1]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([3.]))</span>
<span class="sd">        &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        3</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Append a single item to the dataset.</span>

<span class="sd">        Note **append is blocking**. For better performance use extend instead with batches.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">        Args:</span>
<span class="sd">            item: The item to append to the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extend threadpool method.</span>

<span class="sd">        To be called by :meth:`extend`.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: A batch of items to add to the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Unstack to a list of tensors</span>
            <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">=</span> <span class="n">resize_to_list_vectors</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># noqa: BLE001</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extend the dataset with multiple items (non-blocking).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">            &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">            &gt;&gt;&gt; async_result = store.extend(batch)</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            10</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: A batch of items to add to the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Schedule _extend to run in a separate process</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
            <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

        <span class="c1"># Fallback to synchronous execution if not multiprocessing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">        Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.randn(3, 100))</span>
<span class="sd">        &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        3</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Restart the pool</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">:</span>
            <span class="n">exceptions_report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">])</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Exceptions occurred in background workers:</span><span class="se">\n</span><span class="si">{</span><span class="n">exceptions_report</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>

        <span class="c1"># Clearing a list like this works for both standard and multiprocessing lists</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__del__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__del__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Delete Dunder Method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.zeros(5))
store.append(torch.ones(5))
store[1]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>index</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The index of the tensor to fetch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
            –
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">    &gt;&gt;&gt; store[1]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">multiprocessing_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the List Activation Store.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>data</code></b>
                  (<code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a>] | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Data to initialize the dataset with.</p>
              </div>
            </li>
            <li>
              <b><code>device</code></b>
                  (<code><span title="torch.device">device</span> | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Device to store the activation vectors on.</p>
              </div>
            </li>
            <li>
              <b><code>max_workers</code></b>
                  (<code>int | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Max CPU workers if multiprocessing is enabled, for writing to the list.
Default is the number of cores you have.</p>
              </div>
            </li>
            <li>
              <b><code>multiprocessing_enabled</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Support reading/writing to the dataset with multiple GPU
workers. This creates significant overhead, so you should only enable it if you have
multiple GPUs (and experiment with enabling/disabling it).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">multiprocessing_enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the List Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: Data to initialize the dataset with.</span>
<span class="sd">        device: Device to store the activation vectors on.</span>
<span class="sd">        max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.</span>
<span class="sd">            Default is the number of cores you have.</span>
<span class="sd">        multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU</span>
<span class="sd">            workers. This creates significant overhead, so you should only enable it if you have</span>
<span class="sd">            multiple GPUs (and experiment with enabling/disabling it).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Default to empty</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># If multiprocessing is enabled, use a multiprocessing manager to create a shared list</span>
    <span class="c1"># between processes. Otherwise, just use a normal list.</span>
    <span class="k">if</span> <span class="n">multiprocessing_enabled</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span> <span class="o">=</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">max_workers</span><span class="p">)</span>
        <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Device for storing the activation vectors</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>
<p>Returns the number of activation vectors in the dataset.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Returns the number of activation vectors in the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__sizeof__" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">__sizeof__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Sizeof Dunder Method.</p>
<p>Returns the size of the dataset in bytes.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">    Returns the size of the dataset in bytes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># The list of tensors is really a list of pointers to tensors, so we need to account for</span>
    <span class="c1"># this as well as the size of the tensors themselves.</span>
    <span class="n">list_of_pointers_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span>

    <span class="c1"># Handle 0 items</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">list_of_pointers_size</span>

    <span class="c1"># Otherwise, get the size of the first tensor</span>
    <span class="n">first_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">first_tensor_size</span> <span class="o">=</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
    <span class="n">num_tensors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
    <span class="n">total_tensors_size</span> <span class="o">=</span> <span class="n">first_tensor_size</span> <span class="o">*</span> <span class="n">num_tensors</span>

    <span class="k">return</span> <span class="n">total_tensors_size</span> <span class="o">+</span> <span class="n">list_of_pointers_size</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Append a single item to the dataset.</p>
<p>Note <strong>append is blocking</strong>. For better performance use extend instead with batches.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>item</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>)
              –
              <div class="doc-md-description">
                <p>The item to append to the dataset.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Append a single item to the dataset.</span>

<span class="sd">    Note **append is blocking**. For better performance use extend instead with batches.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>

<span class="sd">    Args:</span>
<span class="sd">        item: The item to append to the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Empty the dataset.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
<p>store.empty()
len(store)
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>

<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>

    <span class="c1"># Clearing a list like this works for both standard and multiprocessing lists</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Extend the dataset with multiple items (non-blocking).</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
batch = torch.randn(10, 100)
async_result = store.extend(batch)
len(store)
10</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SourceModelActivations" href="tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations">SourceModelActivations</a></code>)
              –
              <div class="doc-md-description">
                <p>A batch of items to add to the dataset.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extend the dataset with multiple items (non-blocking).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">        &gt;&gt;&gt; async_result = store.extend(batch)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        10</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: A batch of items to add to the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Schedule _extend to run in a separate process</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
        <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

    <span class="c1"># Fallback to synchronous execution if not multiprocessing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.shuffle" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">shuffle</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Shuffle the Data In-Place.</p>
<p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
_seed = torch.manual_seed(42)
store = ListActivationStore()
store.append(torch.tensor([1.]))
store.append(torch.tensor([2.]))
store.append(torch.tensor([3.]))
store.shuffle()
len(store)
3</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([3.]))</span>
<span class="sd">    &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    3</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">wait_for_writes_to_complete</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Wait for Writes to Complete.</p>
<p>Wait for any non-blocking writes (e.g. calls to :meth:<code>append</code>) to complete.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore(multiprocessing_enabled=True)
store.extend(torch.randn(3, 100))
store.wait_for_writes_to_complete()
len(store)
3</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">    Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.randn(3, 100))</span>
<span class="sd">    &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Restart the pool</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="p">):</span>
            <span class="k">pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">:</span>
        <span class="n">exceptions_report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">])</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Exceptions occurred in background workers:</span><span class="se">\n</span><span class="si">{</span><span class="n">exceptions_report</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.LossReducer" class="doc doc-heading">
          <code>LossReducer</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>Loss reducer.</p>
<p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to
nn.Sequential.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss
from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss
LossReducer(
...     MSEReconstructionLoss(),
...     LearnedActivationsL1Loss(0.001),
... )
LossReducer(
  (0): MSEReconstructionLoss()
  (1): LearnedActivationsL1Loss(l1_coefficient=0.001)
)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LossReducer</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss reducer.</span>

<span class="sd">    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to</span>
<span class="sd">    nn.Sequential.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss</span>
<span class="sd">        &gt;&gt;&gt; LossReducer(</span>
<span class="sd">        ...     MSEReconstructionLoss(),</span>
<span class="sd">        ...     LearnedActivationsL1Loss(0.001),</span>
<span class="sd">        ... )</span>
<span class="sd">        LossReducer(</span>
<span class="sd">          (0): MSEReconstructionLoss()</span>
<span class="sd">          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)</span>
<span class="sd">        )</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_modules</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;AbstractLoss&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Children loss modules.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">loss_modules</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the loss reducer.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss_modules: Loss modules to reduce.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the loss reducer has no loss modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_modules</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss_module</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Loss reducer must have at least one loss module.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ItemTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reduce loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Mean loss across the batch, summed across the loss modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_modules_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;module train_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">loss_module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">all_modules_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Dir dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractLoss</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get item dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">AbstractLoss</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterator dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__dir__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__dir__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Dir dunder method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dir dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get item dunder method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractLoss</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get item dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">loss_modules</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the loss reducer.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>loss_modules</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>, default:
                      <code>()</code>
)
              –
              <div class="doc-md-description">
                <p>Loss modules to reduce.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If the loss reducer has no loss modules.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">loss_modules</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the loss reducer.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss_modules: Loss modules to reduce.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the loss reducer has no loss modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_modules</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss_module</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Loss reducer must have at least one loss module.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__iter__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__iter__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Iterator dunder method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">AbstractLoss</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterator dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Length dunder method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reduce loss.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.ItemTensor" href="tensor_types/#sparse_autoencoder.tensor_types.ItemTensor">ItemTensor</a></code>
            –
            <div class="doc-md-description">
              <p>Mean loss across the batch, summed across the loss modules.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ItemTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Mean loss across the batch, summed across the loss modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_modules_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;module train_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">loss_module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">all_modules_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.LossReductionType" class="doc doc-heading">
          <code>LossReductionType</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Loss reduction type (across batch items).</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LossReductionType</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss reduction type (across batch items).&quot;&quot;&quot;</span>

    <span class="n">MEAN</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mean loss across batch items.&quot;&quot;&quot;</span>

    <span class="n">SUM</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sum the loss from all batch items.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.LossReductionType.MEAN" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">MEAN</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Mean loss across batch items.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.LossReductionType.SUM" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">SUM</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Sum the loss from all batch items.</p>
  </div>

</div>





  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.MSEReconstructionLoss" class="doc doc-heading">
          <code>MSEReconstructionLoss</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>MSE Reconstruction loss.</p>
<p>MSE reconstruction loss is calculated as the mean squared error between each each input vector
and it's corresponding decoded vector. The original paper found that models trained with some
loss functions such as cross-entropy loss generally prefer to represent features
polysemantically, whereas models trained with MSE may achieve the same loss for both
polysemantic and monosemantic representations of true features.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
loss = MSEReconstructionLoss()
input_activations = torch.tensor([[5.0, 4], [3.0, 4]])
output_activations = torch.tensor([[1.0, 5], [1.0, 5]])
unused_activations = torch.zeros_like(input_activations)</p>
<h3 id="sparse_autoencoder.MSEReconstructionLoss--outputs-both-loss-and-metrics-to-log">Outputs both loss and metrics to log</h3>
<p>loss(input_activations, unused_activations, output_activations)
(tensor(5.5000), {'MSEReconstructionLoss': 5.5})</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/mse_reconstruction_loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">MSEReconstructionLoss</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MSE Reconstruction loss.</span>

<span class="sd">    MSE reconstruction loss is calculated as the mean squared error between each each input vector</span>
<span class="sd">    and it&#39;s corresponding decoded vector. The original paper found that models trained with some</span>
<span class="sd">    loss functions such as cross-entropy loss generally prefer to represent features</span>
<span class="sd">    polysemantically, whereas models trained with MSE may achieve the same loss for both</span>
<span class="sd">    polysemantic and monosemantic representations of true features.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; loss = MSEReconstructionLoss()</span>
<span class="sd">        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])</span>
<span class="sd">        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])</span>
<span class="sd">        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)</span>
<span class="sd">        &gt;&gt;&gt; # Outputs both loss and metrics to log</span>
<span class="sd">        &gt;&gt;&gt; loss(input_activations, unused_activations, output_activations)</span>
<span class="sd">        (tensor(5.5000), {&#39;MSEReconstructionLoss&#39;: 5.5})</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;MSE Reconstruction loss (mean across features dimension).</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">square_error_loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

        <span class="c1"># Mean over just the features dimension (i.e. batch itemwise loss)</span>
        <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.MSEReconstructionLoss.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>MSE Reconstruction loss (mean across features dimension).</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Source activations (input activations to the autoencoder from the
source model).</p>
              </div>
            </li>
            <li>
              <b><code>learned_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations (intermediate activations in the autoencoder).</p>
              </div>
            </li>
            <li>
              <b><code>decoded_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Decoded activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
            –
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/loss/mse_reconstruction_loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MSE Reconstruction loss (mean across features dimension).</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_error_loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

    <span class="c1"># Mean over just the features dimension (i.e. batch itemwise loss)</span>
    <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.SparseAutoencoder" class="doc doc-heading">
          <code>SparseAutoencoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Sparse Autoencoder Model.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse Autoencoder Model.&quot;&quot;&quot;</span>

    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimated Geometric Median of the Dataset.</span>

<span class="sd">    Used for initialising :attr:`tied_bias`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tied_bias</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tied Bias Parameter.</span>

<span class="sd">    The same bias is used pre-encoder and post-decoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of Input Features.&quot;&quot;&quot;</span>

    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of Learned Features.&quot;&quot;&quot;</span>

    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Device to run the model on.&quot;&quot;&quot;</span>

    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Data type to use for the model.&quot;&quot;&quot;</span>

    <span class="n">encoder</span><span class="p">:</span> <span class="n">Sequential</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encoder Module.&quot;&quot;&quot;</span>

    <span class="n">decoder</span><span class="p">:</span> <span class="n">Sequential</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decoder Module.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">        Args:</span>
<span class="sd">            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations</span>
<span class="sd">                from TransformerLens).</span>
<span class="sd">            n_learned_features: Number of learned features. The initial paper experimented with 1 to</span>
<span class="sd">                256 times the number of input features, and primarily used a multiple of 8.</span>
<span class="sd">            geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">            device: Device to run the model on.</span>
<span class="sd">            dtype: Data type to use for the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

        <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
        <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Initialize the tied bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_input_features</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="n">OrderedDict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;TiedBias&quot;</span><span class="p">:</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">),</span>
                    <span class="s2">&quot;Linear&quot;</span><span class="p">:</span> <span class="n">Linear</span><span class="p">(</span>
                        <span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;ReLU&quot;</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(),</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="n">OrderedDict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;ConstrainedUnitNormLinear&quot;</span><span class="p">:</span> <span class="n">ConstrainedUnitNormLinear</span><span class="p">(</span>
                        <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">n_input_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;TiedBias&quot;</span><span class="p">:</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
        <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of learned activations and decoded activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>

    <span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
        <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">save_to_hf</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the model to Hugging Face.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">load_from_hf</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the model from Hugging Face.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.decoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">decoder</span><span class="p">:</span> <span class="n">Sequential</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">({</span><span class="s1">&#39;ConstrainedUnitNormLinear&#39;</span><span class="p">:</span> <span class="n">ConstrainedUnitNormLinear</span><span class="p">(</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">n_input_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="s1">&#39;TiedBias&#39;</span><span class="p">:</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)}))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Decoder Module.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.device" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">device</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Device to run the model on.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.dtype" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">dtype</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Data type to use for the model.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.encoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">encoder</span><span class="p">:</span> <span class="n">Sequential</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">({</span><span class="s1">&#39;TiedBias&#39;</span><span class="p">:</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">),</span> <span class="s1">&#39;Linear&#39;</span><span class="p">:</span> <span class="n">Linear</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">()}))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Encoder Module.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Estimated Geometric Median of the Dataset.</p>
<p>Used for initialising :attr:<code>tied_bias</code>.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.n_input_features" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_input_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of Input Features.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.n_learned_features" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_learned_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of Learned Features.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.tied_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">tied_bias</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Tied Bias Parameter.</p>
<p>The same bias is used pre-encoder and post-decoder.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">geometric_median_dataset</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Sparse Autoencoder Model.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>n_input_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations
from TransformerLens).</p>
              </div>
            </li>
            <li>
              <b><code>n_learned_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of learned features. The initial paper experimented with 1 to
256 times the number of input features, and primarily used a multiple of 8.</p>
              </div>
            </li>
            <li>
              <b><code>geometric_median_dataset</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>)
              –
              <div class="doc-md-description">
                <p>Estimated geometric median of the dataset.</p>
              </div>
            </li>
            <li>
              <b><code>device</code></b>
                  (<code><span title="torch.device">device</span> | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Device to run the model on.</p>
              </div>
            </li>
            <li>
              <b><code>dtype</code></b>
                  (<code><span title="torch.dtype">dtype</span> | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Data type to use for the model.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations</span>
<span class="sd">            from TransformerLens).</span>
<span class="sd">        n_learned_features: Number of learned features. The initial paper experimented with 1 to</span>
<span class="sd">            256 times the number of input features, and primarily used a multiple of 8.</span>
<span class="sd">        geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">        device: Device to run the model on.</span>
<span class="sd">        dtype: Data type to use for the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
    <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Initialize the tied bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_input_features</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="n">OrderedDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;TiedBias&quot;</span><span class="p">:</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">),</span>
                <span class="s2">&quot;Linear&quot;</span><span class="p">:</span> <span class="n">Linear</span><span class="p">(</span>
                    <span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
                <span class="p">),</span>
                <span class="s2">&quot;ReLU&quot;</span><span class="p">:</span> <span class="n">ReLU</span><span class="p">(),</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="n">OrderedDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;ConstrainedUnitNormLinear&quot;</span><span class="p">:</span> <span class="n">ConstrainedUnitNormLinear</span><span class="p">(</span>
                    <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">n_input_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
                <span class="p">),</span>
                <span class="s2">&quot;TiedBias&quot;</span><span class="p">:</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">),</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a>]</code>
            –
            <div class="doc-md-description">
              <p>Tuple of learned activations and decoded activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
    <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of learned activations and decoded activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">initialize_tied_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the tied parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
    <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.load_from_hf" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_from_hf</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Load the model from Hugging Face.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_from_hf</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load the model from Hugging Face.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reset the parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.save_to_hf" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">save_to_hf</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Save the model to Hugging Face.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_to_hf</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save the model to Hugging Face.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.TensorActivationStore" class="doc doc-heading">
          <code>TensorActivationStore</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>Tensor Activation Store.</p>
<p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation
vectors to be stored to be known in advance. Multiprocess safe.</p>
<p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with
additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p>
<p>Examples:
Create an empty activation dataset:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)
</code></pre>
<p>Add a single activation vector to the dataset:</p>
<pre><code>&gt;&gt;&gt; store.append(torch.randn(100))
&gt;&gt;&gt; len(store)
1
</code></pre>
<p>Add a [batch, pos, neurons] activation tensor to the dataset:</p>
<pre><code>&gt;&gt;&gt; store.empty()
&gt;&gt;&gt; batch = torch.randn(10, 10, 100)
&gt;&gt;&gt; store.extend(batch)
&gt;&gt;&gt; len(store)
100
</code></pre>
<p>Shuffle the dataset <strong>before passing it to the DataLoader</strong>:</p>
<pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument
&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)
</code></pre>
<p>Use the dataloader to iterate over the dataset:</p>
<pre><code>&gt;&gt;&gt; next_item = next(iter(loader))
&gt;&gt;&gt; next_item.shape
torch.Size([2, 100])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TensorActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tensor Activation Store.</span>

<span class="sd">    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation</span>
<span class="sd">    vectors to be stored to be known in advance. Multiprocess safe.</span>

<span class="sd">    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with</span>
<span class="sd">    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).</span>

<span class="sd">    Examples:</span>
<span class="sd">    Create an empty activation dataset:</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)</span>

<span class="sd">    Add a single activation vector to the dataset:</span>

<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        1</span>

<span class="sd">    Add a [batch, pos, neurons] activation tensor to the dataset:</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 10, 100)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(batch)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        100</span>

<span class="sd">    Shuffle the dataset **before passing it to the DataLoader**:</span>

<span class="sd">        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument</span>
<span class="sd">        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)</span>

<span class="sd">    Use the dataloader to iterate over the dataset:</span>

<span class="sd">        &gt;&gt;&gt; next_item = next(iter(loader))</span>
<span class="sd">        &gt;&gt;&gt; next_item.shape</span>
<span class="sd">        torch.Size([2, 100])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_data</span><span class="p">:</span> <span class="n">StoreActivations</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Underlying Tensor Data Store.&quot;&quot;&quot;</span>

    <span class="n">items_stored</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of items stored.&quot;&quot;&quot;</span>

    <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum Number of Items to Store.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the Tensor Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_items: Maximum number of items to store (individual activation vectors)</span>
<span class="sd">            num_neurons: Number of neurons in each activation vector.</span>
<span class="sd">            device: Device to store the activation vectors on.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_items</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Returns the number of activation vectors in the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">        Returns the size of the underlying tensor in bytes.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)</span>
<span class="sd">        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100</span>
<span class="sd">        800</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; store[1]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If the index is out of range.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check in range</span>
        <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> out of range (only </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="si">}</span><span class="s2"> items stored)&quot;</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([0.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">        &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">        &gt;&gt;&gt; [store[i].item() for i in range(3)]</span>
<span class="sd">        [0.0, 2.0, 1.0]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Generate a permutation of the indices for the active data</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">)</span>

        <span class="c1"># Use this permutation to shuffle the active data in-place</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a single item to the store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; store[1]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            item: The item to append to the dataset.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If there is no space remaining.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check we have space</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">StoreFullError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a batch to the store.</span>

<span class="sd">        Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        2</span>

<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        9</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The batch to append to the dataset.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If there is no space remaining.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reshaped</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">resize_to_single_item_dimension</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Check we have space</span>
        <span class="n">num_activation_tensors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Single batch of </span><span class="si">{</span><span class="n">num_activation_tensors</span><span class="si">}</span><span class="s2"> activations is larger than the </span><span class="se">\</span>
<span class="s2">                    total maximum in the store of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="k">raise</span> <span class="n">StoreFullError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span><span class="p">]</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="n">num_activation_tensors</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We don&#39;t need to zero the data, just reset the number of items stored</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.TensorActivationStore.items_stored" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">items_stored</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of items stored.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.TensorActivationStore.max_items" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Maximum Number of Items to Store.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=2, num_neurons=5)
store.append(torch.zeros(5))
store.append(torch.ones(5))
store[1]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>index</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The index of the tensor to fetch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
            –
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>IndexError</code>
              –
              <div class="doc-md-description">
                <p>If the index is out of range.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">    &gt;&gt;&gt; store[1]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If the index is out of range.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check in range</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> out of range (only </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="si">}</span><span class="s2"> items stored)&quot;</span>
        <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">max_items</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the Tensor Activation Store.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>max_items</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Maximum number of items to store (individual activation vectors)</p>
              </div>
            </li>
            <li>
              <b><code>num_neurons</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of neurons in each activation vector.</p>
              </div>
            </li>
            <li>
              <b><code>device</code></b>
                  (<code><span title="torch.device">device</span> | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Device to store the activation vectors on.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the Tensor Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        max_items: Maximum number of items to store (individual activation vectors)</span>
<span class="sd">        num_neurons: Number of neurons in each activation vector.</span>
<span class="sd">        device: Device to store the activation vectors on.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_items</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>
<p>Returns the number of activation vectors in the dataset.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10_000_000, num_neurons=100)
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Returns the number of activation vectors in the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__sizeof__" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">__sizeof__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Sizeof Dunder Method.</p>
<p>Returns the size of the underlying tensor in bytes.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=2, num_neurons=100)
store.<strong>sizeof</strong>() # Pre-allocated tensor of 2x100
800</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">    Returns the size of the underlying tensor in bytes.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)</span>
<span class="sd">    &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100</span>
<span class="sd">    800</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Add a single item to the store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, num_neurons=5)
store.append(torch.zeros(5))
store.append(torch.ones(5))
store[1]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>item</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>)
              –
              <div class="doc-md-description">
                <p>The item to append to the dataset.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>IndexError</code>
              –
              <div class="doc-md-description">
                <p>If there is no space remaining.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a single item to the store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">    &gt;&gt;&gt; store[1]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        item: The item to append to the dataset.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If there is no space remaining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check we have space</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StoreFullError</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Empty the store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, num_neurons=5)
store.extend(torch.zeros(2, 5))
store.items_stored
2
store.empty()
store.items_stored
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    2</span>
<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We don&#39;t need to zero the data, just reset the number of items stored</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Add a batch to the store.</p>
<p>Examples:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, num_neurons=5)
store.extend(torch.zeros(2, 5))
store.items_stored
2</p>
<p>store = TensorActivationStore(max_items=10, num_neurons=5)
store.extend(torch.zeros(3, 3, 5))
store.items_stored
9</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SourceModelActivations" href="tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations">SourceModelActivations</a></code>)
              –
              <div class="doc-md-description">
                <p>The batch to append to the dataset.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>IndexError</code>
              –
              <div class="doc-md-description">
                <p>If there is no space remaining.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a batch to the store.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    2</span>

<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    9</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batch to append to the dataset.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If there is no space remaining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reshaped</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">resize_to_single_item_dimension</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Check we have space</span>
    <span class="n">num_activation_tensors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Single batch of </span><span class="si">{</span><span class="n">num_activation_tensors</span><span class="si">}</span><span class="s2"> activations is larger than the </span><span class="se">\</span>
<span class="s2">                total maximum in the store of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">raise</span> <span class="n">StoreFullError</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span><span class="p">]</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="n">num_activation_tensors</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.shuffle" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">shuffle</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Shuffle the Data In-Place.</p>
<p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
_seed = torch.manual_seed(42)
store = TensorActivationStore(max_items=10, num_neurons=1)
store.append(torch.tensor([0.]))
store.append(torch.tensor([1.]))
store.append(torch.tensor([2.]))
store.shuffle()
[store[i].item() for i in range(3)]
[0.0, 2.0, 1.0]</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([0.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">    &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">    &gt;&gt;&gt; [store[i].item() for i in range(3)]</span>
<span class="sd">    [0.0, 2.0, 1.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate a permutation of the indices for the active data</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">)</span>

    <span class="c1"># Use this permutation to shuffle the active data in-place</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>



<div class="doc doc-object doc-function">




<h2 id="sparse_autoencoder.pipeline" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">pipeline</span><span class="p">(</span><span class="n">src_model</span><span class="p">,</span> <span class="n">src_model_activation_hook_point</span><span class="p">,</span> <span class="n">src_model_activation_layer</span><span class="p">,</span> <span class="n">source_dataset</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">,</span> <span class="n">num_activations_before_training</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">source_dataset_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">resample_frequency</span><span class="o">=</span><span class="mi">25000000</span><span class="p">,</span> <span class="n">sweep_parameters</span><span class="o">=</span><span class="n">SweepParametersRuntime</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_activations</span><span class="o">=</span><span class="mi">100000000</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Full pipeline for training the sparse autoEncoder.</p>
<p>The pipeline alternates between generating activations and training the autoencoder.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>src_model</code></b>
                  (<code><span title="transformer_lens.HookedTransformer">HookedTransformer</span></code>)
              –
              <div class="doc-md-description">
                <p>The model to get activations from.</p>
              </div>
            </li>
            <li>
              <b><code>src_model_activation_hook_point</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>The hook point to get activations from.</p>
              </div>
            </li>
            <li>
              <b><code>src_model_activation_layer</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The layer to get activations from. This is used to stop the
model after this layer, as we don't need the final logits.</p>
              </div>
            </li>
            <li>
              <b><code>source_dataset</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a></code>)
              –
              <div class="doc-md-description">
                <p>Source dataset containing source model inputs (typically batches of prompts)
that are used to generate the activations data.</p>
              </div>
            </li>
            <li>
              <b><code>activation_store</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>)
              –
              <div class="doc-md-description">
                <p>The store to buffer activations in once generated, before training the
autoencoder.</p>
              </div>
            </li>
            <li>
              <b><code>num_activations_before_training</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of activations to generate before training the
autoencoder. As a guide, 1 million activations, each of size 1024, will take up about
2GB of memory (assuming float16/bfloat16).</p>
              </div>
            </li>
            <li>
              <b><code>autoencoder</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>)
              –
              <div class="doc-md-description">
                <p>The autoencoder to train.</p>
              </div>
            </li>
            <li>
              <b><code>source_dataset_batch_size</code></b>
                  (<code>int</code>, default:
                      <code>16</code>
)
              –
              <div class="doc-md-description">
                <p>Batch size of tokenized prompts for generating the source data.</p>
              </div>
            </li>
            <li>
              <b><code>resample_frequency</code></b>
                  (<code>int</code>, default:
                      <code>25000000</code>
)
              –
              <div class="doc-md-description">
                <p>How often to resample neurons (number of activations learnt on).</p>
              </div>
            </li>
            <li>
              <b><code>sweep_parameters</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.sweep_config.SweepParametersRuntime" href="train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParametersRuntime">SweepParametersRuntime</a></code>, default:
                      <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.train.sweep_config.SweepParametersRuntime" href="train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParametersRuntime">SweepParametersRuntime</a>()</code>
)
              –
              <div class="doc-md-description">
                <p>Parameter config to use.</p>
              </div>
            </li>
            <li>
              <b><code>device</code></b>
                  (<code><span title="torch.device">device</span> | None</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Device to run pipeline on.</p>
              </div>
            </li>
            <li>
              <b><code>max_activations</code></b>
                  (<code>int</code>, default:
                      <code>100000000</code>
)
              –
              <div class="doc-md-description">
                <p>Maximum number of activations to train with. May train for less if the
source dataset is exhausted.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913</span>
    <span class="n">src_model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span>
    <span class="n">src_model_activation_hook_point</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">src_model_activation_layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span><span class="p">,</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">num_activations_before_training</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">source_dataset_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">resample_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25_000_000</span><span class="p">,</span>
    <span class="n">sweep_parameters</span><span class="p">:</span> <span class="n">SweepParametersRuntime</span> <span class="o">=</span> <span class="n">SweepParametersRuntime</span><span class="p">(),</span>  <span class="c1"># noqa: B008</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_activations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000_000</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Full pipeline for training the sparse autoEncoder.</span>

<span class="sd">    The pipeline alternates between generating activations and training the autoencoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_model: The model to get activations from.</span>
<span class="sd">        src_model_activation_hook_point: The hook point to get activations from.</span>
<span class="sd">        src_model_activation_layer: The layer to get activations from. This is used to stop the</span>
<span class="sd">            model after this layer, as we don&#39;t need the final logits.</span>
<span class="sd">        source_dataset: Source dataset containing source model inputs (typically batches of prompts)</span>
<span class="sd">            that are used to generate the activations data.</span>
<span class="sd">        activation_store: The store to buffer activations in once generated, before training the</span>
<span class="sd">            autoencoder.</span>
<span class="sd">        num_activations_before_training: The number of activations to generate before training the</span>
<span class="sd">            autoencoder. As a guide, 1 million activations, each of size 1024, will take up about</span>
<span class="sd">            2GB of memory (assuming float16/bfloat16).</span>
<span class="sd">        autoencoder: The autoencoder to train.</span>
<span class="sd">        source_dataset_batch_size: Batch size of tokenized prompts for generating the source data.</span>
<span class="sd">        resample_frequency: How often to resample neurons (number of activations learnt on).</span>
<span class="sd">        sweep_parameters: Parameter config to use.</span>
<span class="sd">        device: Device to run pipeline on.</span>
<span class="sd">        max_activations: Maximum number of activations to train with. May train for less if the</span>
<span class="sd">            source dataset is exhausted.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamWithReset</span><span class="p">(</span>
        <span class="n">autoencoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">sweep_parameters</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">sweep_parameters</span><span class="o">.</span><span class="n">adam_beta_1</span><span class="p">,</span> <span class="n">sweep_parameters</span><span class="o">.</span><span class="n">adam_beta_2</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">sweep_parameters</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">sweep_parameters</span><span class="o">.</span><span class="n">adam_weight_decay</span><span class="p">,</span>
        <span class="n">named_parameters</span><span class="o">=</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="n">source_dataloader</span> <span class="o">=</span> <span class="n">source_dataset</span><span class="o">.</span><span class="n">get_dataloader</span><span class="p">(</span><span class="n">source_dataset_batch_size</span><span class="p">)</span>
    <span class="n">source_data_iterator</span> <span class="o">=</span> <span class="n">stateful_dataloader_iterable</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span>

    <span class="n">total_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">activations_since_resampling</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="n">autoencoder</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">total_activations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Run loop until source data is exhausted:</span>
    <span class="k">with</span> <span class="n">logging_redirect_tqdm</span><span class="p">(),</span> <span class="n">tqdm</span><span class="p">(</span>
        <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Total activations trained on&quot;</span><span class="p">,</span>
        <span class="n">dynamic_ncols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">total</span><span class="o">=</span><span class="n">max_activations</span><span class="p">,</span>
        <span class="n">postfix</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Current mode&quot;</span><span class="p">:</span> <span class="s2">&quot;initializing&quot;</span><span class="p">},</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">total_activations</span> <span class="o">&lt;</span> <span class="n">max_activations</span><span class="p">:</span>
            <span class="c1"># Add activations to the store</span>
            <span class="n">activation_store</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>  <span class="c1"># In case it was filled by a different run</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;Current mode&quot;</span><span class="p">:</span> <span class="s2">&quot;generating&quot;</span><span class="p">})</span>
            <span class="n">generate_activations</span><span class="p">(</span>
                <span class="n">src_model</span><span class="p">,</span>
                <span class="n">src_model_activation_layer</span><span class="p">,</span>
                <span class="n">src_model_activation_hook_point</span><span class="p">,</span>
                <span class="n">activation_store</span><span class="p">,</span>
                <span class="n">source_data_iterator</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                <span class="n">context_size</span><span class="o">=</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span><span class="p">,</span>
                <span class="n">num_items</span><span class="o">=</span><span class="n">num_activations_before_training</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">source_dataset_batch_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Shuffle the store if it has a shuffle method - it is often more efficient to</span>
            <span class="c1"># create a shuffle method ourselves rather than get the DataLoader to shuffle</span>
            <span class="n">activation_store</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>

            <span class="c1"># Train the autoencoder</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;Current mode&quot;</span><span class="p">:</span> <span class="s2">&quot;training&quot;</span><span class="p">})</span>
            <span class="n">train_steps</span><span class="p">,</span> <span class="n">learned_activations_fired_count</span> <span class="o">=</span> <span class="n">train_autoencoder</span><span class="p">(</span>
                <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                <span class="n">sweep_parameters</span><span class="o">=</span><span class="n">sweep_parameters</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                <span class="n">previous_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">total_steps</span> <span class="o">+=</span> <span class="n">train_steps</span>

            <span class="k">if</span> <span class="n">activations_since_resampling</span> <span class="o">&gt;=</span> <span class="n">resample_frequency</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">neuron_activity</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">learned_activations_fired_count</span><span class="p">)</span>

            <span class="n">activations_since_resampling</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
            <span class="n">total_activations</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">))</span>

            <span class="c1"># Resample neurons if required</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">DEFAULT_RESAMPLE_N</span><span class="p">:</span>
                <span class="n">warn_str</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Warning: activation store len </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span><span class="si">}</span><span class="s2"> is less than &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;DEFAULT_RESAMPLE_N (</span><span class="si">{</span><span class="n">DEFAULT_RESAMPLE_N</span><span class="si">}</span><span class="s2">). Resampling with&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;num_resample_inputs as </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="n">warn_str</span><span class="p">,</span>
                    <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">num_resample_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_resample_inputs</span> <span class="o">=</span> <span class="n">DEFAULT_RESAMPLE_N</span>

            <span class="k">if</span> <span class="n">activations_since_resampling</span> <span class="o">&gt;=</span> <span class="n">resample_frequency</span><span class="p">:</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;Current mode&quot;</span><span class="p">:</span> <span class="s2">&quot;resampling&quot;</span><span class="p">})</span>
                <span class="n">activations_since_resampling</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="n">resample_dead_neurons</span><span class="p">(</span>
                    <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                    <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                    <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                    <span class="n">sweep_parameters</span><span class="o">=</span><span class="n">sweep_parameters</span><span class="p">,</span>
                    <span class="n">num_inputs</span><span class="o">=</span><span class="n">num_resample_inputs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">learned_activations_fired_count</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_state_all_parameters</span><span class="p">()</span>

            <span class="n">activation_store</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../citation/" class="btn btn-neutral float-left" title="Citation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="activation_resampler/" class="btn btn-neutral float-right" title="Index">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../citation/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="activation_resampler/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
