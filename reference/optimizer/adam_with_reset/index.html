<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>adam_with_reset - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "adam_with_reset";
        var mkdocs_page_input_path = "reference/optimizer/adam_with_reset.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_resampler</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">autoencoder</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/abstract_metric/">abstract_metric</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">optimizer</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="./">adam_with_reset</a>
    <ul class="current">
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_data</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/abstract_dataset/">abstract_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/random_int/">random_int</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
          <li class="breadcrumb-item">optimizer</li>
      <li class="breadcrumb-item active">adam_with_reset</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder.optimizer.adam_with_reset"></a>
  <div class="doc doc-contents first">
  
      <p>Adam Optimizer with a reset method.</p>
<p>This reset method is useful when resampling dead neurons during training.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset" class="doc doc-heading">
          <code>AdamWithReset</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.optim.Adam">Adam</span></code>, <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset" href="../abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset">AbstractOptimizerWithReset</a></code></p>

  
      <p>Adam Optimizer with a reset method.</p>
<p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when
manually editing the model parameters during training (e.g. when resampling dead neurons). This
is because Adam maintains running averages of the gradients and the squares of gradients, which
will be incorrect if the parameters are changed.</p>
<p>Otherwise this is the same as the standard Adam optimizer.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">AdamWithReset</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adam Optimizer with a reset method.</span>

<span class="sd">    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when</span>
<span class="sd">    manually editing the model parameters during training (e.g. when resampling dead neurons). This</span>
<span class="sd">    is because Adam maintains running averages of the gradients and the squares of gradients, which</span>
<span class="sd">    will be incorrect if the parameters are changed.</span>

<span class="sd">    Otherwise this is the same as the standard Adam optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">parameter_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parameter Names.</span>

<span class="sd">    The names of the parameters, so that we can find them later when resetting the state.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913 , D417 (extending existing implementation)</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">params_t</span><span class="p">,</span>
        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">betas</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">foreach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">named_parameters</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the optimizer.</span>

<span class="sd">        Warning:</span>
<span class="sd">            Named parameters must be with default settings (remove duplicates and not recursive).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">            &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">            ...     model.parameters(),</span>
<span class="sd">            ...     named_parameters=model.named_parameters(),</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_state_all_parameters()</span>

<span class="sd">        Args:</span>
<span class="sd">            named_parameters (Iterator[tuple[str, Parameter]]): An iterator over the named</span>
<span class="sd">                parameters of the model. This is used to find the parameters when resetting their</span>
<span class="sd">                state. You should set this as `model.named_parameters()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialise the parent class (note we repeat the parameter names so that type hints work).</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span>
            <span class="n">foreach</span><span class="o">=</span><span class="n">foreach</span><span class="p">,</span>
            <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">,</span>
            <span class="n">capturable</span><span class="o">=</span><span class="n">capturable</span><span class="p">,</span>
            <span class="n">differentiable</span><span class="o">=</span><span class="n">differentiable</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="n">fused</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Store the names of the parameters, so that we can find them later when resetting the</span>
        <span class="c1"># state.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;The number of parameter names does not match the number of parameters. &quot;</span>
                <span class="s2">&quot;If using model.named_parameters() make sure remove_duplicates is True &quot;</span>
                <span class="s2">&quot;and recursive is False (the default settings).&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_state_all_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the state for all parameters.</span>

<span class="sd">        Iterates over all parameters and resents both the running averages of the gradients and the</span>
<span class="sd">        squares of gradients.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Iterate over every parameter</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="c1"># Get the state</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

                <span class="c1"># Check if state is initialized</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Reset running averages</span>
                <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

                <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
                <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
                    <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_parameter_name_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the index of a parameter name.</span>

<span class="sd">        Args:</span>
<span class="sd">            parameter_name (str): The name of the parameter.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The index of the parameter name.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the parameter name is not found.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">parameter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Parameter name </span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2"> not found.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_neurons_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">neuron_indices</span><span class="p">:</span> <span class="n">DeadNeuronIndices</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">parameter_group</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the state for specific neurons, on a specific parameter.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">            &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">            ...     model.parameters(),</span>
<span class="sd">            ...     named_parameters=model.named_parameters(),</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...</span>
<span class="sd">            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</span>
<span class="sd">            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;encoder.Linear.weight&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;encoder.Linear.bias&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(</span>
<span class="sd">            ...     &quot;decoder.ConstrainedUnitNormLinear.weight&quot;,</span>
<span class="sd">            ...     dead_neurons_indices,</span>
<span class="sd">            ...     axis=1</span>
<span class="sd">            ... )</span>

<span class="sd">        Args:</span>
<span class="sd">            parameter_name: The name of the parameter. Examples from the standard sparse autoencoder</span>
<span class="sd">                implementation  include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`,</span>
<span class="sd">                `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`.</span>
<span class="sd">            neuron_indices: The indices of the neurons to reset.</span>
<span class="sd">            axis: The axis of the parameter to reset.</span>
<span class="sd">            parameter_group: The index of the parameter group to reset (typically this is just zero,</span>
<span class="sd">                unless you have setup multiple parameter groups for e.g. different learning rates</span>
<span class="sd">                for different parameters).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the parameter name is not found.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the state of the parameter</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="n">parameter_group</span><span class="p">]</span>
        <span class="n">parameter_name_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_parameter_name_idx</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">)</span>
        <span class="n">parameter</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">parameter_name_idx</span><span class="p">]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

        <span class="c1"># Check if state is initialized</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Reset running averages for the specified neurons</span>
        <span class="k">if</span> <span class="s2">&quot;exp_avg&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
            <span class="n">exp_avg</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
        <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.parameter_names" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">parameter_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_value</span><span class="p">)</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Parameter Names.</p>
<p>The names of the parameters, so that we can find them later when resetting the state.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">foreach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">capturable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">differentiable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fused</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">named_parameters</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the optimizer.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>Named parameters must be with default settings (remove duplicates and not recursive).</p>
</details>
<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
from sparse_autoencoder.autoencoder.model import SparseAutoencoder
model = SparseAutoencoder(5, 10, torch.zeros(5))
optimizer = AdamWithReset(
...     model.parameters(),
...     named_parameters=model.named_parameters(),
... )
optimizer.reset_state_all_parameters()</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>named_parameters</code></b>
                  (<code><span title="collections.abc.Iterator">Iterator</span>[tuple[str, <span title="torch.nn.parameter.Parameter">Parameter</span>]]</code>)
              â€“
              <div class="doc-md-description">
                <p>An iterator over the named
parameters of the model. This is used to find the parameters when resetting their
state. You should set this as <code>model.named_parameters()</code>.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913 , D417 (extending existing implementation)</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">params_t</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">betas</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">foreach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">named_parameters</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the optimizer.</span>

<span class="sd">    Warning:</span>
<span class="sd">        Named parameters must be with default settings (remove duplicates and not recursive).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">        ...     model.parameters(),</span>
<span class="sd">        ...     named_parameters=model.named_parameters(),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_state_all_parameters()</span>

<span class="sd">    Args:</span>
<span class="sd">        named_parameters (Iterator[tuple[str, Parameter]]): An iterator over the named</span>
<span class="sd">            parameters of the model. This is used to find the parameters when resetting their</span>
<span class="sd">            state. You should set this as `model.named_parameters()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialise the parent class (note we repeat the parameter names so that type hints work).</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span>
        <span class="n">foreach</span><span class="o">=</span><span class="n">foreach</span><span class="p">,</span>
        <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">,</span>
        <span class="n">capturable</span><span class="o">=</span><span class="n">capturable</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="o">=</span><span class="n">differentiable</span><span class="p">,</span>
        <span class="n">fused</span><span class="o">=</span><span class="n">fused</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Store the names of the parameters, so that we can find them later when resetting the</span>
    <span class="c1"># state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;The number of parameter names does not match the number of parameters. &quot;</span>
            <span class="s2">&quot;If using model.named_parameters() make sure remove_duplicates is True &quot;</span>
            <span class="s2">&quot;and recursive is False (the default settings).&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_neurons_state</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">parameter_group</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reset the state for specific neurons, on a specific parameter.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
from sparse_autoencoder.autoencoder.model import SparseAutoencoder
model = SparseAutoencoder(5, 10, torch.zeros(5))
optimizer = AdamWithReset(
...     model.parameters(),
...     named_parameters=model.named_parameters(),
... )</p>
<h4 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this">... train the model and then resample some dead neurons, then do this ...</h4>
<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>
<h4 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated">Reset the optimizer state for parameters that have been updated</h4>
<p>optimizer.reset_neurons_state("encoder.Linear.weight", dead_neurons_indices, axis=0)
optimizer.reset_neurons_state("encoder.Linear.bias", dead_neurons_indices, axis=0)
optimizer.reset_neurons_state(
...     "decoder.ConstrainedUnitNormLinear.weight",
...     dead_neurons_indices,
...     axis=1
... )</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>parameter_name</code></b>
                  (<code>str</code>)
              â€“
              <div class="doc-md-description">
                <p>The name of the parameter. Examples from the standard sparse autoencoder
implementation  include <code>tied_bias</code>, <code>encoder.Linear.weight</code>, <code>encoder.Linear.bias</code>,
<code>decoder.Linear.weight</code>, and <code>decoder.ConstrainedUnitNormLinear.weight</code>.</p>
              </div>
            </li>
            <li>
              <b><code>neuron_indices</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.DeadNeuronIndices" href="../../tensor_types/#sparse_autoencoder.tensor_types.DeadNeuronIndices">DeadNeuronIndices</a></code>)
              â€“
              <div class="doc-md-description">
                <p>The indices of the neurons to reset.</p>
              </div>
            </li>
            <li>
              <b><code>axis</code></b>
                  (<code>int</code>)
              â€“
              <div class="doc-md-description">
                <p>The axis of the parameter to reset.</p>
              </div>
            </li>
            <li>
              <b><code>parameter_group</code></b>
                  (<code>int</code>, default:
                      <code>0</code>
)
              â€“
              <div class="doc-md-description">
                <p>The index of the parameter group to reset (typically this is just zero,
unless you have setup multiple parameter groups for e.g. different learning rates
for different parameters).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              â€“
              <div class="doc-md-description">
                <p>If the parameter name is not found.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_neurons_state</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">neuron_indices</span><span class="p">:</span> <span class="n">DeadNeuronIndices</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">parameter_group</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the state for specific neurons, on a specific parameter.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">        ...     model.parameters(),</span>
<span class="sd">        ...     named_parameters=model.named_parameters(),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...</span>
<span class="sd">        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</span>
<span class="sd">        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;encoder.Linear.weight&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;encoder.Linear.bias&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(</span>
<span class="sd">        ...     &quot;decoder.ConstrainedUnitNormLinear.weight&quot;,</span>
<span class="sd">        ...     dead_neurons_indices,</span>
<span class="sd">        ...     axis=1</span>
<span class="sd">        ... )</span>

<span class="sd">    Args:</span>
<span class="sd">        parameter_name: The name of the parameter. Examples from the standard sparse autoencoder</span>
<span class="sd">            implementation  include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`,</span>
<span class="sd">            `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`.</span>
<span class="sd">        neuron_indices: The indices of the neurons to reset.</span>
<span class="sd">        axis: The axis of the parameter to reset.</span>
<span class="sd">        parameter_group: The index of the parameter group to reset (typically this is just zero,</span>
<span class="sd">            unless you have setup multiple parameter groups for e.g. different learning rates</span>
<span class="sd">            for different parameters).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the parameter name is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the state of the parameter</span>
    <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="n">parameter_group</span><span class="p">]</span>
    <span class="n">parameter_name_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_parameter_name_idx</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">)</span>
    <span class="n">parameter</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">parameter_name_idx</span><span class="p">]</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

    <span class="c1"># Check if state is initialized</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Reset running averages for the specified neurons</span>
    <span class="k">if</span> <span class="s2">&quot;exp_avg&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
        <span class="n">exp_avg</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
        <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
    <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
        <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_state_all_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_state_all_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reset the state for all parameters.</p>
<p>Iterates over all parameters and resents both the running averages of the gradients and the
squares of gradients.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_state_all_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the state for all parameters.</span>

<span class="sd">    Iterates over all parameters and resents both the running averages of the gradients and the</span>
<span class="sd">    squares of gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Iterate over every parameter</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="c1"># Get the state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

            <span class="c1"># Check if state is initialized</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="c1"># Reset running averages</span>
            <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
            <span class="n">exp_avg</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
            <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../abstract_optimizer/" class="btn btn-neutral float-left" title="abstract_optimizer"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../source_data/" class="btn btn-neutral float-right" title="Index">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../abstract_optimizer/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../source_data/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
