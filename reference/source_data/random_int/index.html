<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>random_int - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "random_int";
        var mkdocs_page_input_path = "reference/source_data/random_int.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_resampler</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">autoencoder</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/abstract_metric/">abstract_metric</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">optimizer</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/adam_with_reset/">adam_with_reset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">source_data</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../abstract_dataset/">abstract_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="./">random_int</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
          <li class="breadcrumb-item">source_data</li>
      <li class="breadcrumb-item active">random_int</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder.source_data.random_int"></a>
  <div class="doc doc-contents first">
  
      <p>Random Int Dummy Source Data.</p>
<p>For use with tests and simple examples.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.source_data.random_int.RandomIntDummyDataset" class="doc doc-heading">
          <code>RandomIntDummyDataset</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="../abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.random_int.RandomIntSourceData" href="#sparse_autoencoder.source_data.random_int.RandomIntSourceData">RandomIntSourceData</a>]</code></p>

  
      <p>Random Int Dummy Dataset.</p>
<p>For use with tests and simple examples.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/random_int.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">RandomIntDummyDataset</span><span class="p">(</span><span class="n">SourceDataset</span><span class="p">[</span><span class="n">RandomIntSourceData</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Random Int Dummy Dataset.</span>

<span class="sd">    For use with tests and simple examples.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerFast</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_batch</span><span class="p">:</span> <span class="n">RandomIntSourceData</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">        Not implemented for this dummy dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">250</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dummy&quot;</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Random Int Dummy dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; data = RandomIntDummyDataset()</span>
<span class="sd">            &gt;&gt;&gt; first_item = next(iter(data))</span>
<span class="sd">            &gt;&gt;&gt; len(first_item[&quot;input_ids&quot;])</span>
<span class="sd">            250</span>

<span class="sd">        Args:</span>
<span class="sd">            context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">                a context size of 250.</span>
<span class="sd">            buffer_size: The buffer size to use when shuffling the dataset. As the dataset is</span>
<span class="sd">                streamed, this just pre-downloads at least `buffer_size` items and then shuffles</span>
<span class="sd">                just that buffer. Note that the generated activations should also be shuffled before</span>
<span class="sd">                training the sparse autoencoder, so a large buffer may not be strictly necessary</span>
<span class="sd">                here. Note also that this is the number of items in the dataset (e.g. number of</span>
<span class="sd">                prompts) and is typically significantly less than the number of tokenized prompts</span>
<span class="sd">                once the preprocessing function has been applied.</span>
<span class="sd">            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.</span>
<span class="sd">                tokenizing prompts).</span>
<span class="sd">            dataset_path: The path to the dataset on Hugging Face.</span>
<span class="sd">            dataset_split: Dataset split (e.g. `train`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">RandomIntHuggingFaceDataset</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]:</span>  <span class="c1"># type: ignore</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntDummyDataset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">context_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dataset_path</span><span class="o">=</span><span class="s1">&#39;dummy&#39;</span><span class="p">,</span> <span class="n">dataset_split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Random Int Dummy dataset.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>data = RandomIntDummyDataset()
first_item = next(iter(data))
len(first_item["input_ids"])
250</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>context_size</code></b>
                  (<code>int</code>, default:
                      <code>250</code>
)
              –
              <div class="doc-md-description">
                <p>The context size to use when returning a list of tokenized prompts.
<em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em> used
a context size of 250.</p>
              </div>
            </li>
            <li>
              <b><code>buffer_size</code></b>
                  (<code>int</code>, default:
                      <code>1000</code>
)
              –
              <div class="doc-md-description">
                <p>The buffer size to use when shuffling the dataset. As the dataset is
streamed, this just pre-downloads at least <code>buffer_size</code> items and then shuffles
just that buffer. Note that the generated activations should also be shuffled before
training the sparse autoencoder, so a large buffer may not be strictly necessary
here. Note also that this is the number of items in the dataset (e.g. number of
prompts) and is typically significantly less than the number of tokenized prompts
once the preprocessing function has been applied.</p>
              </div>
            </li>
            <li>
              <b><code>preprocess_batch_size</code></b>
                  (<code>int</code>, default:
                      <code>1000</code>
)
              –
              <div class="doc-md-description">
                <p>The batch size to use just for preprocessing the dataset (e.g.
tokenizing prompts).</p>
              </div>
            </li>
            <li>
              <b><code>dataset_path</code></b>
                  (<code>str</code>, default:
                      <code>&#39;dummy&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The path to the dataset on Hugging Face.</p>
              </div>
            </li>
            <li>
              <b><code>dataset_split</code></b>
                  (<code>str</code>, default:
                      <code>&#39;train&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Dataset split (e.g. <code>train</code>).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">250</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dummy&quot;</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Random Int Dummy dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; data = RandomIntDummyDataset()</span>
<span class="sd">        &gt;&gt;&gt; first_item = next(iter(data))</span>
<span class="sd">        &gt;&gt;&gt; len(first_item[&quot;input_ids&quot;])</span>
<span class="sd">        250</span>

<span class="sd">    Args:</span>
<span class="sd">        context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">            a context size of 250.</span>
<span class="sd">        buffer_size: The buffer size to use when shuffling the dataset. As the dataset is</span>
<span class="sd">            streamed, this just pre-downloads at least `buffer_size` items and then shuffles</span>
<span class="sd">            just that buffer. Note that the generated activations should also be shuffled before</span>
<span class="sd">            training the sparse autoencoder, so a large buffer may not be strictly necessary</span>
<span class="sd">            here. Note also that this is the number of items in the dataset (e.g. number of</span>
<span class="sd">            prompts) and is typically significantly less than the number of tokenized prompts</span>
<span class="sd">            once the preprocessing function has been applied.</span>
<span class="sd">        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.</span>
<span class="sd">            tokenizing prompts).</span>
<span class="sd">        dataset_path: The path to the dataset on Hugging Face.</span>
<span class="sd">        dataset_split: Dataset split (e.g. `train`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">RandomIntHuggingFaceDataset</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntDummyDataset.get_dataloader" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get Dataloader.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]:</span>  <span class="c1"># type: ignore</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Dataloader.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntDummyDataset.preprocess" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">preprocess</span><span class="p">(</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Preprocess a batch of prompts.</p>
<p>Not implemented for this dummy dataset.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_batch</span><span class="p">:</span> <span class="n">RandomIntSourceData</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">    Not implemented for this dummy dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset" class="doc doc-heading">
          <code>RandomIntHuggingFaceDataset</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.utils.data.Dataset">Dataset</span></code></p>

  
      <p>Dummy Hugging Face Dataset.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/random_int.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">RandomIntHuggingFaceDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dummy Hugging Face Dataset.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Random Int Dummy Hugging Face Dataset.</span>

<span class="sd">        Args:</span>
<span class="sd">            vocab_size: The size of the vocabulary to use.</span>
<span class="sd">            context_size: The number of tokens in the context window</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_size</span> <span class="o">=</span> <span class="n">context_size</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;RandomIntHuggingFaceDataset&quot;</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iter Dunder Method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Next Dunder Method.&quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_size</span><span class="p">)]</span>  <span class="c1"># noqa: S311</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">}</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Len Dunder Method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mi">1000</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__next__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get Item.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__next__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Random Int Dummy Hugging Face Dataset.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>vocab_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The size of the vocabulary to use.</p>
              </div>
            </li>
            <li>
              <b><code>context_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of tokens in the context window</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Random Int Dummy Hugging Face Dataset.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size: The size of the vocabulary to use.</span>
<span class="sd">        context_size: The number of tokens in the context window</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context_size</span> <span class="o">=</span> <span class="n">context_size</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__iter__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__iter__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Iter Dunder Method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;RandomIntHuggingFaceDataset&quot;</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iter Dunder Method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Len Dunder Method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Len Dunder Method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1000</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.random_int.RandomIntHuggingFaceDataset.__next__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__next__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Next Dunder Method.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/random_int.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Next Dunder Method.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_size</span><span class="p">)]</span>  <span class="c1"># noqa: S311</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.source_data.random_int.RandomIntSourceData" class="doc doc-heading">
          <code>RandomIntSourceData</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="typing.TypedDict">TypedDict</span></code></p>

  
      <p>Random Int Dummy Source Data.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/random_int.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">RandomIntSourceData</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Random Int Dummy Source Data.&quot;&quot;&quot;</span>

    <span class="n">input_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../pretokenized_dataset/" class="btn btn-neutral float-left" title="pretokenized_dataset"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../text_dataset/" class="btn btn-neutral float-right" title="text_dataset">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../pretokenized_dataset/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../text_dataset/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
