{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Benchmarks to help with design/architecture decisions of the lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n",
    "from jaxtyping import Float\n",
    "from sparse_autoencoder.activation_store.list_store import ListActivationStore\n",
    "from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n",
    "from sparse_autoencoder.activation_store.disk_store import DiskActivationStore\n",
    "from sparse_autoencoder.train import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Tensor Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to know both the size and how much they can be compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of text data\n",
    "dataset = load_dataset(\"NeelNanda/c4-code-tokenized-2b\", split=\"train\", streaming=True)\n",
    "first_batch = []\n",
    "for idx, example in enumerate(dataset):\n",
    "    if not idx <= 24:\n",
    "        break\n",
    "    first_batch.append(example[\"tokens\"])\n",
    "first_batch = torch.tensor(first_batch)\n",
    "f\"Number of activations to store in this benchmark test: {first_batch.numel()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the activations\n",
    "src_model = HookedTransformer.from_pretrained(\"NeelNanda/GELU_1L512W_C4_Code\")\n",
    "logits, cache = src_model.run_with_cache(first_batch)\n",
    "activations = cache[\"blocks.0.mlp.hook_post\"].half()\n",
    "number_activations = activations.numel()\n",
    "size_bytes_activations = number_activations * 2  # Assume float 16\n",
    "size_mb_activations = f\"{size_bytes_activations / (10**6):.2f} MB\"\n",
    "f\"With {activations.numel()} features at half precision, the features take up {size_mb_activations} of memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try compressing on the disk (and find the impact is small so probably not worth it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to temp dir\n",
    "temp_dir = tempfile.gettempdir()\n",
    "temp_file = temp_dir + \"/temp.pt\"\n",
    "temp_file_gz = temp_file + \".gz\"\n",
    "torch.save(activations, temp_file)\n",
    "\n",
    "# Zip it\n",
    "with open(temp_file, \"rb\") as f_in:\n",
    "    with gzip.open(temp_file_gz, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Get the file size back\n",
    "fs_bytes = os.path.getsize(temp_file_gz)\n",
    "f\"Compressed file size is {fs_bytes / (10**6):.2f} MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate assuming 8 billion activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumed_n_activation_batches = 8 * (10**9)\n",
    "assumed_n_activations_per_batch = 2048\n",
    "uncompressed_size_per_activation = 2  # float16\n",
    "estimated_size = (\n",
    "    assumed_n_activation_batches\n",
    "    * assumed_n_activations_per_batch\n",
    "    * uncompressed_size_per_activation\n",
    ")\n",
    "f\"With {assumed_n_activation_batches/10**9}B activations with {assumed_n_activations_per_batch} features, \\\n",
    "the estimated size is {estimated_size / (10**12):.2f} TB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of activations you can store with different sizes\n",
    "sizes_gb = [10, 50, 100, 300, 500, 1000]\n",
    "activations_per_size = [\n",
    "    i * (10**9) / uncompressed_size_per_activation / assumed_n_activations_per_batch\n",
    "    for i in sizes_gb\n",
    "]\n",
    "\n",
    "table = pd.DataFrame({\"Size (GB)\": sizes_gb, \"Activations\": activations_per_size})\n",
    "table[\"Activations\"] = table[\"Activations\"].apply(\n",
    "    lambda x: \"{:,.0f}\".format(x / 10**6) + \"M\"\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VastAI systems often have quite a lot of HD space (e.g. 300GB) but available ram is often smaller\n",
    "(e.g. 50GB and we need a reasonable amount left over for moving tensors around etc). This means that\n",
    "we can store c. 5-10M activations on a typical instance in CPU RAM (sometimes 25M+), or 50-100M on\n",
    "disk. Both seem like plenty!\n",
    "\n",
    "To note that replenishing a buffer of cached activations when half used in training seems like a lot\n",
    "of pain, considering that the improvement is likely marginal. Particularly if we also randomly sort\n",
    "the prompts for the forward pass of the source model, we'll have a chance of two tokens coming from\n",
    "the same/nearby prompts as very small.\n",
    "\n",
    "The conclusion is therefore that we do a need some sort of buffer, as we can't store 40TB on disk\n",
    "easily, and this buffer can be disk or ram. It needs to store asynchronously (so it doesn't block\n",
    "the forward pass), and it needs to be able to handle multiple simultaneous writes from e.g.\n",
    "distributed GPUs. The best approaches here are probably (a) pre-allocating a cpu ram space with\n",
    "torch.empty, or (b) writing asynchronously to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Activations (Forward Pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_items: int = 1_000_000\n",
    "# num_neurons: int = 2048\n",
    "\n",
    "\n",
    "# def create_dummy_activations(\n",
    "#     n_items: int, n_neurons: int\n",
    "# ) -> list[Float[Tensor, \"batch neurons\"]]:\n",
    "#     \"\"\"Create Dummy Activations for Benchmarks.\"\"\"\n",
    "#     batch_size = 1_000\n",
    "#     n_batches = int(n_items // batch_size)\n",
    "#     activations = [torch.rand(batch_size, n_neurons) for _ in range(n_batches)]\n",
    "#     return activations\n",
    "\n",
    "\n",
    "# dummy_activations = create_dummy_activations(num_items, num_neurons)\n",
    "# dummy_activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks_to_run = {\n",
    "#     \"GPU Tensor\": TensorActivationStore(\n",
    "#         max_items=num_items, num_neurons=num_neurons, device=torch.device(\"cuda\")\n",
    "#     ),\n",
    "#     \"CPU Tensor\": TensorActivationStore(\n",
    "#         max_items=num_items, num_neurons=num_neurons, device=torch.device(\"cpu\")\n",
    "#     ),\n",
    "#     \"CPU List, No Multiprocessing\": ListActivationStore(),\n",
    "#     \"CPU List, Multiprocessing (multiple GPUs)\": ListActivationStore(\n",
    "#         multiprocessing_enabled=True\n",
    "#     ),\n",
    "#     \"Disk\": DiskActivationStore(empty_dir=True, max_cache_size=100_000),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that where multiprocessing is enabled, this will incur a large time cost and no significant benefits\n",
    "realised here. With multiple GPUs however this may be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "\n",
    "# for name, store in tqdm.tqdm(benchmarks_to_run.items()):\n",
    "#     store.empty()\n",
    "#     start_time = time.time()\n",
    "#     for batch in dummy_activations:\n",
    "#         store.extend(batch)\n",
    "#     if hasattr(store, \"wait_for_writes_to_complete\"):\n",
    "#         store.wait_for_writes_to_complete()\n",
    "#     end_time = time.time()\n",
    "#     results[name] = end_time - start_time\n",
    "\n",
    "# df = pd.DataFrame(results, index=[\"Time (s)\"]).T\n",
    "# df[\"Time 10B (h estimate)\"] = df[\"Time (s)\"] * 10**10 / num_items / 3600\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
