{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sparse_autoencoder import (\n",
    "    SparseAutoencoder,\n",
    "    TensorActivationStore,\n",
    "    pipeline,\n",
    "    create_src_dataloader,\n",
    ")\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "from sparse_autoencoder.src_data.datasets.neel_c4_tokenized import (\n",
    "    collate_neel_c4_tokenized,\n",
    ")\n",
    "import torch\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129121d84045498ea87783e8c7e32e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_dataloader = create_src_dataloader(\n",
    "    \"NeelNanda/c4-code-tokenized-2b\",\n",
    "    collate_fn=collate_neel_c4_tokenized,\n",
    "    shuffle_buffer_size=10_000,\n",
    "    random_seed=0,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model solu-1l into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model = HookedTransformer.from_pretrained(\"solu-1l\", dtype=torch.float32)\n",
    "src_d_mlp = src_model.cfg.d_mlp\n",
    "src_d_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_items = 2_000_000\n",
    "store = TensorActivationStore(max_items, src_d_mlp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = SparseAutoencoder(src_d_mlp, src_d_mlp * 8, torch.zeros(src_d_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialise [wandb](https://wandb.ai/site), the pipeline will automatically log all metrics to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malan-cooney\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alan/Documents/Repos/sparse_autoencoder/wandb/run-20231104_163710-v4rzv049</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/v4rzv049' target=\"_blank\">toasty-jazz-13</a></strong> to <a href='https://wandb.ai/alan-cooney/sparse-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alan-cooney/sparse-autoencoder' target=\"_blank\">https://wandb.ai/alan-cooney/sparse-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/v4rzv049' target=\"_blank\">https://wandb.ai/alan-cooney/sparse-autoencoder/runs/v4rzv049</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alan-cooney/sparse-autoencoder/runs/v4rzv049?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2ccffdbd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x291cabfd0>> (for post_run_cell), with arguments args (<ExecutionResult object at 291ae3250, execution_count=7 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 29fbdc450, raw_cell=\"wandb.init(project=\"sparse-autoencoder\")\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D> result=<wandb.sdk.wandb_run.Run object at 0x2ccffdbd0>>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"sparse-autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183144c8bf514fdf899ae1370411c41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate/Train Cycles: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52050dd0401149a0b3fde019b5423e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate Activations:   0%|          | 0/1966080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2064341ffd954619b82978c231d4b0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Autoencoder:   0%|          | 0/1966080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     src_model\u001b[39m=\u001b[39;49msrc_model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     src_model_activation_hook_point\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mblocks.0.mlp.hook_post\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     src_model_activation_layer\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     src_dataloader\u001b[39m=\u001b[39;49msrc_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     activation_store\u001b[39m=\u001b[39;49mstore,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_activations_before_training\u001b[39m=\u001b[39;49mmax_items,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     autoencoder\u001b[39m=\u001b[39;49mautoencoder,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/pipeline.py:88\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(src_model, src_model_activation_hook_point, src_model_activation_layer, src_dataloader, activation_store, num_activations_before_training, autoencoder, sweep_parameters, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     83\u001b[0m     activation_store,\n\u001b[1;32m     84\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m8192\u001b[39m,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[39m# Train the autoencoder\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m train_autoencoder(\n\u001b[1;32m     89\u001b[0m     activations_dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[1;32m     90\u001b[0m     autoencoder\u001b[39m=\u001b[39;49mautoencoder,\n\u001b[1;32m     91\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     92\u001b[0m     sweep_parameters\u001b[39m=\u001b[39;49msweep_parameters,\n\u001b[1;32m     93\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     96\u001b[0m \u001b[39m# Empty the store so we can fill it up again\u001b[39;00m\n\u001b[1;32m     97\u001b[0m activation_store\u001b[39m.\u001b[39mempty()\n",
      "File \u001b[0;32m~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/train_autoencoder.py:73\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(activations_dataloader, autoencoder, optimizer, sweep_parameters, log_interval, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m total_loss \u001b[39m=\u001b[39m sae_training_loss(\n\u001b[1;32m     66\u001b[0m     reconstruction_loss_mse,\n\u001b[1;32m     67\u001b[0m     l1_loss_learned_activations,\n\u001b[1;32m     68\u001b[0m     sweep_parameters\u001b[39m.\u001b[39ml1_coefficient,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[39m# TODO: Log dead neurons metric (get_frequencies in Neel's code)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m# Backwards pass\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     75\u001b[0m \u001b[39m# TODO: Make decoder weights and grad unit norm\u001b[39;00m\n\u001b[1;32m     77\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline(\n",
    "    src_model=src_model,\n",
    "    src_model_activation_hook_point=\"blocks.0.mlp.hook_post\",\n",
    "    src_model_activation_layer=0,\n",
    "    src_dataloader=src_dataloader,\n",
    "    activation_store=store,\n",
    "    num_activations_before_training=max_items,\n",
    "    autoencoder=autoencoder,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
